[32;1mConfigured folder ./cpg/wandb/run-20220708_062953-suopeg7d/files for saving[0m
[32;1mName: RG-v0_CRG-v0_dnc_True_dno_True_dnr_True_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
Using cpu device
/home/mwbaert/Documents/research/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -6.39e+03 |
|    mean_ep_length   | 179       |
|    mean_reward      | -6.39e+03 |
|    true_cost        | 0.378     |
| infos/              |           |
|    cost             | 0.34      |
| rollout/            |           |
|    adjusted_reward  | -1.22     |
|    ep_len_mean      | 175       |
|    ep_rew_mean      | -147      |
| time/               |           |
|    fps              | 2364      |
|    iterations       | 1         |
|    time_elapsed     | 4         |
|    total_timesteps  | 10240     |
-----------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.94e+03   |
|    mean_ep_length       | 143         |
|    mean_reward          | -3.94e+03   |
|    true_cost            | 0.258       |
| infos/                  |             |
|    cost                 | 0.23        |
| rollout/                |             |
|    adjusted_reward      | -1.12       |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | -144        |
| time/                   |             |
|    fps                  | 1995        |
|    iterations           | 2           |
|    time_elapsed         | 10          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.043860372 |
|    average_cost         | 0.37773436  |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -216        |
|    cost_value_loss      | 18.4        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    mean_cost_advantages | 5.8334517   |
|    mean_reward_advan... | -12.768271  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.378      |
|    policy_gradient_loss | -0.0207     |
|    reward_explained_... | -577        |
|    reward_value_loss    | 111         |
|    total_cost           | 3868.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -3.4e+03   |
|    mean_ep_length       | 181        |
|    mean_reward          | -3.4e+03   |
|    true_cost            | 0.184      |
| infos/                  |            |
|    cost                 | 0.16       |
| rollout/                |            |
|    adjusted_reward      | -0.955     |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | -136       |
| time/                   |            |
|    fps                  | 1889       |
|    iterations           | 3          |
|    time_elapsed         | 16         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.02459438 |
|    average_cost         | 0.25830078 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -0.52      |
|    cost_value_loss      | 4.72       |
|    early_stop_epoch     | 1          |
|    entropy_loss         | -1.24      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.6       |
|    mean_cost_advantages | 2.621684   |
|    mean_reward_advan... | -10.442154 |
|    n_updates            | 20         |
|    nu                   | 1.13       |
|    nu_loss              | -0.275     |
|    policy_gradient_loss | -0.0186    |
|    reward_explained_... | -1.06      |
|    reward_value_loss    | 72.5       |
|    total_cost           | 2645.0     |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.74e+03   |
|    mean_ep_length       | 157         |
|    mean_reward          | -1.74e+03   |
|    true_cost            | 0.107       |
| infos/                  |             |
|    cost                 | 0.11        |
| rollout/                |             |
|    adjusted_reward      | -0.85       |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | -118        |
| time/                   |             |
|    fps                  | 1894        |
|    iterations           | 4           |
|    time_elapsed         | 21          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.020995885 |
|    average_cost         | 0.18447265  |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.38       |
|    cost_value_loss      | 4.25        |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -1.1        |
|    learning_rate        | 0.0003      |
|    loss                 | 26.5        |
|    mean_cost_advantages | 0.9220932   |
|    mean_reward_advan... | -7.0060234  |
|    n_updates            | 30          |
|    nu                   | 1.19        |
|    nu_loss              | -0.208      |
|    policy_gradient_loss | -0.0193     |
|    reward_explained_... | -0.0649     |
|    reward_value_loss    | 68.8        |
|    total_cost           | 1889.0      |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1e+03      |
|    mean_ep_length       | 184         |
|    mean_reward          | -1e+03      |
|    true_cost            | 0.0387      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.735      |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | -109        |
| time/                   |             |
|    fps                  | 1925        |
|    iterations           | 5           |
|    time_elapsed         | 26          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.02348655  |
|    average_cost         | 0.1071289   |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.42       |
|    cost_value_loss      | 3.33        |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.898      |
|    learning_rate        | 0.0003      |
|    loss                 | 27          |
|    mean_cost_advantages | -0.53576005 |
|    mean_reward_advan... | -5.2534947  |
|    n_updates            | 40          |
|    nu                   | 1.26        |
|    nu_loss              | -0.128      |
|    policy_gradient_loss | -0.0199     |
|    reward_explained_... | -0.509      |
|    reward_value_loss    | 63.6        |
|    total_cost           | 1097.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -229        |
|    mean_ep_length       | 107         |
|    mean_reward          | -229        |
|    true_cost            | 0.0134      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.596      |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | -84         |
| time/                   |             |
|    fps                  | 1921        |
|    iterations           | 6           |
|    time_elapsed         | 31          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.026961774 |
|    average_cost         | 0.038671874 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.107      |
|    cost_value_loss      | 1.85        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.738      |
|    learning_rate        | 0.0003      |
|    loss                 | 15.7        |
|    mean_cost_advantages | -1.3581271  |
|    mean_reward_advan... | -4.34953    |
|    n_updates            | 50          |
|    nu                   | 1.31        |
|    nu_loss              | -0.0486     |
|    policy_gradient_loss | -0.0136     |
|    reward_explained_... | 0.109       |
|    reward_value_loss    | 53          |
|    total_cost           | 396.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -39.4       |
|    mean_ep_length       | 70.8        |
|    mean_reward          | -39.4       |
|    true_cost            | 0.00361     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.526      |
|    ep_len_mean          | 82.8        |
|    ep_rew_mean          | -43.1       |
| time/                   |             |
|    fps                  | 1829        |
|    iterations           | 7           |
|    time_elapsed         | 39          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.01610249  |
|    average_cost         | 0.013378906 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.253       |
|    cost_value_loss      | 1.05        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.629      |
|    learning_rate        | 0.0003      |
|    loss                 | 19          |
|    mean_cost_advantages | -1.4865966  |
|    mean_reward_advan... | -1.2249616  |
|    n_updates            | 60          |
|    nu                   | 1.37        |
|    nu_loss              | -0.0176     |
|    policy_gradient_loss | -0.012      |
|    reward_explained_... | 0.482       |
|    reward_value_loss    | 39.3        |
|    total_cost           | 137.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -29.5        |
|    mean_ep_length       | 56.2         |
|    mean_reward          | -29.5        |
|    true_cost            | 0.00361      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | -0.501       |
|    ep_len_mean          | 57.2         |
|    ep_rew_mean          | -28.7        |
| time/                   |              |
|    fps                  | 1792         |
|    iterations           | 8            |
|    time_elapsed         | 45           |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.015981136  |
|    average_cost         | 0.0036132813 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.643        |
|    cost_value_loss      | 0.511        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.546       |
|    learning_rate        | 0.0003       |
|    loss                 | 14           |
|    mean_cost_advantages | -1.4688765   |
|    mean_reward_advan... | 1.9898682    |
|    n_updates            | 70           |
|    nu                   | 1.41         |
|    nu_loss              | -0.00494     |
|    policy_gradient_loss | -0.0105      |
|    reward_explained_... | 0.673        |
|    reward_value_loss    | 27.8         |
|    total_cost           | 37.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -29.5        |
|    mean_ep_length       | 57.4         |
|    mean_reward          | -45.6        |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.453       |
|    ep_len_mean          | 49.8         |
|    ep_rew_mean          | -22.3        |
| time/                   |              |
|    fps                  | 1797         |
|    iterations           | 9            |
|    time_elapsed         | 51           |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.016660918  |
|    average_cost         | 0.0036132813 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.734        |
|    cost_value_loss      | 0.278        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.497       |
|    learning_rate        | 0.0003       |
|    loss                 | 10.3         |
|    mean_cost_advantages | -1.1965972   |
|    mean_reward_advan... | 4.4377856    |
|    n_updates            | 80           |
|    nu                   | 1.45         |
|    nu_loss              | -0.0051      |
|    policy_gradient_loss | -0.00904     |
|    reward_explained_... | 0.794        |
|    reward_value_loss    | 19.8         |
|    total_cost           | 37.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -14.8        |
|    mean_ep_length       | 33.2         |
|    mean_reward          | -14.8        |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.411       |
|    ep_len_mean          | 39.8         |
|    ep_rew_mean          | -16.7        |
| time/                   |              |
|    fps                  | 1802         |
|    iterations           | 10           |
|    time_elapsed         | 56           |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.01789381   |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.0943       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.688        |
|    cost_value_loss      | 0.101        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.424       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.42         |
|    mean_cost_advantages | -0.7118637   |
|    mean_reward_advan... | 3.7562962    |
|    n_updates            | 90           |
|    nu                   | 1.49         |
|    nu_loss              | -0.00497     |
|    policy_gradient_loss | -0.00737     |
|    reward_explained_... | 0.842        |
|    reward_value_loss    | 11.3         |
|    total_cost           | 35.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -11.7         |
|    mean_ep_length       | 34.2          |
|    mean_reward          | -11.7         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.403        |
|    ep_len_mean          | 34            |
|    ep_rew_mean          | -13.7         |
| time/                   |               |
|    fps                  | 1663          |
|    iterations           | 11            |
|    time_elapsed         | 67            |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.010627342   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.13          |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.675         |
|    cost_value_loss      | 0.0157        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.359        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.42          |
|    mean_cost_advantages | -0.43415195   |
|    mean_reward_advan... | 3.0758126     |
|    n_updates            | 100           |
|    nu                   | 1.53          |
|    nu_loss              | -0.000728     |
|    policy_gradient_loss | -0.00781      |
|    reward_explained_... | 0.828         |
|    reward_value_loss    | 5.69          |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -10.3         |
|    mean_ep_length       | 24.2          |
|    mean_reward          | -10.3         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.398        |
|    ep_len_mean          | 29.4          |
|    ep_rew_mean          | -11.4         |
| time/                   |               |
|    fps                  | 1665          |
|    iterations           | 12            |
|    time_elapsed         | 73            |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 0.023091208   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0814        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.549         |
|    cost_value_loss      | 0.00753       |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.297        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.76          |
|    mean_cost_advantages | -0.20671213   |
|    mean_reward_advan... | 1.9657557     |
|    n_updates            | 110           |
|    nu                   | 1.56          |
|    nu_loss              | -0.000745     |
|    policy_gradient_loss | -0.00634      |
|    reward_explained_... | 0.841         |
|    reward_value_loss    | 3.67          |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -9.6         |
|    mean_ep_length       | 24.6         |
|    mean_reward          | -9.6         |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.399       |
|    ep_len_mean          | 24           |
|    ep_rew_mean          | -9.56        |
| time/                   |              |
|    fps                  | 1589         |
|    iterations           | 13           |
|    time_elapsed         | 83           |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.01910479   |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0775       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.373        |
|    cost_value_loss      | 0.00194      |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.213       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.917        |
|    mean_cost_advantages | -0.08721129  |
|    mean_reward_advan... | 1.1846977    |
|    n_updates            | 120          |
|    nu                   | 1.58         |
|    nu_loss              | -0.000304    |
|    policy_gradient_loss | -0.00467     |
|    reward_explained_... | 0.84         |
|    reward_value_loss    | 2.34         |
|    total_cost           | 2.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.78        |
|    mean_ep_length       | 22.4         |
|    mean_reward          | -8.78        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.415       |
|    ep_len_mean          | 23.3         |
|    ep_rew_mean          | -9.41        |
| time/                   |              |
|    fps                  | 1529         |
|    iterations           | 14           |
|    time_elapsed         | 93           |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.014277625  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0481       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.774       |
|    cost_value_loss      | 0.00172      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.137       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.385        |
|    mean_cost_advantages | -0.027896976 |
|    mean_reward_advan... | 1.0621595    |
|    n_updates            | 130          |
|    nu                   | 1.61         |
|    nu_loss              | -0.000309    |
|    policy_gradient_loss | -0.00252     |
|    reward_explained_... | 0.877        |
|    reward_value_loss    | 1.3          |
|    total_cost           | 2.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.58        |
|    mean_ep_length       | 19.4         |
|    mean_reward          | -8.58        |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.421       |
|    ep_len_mean          | 23.4         |
|    ep_rew_mean          | -9.84        |
| time/                   |              |
|    fps                  | 1480         |
|    iterations           | 15           |
|    time_elapsed         | 103          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0028889866 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.107        |
|    cost_value_loss      | 2.07e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0938      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.528        |
|    mean_cost_advantages | -0.012674769 |
|    mean_reward_advan... | 0.5571121    |
|    n_updates            | 140          |
|    nu                   | 1.63         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00117     |
|    reward_explained_... | 0.88         |
|    reward_value_loss    | 1.21         |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -8.58         |
|    mean_ep_length       | 22.4          |
|    mean_reward          | -9.13         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.43         |
|    ep_len_mean          | 21.9          |
|    ep_rew_mean          | -9.41         |
| time/                   |               |
|    fps                  | 1436          |
|    iterations           | 16            |
|    time_elapsed         | 114           |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.00069033075 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.0111        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -37.1         |
|    cost_value_loss      | 0.000563      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0693       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.582         |
|    mean_cost_advantages | -0.0047327625 |
|    mean_reward_advan... | 0.25578576    |
|    n_updates            | 150           |
|    nu                   | 1.65          |
|    nu_loss              | -0.000159     |
|    policy_gradient_loss | -0.000443     |
|    reward_explained_... | 0.895         |
|    reward_value_loss    | 0.925         |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -8.15         |
|    mean_ep_length       | 19.8          |
|    mean_reward          | -8.15         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.428        |
|    ep_len_mean          | 21.2          |
|    ep_rew_mean          | -9.14         |
| time/                   |               |
|    fps                  | 1412          |
|    iterations           | 17            |
|    time_elapsed         | 123           |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 0.00017494074 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00842       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.114         |
|    cost_value_loss      | 8.76e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0548       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.482         |
|    mean_cost_advantages | -0.017867243  |
|    mean_reward_advan... | 0.10351608    |
|    n_updates            | 160           |
|    nu                   | 1.67          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000193     |
|    reward_explained_... | 0.893         |
|    reward_value_loss    | 0.886         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -8.15         |
|    mean_ep_length       | 21.2          |
|    mean_reward          | -9.77         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.428        |
|    ep_len_mean          | 20.9          |
|    ep_rew_mean          | -8.97         |
| time/                   |               |
|    fps                  | 1381          |
|    iterations           | 18            |
|    time_elapsed         | 133           |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 0.0020380705  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0115        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.503         |
|    cost_value_loss      | 4.91e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0504       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.448         |
|    mean_cost_advantages | -0.0052579306 |
|    mean_reward_advan... | -0.02253269   |
|    n_updates            | 170           |
|    nu                   | 1.69          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000345     |
|    reward_explained_... | 0.876         |
|    reward_value_loss    | 0.94          |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -8.15          |
|    mean_ep_length       | 23.6           |
|    mean_reward          | -11.1          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.432         |
|    ep_len_mean          | 21.2           |
|    ep_rew_mean          | -9.11          |
| time/                   |                |
|    fps                  | 1356           |
|    iterations           | 19             |
|    time_elapsed         | 143            |
|    total_timesteps      | 194560         |
| train/                  |                |
|    approx_kl            | 0.0012677419   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00963        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.328          |
|    cost_value_loss      | 2.3e-05        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0344        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.415          |
|    mean_cost_advantages | -0.00095893006 |
|    mean_reward_advan... | 0.03703403     |
|    n_updates            | 180            |
|    nu                   | 1.71           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000122      |
|    reward_explained_... | 0.887          |
|    reward_value_loss    | 0.84           |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -7.68         |
|    mean_ep_length       | 19            |
|    mean_reward          | -7.68         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.436        |
|    ep_len_mean          | 20.9          |
|    ep_rew_mean          | -9.19         |
| time/                   |               |
|    fps                  | 1324          |
|    iterations           | 20            |
|    time_elapsed         | 154           |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 0.0022663358  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00987       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.151         |
|    cost_value_loss      | 4.71e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0232       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.569         |
|    mean_cost_advantages | -7.194257e-05 |
|    mean_reward_advan... | -0.047598597  |
|    n_updates            | 190           |
|    nu                   | 1.72          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000148     |
|    reward_explained_... | 0.878         |
|    reward_value_loss    | 0.846         |
|    total_cost           | 0.0           |
-------------------------------------------
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/wrappers/monitoring/video_recorder.py:57: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  logger.deprecation(
Mean reward: -8.000000 +/- 2.084999.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/util.py:37: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping, Sequence
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. [0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:305: UserWarning: [33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps[0m
  logger.warn(
[32;1mTime taken: 03.06 minutes[0m
