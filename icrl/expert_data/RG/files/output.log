[32;1mConfigured folder ./cpg/wandb/run-20220714_143547-ejzhwggk/files for saving[0m
[32;1mName: RG-v0_CRG-v0_dnc_True_dno_True_dnr_True_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
Using cpu device
/home/mwbaert/Documents/research/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
----------------------------------
| eval/               |          |
|    best_mean_reward | -7.7e+03 |
|    mean_ep_length   | 200      |
|    mean_reward      | -7.7e+03 |
|    true_cost        | 0.384    |
| infos/              |          |
|    cost             | 0.4      |
| rollout/            |          |
|    adjusted_reward  | -1.22    |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | -143     |
| time/               |          |
|    fps              | 1604     |
|    iterations       | 1        |
|    time_elapsed     | 6        |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.06e+03   |
|    mean_ep_length       | 200         |
|    mean_reward          | -5.06e+03   |
|    true_cost            | 0.253       |
| infos/                  |             |
|    cost                 | 0.22        |
| rollout/                |             |
|    adjusted_reward      | -1.06       |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | -138        |
| time/                   |             |
|    fps                  | 1598        |
|    iterations           | 2           |
|    time_elapsed         | 12          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.040104702 |
|    average_cost         | 0.38359374  |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -196        |
|    cost_value_loss      | 18.4        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    mean_cost_advantages | 5.877084    |
|    mean_reward_advan... | -12.663411  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.384      |
|    policy_gradient_loss | -0.0215     |
|    reward_explained_... | -634        |
|    reward_value_loss    | 110         |
|    total_cost           | 3928.0      |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.29e+03   |
|    mean_ep_length       | 174         |
|    mean_reward          | -3.29e+03   |
|    true_cost            | 0.184       |
| infos/                  |             |
|    cost                 | 0.22        |
| rollout/                |             |
|    adjusted_reward      | -1.02       |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -130        |
| time/                   |             |
|    fps                  | 1535        |
|    iterations           | 3           |
|    time_elapsed         | 20          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.018610477 |
|    average_cost         | 0.25283203  |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.579      |
|    cost_value_loss      | 6.29        |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -1.23       |
|    learning_rate        | 0.0003      |
|    loss                 | 31          |
|    mean_cost_advantages | 2.5450177   |
|    mean_reward_advan... | -9.714003   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.269      |
|    policy_gradient_loss | -0.0194     |
|    reward_explained_... | -0.825      |
|    reward_value_loss    | 88.3        |
|    total_cost           | 2589.0      |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -1.47e+03  |
|    mean_ep_length       | 144        |
|    mean_reward          | -1.47e+03  |
|    true_cost            | 0.103      |
| infos/                  |            |
|    cost                 | 0.06       |
| rollout/                |            |
|    adjusted_reward      | -0.927     |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | -136       |
| time/                   |            |
|    fps                  | 1502       |
|    iterations           | 4          |
|    time_elapsed         | 27         |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.01841465 |
|    average_cost         | 0.18369141 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -1.9       |
|    cost_value_loss      | 3.99       |
|    early_stop_epoch     | 0          |
|    entropy_loss         | -1.12      |
|    learning_rate        | 0.0003     |
|    loss                 | 34.2       |
|    mean_cost_advantages | 0.92523307 |
|    mean_reward_advan... | -8.810882  |
|    n_updates            | 30         |
|    nu                   | 1.19       |
|    nu_loss              | -0.207     |
|    policy_gradient_loss | -0.0176    |
|    reward_explained_... | -1.77      |
|    reward_value_loss    | 84.2       |
|    total_cost           | 1881.0     |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -381        |
|    mean_ep_length       | 97.6        |
|    mean_reward          | -381        |
|    true_cost            | 0.0339      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.77       |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | -133        |
| time/                   |             |
|    fps                  | 1561        |
|    iterations           | 5           |
|    time_elapsed         | 32          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.030600334 |
|    average_cost         | 0.10263672  |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.77       |
|    cost_value_loss      | 3.22        |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.878      |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    mean_cost_advantages | -0.47646016 |
|    mean_reward_advan... | -7.629819   |
|    n_updates            | 40          |
|    nu                   | 1.26        |
|    nu_loss              | -0.123      |
|    policy_gradient_loss | -0.0206     |
|    reward_explained_... | -1.6        |
|    reward_value_loss    | 80.6        |
|    total_cost           | 1051.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -223       |
|    mean_ep_length       | 187        |
|    mean_reward          | -223       |
|    true_cost            | 0.0085     |
| infos/                  |            |
|    cost                 | 0.02       |
| rollout/                |            |
|    adjusted_reward      | -0.651     |
|    ep_len_mean          | 158        |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 1567       |
|    iterations           | 6          |
|    time_elapsed         | 39         |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.02965781 |
|    average_cost         | 0.03388672 |
|    clip_fraction        | 0.0711     |
|    clip_range           | 0.2        |
|    cost_explained_va... | -0.763     |
|    cost_value_loss      | 1.98       |
|    early_stop_epoch     | 1          |
|    entropy_loss         | -0.717     |
|    learning_rate        | 0.0003     |
|    loss                 | 35.5       |
|    mean_cost_advantages | -1.4325359 |
|    mean_reward_advan... | -5.4063706 |
|    n_updates            | 50         |
|    nu                   | 1.31       |
|    nu_loss              | -0.0426    |
|    policy_gradient_loss | -0.0111    |
|    reward_explained_... | -1.03      |
|    reward_value_loss    | 58         |
|    total_cost           | 347.0      |
----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -40.7       |
|    mean_ep_length       | 93.4        |
|    mean_reward          | -40.7       |
|    true_cost            | 0.00254     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.541      |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 1549        |
|    iterations           | 7           |
|    time_elapsed         | 46          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.018357104 |
|    average_cost         | 0.008496094 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.193       |
|    cost_value_loss      | 0.903       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.593      |
|    learning_rate        | 0.0003      |
|    loss                 | 24          |
|    mean_cost_advantages | -1.4619246  |
|    mean_reward_advan... | -2.8469472  |
|    n_updates            | 60          |
|    nu                   | 1.36        |
|    nu_loss              | -0.0112     |
|    policy_gradient_loss | -0.00924    |
|    reward_explained_... | 0.0616      |
|    reward_value_loss    | 46.4        |
|    total_cost           | 87.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -40.7        |
|    mean_ep_length       | 116          |
|    mean_reward          | -94.9        |
|    true_cost            | 0.00293      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.47        |
|    ep_len_mean          | 86.7         |
|    ep_rew_mean          | -40.1        |
| time/                   |              |
|    fps                  | 1556         |
|    iterations           | 8            |
|    time_elapsed         | 52           |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.017031947  |
|    average_cost         | 0.0025390624 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.548        |
|    cost_value_loss      | 0.499        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.524       |
|    learning_rate        | 0.0003       |
|    loss                 | 18.5         |
|    mean_cost_advantages | -1.3253921   |
|    mean_reward_advan... | 0.75260174   |
|    n_updates            | 70           |
|    nu                   | 1.41         |
|    nu_loss              | -0.00346     |
|    policy_gradient_loss | -0.0102      |
|    reward_explained_... | 0.551        |
|    reward_value_loss    | 34.4         |
|    total_cost           | 26.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -28          |
|    mean_ep_length       | 57.4         |
|    mean_reward          | -28          |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.439       |
|    ep_len_mean          | 57.3         |
|    ep_rew_mean          | -24.9        |
| time/                   |              |
|    fps                  | 1569         |
|    iterations           | 9            |
|    time_elapsed         | 58           |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.015933752  |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.523        |
|    cost_value_loss      | 0.241        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.48        |
|    learning_rate        | 0.0003       |
|    loss                 | 12.1         |
|    mean_cost_advantages | -0.9292369   |
|    mean_reward_advan... | 1.9881208    |
|    n_updates            | 80           |
|    nu                   | 1.45         |
|    nu_loss              | -0.00413     |
|    policy_gradient_loss | -0.00583     |
|    reward_explained_... | 0.634        |
|    reward_value_loss    | 22.3         |
|    total_cost           | 30.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -19.4         |
|    mean_ep_length       | 39.4          |
|    mean_reward          | -19.4         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.423        |
|    ep_len_mean          | 42.8          |
|    ep_rew_mean          | -18           |
| time/                   |               |
|    fps                  | 1557          |
|    iterations           | 10            |
|    time_elapsed         | 65            |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.016473029   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0986        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.658         |
|    cost_value_loss      | 0.104         |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.435        |
|    learning_rate        | 0.0003        |
|    loss                 | 8.21          |
|    mean_cost_advantages | -0.7584783    |
|    mean_reward_advan... | 3.8245625     |
|    n_updates            | 90            |
|    nu                   | 1.48          |
|    nu_loss              | -0.000707     |
|    policy_gradient_loss | -0.0064       |
|    reward_explained_... | 0.72          |
|    reward_value_loss    | 18.9          |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -13.1        |
|    mean_ep_length       | 31.8         |
|    mean_reward          | -13.1        |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.423       |
|    ep_len_mean          | 36.7         |
|    ep_rew_mean          | -15.4        |
| time/                   |              |
|    fps                  | 1535         |
|    iterations           | 11           |
|    time_elapsed         | 73           |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.015093011  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.693        |
|    cost_value_loss      | 0.0265       |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.375       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.59         |
|    mean_cost_advantages | -0.49257153  |
|    mean_reward_advan... | 4.16298      |
|    n_updates            | 100          |
|    nu                   | 1.52         |
|    nu_loss              | -0.00029     |
|    policy_gradient_loss | -0.00764     |
|    reward_explained_... | 0.805        |
|    reward_value_loss    | 9.25         |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -13.1       |
|    mean_ep_length       | 28.4        |
|    mean_reward          | -30.6       |
|    true_cost            | 0.000488    |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.399      |
|    ep_len_mean          | 29.5        |
|    ep_rew_mean          | -12         |
| time/                   |             |
|    fps                  | 1530        |
|    iterations           | 12          |
|    time_elapsed         | 80          |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.026006484 |
|    average_cost         | 0.000390625 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.642       |
|    cost_value_loss      | 0.00792     |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.301      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.63        |
|    mean_cost_advantages | -0.251779   |
|    mean_reward_advan... | 2.915931    |
|    n_updates            | 110         |
|    nu                   | 1.55        |
|    nu_loss              | -0.000593   |
|    policy_gradient_loss | -0.00455    |
|    reward_explained_... | 0.854       |
|    reward_value_loss    | 4.33        |
|    total_cost           | 4.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -10.1         |
|    mean_ep_length       | 27            |
|    mean_reward          | -10.1         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.398        |
|    ep_len_mean          | 26.5          |
|    ep_rew_mean          | -10.4         |
| time/                   |               |
|    fps                  | 1557          |
|    iterations           | 13            |
|    time_elapsed         | 85            |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.01513744    |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0527        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.395         |
|    cost_value_loss      | 0.00419       |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.216        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.37          |
|    mean_cost_advantages | -0.11239797   |
|    mean_reward_advan... | 1.8616459     |
|    n_updates            | 120           |
|    nu                   | 1.57          |
|    nu_loss              | -0.000756     |
|    policy_gradient_loss | -0.00343      |
|    reward_explained_... | 0.854         |
|    reward_value_loss    | 2.69          |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -9.33         |
|    mean_ep_length       | 23            |
|    mean_reward          | -9.33         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.406        |
|    ep_len_mean          | 24.1          |
|    ep_rew_mean          | -9.91         |
| time/                   |               |
|    fps                  | 1469          |
|    iterations           | 14            |
|    time_elapsed         | 97            |
|    total_timesteps      | 143360        |
| train/                  |               |
|    approx_kl            | 0.008656095   |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0372        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.404        |
|    cost_value_loss      | 0.00193       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.152        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.794         |
|    mean_cost_advantages | -0.044737086  |
|    mean_reward_advan... | 1.2943321     |
|    n_updates            | 130           |
|    nu                   | 1.6           |
|    nu_loss              | -0.000461     |
|    policy_gradient_loss | -0.00256      |
|    reward_explained_... | 0.875         |
|    reward_value_loss    | 1.55          |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.87        |
|    mean_ep_length       | 23           |
|    mean_reward          | -8.87        |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.42        |
|    ep_len_mean          | 23.1         |
|    ep_rew_mean          | -9.85        |
| time/                   |              |
|    fps                  | 1384         |
|    iterations           | 15           |
|    time_elapsed         | 110          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0033852593 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.598        |
|    cost_value_loss      | 2.1e-05      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.119       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.541        |
|    mean_cost_advantages | -0.023653034 |
|    mean_reward_advan... | 0.77478856   |
|    n_updates            | 140          |
|    nu                   | 1.62         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00157     |
|    reward_explained_... | 0.883        |
|    reward_value_loss    | 1.2          |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.13        |
|    mean_ep_length       | 19.2         |
|    mean_reward          | -8.13        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.419       |
|    ep_len_mean          | 21.9         |
|    ep_rew_mean          | -9.06        |
| time/                   |              |
|    fps                  | 1342         |
|    iterations           | 16           |
|    time_elapsed         | 122          |
|    total_timesteps      | 163840       |
| train/                  |              |
|    approx_kl            | 0.001187902  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.08        |
|    cost_value_loss      | 0.000413     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0938      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.595        |
|    mean_cost_advantages | -0.009894618 |
|    mean_reward_advan... | 0.3539566    |
|    n_updates            | 150          |
|    nu                   | 1.64         |
|    nu_loss              | -0.000158    |
|    policy_gradient_loss | -0.000721    |
|    reward_explained_... | 0.898        |
|    reward_value_loss    | 0.944        |
|    total_cost           | 1.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.13        |
|    mean_ep_length       | 20.4         |
|    mean_reward          | -8.97        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.426       |
|    ep_len_mean          | 22.2         |
|    ep_rew_mean          | -9.67        |
| time/                   |              |
|    fps                  | 1313         |
|    iterations           | 17           |
|    time_elapsed         | 132          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0016919263 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.369        |
|    cost_value_loss      | 3.19e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0801      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.313        |
|    mean_cost_advantages | -0.003470411 |
|    mean_reward_advan... | 0.18074949   |
|    n_updates            | 160          |
|    nu                   | 1.66         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000621    |
|    reward_explained_... | 0.895        |
|    reward_value_loss    | 0.896        |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -8.13          |
|    mean_ep_length       | 24.8           |
|    mean_reward          | -10.6          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.434         |
|    ep_len_mean          | 21.9           |
|    ep_rew_mean          | -9.61          |
| time/                   |                |
|    fps                  | 1292           |
|    iterations           | 18             |
|    time_elapsed         | 142            |
|    total_timesteps      | 184320         |
| train/                  |                |
|    approx_kl            | 0.0029239382   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.016          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.158          |
|    cost_value_loss      | 5.93e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0549        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.522          |
|    mean_cost_advantages | -0.00049457944 |
|    mean_reward_advan... | 0.027905798    |
|    n_updates            | 170            |
|    nu                   | 1.68           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000548      |
|    reward_explained_... | 0.885          |
|    reward_value_loss    | 0.875          |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -8.13         |
|    mean_ep_length       | 20.8          |
|    mean_reward          | -9.38         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.439        |
|    ep_len_mean          | 21.2          |
|    ep_rew_mean          | -9.15         |
| time/                   |               |
|    fps                  | 1255          |
|    iterations           | 19            |
|    time_elapsed         | 154           |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 0.0014229368  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00711       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0929        |
|    cost_value_loss      | 5.07e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0365       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.412         |
|    mean_cost_advantages | -8.087618e-05 |
|    mean_reward_advan... | 0.09980806    |
|    n_updates            | 180           |
|    nu                   | 1.69          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00018      |
|    reward_explained_... | 0.882         |
|    reward_value_loss    | 0.918         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -8.13         |
|    mean_ep_length       | 22.4          |
|    mean_reward          | -10.2         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.435        |
|    ep_len_mean          | 21.1          |
|    ep_rew_mean          | -9.06         |
| time/                   |               |
|    fps                  | 1190          |
|    iterations           | 20            |
|    time_elapsed         | 171           |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 0.0012594626  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00719       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.189         |
|    cost_value_loss      | 1.14e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0218       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.04          |
|    mean_cost_advantages | 3.7080725e-05 |
|    mean_reward_advan... | -0.035432518  |
|    n_updates            | 190           |
|    nu                   | 1.71          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000155     |
|    reward_explained_... | 0.871         |
|    reward_value_loss    | 0.982         |
|    total_cost           | 0.0           |
-------------------------------------------
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/wrappers/monitoring/video_recorder.py:57: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  logger.deprecation(
Mean reward: -7.944444 +/- 2.021108.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/util.py:37: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping, Sequence
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. [0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:305: UserWarning: [33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps[0m
  logger.warn(
[32;1mTime taken: 03.47 minutes[0m
