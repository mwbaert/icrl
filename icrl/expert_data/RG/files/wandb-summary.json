{"eval/mean_reward": -7.683333333333333, "eval/mean_ep_length": 19.0, "eval/best_mean_reward": -7.683333333333333, "rollout/adjusted_reward": -0.43571776151657104, "eval/true_cost": 0.0, "time/iterations": 20, "time/fps": 1324, "time/time_elapsed": 154, "time/total_timesteps": 204800, "infos/cost": 0.0, "rollout/ep_rew_mean": -9.19416671, "rollout/ep_len_mean": 20.87, "_step": 204800, "_runtime": 183, "_timestamp": 1657254776, "train/learning_rate": 0.0003, "train/entropy_loss": -0.023232882022930425, "train/policy_gradient_loss": -0.00014768171035070527, "train/reward_value_loss": 0.8459050481580198, "train/cost_value_loss": 4.71286970729845e-06, "train/approx_kl": 0.002266335766762495, "train/clip_fraction": 0.009873046875, "train/loss": 0.5692912936210632, "train/mean_reward_advantages": -0.047598596662282944, "train/mean_cost_advantages": -7.194257341325283e-05, "train/reward_explained_variance": 0.878135658800602, "train/cost_explained_variance": 0.15098649263381958, "train/nu": 1.7201247215270996, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 190, "train/clip_range": 0.2}