{"eval/mean_reward": -10.2, "eval/mean_ep_length": 22.4, "eval/best_mean_reward": -8.133333333333335, "rollout/adjusted_reward": -0.4345497488975525, "eval/true_cost": 9.765625e-05, "time/iterations": 20, "time/fps": 1190, "time/time_elapsed": 171, "time/total_timesteps": 204800, "infos/cost": 0.0, "rollout/ep_rew_mean": -9.05666664, "rollout/ep_len_mean": 21.11, "_step": 204800, "_runtime": 207, "_timestamp": 1657802354, "train/learning_rate": 0.0003, "train/entropy_loss": -0.021768539138429334, "train/policy_gradient_loss": -0.00015538107194750967, "train/reward_value_loss": 0.982049762327224, "train/cost_value_loss": 1.1443966343714917e-05, "train/approx_kl": 0.0012594625586643815, "train/clip_fraction": 0.0071875, "train/loss": 1.0409395694732666, "train/mean_reward_advantages": -0.035432517528533936, "train/mean_cost_advantages": 3.7080724723637104e-05, "train/reward_explained_variance": 0.8709947764873505, "train/cost_explained_variance": 0.18873268365859985, "train/nu": 1.7061107158660889, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 190, "train/clip_range": 0.2}