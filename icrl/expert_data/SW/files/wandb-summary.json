{"eval/mean_reward": 1.0, "eval/mean_ep_length": 22.4, "eval/best_mean_reward": 1.0, "rollout/adjusted_reward": 0.03301393613219261, "eval/true_cost": 0.00048828125, "time/iterations": 196, "time/fps": 741, "time/time_elapsed": 541, "time/total_timesteps": 401408, "infos/info": 0.0, "infos/cost": 0.04029201854824335, "rollout/ep_rew_mean": 1.0, "rollout/ep_len_mean": 20.2, "_step": 401408, "_runtime": 554, "_timestamp": 1650889022, "train/learning_rate": 0.0003, "train/entropy_loss": -0.33809652854688466, "train/policy_gradient_loss": -0.0010354732121474657, "train/reward_value_loss": 0.00839725100013311, "train/cost_value_loss": 0.12703178861647757, "train/approx_kl": 0.018925905227661133, "train/clip_fraction": 0.179443359375, "train/loss": 0.006092129275202751, "train/mean_reward_advantages": 0.06789951026439667, "train/mean_cost_advantages": 0.026888445019721985, "train/reward_explained_variance": 0.8012745827436447, "train/cost_explained_variance": -4.735580921173096, "train/nu": 8.279648780822754, "train/nu_loss": -0.020177438855171204, "train/average_cost": 0.00244140625, "train/total_cost": 5.0, "train/early_stop_epoch": 3, "train/n_updates": 1950, "train/clip_range": 0.2}