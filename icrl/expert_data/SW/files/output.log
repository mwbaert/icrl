[32;1mConfigured folder ./cpg/wandb/run-20220425_140748-3u7afptf/files for saving[0m
[32;1mName: SW-v0_CLGW-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
----------------------------------
| eval/               |          |
|    best_mean_reward | -1       |
|    mean_ep_length   | 64.6     |
|    mean_reward      | -1       |
|    true_cost        | 0.0112   |
| infos/              |          |
|    cost             | 0        |
|    info             | 0        |
| rollout/            |          |
|    adjusted_reward  | -0.0178  |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 0.2      |
| time/               |          |
|    fps              | 682      |
|    iterations       | 1        |
|    time_elapsed     | 2        |
|    total_timesteps  | 2048     |
----------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -0.4        |
|    mean_ep_length       | 90.6        |
|    mean_reward          | -0.4        |
|    true_cost            | 0.00586     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.00399    |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 0.444       |
| time/                   |             |
|    fps                  | 571         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009937109 |
|    average_cost         | 0.011230469 |
|    clip_fraction        | 0.0818      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.54       |
|    cost_value_loss      | 1.65        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.6        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    mean_cost_advantages | 0.2984632   |
|    mean_reward_advan... | 0.15619649  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.0112     |
|    policy_gradient_loss | -0.00538    |
|    reward_explained_... | -0.807      |
|    reward_value_loss    | 0.85        |
|    total_cost           | 23.0        |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -0.2        |
|    mean_ep_length       | 64.6        |
|    mean_reward          | -0.2        |
|    true_cost            | 0.0288      |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0208     |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 0.746       |
| time/                   |             |
|    fps                  | 597         |
|    iterations           | 3           |
|    time_elapsed         | 10          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012025247 |
|    average_cost         | 0.005859375 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.69       |
|    cost_value_loss      | 0.129       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.58       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.866       |
|    mean_cost_advantages | 0.045471787 |
|    mean_reward_advan... | 0.7281896   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.00624    |
|    policy_gradient_loss | -0.00803    |
|    reward_explained_... | -16.2       |
|    reward_value_loss    | 3.05        |
|    total_cost           | 12.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 74.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.0171       |
| infos/                  |              |
|    cost                 | 0.0299       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00114     |
|    ep_len_mean          | 84.3         |
|    ep_rew_mean          | 0.845        |
| time/                   |              |
|    fps                  | 599          |
|    iterations           | 4            |
|    time_elapsed         | 13           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0138732195 |
|    average_cost         | 0.028808594  |
|    clip_fraction        | 0.174        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -72.6        |
|    cost_value_loss      | 0.616        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.54        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.33         |
|    mean_cost_advantages | 0.32864866   |
|    mean_reward_advan... | 2.1978183    |
|    n_updates            | 30           |
|    nu                   | 1.19         |
|    nu_loss              | -0.0325      |
|    policy_gradient_loss | -0.011       |
|    reward_explained_... | -9.07        |
|    reward_value_loss    | 5.03         |
|    total_cost           | 59.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.0195       |
| infos/                  |              |
|    cost                 | 0.0407       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.00481      |
|    ep_len_mean          | 44.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 571          |
|    iterations           | 5            |
|    time_elapsed         | 17           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.013391074  |
|    average_cost         | 0.017089844  |
|    clip_fraction        | 0.261        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.93        |
|    cost_value_loss      | 0.242        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.48        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.14         |
|    mean_cost_advantages | -0.053039752 |
|    mean_reward_advan... | 1.9690012    |
|    n_updates            | 40           |
|    nu                   | 1.25         |
|    nu_loss              | -0.0203      |
|    policy_gradient_loss | -0.013       |
|    reward_explained_... | 0.267        |
|    reward_value_loss    | 2.33         |
|    total_cost           | 35.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 22.2        |
|    mean_reward          | -0.2        |
|    true_cost            | 0.0112      |
| infos/                  |             |
|    cost                 | 0.0109      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0209      |
|    ep_len_mean          | 30.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 610         |
|    iterations           | 6           |
|    time_elapsed         | 20          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.017447073 |
|    average_cost         | 0.01953125  |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.64       |
|    cost_value_loss      | 0.42        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.953       |
|    mean_cost_advantages | -0.05472324 |
|    mean_reward_advan... | 1.5391359   |
|    n_updates            | 50          |
|    nu                   | 1.32        |
|    nu_loss              | -0.0244     |
|    policy_gradient_loss | -0.0133     |
|    reward_explained_... | 0.669       |
|    reward_value_loss    | 1.5         |
|    total_cost           | 40.0        |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 25.4        |
|    mean_reward          | 0.6         |
|    true_cost            | 0.0132      |
| infos/                  |             |
|    cost                 | 0.0225      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0174      |
|    ep_len_mean          | 27.6        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 639         |
|    iterations           | 7           |
|    time_elapsed         | 22          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.019516425 |
|    average_cost         | 0.011230469 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.8        |
|    cost_value_loss      | 0.107       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.29       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.121       |
|    mean_cost_advantages | -0.15145898 |
|    mean_reward_advan... | 0.34326375  |
|    n_updates            | 60          |
|    nu                   | 1.39        |
|    nu_loss              | -0.0148     |
|    policy_gradient_loss | -0.00605    |
|    reward_explained_... | 0.609       |
|    reward_value_loss    | 0.332       |
|    total_cost           | 23.0        |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18           |
|    mean_reward          | 0.6          |
|    true_cost            | 0.0127       |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0266       |
|    ep_len_mean          | 21.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 668          |
|    iterations           | 8            |
|    time_elapsed         | 24           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.02659454   |
|    average_cost         | 0.013183594  |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.71        |
|    cost_value_loss      | 0.186        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -1.22        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0784       |
|    mean_cost_advantages | -0.041912593 |
|    mean_reward_advan... | -0.36515093  |
|    n_updates            | 70           |
|    nu                   | 1.46         |
|    nu_loss              | -0.0183      |
|    policy_gradient_loss | -0.00578     |
|    reward_explained_... | 0.523        |
|    reward_value_loss    | 0.114        |
|    total_cost           | 27.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.0239       |
| infos/                  |              |
|    cost                 | 0.0248       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0206       |
|    ep_len_mean          | 16.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 693          |
|    iterations           | 9            |
|    time_elapsed         | 26           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.020450935  |
|    average_cost         | 0.0126953125 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.82        |
|    cost_value_loss      | 0.0983       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -1.11        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0626       |
|    mean_cost_advantages | -0.04999617  |
|    mean_reward_advan... | -0.38648188  |
|    n_updates            | 80           |
|    nu                   | 1.53         |
|    nu_loss              | -0.0185      |
|    policy_gradient_loss | -0.00771     |
|    reward_explained_... | 0.426        |
|    reward_value_loss    | 0.0776       |
|    total_cost           | 26.0         |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16          |
|    mean_reward          | 1           |
|    true_cost            | 0.0273      |
| infos/                  |             |
|    cost                 | 0.0128      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0173      |
|    ep_len_mean          | 14.5        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 703         |
|    iterations           | 10          |
|    time_elapsed         | 29          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.016373552 |
|    average_cost         | 0.023925781 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -12         |
|    cost_value_loss      | 0.188       |
|    early_stop_epoch     | 7           |
|    entropy_loss         | -0.943      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0553      |
|    mean_cost_advantages | 0.037513565 |
|    mean_reward_advan... | -0.36011598 |
|    n_updates            | 90          |
|    nu                   | 1.6         |
|    nu_loss              | -0.0366     |
|    policy_gradient_loss | -0.00708    |
|    reward_explained_... | 0.399       |
|    reward_value_loss    | 0.0359      |
|    total_cost           | 49.0        |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 13.6        |
|    mean_reward          | 0.2         |
|    true_cost            | 0.0396      |
| infos/                  |             |
|    cost                 | 0.0396      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0016     |
|    ep_len_mean          | 12.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 719         |
|    iterations           | 11          |
|    time_elapsed         | 31          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.017112864 |
|    average_cost         | 0.02734375  |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -8.07       |
|    cost_value_loss      | 0.209       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.824      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.136       |
|    mean_cost_advantages | 0.035471525 |
|    mean_reward_advan... | -0.37591842 |
|    n_updates            | 100         |
|    nu                   | 1.68        |
|    nu_loss              | -0.0439     |
|    policy_gradient_loss | -0.00555    |
|    reward_explained_... | 0.405       |
|    reward_value_loss    | 0.0331      |
|    total_cost           | 56.0        |
-----------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 7.2         |
|    mean_reward          | -0.2        |
|    true_cost            | 0.0327      |
| infos/                  |             |
|    cost                 | 0.0272      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.00847     |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 719         |
|    iterations           | 12          |
|    time_elapsed         | 34          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.015602817 |
|    average_cost         | 0.03955078  |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -8.01       |
|    cost_value_loss      | 0.25        |
|    early_stop_epoch     | 8           |
|    entropy_loss         | -0.695      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    mean_cost_advantages | 0.07808845  |
|    mean_reward_advan... | -0.32722494 |
|    n_updates            | 110         |
|    nu                   | 1.76        |
|    nu_loss              | -0.0666     |
|    policy_gradient_loss | -0.00656    |
|    reward_explained_... | 0.332       |
|    reward_value_loss    | 0.0165      |
|    total_cost           | 81.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 12           |
|    mean_reward          | 0.6          |
|    true_cost            | 0.0337       |
| infos/                  |              |
|    cost                 | 0.042        |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.003        |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 13           |
|    time_elapsed         | 37           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.004073248  |
|    average_cost         | 0.032714844  |
|    clip_fraction        | 0.0745       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.57        |
|    cost_value_loss      | 0.172        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.595       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0585       |
|    mean_cost_advantages | -0.039426483 |
|    mean_reward_advan... | -0.2866306   |
|    n_updates            | 120          |
|    nu                   | 1.85         |
|    nu_loss              | -0.0577      |
|    policy_gradient_loss | -0.00396     |
|    reward_explained_... | 0.387        |
|    reward_value_loss    | 0.0107       |
|    total_cost           | 67.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 5.8          |
|    mean_reward          | -0.6         |
|    true_cost            | 0.041        |
| infos/                  |              |
|    cost                 | 0.0575       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0164      |
|    ep_len_mean          | 10.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 14           |
|    time_elapsed         | 39           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.012434911  |
|    average_cost         | 0.033691406  |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.92        |
|    cost_value_loss      | 0.183        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.516       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.087        |
|    mean_cost_advantages | -0.001334847 |
|    mean_reward_advan... | -0.22975604  |
|    n_updates            | 130          |
|    nu                   | 1.94         |
|    nu_loss              | -0.0623      |
|    policy_gradient_loss | -0.00347     |
|    reward_explained_... | 0.488        |
|    reward_value_loss    | 0.0075       |
|    total_cost           | 69.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 10.8        |
|    mean_reward          | 0.6         |
|    true_cost            | 0.0386      |
| infos/                  |             |
|    cost                 | 0.0441      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0192     |
|    ep_len_mean          | 10.8        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 717         |
|    iterations           | 15          |
|    time_elapsed         | 42          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.005608582 |
|    average_cost         | 0.041015625 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.67       |
|    cost_value_loss      | 0.219       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.446      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0765      |
|    mean_cost_advantages | 0.037637975 |
|    mean_reward_advan... | -0.1668545  |
|    n_updates            | 140         |
|    nu                   | 2.03        |
|    nu_loss              | -0.0795     |
|    policy_gradient_loss | -0.00306    |
|    reward_explained_... | 0.643       |
|    reward_value_loss    | 0.00502     |
|    total_cost           | 84.0        |
-----------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1              |
|    mean_ep_length       | 8.4            |
|    mean_reward          | 0.2            |
|    true_cost            | 0.0503         |
| infos/                  |                |
|    cost                 | 0.03           |
|    info                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.0567        |
|    ep_len_mean          | 9.98           |
|    ep_rew_mean          | 1              |
| time/                   |                |
|    fps                  | 715            |
|    iterations           | 16             |
|    time_elapsed         | 45             |
|    total_timesteps      | 32768          |
| train/                  |                |
|    approx_kl            | 0.010376869    |
|    average_cost         | 0.03857422     |
|    clip_fraction        | 0.0501         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -3.53          |
|    cost_value_loss      | 0.219          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.388         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.102          |
|    mean_cost_advantages | -0.00047515798 |
|    mean_reward_advan... | -0.15634942    |
|    n_updates            | 150            |
|    nu                   | 2.12           |
|    nu_loss              | -0.0782        |
|    policy_gradient_loss | -0.00382       |
|    reward_explained_... | 0.539          |
|    reward_value_loss    | 0.00568        |
|    total_cost           | 79.0           |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 10.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.0542       |
| infos/                  |              |
|    cost                 | 0.0458       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0772      |
|    ep_len_mean          | 9.7          |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 17           |
|    time_elapsed         | 48           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.005724002  |
|    average_cost         | 0.05029297   |
|    clip_fraction        | 0.0456       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.42        |
|    cost_value_loss      | 0.248        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.335       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.122        |
|    mean_cost_advantages | 0.07578161   |
|    mean_reward_advan... | -0.117375426 |
|    n_updates            | 160          |
|    nu                   | 2.22         |
|    nu_loss              | -0.107       |
|    policy_gradient_loss | -0.0021      |
|    reward_explained_... | 0.739        |
|    reward_value_loss    | 0.00344      |
|    total_cost           | 103.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 6.2          |
|    mean_reward          | -0.6         |
|    true_cost            | 0.0405       |
| infos/                  |              |
|    cost                 | 0.0779       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0462      |
|    ep_len_mean          | 10.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 18           |
|    time_elapsed         | 51           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.009209261  |
|    average_cost         | 0.05419922   |
|    clip_fraction        | 0.0627       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.67        |
|    cost_value_loss      | 0.263        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.324       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0918       |
|    mean_cost_advantages | 0.024124566  |
|    mean_reward_advan... | -0.097656325 |
|    n_updates            | 170          |
|    nu                   | 2.32         |
|    nu_loss              | -0.12        |
|    policy_gradient_loss | -0.00151     |
|    reward_explained_... | 0.807        |
|    reward_value_loss    | 0.00258      |
|    total_cost           | 111.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.0381       |
| infos/                  |              |
|    cost                 | 0.0634       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0454      |
|    ep_len_mean          | 9.94         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 711          |
|    iterations           | 19           |
|    time_elapsed         | 54           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 6.722595e-05 |
|    average_cost         | 0.040527344  |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.09        |
|    cost_value_loss      | 0.243        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.322       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.149        |
|    mean_cost_advantages | -0.072379656 |
|    mean_reward_advan... | -0.09919217  |
|    n_updates            | 180          |
|    nu                   | 2.42         |
|    nu_loss              | -0.0939      |
|    policy_gradient_loss | -0.00123     |
|    reward_explained_... | 0.748        |
|    reward_value_loss    | 0.00307      |
|    total_cost           | 83.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 6.4          |
|    mean_reward          | -0.2         |
|    true_cost            | 0.0586       |
| infos/                  |              |
|    cost                 | 0.0643       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.13        |
|    ep_len_mean          | 9.32         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 710          |
|    iterations           | 20           |
|    time_elapsed         | 57           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0043477714 |
|    average_cost         | 0.038085938  |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.25        |
|    cost_value_loss      | 0.245        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.295       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0899       |
|    mean_cost_advantages | -0.006341653 |
|    mean_reward_advan... | -0.0684821   |
|    n_updates            | 190          |
|    nu                   | 2.52         |
|    nu_loss              | -0.092       |
|    policy_gradient_loss | -0.00189     |
|    reward_explained_... | 0.794        |
|    reward_value_loss    | 0.00262      |
|    total_cost           | 78.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.0146       |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0215       |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 21           |
|    time_elapsed         | 59           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.031523705  |
|    average_cost         | 0.05859375   |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.33        |
|    cost_value_loss      | 0.297        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.323       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.153        |
|    mean_cost_advantages | 0.14353958   |
|    mean_reward_advan... | -0.047684807 |
|    n_updates            | 200          |
|    nu                   | 2.62         |
|    nu_loss              | -0.148       |
|    policy_gradient_loss | -0.00224     |
|    reward_explained_... | 0.81         |
|    reward_value_loss    | 0.00205      |
|    total_cost           | 120.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 6.8          |
|    mean_reward          | -0.2         |
|    true_cost            | 0.0356       |
| infos/                  |              |
|    cost                 | 0.0665       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0646      |
|    ep_len_mean          | 10.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 715          |
|    iterations           | 22           |
|    time_elapsed         | 62           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.009845328  |
|    average_cost         | 0.0146484375 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.386       |
|    cost_value_loss      | 0.137        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.355       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0536       |
|    mean_cost_advantages | -0.25503907  |
|    mean_reward_advan... | -0.091187686 |
|    n_updates            | 210          |
|    nu                   | 2.72         |
|    nu_loss              | -0.0384      |
|    policy_gradient_loss | -0.00473     |
|    reward_explained_... | 0.683        |
|    reward_value_loss    | 0.00354      |
|    total_cost           | 30.0         |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1              |
|    mean_ep_length       | 8.4            |
|    mean_reward          | 0.2            |
|    true_cost            | 0.04           |
| infos/                  |                |
|    cost                 | 0.0675         |
|    info                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.0907        |
|    ep_len_mean          | 10.1           |
|    ep_rew_mean          | 1              |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 23             |
|    time_elapsed         | 65             |
|    total_timesteps      | 47104          |
| train/                  |                |
|    approx_kl            | -0.00036306086 |
|    average_cost         | 0.03564453     |
|    clip_fraction        | 0.00742        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -7.49          |
|    cost_value_loss      | 0.263          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.314         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.159          |
|    mean_cost_advantages | 0.10967059     |
|    mean_reward_advan... | -0.027843304   |
|    n_updates            | 220            |
|    nu                   | 2.82           |
|    nu_loss              | -0.097         |
|    policy_gradient_loss | -8.19e-05      |
|    reward_explained_... | 0.786          |
|    reward_value_loss    | 0.00278        |
|    total_cost           | 73.0           |
--------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 8.2         |
|    mean_reward          | 0.2         |
|    true_cost            | 0.0181      |
| infos/                  |             |
|    cost                 | 0.0171      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.00521    |
|    ep_len_mean          | 12.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 719         |
|    iterations           | 24          |
|    time_elapsed         | 68          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.01613117  |
|    average_cost         | 0.040039062 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.05       |
|    cost_value_loss      | 0.286       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.338      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    mean_cost_advantages | 0.050090455 |
|    mean_reward_advan... | -0.03212603 |
|    n_updates            | 230         |
|    nu                   | 2.92        |
|    nu_loss              | -0.113      |
|    policy_gradient_loss | -0.000865   |
|    reward_explained_... | 0.754       |
|    reward_value_loss    | 0.00305     |
|    total_cost           | 82.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 6.8          |
|    mean_reward          | -0.2         |
|    true_cost            | 0.0283       |
| infos/                  |              |
|    cost                 | 0.0348       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0544      |
|    ep_len_mean          | 10.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 25           |
|    time_elapsed         | 71           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.009700041  |
|    average_cost         | 0.018066406  |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.951       |
|    cost_value_loss      | 0.171        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.341       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0755       |
|    mean_cost_advantages | -0.14845079  |
|    mean_reward_advan... | -0.056963135 |
|    n_updates            | 240          |
|    nu                   | 3.02         |
|    nu_loss              | -0.0528      |
|    policy_gradient_loss | -0.00379     |
|    reward_explained_... | 0.772        |
|    reward_value_loss    | 0.00375      |
|    total_cost           | 37.0         |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1              |
|    mean_ep_length       | 7.2            |
|    mean_reward          | -0.2           |
|    true_cost            | 0.0371         |
| infos/                  |                |
|    cost                 | 0.0352         |
|    info                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.104         |
|    ep_len_mean          | 10.2           |
|    ep_rew_mean          | 1              |
| time/                   |                |
|    fps                  | 706            |
|    iterations           | 26             |
|    time_elapsed         | 75             |
|    total_timesteps      | 53248          |
| train/                  |                |
|    approx_kl            | 0.00037936523  |
|    average_cost         | 0.028320312    |
|    clip_fraction        | 0.00601        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -8.04          |
|    cost_value_loss      | 0.247          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.317         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.163          |
|    mean_cost_advantages | 0.0613441      |
|    mean_reward_advan... | -0.00038891018 |
|    n_updates            | 250            |
|    nu                   | 3.12           |
|    nu_loss              | -0.0856        |
|    policy_gradient_loss | -0.000135      |
|    reward_explained_... | 0.741          |
|    reward_value_loss    | 0.00339        |
|    total_cost           | 58.0           |
--------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.0112       |
| infos/                  |              |
|    cost                 | 0.0536       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0159       |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 701          |
|    iterations           | 27           |
|    time_elapsed         | 78           |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.021052785  |
|    average_cost         | 0.037109375  |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.88        |
|    cost_value_loss      | 0.299        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.335       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.206        |
|    mean_cost_advantages | 0.07525192   |
|    mean_reward_advan... | -0.010598818 |
|    n_updates            | 260          |
|    nu                   | 3.22         |
|    nu_loss              | -0.116       |
|    policy_gradient_loss | -0.0015      |
|    reward_explained_... | 0.825        |
|    reward_value_loss    | 0.00243      |
|    total_cost           | 76.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 12.8        |
|    mean_reward          | 0.6         |
|    true_cost            | 0.0239      |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0536     |
|    ep_len_mean          | 11.3        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 691         |
|    iterations           | 28          |
|    time_elapsed         | 82          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.009814016 |
|    average_cost         | 0.011230469 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.25       |
|    cost_value_loss      | 0.13        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.345      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0602      |
|    mean_cost_advantages | -0.16178255 |
|    mean_reward_advan... | -0.06470458 |
|    n_updates            | 270         |
|    nu                   | 3.32        |
|    nu_loss              | -0.0362     |
|    policy_gradient_loss | -0.00404    |
|    reward_explained_... | 0.712       |
|    reward_value_loss    | 0.00442     |
|    total_cost           | 23.0        |
-----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 7.2          |
|    mean_reward          | -0.2         |
|    true_cost            | 0.0112       |
| infos/                  |              |
|    cost                 | 0.0367       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.00884      |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 687          |
|    iterations           | 29           |
|    time_elapsed         | 86           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.019680332  |
|    average_cost         | 0.023925781  |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -14.6        |
|    cost_value_loss      | 0.236        |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.364       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0821       |
|    mean_cost_advantages | 0.07452958   |
|    mean_reward_advan... | 0.0071928483 |
|    n_updates            | 280          |
|    nu                   | 3.41         |
|    nu_loss              | -0.0794      |
|    policy_gradient_loss | -0.000595    |
|    reward_explained_... | 0.776        |
|    reward_value_loss    | 0.0038       |
|    total_cost           | 49.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.0225       |
| infos/                  |              |
|    cost                 | 0.0371       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0571      |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 686          |
|    iterations           | 30           |
|    time_elapsed         | 89           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0034074483 |
|    average_cost         | 0.011230469  |
|    clip_fraction        | 0.0893       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.05        |
|    cost_value_loss      | 0.13         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.363       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0332       |
|    mean_cost_advantages | -0.08030091  |
|    mean_reward_advan... | -0.033821303 |
|    n_updates            | 290          |
|    nu                   | 3.5          |
|    nu_loss              | -0.0383      |
|    policy_gradient_loss | -0.00331     |
|    reward_explained_... | 0.602        |
|    reward_value_loss    | 0.00497      |
|    total_cost           | 23.0         |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 1           |
|    true_cost            | 0.00586     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0309      |
|    ep_len_mean          | 14.1        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 690         |
|    iterations           | 31          |
|    time_elapsed         | 91          |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.022591982 |
|    average_cost         | 0.022460938 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -18.4       |
|    cost_value_loss      | 0.238       |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -0.384      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    mean_cost_advantages | 0.073213965 |
|    mean_reward_advan... | 0.006173703 |
|    n_updates            | 300         |
|    nu                   | 3.59        |
|    nu_loss              | -0.0787     |
|    policy_gradient_loss | -0.000651   |
|    reward_explained_... | 0.759       |
|    reward_value_loss    | 0.00381     |
|    total_cost           | 46.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.0171       |
| infos/                  |              |
|    cost                 | 0.0761       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0373      |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 689          |
|    iterations           | 32           |
|    time_elapsed         | 95           |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0074566263 |
|    average_cost         | 0.005859375  |
|    clip_fraction        | 0.174        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.63        |
|    cost_value_loss      | 0.0738       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.391       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.04         |
|    mean_cost_advantages | -0.113156855 |
|    mean_reward_advan... | -0.059746508 |
|    n_updates            | 310          |
|    nu                   | 3.68         |
|    nu_loss              | -0.0211      |
|    policy_gradient_loss | -0.00508     |
|    reward_explained_... | 0.557        |
|    reward_value_loss    | 0.00609      |
|    total_cost           | 12.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 1           |
|    true_cost            | 0.00635     |
| infos/                  |             |
|    cost                 | 0.0193      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0245      |
|    ep_len_mean          | 14.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 687         |
|    iterations           | 33          |
|    time_elapsed         | 98          |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.013875275 |
|    average_cost         | 0.017089844 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -41.5       |
|    cost_value_loss      | 0.202       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.403      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    mean_cost_advantages | 0.073020756 |
|    mean_reward_advan... | 0.03562253  |
|    n_updates            | 320         |
|    nu                   | 3.76        |
|    nu_loss              | -0.0628     |
|    policy_gradient_loss | -0.000335   |
|    reward_explained_... | 0.724       |
|    reward_value_loss    | 0.00501     |
|    total_cost           | 35.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.0127       |
| infos/                  |              |
|    cost                 | 0.039        |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0152      |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 686          |
|    iterations           | 34           |
|    time_elapsed         | 101          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.007825475  |
|    average_cost         | 0.0063476562 |
|    clip_fraction        | 0.167        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.02        |
|    cost_value_loss      | 0.0877       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.395       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0669       |
|    mean_cost_advantages | -0.07313061  |
|    mean_reward_advan... | -0.032640807 |
|    n_updates            | 330          |
|    nu                   | 3.84         |
|    nu_loss              | -0.0239      |
|    policy_gradient_loss | -0.00447     |
|    reward_explained_... | 0.691        |
|    reward_value_loss    | 0.00671      |
|    total_cost           | 13.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 10           |
|    mean_reward          | 0.2          |
|    true_cost            | 0.0186       |
| infos/                  |              |
|    cost                 | 0.0394       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0541      |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 682          |
|    iterations           | 35           |
|    time_elapsed         | 105          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.00760996   |
|    average_cost         | 0.0126953125 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -34.2        |
|    cost_value_loss      | 0.164        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.361       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0923       |
|    mean_cost_advantages | 0.04314881   |
|    mean_reward_advan... | 0.015909297  |
|    n_updates            | 340          |
|    nu                   | 3.91         |
|    nu_loss              | -0.0487      |
|    policy_gradient_loss | -0.00103     |
|    reward_explained_... | 0.706        |
|    reward_value_loss    | 0.00545      |
|    total_cost           | 26.0         |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 13.6        |
|    mean_reward          | 1           |
|    true_cost            | 0.00928     |
| infos/                  |             |
|    cost                 | 0.0398      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.00186     |
|    ep_len_mean          | 13.1        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 683         |
|    iterations           | 36          |
|    time_elapsed         | 107         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.016274288 |
|    average_cost         | 0.018554688 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -22.1       |
|    cost_value_loss      | 0.231       |
|    early_stop_epoch     | 6           |
|    entropy_loss         | -0.371      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    mean_cost_advantages | 0.055857845 |
|    mean_reward_advan... | 0.021498289 |
|    n_updates            | 350         |
|    nu                   | 3.99        |
|    nu_loss              | -0.0726     |
|    policy_gradient_loss | -0.00146    |
|    reward_explained_... | 0.817       |
|    reward_value_loss    | 0.00311     |
|    total_cost           | 38.0        |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 9.6          |
|    mean_reward          | 0.6          |
|    true_cost            | 0.0137       |
| infos/                  |              |
|    cost                 | 0.0201       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0265      |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 686          |
|    iterations           | 37           |
|    time_elapsed         | 110          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.015078852  |
|    average_cost         | 0.009277344  |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.03        |
|    cost_value_loss      | 0.131        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.356       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.105        |
|    mean_cost_advantages | -0.06578693  |
|    mean_reward_advan... | -0.041223783 |
|    n_updates            | 360          |
|    nu                   | 4.06         |
|    nu_loss              | -0.037       |
|    policy_gradient_loss | -0.00241     |
|    reward_explained_... | 0.702        |
|    reward_value_loss    | 0.00572      |
|    total_cost           | 19.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 8            |
|    mean_reward          | 0.2          |
|    true_cost            | 0.0273       |
| infos/                  |              |
|    cost                 | 0.101        |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.137       |
|    ep_len_mean          | 10.7         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 683          |
|    iterations           | 38           |
|    time_elapsed         | 113          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0074336757 |
|    average_cost         | 0.013671875  |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -8.26        |
|    cost_value_loss      | 0.187        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.34        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.024        |
|    mean_cost_advantages | 0.026899727  |
|    mean_reward_advan... | 0.0055545755 |
|    n_updates            | 370          |
|    nu                   | 4.13         |
|    nu_loss              | -0.0555      |
|    policy_gradient_loss | -0.000418    |
|    reward_explained_... | 0.742        |
|    reward_value_loss    | 0.00419      |
|    total_cost           | 28.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.13
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0399       |
|    ep_len_mean          | 22.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 686          |
|    iterations           | 39           |
|    time_elapsed         | 116          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.12959565   |
|    average_cost         | 0.02734375   |
|    clip_fraction        | 0.302        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -14          |
|    cost_value_loss      | 0.333        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.365       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.155        |
|    mean_cost_advantages | 0.12362912   |
|    mean_reward_advan... | 0.0025857664 |
|    n_updates            | 380          |
|    nu                   | 4.2          |
|    nu_loss              | -0.113       |
|    policy_gradient_loss | -0.0137      |
|    reward_explained_... | 0.746        |
|    reward_value_loss    | 0.00342      |
|    total_cost           | 56.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 13.2        |
|    mean_reward          | 0.6         |
|    true_cost            | 0.00293     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0416      |
|    ep_len_mean          | 15.1        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 690         |
|    iterations           | 40          |
|    time_elapsed         | 118         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.06430615  |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.356       |
|    cost_value_loss      | 0.0265      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.427      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00176     |
|    mean_cost_advantages | -0.19435045 |
|    mean_reward_advan... | -0.17551874 |
|    n_updates            | 390         |
|    nu                   | 4.27        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00788    |
|    reward_explained_... | 0.332       |
|    reward_value_loss    | 0.00918     |
|    total_cost           | 0.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.0146       |
| infos/                  |              |
|    cost                 | 0.0209       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0532      |
|    ep_len_mean          | 12.7         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 689          |
|    iterations           | 41           |
|    time_elapsed         | 121          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.008925213  |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.215        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.15        |
|    cost_value_loss      | 0.0488       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.386       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0309       |
|    mean_cost_advantages | -0.016481    |
|    mean_reward_advan... | 0.08432612   |
|    n_updates            | 400          |
|    nu                   | 4.33         |
|    nu_loss              | -0.0125      |
|    policy_gradient_loss | -0.00577     |
|    reward_explained_... | 0.788        |
|    reward_value_loss    | 0.00777      |
|    total_cost           | 6.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 17           |
|    mean_reward          | 1            |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.035        |
|    ep_len_mean          | 17.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 694          |
|    iterations           | 42           |
|    time_elapsed         | 123          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.036897086  |
|    average_cost         | 0.0146484375 |
|    clip_fraction        | 0.247        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -60.7        |
|    cost_value_loss      | 0.22         |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.384       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.121        |
|    mean_cost_advantages | 0.10790999   |
|    mean_reward_advan... | 0.07132253   |
|    n_updates            | 410          |
|    nu                   | 4.4          |
|    nu_loss              | -0.0635      |
|    policy_gradient_loss | -0.00265     |
|    reward_explained_... | 0.794        |
|    reward_value_loss    | 0.00598      |
|    total_cost           | 30.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 12           |
|    mean_reward          | 1            |
|    true_cost            | 0.00781      |
| infos/                  |              |
|    cost                 | 0.064        |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00422     |
|    ep_len_mean          | 14.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 693          |
|    iterations           | 43           |
|    time_elapsed         | 126          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0075973338 |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.237        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.37        |
|    cost_value_loss      | 0.0428       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.405       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0091      |
|    mean_cost_advantages | -0.09076311  |
|    mean_reward_advan... | -0.07819831  |
|    n_updates            | 420          |
|    nu                   | 4.46         |
|    nu_loss              | -0.0107      |
|    policy_gradient_loss | -0.00577     |
|    reward_explained_... | 0.684        |
|    reward_value_loss    | 0.008        |
|    total_cost           | 5.0          |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 10.6        |
|    mean_reward          | 0.6         |
|    true_cost            | 0.00732     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.00582     |
|    ep_len_mean          | 13.3        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 693         |
|    iterations           | 44          |
|    time_elapsed         | 129         |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.00756388  |
|    average_cost         | 0.0078125   |
|    clip_fraction        | 0.0569      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -41.6       |
|    cost_value_loss      | 0.128       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.386      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0797      |
|    mean_cost_advantages | 0.03386275  |
|    mean_reward_advan... | 0.029451853 |
|    n_updates            | 430         |
|    nu                   | 4.51        |
|    nu_loss              | -0.0348     |
|    policy_gradient_loss | -0.00118    |
|    reward_explained_... | 0.656       |
|    reward_value_loss    | 0.00836     |
|    total_cost           | 16.0        |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 0.2           |
|    true_cost            | 0.0171        |
| infos/                  |               |
|    cost                 | 0.0651        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0843       |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 693           |
|    iterations           | 45            |
|    time_elapsed         | 132           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.0038991938  |
|    average_cost         | 0.0073242188  |
|    clip_fraction        | 0.0874        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -13           |
|    cost_value_loss      | 0.121         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.352        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.106         |
|    mean_cost_advantages | 0.00049978774 |
|    mean_reward_advan... | 0.05176855    |
|    n_updates            | 440           |
|    nu                   | 4.57          |
|    nu_loss              | -0.033        |
|    policy_gradient_loss | -0.00183      |
|    reward_explained_... | 0.797         |
|    reward_value_loss    | 0.00503       |
|    total_cost           | 15.0          |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 1           |
|    true_cost            | 0.00195     |
| infos/                  |             |
|    cost                 | 0.0219      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0354      |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 697         |
|    iterations           | 46          |
|    time_elapsed         | 134         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.050500985 |
|    average_cost         | 0.017089844 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -34.4       |
|    cost_value_loss      | 0.264       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.376      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    mean_cost_advantages | 0.09467507  |
|    mean_reward_advan... | 0.038944542 |
|    n_updates            | 450         |
|    nu                   | 4.62        |
|    nu_loss              | -0.078      |
|    policy_gradient_loss | -0.00437    |
|    reward_explained_... | 0.808       |
|    reward_value_loss    | 0.00419     |
|    total_cost           | 35.0        |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 1           |
|    true_cost            | 0.00635     |
| infos/                  |             |
|    cost                 | 0.0664      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.00155     |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 697         |
|    iterations           | 47          |
|    time_elapsed         | 137         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.013327021 |
|    average_cost         | 0.001953125 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.15       |
|    cost_value_loss      | 0.0374      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.4        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0242      |
|    mean_cost_advantages | -0.10969916 |
|    mean_reward_advan... | -0.08678572 |
|    n_updates            | 460         |
|    nu                   | 4.67        |
|    nu_loss              | -0.00903    |
|    policy_gradient_loss | -0.00479    |
|    reward_explained_... | 0.562       |
|    reward_value_loss    | 0.00735     |
|    total_cost           | 4.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00928      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0211      |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 698          |
|    iterations           | 48           |
|    time_elapsed         | 140          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.008931943  |
|    average_cost         | 0.0063476562 |
|    clip_fraction        | 0.0788       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -23.2        |
|    cost_value_loss      | 0.112        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.381       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.114        |
|    mean_cost_advantages | 0.0149434935 |
|    mean_reward_advan... | 0.017832462  |
|    n_updates            | 470          |
|    nu                   | 4.72         |
|    nu_loss              | -0.0297      |
|    policy_gradient_loss | -0.00144     |
|    reward_explained_... | 0.608        |
|    reward_value_loss    | 0.00872      |
|    total_cost           | 13.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16          |
|    mean_reward          | 1           |
|    true_cost            | 0.00439     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0238      |
|    ep_len_mean          | 14.3        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 697         |
|    iterations           | 49          |
|    time_elapsed         | 143         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.010751927 |
|    average_cost         | 0.009277344 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -65         |
|    cost_value_loss      | 0.18        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.367      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0371      |
|    mean_cost_advantages | 0.03540014  |
|    mean_reward_advan... | 0.03522715  |
|    n_updates            | 480         |
|    nu                   | 4.77        |
|    nu_loss              | -0.0438     |
|    policy_gradient_loss | -0.00128    |
|    reward_explained_... | 0.716       |
|    reward_value_loss    | 0.00573     |
|    total_cost           | 19.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.0127       |
| infos/                  |              |
|    cost                 | 0.0453       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0585      |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 686          |
|    iterations           | 50           |
|    time_elapsed         | 149          |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0101496475 |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.16         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -8.15        |
|    cost_value_loss      | 0.0811       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.352       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00419     |
|    mean_cost_advantages | -0.04191143  |
|    mean_reward_advan... | -0.008821988 |
|    n_updates            | 490          |
|    nu                   | 4.82         |
|    nu_loss              | -0.021       |
|    policy_gradient_loss | -0.00344     |
|    reward_explained_... | 0.769        |
|    reward_value_loss    | 0.00568      |
|    total_cost           | 9.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00391      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0197       |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 689          |
|    iterations           | 51           |
|    time_elapsed         | 151          |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.018331274  |
|    average_cost         | 0.0126953125 |
|    clip_fraction        | 0.211        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -52          |
|    cost_value_loss      | 0.221        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.359       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0878       |
|    mean_cost_advantages | 0.073354274  |
|    mean_reward_advan... | 0.024680315  |
|    n_updates            | 500          |
|    nu                   | 4.87         |
|    nu_loss              | -0.0612      |
|    policy_gradient_loss | -0.00236     |
|    reward_explained_... | 0.788        |
|    reward_value_loss    | 0.00497      |
|    total_cost           | 26.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 1           |
|    true_cost            | 0.00488     |
| infos/                  |             |
|    cost                 | 0.023       |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0143      |
|    ep_len_mean          | 14.4        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 686         |
|    iterations           | 52          |
|    time_elapsed         | 155         |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.008636327 |
|    average_cost         | 0.00390625  |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -6.39       |
|    cost_value_loss      | 0.0746      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.377      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0261      |
|    mean_cost_advantages | -0.06776    |
|    mean_reward_advan... | -0.05156542 |
|    n_updates            | 510         |
|    nu                   | 4.91        |
|    nu_loss              | -0.019      |
|    policy_gradient_loss | -0.00268    |
|    reward_explained_... | 0.662       |
|    reward_value_loss    | 0.00704     |
|    total_cost           | 8.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00781      |
| infos/                  |              |
|    cost                 | 0.0232       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00949     |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 681          |
|    iterations           | 53           |
|    time_elapsed         | 159          |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 0.011427131  |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -20.9        |
|    cost_value_loss      | 0.0941       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.349       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0666       |
|    mean_cost_advantages | -0.00837028  |
|    mean_reward_advan... | 0.016207822  |
|    n_updates            | 520          |
|    nu                   | 4.96         |
|    nu_loss              | -0.024       |
|    policy_gradient_loss | -0.00252     |
|    reward_explained_... | 0.738        |
|    reward_value_loss    | 0.00674      |
|    total_cost           | 10.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 7.6         |
|    mean_reward          | -0.2        |
|    true_cost            | 0.0186      |
| infos/                  |             |
|    cost                 | 0.0233      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.128      |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 679         |
|    iterations           | 54          |
|    time_elapsed         | 162         |
|    total_timesteps      | 110592      |
| train/                  |             |
|    approx_kl            | 0.006433851 |
|    average_cost         | 0.0078125   |
|    clip_fraction        | 0.0361      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -27.8       |
|    cost_value_loss      | 0.148       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.321      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0273      |
|    mean_cost_advantages | 0.027356993 |
|    mean_reward_advan... | 0.037216946 |
|    n_updates            | 530         |
|    nu                   | 5           |
|    nu_loss              | -0.0387     |
|    policy_gradient_loss | -0.000701   |
|    reward_explained_... | 0.807       |
|    reward_value_loss    | 0.00406     |
|    total_cost           | 16.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 24.8        |
|    mean_reward          | 1           |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0434      |
|    ep_len_mean          | 21.7        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 680         |
|    iterations           | 55          |
|    time_elapsed         | 165         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.09928028  |
|    average_cost         | 0.018554688 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -30.3       |
|    cost_value_loss      | 0.319       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.348      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    mean_cost_advantages | 0.10804793  |
|    mean_reward_advan... | 0.028150799 |
|    n_updates            | 540         |
|    nu                   | 5.04        |
|    nu_loss              | -0.0927     |
|    policy_gradient_loss | -0.0106     |
|    reward_explained_... | 0.805       |
|    reward_value_loss    | 0.00379     |
|    total_cost           | 38.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 1           |
|    true_cost            | 0.000488    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0517      |
|    ep_len_mean          | 17.3        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 681         |
|    iterations           | 56          |
|    time_elapsed         | 168         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.03581331  |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.286       |
|    cost_value_loss      | 0.0144      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.402      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00886    |
|    mean_cost_advantages | -0.14441745 |
|    mean_reward_advan... | -0.14316118 |
|    n_updates            | 550         |
|    nu                   | 5.09        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00641    |
|    reward_explained_... | 0.407       |
|    reward_value_loss    | 0.00888     |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 1             |
|    true_cost            | 0.00977       |
| infos/                  |               |
|    cost                 | 0.0478        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0471       |
|    ep_len_mean          | 13.6          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 683           |
|    iterations           | 57            |
|    time_elapsed         | 170           |
|    total_timesteps      | 116736        |
| train/                  |               |
|    approx_kl            | 0.016807847   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.259         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.84         |
|    cost_value_loss      | 0.0126        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.382        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00247      |
|    mean_cost_advantages | -0.027659498  |
|    mean_reward_advan... | 0.030203238   |
|    n_updates            | 560           |
|    nu                   | 5.13          |
|    nu_loss              | -0.00248      |
|    policy_gradient_loss | -0.0059       |
|    reward_explained_... | 0.765         |
|    reward_value_loss    | 0.00897       |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 1           |
|    true_cost            | 0.000977    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.044       |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 683         |
|    iterations           | 58          |
|    time_elapsed         | 173         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.018445615 |
|    average_cost         | 0.009765625 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -55.8       |
|    cost_value_loss      | 0.196       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.366      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    mean_cost_advantages | 0.09198344  |
|    mean_reward_advan... | 0.060131215 |
|    n_updates            | 570         |
|    nu                   | 5.17        |
|    nu_loss              | -0.0501     |
|    policy_gradient_loss | -0.00174    |
|    reward_explained_... | 0.765       |
|    reward_value_loss    | 0.00776     |
|    total_cost           | 20.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00586      |
| infos/                  |              |
|    cost                 | 0.0243       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00733     |
|    ep_len_mean          | 15.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 685          |
|    iterations           | 59           |
|    time_elapsed         | 176          |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.023037508  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.268        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.28        |
|    cost_value_loss      | 0.025        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.388       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0465       |
|    mean_cost_advantages | -0.07271072  |
|    mean_reward_advan... | -0.041535523 |
|    n_updates            | 580          |
|    nu                   | 5.2          |
|    nu_loss              | -0.00505     |
|    policy_gradient_loss | -0.00508     |
|    reward_explained_... | 0.757        |
|    reward_value_loss    | 0.00826      |
|    total_cost           | 2.0          |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 12          |
|    mean_reward          | 0.6         |
|    true_cost            | 0.00879     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0345     |
|    ep_len_mean          | 12.4        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 684         |
|    iterations           | 60          |
|    time_elapsed         | 179         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.009520087 |
|    average_cost         | 0.005859375 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -29.5       |
|    cost_value_loss      | 0.123       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0771      |
|    mean_cost_advantages | 0.041193    |
|    mean_reward_advan... | 0.030207904 |
|    n_updates            | 590         |
|    nu                   | 5.24        |
|    nu_loss              | -0.0305     |
|    policy_gradient_loss | -0.000234   |
|    reward_explained_... | 0.746       |
|    reward_value_loss    | 0.00781     |
|    total_cost           | 12.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14           |
|    mean_reward          | 1            |
|    true_cost            | 0.00439      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0086       |
|    ep_len_mean          | 15.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 683          |
|    iterations           | 61           |
|    time_elapsed         | 182          |
|    total_timesteps      | 124928       |
| train/                  |              |
|    approx_kl            | 0.009032179  |
|    average_cost         | 0.0087890625 |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.2          |
|    cost_explained_va... | -28.8        |
|    cost_value_loss      | 0.18         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.368       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0918       |
|    mean_cost_advantages | 0.037704416  |
|    mean_reward_advan... | 0.036753517  |
|    n_updates            | 600          |
|    nu                   | 5.28         |
|    nu_loss              | -0.046       |
|    policy_gradient_loss | -0.00061     |
|    reward_explained_... | 0.72         |
|    reward_value_loss    | 0.00662      |
|    total_cost           | 18.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 12           |
|    mean_reward          | 1            |
|    true_cost            | 0.0103       |
| infos/                  |              |
|    cost                 | 0.0247       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0552      |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 682          |
|    iterations           | 62           |
|    time_elapsed         | 186          |
|    total_timesteps      | 126976       |
| train/                  |              |
|    approx_kl            | 0.0075297104 |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.0912       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.86        |
|    cost_value_loss      | 0.0957       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.365       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0216       |
|    mean_cost_advantages | -0.033735763 |
|    mean_reward_advan... | -0.032587133 |
|    n_updates            | 610          |
|    nu                   | 5.31         |
|    nu_loss              | -0.0232      |
|    policy_gradient_loss | -0.0016      |
|    reward_explained_... | 0.655        |
|    reward_value_loss    | 0.00671      |
|    total_cost           | 9.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 19.4        |
|    mean_reward          | 1           |
|    true_cost            | 0.00195     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0301      |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 685         |
|    iterations           | 63          |
|    time_elapsed         | 188         |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.020718578 |
|    average_cost         | 0.010253906 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -47         |
|    cost_value_loss      | 0.211       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.358      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0902      |
|    mean_cost_advantages | 0.06010969  |
|    mean_reward_advan... | 0.046200402 |
|    n_updates            | 620         |
|    nu                   | 5.35        |
|    nu_loss              | -0.0545     |
|    policy_gradient_loss | -0.00147    |
|    reward_explained_... | 0.766       |
|    reward_value_loss    | 0.00527     |
|    total_cost           | 21.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 10           |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00488      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.000229     |
|    ep_len_mean          | 15.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 686          |
|    iterations           | 64           |
|    time_elapsed         | 190          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.016298393  |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.172        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.88        |
|    cost_value_loss      | 0.0484       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.381       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0395       |
|    mean_cost_advantages | -0.07361526  |
|    mean_reward_advan... | -0.052603044 |
|    n_updates            | 630          |
|    nu                   | 5.38         |
|    nu_loss              | -0.0104      |
|    policy_gradient_loss | -0.00315     |
|    reward_explained_... | 0.681        |
|    reward_value_loss    | 0.0077       |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14           |
|    mean_reward          | 1            |
|    true_cost            | 0.0142       |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.111       |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 687          |
|    iterations           | 65           |
|    time_elapsed         | 193          |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.015556617  |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -18.3        |
|    cost_value_loss      | 0.11         |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.361       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0666       |
|    mean_cost_advantages | 0.013017816  |
|    mean_reward_advan... | 0.019428816  |
|    n_updates            | 640          |
|    nu                   | 5.42         |
|    nu_loss              | -0.0263      |
|    policy_gradient_loss | -0.000917    |
|    reward_explained_... | 0.667        |
|    reward_value_loss    | 0.00821      |
|    total_cost           | 10.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16          |
|    mean_reward          | 1           |
|    true_cost            | 0.00195     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0277      |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 690         |
|    iterations           | 66          |
|    time_elapsed         | 195         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.0165574   |
|    average_cost         | 0.014160156 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -39         |
|    cost_value_loss      | 0.297       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.367      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    mean_cost_advantages | 0.10112299  |
|    mean_reward_advan... | 0.06709026  |
|    n_updates            | 650         |
|    nu                   | 5.46        |
|    nu_loss              | -0.0767     |
|    policy_gradient_loss | -0.00571    |
|    reward_explained_... | 0.773       |
|    reward_value_loss    | 0.00604     |
|    total_cost           | 29.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.00439      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0049       |
|    ep_len_mean          | 15.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 692          |
|    iterations           | 67           |
|    time_elapsed         | 198          |
|    total_timesteps      | 137216       |
| train/                  |              |
|    approx_kl            | 0.018616825  |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.19         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.6         |
|    cost_value_loss      | 0.0502       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.394       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0388       |
|    mean_cost_advantages | -0.08249343  |
|    mean_reward_advan... | -0.069080845 |
|    n_updates            | 660          |
|    nu                   | 5.49         |
|    nu_loss              | -0.0107      |
|    policy_gradient_loss | -0.00201     |
|    reward_explained_... | 0.563        |
|    reward_value_loss    | 0.00835      |
|    total_cost           | 4.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14           |
|    mean_reward          | 1            |
|    true_cost            | 0.00732      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0274      |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 692          |
|    iterations           | 68           |
|    time_elapsed         | 201          |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0047301566 |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.0771       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -17          |
|    cost_value_loss      | 0.102        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.369       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00195      |
|    mean_cost_advantages | 0.010308791  |
|    mean_reward_advan... | 0.019490536  |
|    n_updates            | 670          |
|    nu                   | 5.52         |
|    nu_loss              | -0.0241      |
|    policy_gradient_loss | -0.000935    |
|    reward_explained_... | 0.653        |
|    reward_value_loss    | 0.0079       |
|    total_cost           | 9.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.00391      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.00703      |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 692          |
|    iterations           | 69           |
|    time_elapsed         | 204          |
|    total_timesteps      | 141312       |
| train/                  |              |
|    approx_kl            | 0.01029129   |
|    average_cost         | 0.0073242188 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -25.3        |
|    cost_value_loss      | 0.164        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.365       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.116        |
|    mean_cost_advantages | 0.034541063  |
|    mean_reward_advan... | 0.040945604  |
|    n_updates            | 680          |
|    nu                   | 5.56         |
|    nu_loss              | -0.0405      |
|    policy_gradient_loss | -0.000647    |
|    reward_explained_... | 0.813        |
|    reward_value_loss    | 0.00495      |
|    total_cost           | 15.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14           |
|    mean_reward          | 1            |
|    true_cost            | 0.0103       |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0723      |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 692          |
|    iterations           | 70           |
|    time_elapsed         | 207          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.009070437  |
|    average_cost         | 0.00390625   |
|    clip_fraction        | 0.0827       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.35        |
|    cost_value_loss      | 0.0931       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.365       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0873       |
|    mean_cost_advantages | -0.03078969  |
|    mean_reward_advan... | -0.031185273 |
|    n_updates            | 690          |
|    nu                   | 5.59         |
|    nu_loss              | -0.0217      |
|    policy_gradient_loss | -0.00107     |
|    reward_explained_... | 0.659        |
|    reward_value_loss    | 0.00763      |
|    total_cost           | 8.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 22.2        |
|    mean_reward          | 1           |
|    true_cost            | 0.00146     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0316      |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 695         |
|    iterations           | 71          |
|    time_elapsed         | 209         |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.039513357 |
|    average_cost         | 0.010253906 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -31.7       |
|    cost_value_loss      | 0.23        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.358      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.185       |
|    mean_cost_advantages | 0.06544208  |
|    mean_reward_advan... | 0.038091235 |
|    n_updates            | 700         |
|    nu                   | 5.63        |
|    nu_loss              | -0.0573     |
|    policy_gradient_loss | -0.00404    |
|    reward_explained_... | 0.719       |
|    reward_value_loss    | 0.00687     |
|    total_cost           | 21.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16           |
|    mean_reward          | 1            |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0.0263       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0237       |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 698          |
|    iterations           | 72           |
|    time_elapsed         | 211          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.01640088   |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.199        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.51        |
|    cost_value_loss      | 0.0415       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.384       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00754     |
|    mean_cost_advantages | -0.073873624 |
|    mean_reward_advan... | -0.08755526  |
|    n_updates            | 710          |
|    nu                   | 5.66         |
|    nu_loss              | -0.00824     |
|    policy_gradient_loss | -0.00329     |
|    reward_explained_... | 0.696        |
|    reward_value_loss    | 0.00831      |
|    total_cost           | 3.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0.0265        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0177       |
|    ep_len_mean          | 13.7          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 698           |
|    iterations           | 73            |
|    time_elapsed         | 214           |
|    total_timesteps      | 149504        |
| train/                  |               |
|    approx_kl            | 0.007862719   |
|    average_cost         | 0.0024414062  |
|    clip_fraction        | 0.143         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -10.5         |
|    cost_value_loss      | 0.062         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.372        |
|    learning_rate        | 0.0003        |
|    loss                 | -5.26e-05     |
|    mean_cost_advantages | -0.0062183365 |
|    mean_reward_advan... | 0.01838785    |
|    n_updates            | 720           |
|    nu                   | 5.69          |
|    nu_loss              | -0.0138       |
|    policy_gradient_loss | -0.00258      |
|    reward_explained_... | 0.738         |
|    reward_value_loss    | 0.0084        |
|    total_cost           | 5.0           |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 9.6         |
|    mean_reward          | 0.2         |
|    true_cost            | 0.0137      |
| infos/                  |             |
|    cost                 | 0.0797      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.126      |
|    ep_len_mean          | 12.3        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 697         |
|    iterations           | 74          |
|    time_elapsed         | 217         |
|    total_timesteps      | 151552      |
| train/                  |             |
|    approx_kl            | 0.008380228 |
|    average_cost         | 0.005859375 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -28.9       |
|    cost_value_loss      | 0.143       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.352      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.161       |
|    mean_cost_advantages | 0.0372859   |
|    mean_reward_advan... | 0.039250597 |
|    n_updates            | 730         |
|    nu                   | 5.72        |
|    nu_loss              | -0.0333     |
|    policy_gradient_loss | -0.000299   |
|    reward_explained_... | 0.614       |
|    reward_value_loss    | 0.00919     |
|    total_cost           | 12.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.13
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 30.8        |
|    mean_reward          | 1           |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0351      |
|    ep_len_mean          | 23.9        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 699         |
|    iterations           | 75          |
|    time_elapsed         | 219         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.13054204  |
|    average_cost         | 0.013671875 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -21.4       |
|    cost_value_loss      | 0.308       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.339      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    mean_cost_advantages | 0.09261262  |
|    mean_reward_advan... | 0.053791117 |
|    n_updates            | 740         |
|    nu                   | 5.76        |
|    nu_loss              | -0.0782     |
|    policy_gradient_loss | -0.0143     |
|    reward_explained_... | 0.799       |
|    reward_value_loss    | 0.00551     |
|    total_cost           | 28.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 13.2        |
|    mean_reward          | 1           |
|    true_cost            | 0.000977    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0399      |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 702         |
|    iterations           | 76          |
|    time_elapsed         | 221         |
|    total_timesteps      | 155648      |
| train/                  |             |
|    approx_kl            | 0.05057669  |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.237       |
|    cost_value_loss      | 0.0146      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.402      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00027    |
|    mean_cost_advantages | -0.11968987 |
|    mean_reward_advan... | -0.18872145 |
|    n_updates            | 750         |
|    nu                   | 5.79        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00557    |
|    reward_explained_... | 0.297       |
|    reward_value_loss    | 0.0137      |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00488      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00928     |
|    ep_len_mean          | 14.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 704          |
|    iterations           | 77           |
|    time_elapsed         | 223          |
|    total_timesteps      | 157696       |
| train/                  |              |
|    approx_kl            | 0.016993465  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.192        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.59        |
|    cost_value_loss      | 0.0284       |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.388       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00299      |
|    mean_cost_advantages | -0.028683994 |
|    mean_reward_advan... | 0.04411917   |
|    n_updates            | 760          |
|    nu                   | 5.82         |
|    nu_loss              | -0.00565     |
|    policy_gradient_loss | -0.00393     |
|    reward_explained_... | 0.778        |
|    reward_value_loss    | 0.0105       |
|    total_cost           | 2.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0.0272       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0127       |
|    ep_len_mean          | 14.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 704          |
|    iterations           | 78           |
|    time_elapsed         | 226          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0013468814 |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -21.4        |
|    cost_value_loss      | 0.124        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.367       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.114        |
|    mean_cost_advantages | 0.038764462  |
|    mean_reward_advan... | 0.06883675   |
|    n_updates            | 770          |
|    nu                   | 5.85         |
|    nu_loss              | -0.0284      |
|    policy_gradient_loss | 0.000178     |
|    reward_explained_... | 0.782        |
|    reward_value_loss    | 0.0078       |
|    total_cost           | 10.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00977       |
| infos/                  |               |
|    cost                 | 0.0546        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0793       |
|    ep_len_mean          | 13.2          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 704           |
|    iterations           | 79            |
|    time_elapsed         | 229           |
|    total_timesteps      | 161792        |
| train/                  |               |
|    approx_kl            | 0.00496742    |
|    average_cost         | 0.0034179688  |
|    clip_fraction        | 0.103         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -11.6         |
|    cost_value_loss      | 0.0886        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.366        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0297        |
|    mean_cost_advantages | -0.0075026597 |
|    mean_reward_advan... | 0.007332176   |
|    n_updates            | 780           |
|    nu                   | 5.88          |
|    nu_loss              | -0.02         |
|    policy_gradient_loss | -0.0017       |
|    reward_explained_... | 0.722         |
|    reward_value_loss    | 0.00727       |
|    total_cost           | 7.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 23          |
|    mean_reward          | 1           |
|    true_cost            | 0.000977    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0315      |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 706         |
|    iterations           | 80          |
|    time_elapsed         | 231         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.0707618   |
|    average_cost         | 0.009765625 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    cost_explained_va... | -24.7       |
|    cost_value_loss      | 0.24        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.356      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0779      |
|    mean_cost_advantages | 0.07242367  |
|    mean_reward_advan... | 0.033097126 |
|    n_updates            | 790         |
|    nu                   | 5.91        |
|    nu_loss              | -0.0574     |
|    policy_gradient_loss | -0.00546    |
|    reward_explained_... | 0.693       |
|    reward_value_loss    | 0.00676     |
|    total_cost           | 20.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0242       |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 705          |
|    iterations           | 81           |
|    time_elapsed         | 235          |
|    total_timesteps      | 165888       |
| train/                  |              |
|    approx_kl            | 0.011588585  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.209        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.26        |
|    cost_value_loss      | 0.0289       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.401       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00674     |
|    mean_cost_advantages | -0.077908404 |
|    mean_reward_advan... | -0.09873883  |
|    n_updates            | 800          |
|    nu                   | 5.94         |
|    nu_loss              | -0.00577     |
|    policy_gradient_loss | -0.0033      |
|    reward_explained_... | 0.722        |
|    reward_value_loss    | 0.00759      |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 13.6         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00488      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0153      |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 706          |
|    iterations           | 82           |
|    time_elapsed         | 237          |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 0.0157657    |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -10.9        |
|    cost_value_loss      | 0.054        |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.406       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.109        |
|    mean_cost_advantages | -0.015982706 |
|    mean_reward_advan... | 0.03152356   |
|    n_updates            | 810          |
|    nu                   | 5.96         |
|    nu_loss              | -0.0116      |
|    policy_gradient_loss | -0.00227     |
|    reward_explained_... | 0.732        |
|    reward_value_loss    | 0.00913      |
|    total_cost           | 4.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0455       |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 705          |
|    iterations           | 83           |
|    time_elapsed         | 240          |
|    total_timesteps      | 169984       |
| train/                  |              |
|    approx_kl            | 0.014849091  |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.0681       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -29.4        |
|    cost_value_loss      | 0.129        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.433       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0119       |
|    mean_cost_advantages | 0.03405362   |
|    mean_reward_advan... | 0.06202509   |
|    n_updates            | 820          |
|    nu                   | 5.99         |
|    nu_loss              | -0.0291      |
|    policy_gradient_loss | -0.000364    |
|    reward_explained_... | 0.74         |
|    reward_value_loss    | 0.00822      |
|    total_cost           | 10.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 1             |
|    true_cost            | 0.00537       |
| infos/                  |               |
|    cost                 | 0.0281        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0246       |
|    ep_len_mean          | 15.1          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 707           |
|    iterations           | 84            |
|    time_elapsed         | 243           |
|    total_timesteps      | 172032        |
| train/                  |               |
|    approx_kl            | 0.019775037   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.206         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.14         |
|    cost_value_loss      | 0.0177        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.452        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00154       |
|    mean_cost_advantages | -0.04281739   |
|    mean_reward_advan... | -0.034339648  |
|    n_updates            | 830           |
|    nu                   | 6.02          |
|    nu_loss              | -0.00293      |
|    policy_gradient_loss | -0.00335      |
|    reward_explained_... | 0.72          |
|    reward_value_loss    | 0.0103        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0132       |
|    ep_len_mean          | 18.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 709          |
|    iterations           | 85           |
|    time_elapsed         | 245          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.01922338   |
|    average_cost         | 0.0053710938 |
|    clip_fraction        | 0.237        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -22.7        |
|    cost_value_loss      | 0.147        |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.419       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0861       |
|    mean_cost_advantages | 0.05045286   |
|    mean_reward_advan... | 0.042688213  |
|    n_updates            | 840          |
|    nu                   | 6.04         |
|    nu_loss              | -0.0323      |
|    policy_gradient_loss | -0.00169     |
|    reward_explained_... | 0.725        |
|    reward_value_loss    | 0.00983      |
|    total_cost           | 11.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00537      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0306      |
|    ep_len_mean          | 16.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 709          |
|    iterations           | 86           |
|    time_elapsed         | 248          |
|    total_timesteps      | 176128       |
| train/                  |              |
|    approx_kl            | 0.00771824   |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0992       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.58        |
|    cost_value_loss      | 0.0694       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.434       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00164     |
|    mean_cost_advantages | -0.028163714 |
|    mean_reward_advan... | -0.036233686 |
|    n_updates            | 850          |
|    nu                   | 6.07         |
|    nu_loss              | -0.0148      |
|    policy_gradient_loss | -0.00144     |
|    reward_explained_... | 0.72         |
|    reward_value_loss    | 0.00854      |
|    total_cost           | 5.0          |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0324       |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 710          |
|    iterations           | 87           |
|    time_elapsed         | 250          |
|    total_timesteps      | 178176       |
| train/                  |              |
|    approx_kl            | 0.016414613  |
|    average_cost         | 0.0053710938 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -20.8        |
|    cost_value_loss      | 0.147        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.412       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.131        |
|    mean_cost_advantages | 0.030861469  |
|    mean_reward_advan... | 0.01577655   |
|    n_updates            | 860          |
|    nu                   | 6.09         |
|    nu_loss              | -0.0326      |
|    policy_gradient_loss | -0.00128     |
|    reward_explained_... | 0.65         |
|    reward_value_loss    | 0.0108       |
|    total_cost           | 11.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15           |
|    mean_reward          | 1            |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0.0286       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0259       |
|    ep_len_mean          | 16.7         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 88           |
|    time_elapsed         | 253          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.020282872  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.75        |
|    cost_value_loss      | 0.0314       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.426       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00225     |
|    mean_cost_advantages | -0.044032857 |
|    mean_reward_advan... | -0.04868693  |
|    n_updates            | 870          |
|    nu                   | 6.11         |
|    nu_loss              | -0.00595     |
|    policy_gradient_loss | -0.00314     |
|    reward_explained_... | 0.663        |
|    reward_value_loss    | 0.0113       |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.00879      |
| infos/                  |              |
|    cost                 | 0.0288       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.083       |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 713          |
|    iterations           | 89           |
|    time_elapsed         | 255          |
|    total_timesteps      | 182272       |
| train/                  |              |
|    approx_kl            | 0.015767824  |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -12          |
|    cost_value_loss      | 0.0584       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.421       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.104        |
|    mean_cost_advantages | 0.0012890715 |
|    mean_reward_advan... | 0.031046508  |
|    n_updates            | 880          |
|    nu                   | 6.14         |
|    nu_loss              | -0.0119      |
|    policy_gradient_loss | -0.00196     |
|    reward_explained_... | 0.738        |
|    reward_value_loss    | 0.0102       |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 25           |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0395       |
|    ep_len_mean          | 23.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 90           |
|    time_elapsed         | 258          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.056406863  |
|    average_cost         | 0.0087890625 |
|    clip_fraction        | 0.319        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -34.3        |
|    cost_value_loss      | 0.244        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.386       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0488       |
|    mean_cost_advantages | 0.089964435  |
|    mean_reward_advan... | 0.06292027   |
|    n_updates            | 890          |
|    nu                   | 6.16         |
|    nu_loss              | -0.0539      |
|    policy_gradient_loss | -0.0085      |
|    reward_explained_... | 0.768        |
|    reward_value_loss    | 0.00781      |
|    total_cost           | 18.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 1           |
|    true_cost            | 0.00342     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.00221    |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 714         |
|    iterations           | 91          |
|    time_elapsed         | 260         |
|    total_timesteps      | 186368      |
| train/                  |             |
|    approx_kl            | 0.04757684  |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.169       |
|    cost_value_loss      | 0.00616     |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.433      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00889    |
|    mean_cost_advantages | -0.08269931 |
|    mean_reward_advan... | -0.11780669 |
|    n_updates            | 900         |
|    nu                   | 6.19        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00614    |
|    reward_explained_... | 0.646       |
|    reward_value_loss    | 0.011       |
|    total_cost           | 0.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16           |
|    mean_reward          | 1            |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0341       |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 92           |
|    time_elapsed         | 264          |
|    total_timesteps      | 188416       |
| train/                  |              |
|    approx_kl            | 0.0006685003 |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -17.6        |
|    cost_value_loss      | 0.0998       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.405       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00336      |
|    mean_cost_advantages | 0.012488886  |
|    mean_reward_advan... | 0.062982805  |
|    n_updates            | 910          |
|    nu                   | 6.21         |
|    nu_loss              | -0.0211      |
|    policy_gradient_loss | 0.000293     |
|    reward_explained_... | 0.782        |
|    reward_value_loss    | 0.00994      |
|    total_cost           | 7.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 13.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.00635      |
| infos/                  |              |
|    cost                 | 0.0293       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0424      |
|    ep_len_mean          | 13.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 711          |
|    iterations           | 93           |
|    time_elapsed         | 267          |
|    total_timesteps      | 190464       |
| train/                  |              |
|    approx_kl            | 0.014135815  |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -8.78        |
|    cost_value_loss      | 0.0472       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.392       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00139     |
|    mean_cost_advantages | -0.019320082 |
|    mean_reward_advan... | 0.023063779  |
|    n_updates            | 920          |
|    nu                   | 6.23         |
|    nu_loss              | -0.00909     |
|    policy_gradient_loss | -0.00337     |
|    reward_explained_... | 0.765        |
|    reward_value_loss    | 0.00917      |
|    total_cost           | 3.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 13.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.00439      |
| infos/                  |              |
|    cost                 | 0.0294       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0207      |
|    ep_len_mean          | 16.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 713          |
|    iterations           | 94           |
|    time_elapsed         | 269          |
|    total_timesteps      | 192512       |
| train/                  |              |
|    approx_kl            | 0.0152542945 |
|    average_cost         | 0.0063476562 |
|    clip_fraction        | 0.218        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -29.3        |
|    cost_value_loss      | 0.184        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.379       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.161        |
|    mean_cost_advantages | 0.05497296   |
|    mean_reward_advan... | 0.055000134  |
|    n_updates            | 930          |
|    nu                   | 6.25         |
|    nu_loss              | -0.0395      |
|    policy_gradient_loss | -0.00149     |
|    reward_explained_... | 0.804        |
|    reward_value_loss    | 0.00603      |
|    total_cost           | 13.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0317       |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 711          |
|    iterations           | 95           |
|    time_elapsed         | 273          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.00945564   |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -9.83        |
|    cost_value_loss      | 0.129        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.374       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.116        |
|    mean_cost_advantages | -0.010482102 |
|    mean_reward_advan... | -0.03976313  |
|    n_updates            | 940          |
|    nu                   | 6.28         |
|    nu_loss              | -0.0275      |
|    policy_gradient_loss | -0.000704    |
|    reward_explained_... | 0.63         |
|    reward_value_loss    | 0.009        |
|    total_cost           | 9.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00537      |
| infos/                  |              |
|    cost                 | 0.0594       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0398      |
|    ep_len_mean          | 16.7         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 96           |
|    time_elapsed         | 275          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.018076107  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.166        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.87        |
|    cost_value_loss      | 0.0346       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.407       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.94e-06     |
|    mean_cost_advantages | -0.042171374 |
|    mean_reward_advan... | -0.04963358  |
|    n_updates            | 950          |
|    nu                   | 6.3          |
|    nu_loss              | -0.00613     |
|    policy_gradient_loss | -0.00239     |
|    reward_explained_... | 0.615        |
|    reward_value_loss    | 0.0102       |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0347       |
|    ep_len_mean          | 22.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 97           |
|    time_elapsed         | 278          |
|    total_timesteps      | 198656       |
| train/                  |              |
|    approx_kl            | 0.016224962  |
|    average_cost         | 0.0053710938 |
|    clip_fraction        | 0.258        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -24          |
|    cost_value_loss      | 0.16         |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.369       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0692       |
|    mean_cost_advantages | 0.043810125  |
|    mean_reward_advan... | 0.044708423  |
|    n_updates            | 960          |
|    nu                   | 6.32         |
|    nu_loss              | -0.0338      |
|    policy_gradient_loss | -0.00256     |
|    reward_explained_... | 0.688        |
|    reward_value_loss    | 0.0105       |
|    total_cost           | 11.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.00736       |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 98            |
|    time_elapsed         | 280           |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 0.030949533   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.194         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.81         |
|    cost_value_loss      | 0.0176        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.41         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00109       |
|    mean_cost_advantages | -0.047905296  |
|    mean_reward_advan... | -0.070092484  |
|    n_updates            | 970           |
|    nu                   | 6.34          |
|    nu_loss              | -0.00309      |
|    policy_gradient_loss | -0.00344      |
|    reward_explained_... | 0.671         |
|    reward_value_loss    | 0.0107        |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00439      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0186      |
|    ep_len_mean          | 15.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 99           |
|    time_elapsed         | 283          |
|    total_timesteps      | 202752       |
| train/                  |              |
|    approx_kl            | 0.006293322  |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.058        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -16          |
|    cost_value_loss      | 0.0904       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.379       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0534       |
|    mean_cost_advantages | 0.014006215  |
|    mean_reward_advan... | 0.07700226   |
|    n_updates            | 980          |
|    nu                   | 6.36         |
|    nu_loss              | -0.0186      |
|    policy_gradient_loss | -0.000943    |
|    reward_explained_... | 0.781        |
|    reward_value_loss    | 0.00879      |
|    total_cost           | 6.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0.0302       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0107      |
|    ep_len_mean          | 18.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 713          |
|    iterations           | 100          |
|    time_elapsed         | 286          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.00865894   |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -13.6        |
|    cost_value_loss      | 0.134        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.366       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0375       |
|    mean_cost_advantages | 0.023227803  |
|    mean_reward_advan... | 0.029991006  |
|    n_updates            | 990          |
|    nu                   | 6.38         |
|    nu_loss              | -0.028       |
|    policy_gradient_loss | -0.000517    |
|    reward_explained_... | 0.737        |
|    reward_value_loss    | 0.00862      |
|    total_cost           | 9.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 1             |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0297        |
|    ep_len_mean          | 17.2          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 713           |
|    iterations           | 101           |
|    time_elapsed         | 289           |
|    total_timesteps      | 206848        |
| train/                  |               |
|    approx_kl            | 0.0037386573  |
|    average_cost         | 0.0034179688  |
|    clip_fraction        | 0.0212        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -8.4          |
|    cost_value_loss      | 0.109         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.376        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0616        |
|    mean_cost_advantages | -0.0033913706 |
|    mean_reward_advan... | -0.042870685  |
|    n_updates            | 1000          |
|    nu                   | 6.41          |
|    nu_loss              | -0.0218       |
|    policy_gradient_loss | -0.000389     |
|    reward_explained_... | 0.658         |
|    reward_value_loss    | 0.0107        |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00538     |
|    ep_len_mean          | 16.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 102          |
|    time_elapsed         | 293          |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 0.005372393  |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.148        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.86        |
|    cost_value_loss      | 0.0471       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.363       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0278       |
|    mean_cost_advantages | -0.024339208 |
|    mean_reward_advan... | 0.0126024345 |
|    n_updates            | 1010         |
|    nu                   | 6.43         |
|    nu_loss              | -0.00938     |
|    policy_gradient_loss | -0.00274     |
|    reward_explained_... | 0.784        |
|    reward_value_loss    | 0.00847      |
|    total_cost           | 3.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00342       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.00337      |
|    ep_len_mean          | 15.5          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 712           |
|    iterations           | 103           |
|    time_elapsed         | 295           |
|    total_timesteps      | 210944        |
| train/                  |               |
|    approx_kl            | -0.0006415686 |
|    average_cost         | 0.0034179688  |
|    clip_fraction        | 0.00488       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -16.8         |
|    cost_value_loss      | 0.107         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.36         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.029         |
|    mean_cost_advantages | 0.021490995   |
|    mean_reward_advan... | 0.017977705   |
|    n_updates            | 1020          |
|    nu                   | 6.45          |
|    nu_loss              | -0.022        |
|    policy_gradient_loss | 0.000276      |
|    reward_explained_... | 0.765         |
|    reward_value_loss    | 0.00803       |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14           |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00391      |
| infos/                  |              |
|    cost                 | 0.0307       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00622     |
|    ep_len_mean          | 14.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 711          |
|    iterations           | 104          |
|    time_elapsed         | 299          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0036308116 |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -12          |
|    cost_value_loss      | 0.107        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.36        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.118        |
|    mean_cost_advantages | 0.0075971414 |
|    mean_reward_advan... | 0.024043987  |
|    n_updates            | 1030         |
|    nu                   | 6.47         |
|    nu_loss              | -0.022       |
|    policy_gradient_loss | -2.69e-05    |
|    reward_explained_... | 0.753        |
|    reward_value_loss    | 0.00799      |
|    total_cost           | 7.0          |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 14          |
|    mean_reward          | 1           |
|    true_cost            | 0.00732     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0721     |
|    ep_len_mean          | 13.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 711         |
|    iterations           | 105         |
|    time_elapsed         | 302         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.003949061 |
|    average_cost         | 0.00390625  |
|    clip_fraction        | 0.0132      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -10.4       |
|    cost_value_loss      | 0.122       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.347      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.072       |
|    mean_cost_advantages | 0.006865938 |
|    mean_reward_advan... | 0.039416507 |
|    n_updates            | 1040        |
|    nu                   | 6.49        |
|    nu_loss              | -0.0253     |
|    policy_gradient_loss | -0.000134   |
|    reward_explained_... | 0.789       |
|    reward_value_loss    | 0.00633     |
|    total_cost           | 8.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0414       |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 106          |
|    time_elapsed         | 304          |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 0.03152795   |
|    average_cost         | 0.0073242188 |
|    clip_fraction        | 0.299        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -10.6        |
|    cost_value_loss      | 0.223        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.356       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0986       |
|    mean_cost_advantages | 0.044161543  |
|    mean_reward_advan... | 0.019812807  |
|    n_updates            | 1050         |
|    nu                   | 6.51         |
|    nu_loss              | -0.0475      |
|    policy_gradient_loss | -0.00467     |
|    reward_explained_... | 0.75         |
|    reward_value_loss    | 0.00625      |
|    total_cost           | 15.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 1             |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0124        |
|    ep_len_mean          | 16.6          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 107           |
|    time_elapsed         | 306           |
|    total_timesteps      | 219136        |
| train/                  |               |
|    approx_kl            | 0.018580392   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.217         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.477        |
|    cost_value_loss      | 0.0219        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.379        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00232      |
|    mean_cost_advantages | -0.07391238   |
|    mean_reward_advan... | -0.08963473   |
|    n_updates            | 1060          |
|    nu                   | 6.53          |
|    nu_loss              | -0.00318      |
|    policy_gradient_loss | -0.00361      |
|    reward_explained_... | 0.614         |
|    reward_value_loss    | 0.00996       |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.0083       |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0938      |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 108          |
|    time_elapsed         | 309          |
|    total_timesteps      | 221184       |
| train/                  |              |
|    approx_kl            | 0.010885068  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -9.81        |
|    cost_value_loss      | 0.0803       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.364       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00214     |
|    mean_cost_advantages | 0.0073102787 |
|    mean_reward_advan... | 0.01985901   |
|    n_updates            | 1070         |
|    nu                   | 6.55         |
|    nu_loss              | -0.016       |
|    policy_gradient_loss | -0.00155     |
|    reward_explained_... | 0.673        |
|    reward_value_loss    | 0.0101       |
|    total_cost           | 5.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 23          |
|    mean_reward          | 1           |
|    true_cost            | 0.000977    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0321      |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 715         |
|    iterations           | 109         |
|    time_elapsed         | 311         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.021865029 |
|    average_cost         | 0.008300781 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -13.6       |
|    cost_value_loss      | 0.261       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.356      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0915      |
|    mean_cost_advantages | 0.07739798  |
|    mean_reward_advan... | 0.05974653  |
|    n_updates            | 1080        |
|    nu                   | 6.58        |
|    nu_loss              | -0.0544     |
|    policy_gradient_loss | -0.0026     |
|    reward_explained_... | 0.752       |
|    reward_value_loss    | 0.00818     |
|    total_cost           | 17.0        |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00391      |
| infos/                  |              |
|    cost                 | 0.0315       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0152      |
|    ep_len_mean          | 15.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 110          |
|    time_elapsed         | 314          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.016063344  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.189        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.81        |
|    cost_value_loss      | 0.0366       |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.368       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00159     |
|    mean_cost_advantages | -0.0481229   |
|    mean_reward_advan... | -0.07146807  |
|    n_updates            | 1090         |
|    nu                   | 6.6          |
|    nu_loss              | -0.00642     |
|    policy_gradient_loss | -0.00276     |
|    reward_explained_... | 0.563        |
|    reward_value_loss    | 0.0089       |
|    total_cost           | 2.0          |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 18.8        |
|    mean_reward          | 1           |
|    true_cost            | 0.000488    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0435      |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 716         |
|    iterations           | 111         |
|    time_elapsed         | 317         |
|    total_timesteps      | 227328      |
| train/                  |             |
|    approx_kl            | 0.012156697 |
|    average_cost         | 0.00390625  |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -13.7       |
|    cost_value_loss      | 0.127       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.346      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.016       |
|    mean_cost_advantages | 0.018073995 |
|    mean_reward_advan... | 0.034328282 |
|    n_updates            | 1100        |
|    nu                   | 6.62        |
|    nu_loss              | -0.0258     |
|    policy_gradient_loss | -0.000225   |
|    reward_explained_... | 0.742       |
|    reward_value_loss    | 0.00864     |
|    total_cost           | 8.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 0.6           |
|    true_cost            | 0.00635       |
| infos/                  |               |
|    cost                 | 0.0317        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0656       |
|    ep_len_mean          | 14.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 717           |
|    iterations           | 112           |
|    time_elapsed         | 319           |
|    total_timesteps      | 229376        |
| train/                  |               |
|    approx_kl            | 0.016918577   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.203         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.02         |
|    cost_value_loss      | 0.0188        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.365        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00192       |
|    mean_cost_advantages | -0.040089533  |
|    mean_reward_advan... | -0.05817778   |
|    n_updates            | 1110          |
|    nu                   | 6.65          |
|    nu_loss              | -0.00323      |
|    policy_gradient_loss | -0.00338      |
|    reward_explained_... | 0.731         |
|    reward_value_loss    | 0.00907       |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0405       |
|    ep_len_mean          | 23           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 719          |
|    iterations           | 113          |
|    time_elapsed         | 321          |
|    total_timesteps      | 231424       |
| train/                  |              |
|    approx_kl            | 0.04456444   |
|    average_cost         | 0.0063476562 |
|    clip_fraction        | 0.301        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -14          |
|    cost_value_loss      | 0.207        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.33        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0285       |
|    mean_cost_advantages | 0.06270094   |
|    mean_reward_advan... | 0.022184161  |
|    n_updates            | 1120         |
|    nu                   | 6.67         |
|    nu_loss              | -0.0422      |
|    policy_gradient_loss | -0.00495     |
|    reward_explained_... | 0.733        |
|    reward_value_loss    | 0.00787      |
|    total_cost           | 13.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0154       |
|    ep_len_mean          | 17.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 720          |
|    iterations           | 114          |
|    time_elapsed         | 323          |
|    total_timesteps      | 233472       |
| train/                  |              |
|    approx_kl            | 0.03790582   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.271        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.132        |
|    cost_value_loss      | 0.00296      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.389       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000333     |
|    mean_cost_advantages | -0.056378953 |
|    mean_reward_advan... | -0.11918566  |
|    n_updates            | 1130         |
|    nu                   | 6.69         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00485     |
|    reward_explained_... | 0.508        |
|    reward_value_loss    | 0.011        |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.031        |
|    ep_len_mean          | 15.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 720          |
|    iterations           | 115          |
|    time_elapsed         | 326          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0073708314 |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.0914       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -12.9        |
|    cost_value_loss      | 0.0672       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.355       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00122      |
|    mean_cost_advantages | 0.0026772534 |
|    mean_reward_advan... | 0.06299605   |
|    n_updates            | 1140         |
|    nu                   | 6.71         |
|    nu_loss              | -0.0131      |
|    policy_gradient_loss | -0.0011      |
|    reward_explained_... | 0.729        |
|    reward_value_loss    | 0.0119       |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 1             |
|    true_cost            | 0.00391       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0093       |
|    ep_len_mean          | 13.4          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 722           |
|    iterations           | 116           |
|    time_elapsed         | 328           |
|    total_timesteps      | 237568        |
| train/                  |               |
|    approx_kl            | 0.017330976   |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0.134         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -8.52         |
|    cost_value_loss      | 0.0517        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.348        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000258      |
|    mean_cost_advantages | -0.0070873257 |
|    mean_reward_advan... | 0.034829255   |
|    n_updates            | 1150          |
|    nu                   | 6.73          |
|    nu_loss              | -0.00983      |
|    policy_gradient_loss | -0.00191      |
|    reward_explained_... | 0.758         |
|    reward_value_loss    | 0.00986       |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0.0647       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0238       |
|    ep_len_mean          | 14.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 722          |
|    iterations           | 117          |
|    time_elapsed         | 331          |
|    total_timesteps      | 239616       |
| train/                  |              |
|    approx_kl            | 0.0068938998 |
|    average_cost         | 0.00390625   |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -12.2        |
|    cost_value_loss      | 0.132        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.342       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0596       |
|    mean_cost_advantages | 0.031918757  |
|    mean_reward_advan... | 0.06943761   |
|    n_updates            | 1160         |
|    nu                   | 6.75         |
|    nu_loss              | -0.0263      |
|    policy_gradient_loss | -9.44e-05    |
|    reward_explained_... | 0.817        |
|    reward_value_loss    | 0.0056       |
|    total_cost           | 8.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.0112       |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.169       |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 722          |
|    iterations           | 118          |
|    time_elapsed         | 334          |
|    total_timesteps      | 241664       |
| train/                  |              |
|    approx_kl            | 0.006832393  |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.22        |
|    cost_value_loss      | 0.0686       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.345       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00154      |
|    mean_cost_advantages | -0.015427144 |
|    mean_reward_advan... | -0.020239986 |
|    n_updates            | 1170         |
|    nu                   | 6.77         |
|    nu_loss              | -0.0132      |
|    policy_gradient_loss | -0.00202     |
|    reward_explained_... | 0.682        |
|    reward_value_loss    | 0.00712      |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 26.8        |
|    mean_reward          | 1           |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0468      |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 723         |
|    iterations           | 119         |
|    time_elapsed         | 336         |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.027933724 |
|    average_cost         | 0.011230469 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -10.8       |
|    cost_value_loss      | 0.373       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.341      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0935      |
|    mean_cost_advantages | 0.11354815  |
|    mean_reward_advan... | 0.024994522 |
|    n_updates            | 1180        |
|    nu                   | 6.79        |
|    nu_loss              | -0.076      |
|    policy_gradient_loss | -0.00952    |
|    reward_explained_... | 0.711       |
|    reward_value_loss    | 0.00669     |
|    total_cost           | 23.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 0.6         |
|    true_cost            | 0.00342     |
| infos/                  |             |
|    cost                 | 0.0327      |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0125     |
|    ep_len_mean          | 16          |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 725         |
|    iterations           | 120         |
|    time_elapsed         | 338         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.036450706 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.211       |
|    cost_value_loss      | 0.00452     |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.376      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00458    |
|    mean_cost_advantages | -0.07350061 |
|    mean_reward_advan... | -0.11088676 |
|    n_updates            | 1190        |
|    nu                   | 6.82        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00393    |
|    reward_explained_... | 0.378       |
|    reward_value_loss    | 0.011       |
|    total_cost           | 0.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14           |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0194      |
|    ep_len_mean          | 17.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 725          |
|    iterations           | 121          |
|    time_elapsed         | 341          |
|    total_timesteps      | 247808       |
| train/                  |              |
|    approx_kl            | 0.004636161  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.056        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -11.5        |
|    cost_value_loss      | 0.12         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.347       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0137       |
|    mean_cost_advantages | 0.018307505  |
|    mean_reward_advan... | 0.045420855  |
|    n_updates            | 1200         |
|    nu                   | 6.84         |
|    nu_loss              | -0.0233      |
|    policy_gradient_loss | -0.000173    |
|    reward_explained_... | 0.755        |
|    reward_value_loss    | 0.00986      |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0434       |
|    ep_len_mean          | 22           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 725          |
|    iterations           | 122          |
|    time_elapsed         | 344          |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 0.016442344  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.49        |
|    cost_value_loss      | 0.12         |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.332       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0184       |
|    mean_cost_advantages | 0.005863835  |
|    mean_reward_advan... | -0.026249241 |
|    n_updates            | 1210         |
|    nu                   | 6.86         |
|    nu_loss              | -0.0234      |
|    policy_gradient_loss | -0.000806    |
|    reward_explained_... | 0.669        |
|    reward_value_loss    | 0.0107       |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0172       |
|    ep_len_mean          | 16.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 726          |
|    iterations           | 123          |
|    time_elapsed         | 346          |
|    total_timesteps      | 251904       |
| train/                  |              |
|    approx_kl            | 0.019482374  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.154        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.148        |
|    cost_value_loss      | 0.00143      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.377       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000497     |
|    mean_cost_advantages | -0.036351524 |
|    mean_reward_advan... | -0.06537157  |
|    n_updates            | 1220         |
|    nu                   | 6.88         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00421     |
|    reward_explained_... | 0.698        |
|    reward_value_loss    | 0.0129       |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 12           |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00488      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0402      |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 726          |
|    iterations           | 124          |
|    time_elapsed         | 349          |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.008821084  |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.0749       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -10.3        |
|    cost_value_loss      | 0.0707       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.353       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0174       |
|    mean_cost_advantages | 0.0021781637 |
|    mean_reward_advan... | 0.07097284   |
|    n_updates            | 1230         |
|    nu                   | 6.9          |
|    nu_loss              | -0.0134      |
|    policy_gradient_loss | -0.00109     |
|    reward_explained_... | 0.783        |
|    reward_value_loss    | 0.0101       |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0332       |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 727          |
|    iterations           | 125          |
|    time_elapsed         | 351          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.019308649  |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.196        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -9.89        |
|    cost_value_loss      | 0.172        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.344       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0836       |
|    mean_cost_advantages | 0.038559876  |
|    mean_reward_advan... | 0.055788487  |
|    n_updates            | 1240         |
|    nu                   | 6.92         |
|    nu_loss              | -0.0337      |
|    policy_gradient_loss | -0.0015      |
|    reward_explained_... | 0.753        |
|    reward_value_loss    | 0.00808      |
|    total_cost           | 10.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0178       |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 726          |
|    iterations           | 126          |
|    time_elapsed         | 354          |
|    total_timesteps      | 258048       |
| train/                  |              |
|    approx_kl            | 0.012109768  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.196        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.64        |
|    cost_value_loss      | 0.0366       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.363       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00912      |
|    mean_cost_advantages | -0.03732805  |
|    mean_reward_advan... | -0.049636222 |
|    n_updates            | 1250         |
|    nu                   | 6.94         |
|    nu_loss              | -0.00676     |
|    policy_gradient_loss | -0.00313     |
|    reward_explained_... | 0.69         |
|    reward_value_loss    | 0.0097       |
|    total_cost           | 2.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 10           |
|    mean_reward          | 0.2          |
|    true_cost            | 0.00781      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.107       |
|    ep_len_mean          | 13.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 726          |
|    iterations           | 127          |
|    time_elapsed         | 357          |
|    total_timesteps      | 260096       |
| train/                  |              |
|    approx_kl            | 0.012275881  |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -9.2         |
|    cost_value_loss      | 0.0718       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.347       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00309      |
|    mean_cost_advantages | 0.0036919015 |
|    mean_reward_advan... | 0.015558929  |
|    n_updates            | 1260         |
|    nu                   | 6.96         |
|    nu_loss              | -0.0136      |
|    policy_gradient_loss | -0.00152     |
|    reward_explained_... | 0.703        |
|    reward_value_loss    | 0.00842      |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 23.8        |
|    mean_reward          | 1           |
|    true_cost            | 0.000488    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0325      |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 728         |
|    iterations           | 128         |
|    time_elapsed         | 359         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.05953374  |
|    average_cost         | 0.0078125   |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -9.48       |
|    cost_value_loss      | 0.275       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.338      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0787      |
|    mean_cost_advantages | 0.08155678  |
|    mean_reward_advan... | 0.057769295 |
|    n_updates            | 1270        |
|    nu                   | 6.99        |
|    nu_loss              | -0.0544     |
|    policy_gradient_loss | -0.00977    |
|    reward_explained_... | 0.747       |
|    reward_value_loss    | 0.00725     |
|    total_cost           | 16.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.025         |
|    ep_len_mean          | 16.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 729           |
|    iterations           | 129           |
|    time_elapsed         | 362           |
|    total_timesteps      | 264192        |
| train/                  |               |
|    approx_kl            | 0.018799998   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.177         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.737        |
|    cost_value_loss      | 0.0215        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.37         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00423      |
|    mean_cost_advantages | -0.06522673   |
|    mean_reward_advan... | -0.11763488   |
|    n_updates            | 1280          |
|    nu                   | 7.01          |
|    nu_loss              | -0.00341      |
|    policy_gradient_loss | -0.0025       |
|    reward_explained_... | 0.588         |
|    reward_value_loss    | 0.011         |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 0.2           |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.00761       |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 130           |
|    time_elapsed         | 364           |
|    total_timesteps      | 266240        |
| train/                  |               |
|    approx_kl            | 0.015232998   |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0.11          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -7.41         |
|    cost_value_loss      | 0.0551        |
|    early_stop_epoch     | 5             |
|    entropy_loss         | -0.358        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0005       |
|    mean_cost_advantages | -0.0060797525 |
|    mean_reward_advan... | 0.049146295   |
|    n_updates            | 1290          |
|    nu                   | 7.03          |
|    nu_loss              | -0.0103       |
|    policy_gradient_loss | -0.00173      |
|    reward_explained_... | 0.793         |
|    reward_value_loss    | 0.00916       |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 12           |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00293      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.000983    |
|    ep_len_mean          | 14.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 131          |
|    time_elapsed         | 367          |
|    total_timesteps      | 268288       |
| train/                  |              |
|    approx_kl            | 0.0040173247 |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0704       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -9.07        |
|    cost_value_loss      | 0.0912       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.351       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00519      |
|    mean_cost_advantages | 0.0132461125 |
|    mean_reward_advan... | 0.045787215  |
|    n_updates            | 1300         |
|    nu                   | 7.05         |
|    nu_loss              | -0.0172      |
|    policy_gradient_loss | -0.000749    |
|    reward_explained_... | 0.717        |
|    reward_value_loss    | 0.0104       |
|    total_cost           | 5.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 8.8          |
|    mean_reward          | 0.2          |
|    true_cost            | 0.0161       |
| infos/                  |              |
|    cost                 | 0.034        |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.302       |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 729          |
|    iterations           | 132          |
|    time_elapsed         | 370          |
|    total_timesteps      | 270336       |
| train/                  |              |
|    approx_kl            | 0.01365892   |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -8.67        |
|    cost_value_loss      | 0.109        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.347       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.184        |
|    mean_cost_advantages | 0.010732062  |
|    mean_reward_advan... | 0.014396554  |
|    n_updates            | 1310         |
|    nu                   | 7.07         |
|    nu_loss              | -0.0206      |
|    policy_gradient_loss | -0.00076     |
|    reward_explained_... | 0.639        |
|    reward_value_loss    | 0.00996      |
|    total_cost           | 6.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 21          |
|    mean_reward          | 1           |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0493      |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 133         |
|    time_elapsed         | 372         |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.02853595  |
|    average_cost         | 0.016113281 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -7.93       |
|    cost_value_loss      | 0.556       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.347      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.291       |
|    mean_cost_advantages | 0.17438442  |
|    mean_reward_advan... | 0.062940314 |
|    n_updates            | 1320        |
|    nu                   | 7.1         |
|    nu_loss              | -0.114      |
|    policy_gradient_loss | -0.0233     |
|    reward_explained_... | 0.788       |
|    reward_value_loss    | 0.00638     |
|    total_cost           | 33.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 11.6        |
|    mean_reward          | 1           |
|    true_cost            | 0.00293     |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.00479    |
|    ep_len_mean          | 15.1        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 134         |
|    time_elapsed         | 375         |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.026308514 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.222       |
|    cost_value_loss      | 0.0117      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.371      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00616    |
|    mean_cost_advantages | -0.11725779 |
|    mean_reward_advan... | -0.11890428 |
|    n_updates            | 1330        |
|    nu                   | 7.12        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00188    |
|    reward_explained_... | 0.545       |
|    reward_value_loss    | 0.00977     |
|    total_cost           | 0.0         |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 16            |
|    mean_reward          | 0.6           |
|    true_cost            | 0.00342       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0157       |
|    ep_len_mean          | 14.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 135           |
|    time_elapsed         | 378           |
|    total_timesteps      | 276480        |
| train/                  |               |
|    approx_kl            | 0.00051815197 |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.00459       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -7.99         |
|    cost_value_loss      | 0.111         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.37         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0175        |
|    mean_cost_advantages | 0.017705882   |
|    mean_reward_advan... | 0.04187658    |
|    n_updates            | 1340          |
|    nu                   | 7.15          |
|    nu_loss              | -0.0209       |
|    policy_gradient_loss | 0.000204      |
|    reward_explained_... | 0.766         |
|    reward_value_loss    | 0.00783       |
|    total_cost           | 6.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 0.6          |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0.0344       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0227      |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 729          |
|    iterations           | 136          |
|    time_elapsed         | 381          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.013001582  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.03        |
|    cost_value_loss      | 0.129        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.375       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0874       |
|    mean_cost_advantages | 0.014435474  |
|    mean_reward_advan... | 0.015865833  |
|    n_updates            | 1350         |
|    nu                   | 7.17         |
|    nu_loss              | -0.0244      |
|    policy_gradient_loss | -0.000475    |
|    reward_explained_... | 0.762        |
|    reward_value_loss    | 0.00735      |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0227       |
|    ep_len_mean          | 21.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 137          |
|    time_elapsed         | 384          |
|    total_timesteps      | 280576       |
| train/                  |              |
|    approx_kl            | 0.01819764   |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.0793       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.04        |
|    cost_value_loss      | 0.13         |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.402       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0134       |
|    mean_cost_advantages | 0.011324681  |
|    mean_reward_advan... | -0.015683876 |
|    n_updates            | 1360         |
|    nu                   | 7.2          |
|    nu_loss              | -0.0245      |
|    policy_gradient_loss | -0.000491    |
|    reward_explained_... | 0.754        |
|    reward_value_loss    | 0.00836      |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 23           |
|    mean_reward          | 1            |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0171       |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 138          |
|    time_elapsed         | 386          |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.018554464  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.18        |
|    cost_value_loss      | 0.0398       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.424       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000968     |
|    mean_cost_advantages | -0.0330118   |
|    mean_reward_advan... | -0.0799654   |
|    n_updates            | 1370         |
|    nu                   | 7.22         |
|    nu_loss              | -0.00703     |
|    policy_gradient_loss | -0.000925    |
|    reward_explained_... | 0.701        |
|    reward_value_loss    | 0.0117       |
|    total_cost           | 2.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 0.6           |
|    true_cost            | 0.00342       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0234       |
|    ep_len_mean          | 16.2          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 139           |
|    time_elapsed         | 389           |
|    total_timesteps      | 284672        |
| train/                  |               |
|    approx_kl            | 0.008296929   |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0.122         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.03         |
|    cost_value_loss      | 0.0574        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.418        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0927        |
|    mean_cost_advantages | -0.0119869895 |
|    mean_reward_advan... | -0.0037760783 |
|    n_updates            | 1380          |
|    nu                   | 7.25          |
|    nu_loss              | -0.0106       |
|    policy_gradient_loss | -0.00175      |
|    reward_explained_... | 0.695         |
|    reward_value_loss    | 0.0122        |
|    total_cost           | 3.0           |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0527       |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 140          |
|    time_elapsed         | 392          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.018240448  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.296        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -8.02        |
|    cost_value_loss      | 0.138        |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.353       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.149        |
|    mean_cost_advantages | 0.02904461   |
|    mean_reward_advan... | 0.038056202  |
|    n_updates            | 1390         |
|    nu                   | 7.27         |
|    nu_loss              | -0.0248      |
|    policy_gradient_loss | -0.00232     |
|    reward_explained_... | 0.744        |
|    reward_value_loss    | 0.0092       |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 9.6          |
|    mean_reward          | 0.2          |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0186      |
|    ep_len_mean          | 14.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 141          |
|    time_elapsed         | 394          |
|    total_timesteps      | 288768       |
| train/                  |              |
|    approx_kl            | 0.025897231  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.284        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.252        |
|    cost_value_loss      | 0.00106      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.379       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00241      |
|    mean_cost_advantages | -0.03760022  |
|    mean_reward_advan... | -0.025760837 |
|    n_updates            | 1400         |
|    nu                   | 7.29         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00475     |
|    reward_explained_... | 0.627        |
|    reward_value_loss    | 0.012        |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0402       |
|    ep_len_mean          | 18.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 142          |
|    time_elapsed         | 397          |
|    total_timesteps      | 290816       |
| train/                  |              |
|    approx_kl            | 0.009613048  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.65        |
|    cost_value_loss      | 0.134        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.353       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0209       |
|    mean_cost_advantages | 0.03228236   |
|    mean_reward_advan... | 0.06482074   |
|    n_updates            | 1410         |
|    nu                   | 7.31         |
|    nu_loss              | -0.0249      |
|    policy_gradient_loss | -0.000794    |
|    reward_explained_... | 0.741        |
|    reward_value_loss    | 0.00996      |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 1             |
|    true_cost            | 0.00537       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0708       |
|    ep_len_mean          | 14.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 731           |
|    iterations           | 143           |
|    time_elapsed         | 400           |
|    total_timesteps      | 292864        |
| train/                  |               |
|    approx_kl            | 0.021915328   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.176         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.17         |
|    cost_value_loss      | 0.0205        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.373        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000268     |
|    mean_cost_advantages | -0.033902682  |
|    mean_reward_advan... | -0.049220763  |
|    n_updates            | 1420          |
|    nu                   | 7.33          |
|    nu_loss              | -0.00357      |
|    policy_gradient_loss | -0.00306      |
|    reward_explained_... | 0.693         |
|    reward_value_loss    | 0.0103        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 24.8         |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.042        |
|    ep_len_mean          | 22.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 732          |
|    iterations           | 144          |
|    time_elapsed         | 402          |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.046679556  |
|    average_cost         | 0.0053710938 |
|    clip_fraction        | 0.268        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.21        |
|    cost_value_loss      | 0.211        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.334       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.082        |
|    mean_cost_advantages | 0.057594255  |
|    mean_reward_advan... | 0.03528303   |
|    n_updates            | 1430         |
|    nu                   | 7.35         |
|    nu_loss              | -0.0394      |
|    policy_gradient_loss | -0.00617     |
|    reward_explained_... | 0.774        |
|    reward_value_loss    | 0.00808      |
|    total_cost           | 11.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0312       |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 733          |
|    iterations           | 145          |
|    time_elapsed         | 404          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.02381963   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.23         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.234        |
|    cost_value_loss      | 0.00128      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.377       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000395     |
|    mean_cost_advantages | -0.04771245  |
|    mean_reward_advan... | -0.108982466 |
|    n_updates            | 1440         |
|    nu                   | 7.37         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0038      |
|    reward_explained_... | 0.628        |
|    reward_value_loss    | 0.0112       |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 0.6           |
|    true_cost            | 0.00342       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0221       |
|    ep_len_mean          | 14.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 732           |
|    iterations           | 146           |
|    time_elapsed         | 408           |
|    total_timesteps      | 299008        |
| train/                  |               |
|    approx_kl            | 0.0076144715  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.146         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.74         |
|    cost_value_loss      | 0.0401        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.368        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0685        |
|    mean_cost_advantages | -0.0070340354 |
|    mean_reward_advan... | 0.044947237   |
|    n_updates            | 1450          |
|    nu                   | 7.39          |
|    nu_loss              | -0.0072       |
|    policy_gradient_loss | -0.00218      |
|    reward_explained_... | 0.747         |
|    reward_value_loss    | 0.0117        |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18           |
|    mean_reward          | 1            |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0047      |
|    ep_len_mean          | 16.7         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 147          |
|    time_elapsed         | 411          |
|    total_timesteps      | 301056       |
| train/                  |              |
|    approx_kl            | 0.007058703  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.216        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.84        |
|    cost_value_loss      | 0.138        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.351       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.248        |
|    mean_cost_advantages | 0.030468421  |
|    mean_reward_advan... | 0.056486227  |
|    n_updates            | 1460         |
|    nu                   | 7.41         |
|    nu_loss              | -0.0253      |
|    policy_gradient_loss | -0.0012      |
|    reward_explained_... | 0.769        |
|    reward_value_loss    | 0.00904      |
|    total_cost           | 7.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0.0357       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0212       |
|    ep_len_mean          | 16.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 148          |
|    time_elapsed         | 414          |
|    total_timesteps      | 303104       |
| train/                  |              |
|    approx_kl            | 0.0017082497 |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.82        |
|    cost_value_loss      | 0.0997       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.351       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.096        |
|    mean_cost_advantages | -0.002360867 |
|    mean_reward_advan... | -0.019343812 |
|    n_updates            | 1470         |
|    nu                   | 7.43         |
|    nu_loss              | -0.0181      |
|    policy_gradient_loss | 0.000229     |
|    reward_explained_... | 0.72         |
|    reward_value_loss    | 0.00935      |
|    total_cost           | 5.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.000476     |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 729          |
|    iterations           | 149          |
|    time_elapsed         | 418          |
|    total_timesteps      | 305152       |
| train/                  |              |
|    approx_kl            | 0.006501683  |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.79        |
|    cost_value_loss      | 0.061        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.364       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00212      |
|    mean_cost_advantages | -0.009974708 |
|    mean_reward_advan... | -0.011818248 |
|    n_updates            | 1480         |
|    nu                   | 7.44         |
|    nu_loss              | -0.0109      |
|    policy_gradient_loss | -0.00109     |
|    reward_explained_... | 0.658        |
|    reward_value_loss    | 0.0106       |
|    total_cost           | 3.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00293      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00612     |
|    ep_len_mean          | 14.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 729          |
|    iterations           | 150          |
|    time_elapsed         | 421          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.004912252  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.05        |
|    cost_value_loss      | 0.101        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.356       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00314      |
|    mean_cost_advantages | 0.012911517  |
|    mean_reward_advan... | 0.03136866   |
|    n_updates            | 1490         |
|    nu                   | 7.46         |
|    nu_loss              | -0.0182      |
|    policy_gradient_loss | 0.000133     |
|    reward_explained_... | 0.602        |
|    reward_value_loss    | 0.00979      |
|    total_cost           | 5.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 1             |
|    true_cost            | 0.0083        |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.148        |
|    ep_len_mean          | 13            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 728           |
|    iterations           | 151           |
|    time_elapsed         | 424           |
|    total_timesteps      | 309248        |
| train/                  |               |
|    approx_kl            | 0.00045331323 |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.89         |
|    cost_value_loss      | 0.121         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.347        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.152         |
|    mean_cost_advantages | 0.010209704   |
|    mean_reward_advan... | 0.04358957    |
|    n_updates            | 1500          |
|    nu                   | 7.48          |
|    nu_loss              | -0.0219       |
|    policy_gradient_loss | 0.000355      |
|    reward_explained_... | 0.778         |
|    reward_value_loss    | 0.00642       |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 22          |
|    mean_reward          | 1           |
|    true_cost            | 0.000977    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.018       |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 729         |
|    iterations           | 152         |
|    time_elapsed         | 426         |
|    total_timesteps      | 311296      |
| train/                  |             |
|    approx_kl            | 0.021947991 |
|    average_cost         | 0.008300781 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.3        |
|    cost_value_loss      | 0.334       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.341      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0829      |
|    mean_cost_advantages | 0.07632598  |
|    mean_reward_advan... | 0.015770853 |
|    n_updates            | 1510        |
|    nu                   | 7.5         |
|    nu_loss              | -0.0621     |
|    policy_gradient_loss | -0.00925    |
|    reward_explained_... | 0.773       |
|    reward_value_loss    | 0.00588     |
|    total_cost           | 17.0        |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.00392     |
|    ep_len_mean          | 16.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 153          |
|    time_elapsed         | 429          |
|    total_timesteps      | 313344       |
| train/                  |              |
|    approx_kl            | 0.023375971  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.51        |
|    cost_value_loss      | 0.0416       |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.378       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000826    |
|    mean_cost_advantages | -0.045488015 |
|    mean_reward_advan... | -0.12419324  |
|    n_updates            | 1520         |
|    nu                   | 7.53         |
|    nu_loss              | -0.00733     |
|    policy_gradient_loss | -0.000626    |
|    reward_explained_... | 0.21         |
|    reward_value_loss    | 0.0123       |
|    total_cost           | 2.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0.0363       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0112       |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 729          |
|    iterations           | 154          |
|    time_elapsed         | 432          |
|    total_timesteps      | 315392       |
| train/                  |              |
|    approx_kl            | 0.013858454  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.63        |
|    cost_value_loss      | 0.103        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.34        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0691       |
|    mean_cost_advantages | 0.0042999205 |
|    mean_reward_advan... | 0.056736406  |
|    n_updates            | 1530         |
|    nu                   | 7.55         |
|    nu_loss              | -0.0184      |
|    policy_gradient_loss | -0.000139    |
|    reward_explained_... | 0.775        |
|    reward_value_loss    | 0.00876      |
|    total_cost           | 5.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0349      |
|    ep_len_mean          | 17           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 728          |
|    iterations           | 155          |
|    time_elapsed         | 435          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.008494878  |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.0777       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.48        |
|    cost_value_loss      | 0.0625       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.373       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000264     |
|    mean_cost_advantages | -0.01383147  |
|    mean_reward_advan... | -0.02844156  |
|    n_updates            | 1540         |
|    nu                   | 7.57         |
|    nu_loss              | -0.0111      |
|    policy_gradient_loss | -0.000832    |
|    reward_explained_... | 0.636        |
|    reward_value_loss    | 0.0129       |
|    total_cost           | 3.0          |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 24.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.026        |
|    ep_len_mean          | 23.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 729          |
|    iterations           | 156          |
|    time_elapsed         | 437          |
|    total_timesteps      | 319488       |
| train/                  |              |
|    approx_kl            | 0.030273227  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.199        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.39        |
|    cost_value_loss      | 0.144        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.339       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0205       |
|    mean_cost_advantages | 0.024515796  |
|    mean_reward_advan... | 0.04012212   |
|    n_updates            | 1550         |
|    nu                   | 7.59         |
|    nu_loss              | -0.0259      |
|    policy_gradient_loss | -0.00226     |
|    reward_explained_... | 0.717        |
|    reward_value_loss    | 0.0117       |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 22            |
|    mean_reward          | 1             |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0513        |
|    ep_len_mean          | 19.4          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 157           |
|    time_elapsed         | 440           |
|    total_timesteps      | 321536        |
| train/                  |               |
|    approx_kl            | 0.017414574   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.113         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.95         |
|    cost_value_loss      | 0.0214        |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.392        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00153       |
|    mean_cost_advantages | -0.029869512  |
|    mean_reward_advan... | -0.09514046   |
|    n_updates            | 1560          |
|    nu                   | 7.61          |
|    nu_loss              | -0.00371      |
|    policy_gradient_loss | -0.0015       |
|    reward_explained_... | 0.601         |
|    reward_value_loss    | 0.0121        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00439      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0555      |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 158          |
|    time_elapsed         | 442          |
|    total_timesteps      | 323584       |
| train/                  |              |
|    approx_kl            | 0.023207618  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.223        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.266        |
|    cost_value_loss      | 0.000392     |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.382       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00253      |
|    mean_cost_advantages | -0.018146684 |
|    mean_reward_advan... | 0.052400302  |
|    n_updates            | 1570         |
|    nu                   | 7.63         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00387     |
|    reward_explained_... | 0.753        |
|    reward_value_loss    | 0.0139       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 21.6         |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0429       |
|    ep_len_mean          | 22.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 732          |
|    iterations           | 159          |
|    time_elapsed         | 444          |
|    total_timesteps      | 325632       |
| train/                  |              |
|    approx_kl            | 0.028532414  |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.204        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.84        |
|    cost_value_loss      | 0.188        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.348       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.104        |
|    mean_cost_advantages | 0.05494025   |
|    mean_reward_advan... | 0.10861796   |
|    n_updates            | 1580         |
|    nu                   | 7.64         |
|    nu_loss              | -0.0335      |
|    policy_gradient_loss | -0.00303     |
|    reward_explained_... | 0.775        |
|    reward_value_loss    | 0.0111       |
|    total_cost           | 9.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.00541      |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 733          |
|    iterations           | 160          |
|    time_elapsed         | 446          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.031218885  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.251        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.261        |
|    cost_value_loss      | 0.0011       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.395       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00106      |
|    mean_cost_advantages | -0.04453069  |
|    mean_reward_advan... | -0.097181104 |
|    n_updates            | 1590         |
|    nu                   | 7.66         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0042      |
|    reward_explained_... | 0.683        |
|    reward_value_loss    | 0.0117       |
|    total_cost           | 0.0          |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 13.2        |
|    mean_reward          | 1           |
|    true_cost            | 0.00244     |
| infos/                  |             |
|    cost                 | 0.037       |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.00677    |
|    ep_len_mean          | 16.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 161         |
|    time_elapsed         | 449         |
|    total_timesteps      | 329728      |
| train/                  |             |
|    approx_kl            | 0.004542885 |
|    average_cost         | 0.001953125 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.72       |
|    cost_value_loss      | 0.0858      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00529     |
|    mean_cost_advantages | 0.010719414 |
|    mean_reward_advan... | 0.062204808 |
|    n_updates            | 1600        |
|    nu                   | 7.68        |
|    nu_loss              | -0.015      |
|    policy_gradient_loss | 1.33e-05    |
|    reward_explained_... | 0.775       |
|    reward_value_loss    | 0.0109      |
|    total_cost           | 4.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 1            |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0.0371       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0316      |
|    ep_len_mean          | 14.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 732          |
|    iterations           | 162          |
|    time_elapsed         | 452          |
|    total_timesteps      | 331776       |
| train/                  |              |
|    approx_kl            | 0.003015607  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.86        |
|    cost_value_loss      | 0.107        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.373       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00667      |
|    mean_cost_advantages | 0.007940674  |
|    mean_reward_advan... | 0.01692142   |
|    n_updates            | 1610         |
|    nu                   | 7.7          |
|    nu_loss              | -0.0187      |
|    policy_gradient_loss | 0.000173     |
|    reward_explained_... | 0.721        |
|    reward_value_loss    | 0.00986      |
|    total_cost           | 5.0          |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 28.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0358       |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 732          |
|    iterations           | 163          |
|    time_elapsed         | 455          |
|    total_timesteps      | 333824       |
| train/                  |              |
|    approx_kl            | 0.028765492  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.319        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.93        |
|    cost_value_loss      | 0.15         |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.363       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.142        |
|    mean_cost_advantages | 0.01942003   |
|    mean_reward_advan... | 0.011920489  |
|    n_updates            | 1620         |
|    nu                   | 7.71         |
|    nu_loss              | -0.0263      |
|    policy_gradient_loss | -0.00228     |
|    reward_explained_... | 0.77         |
|    reward_value_loss    | 0.00713      |
|    total_cost           | 7.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 19            |
|    mean_reward          | 1             |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0208        |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 164           |
|    time_elapsed         | 459           |
|    total_timesteps      | 335872        |
| train/                  |               |
|    approx_kl            | 0.011789611   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.196         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.25         |
|    cost_value_loss      | 0.0221        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.398        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00106      |
|    mean_cost_advantages | -0.031372767  |
|    mean_reward_advan... | -0.06849544   |
|    n_updates            | 1630          |
|    nu                   | 7.73          |
|    nu_loss              | -0.00377      |
|    policy_gradient_loss | -0.00272      |
|    reward_explained_... | 0.648         |
|    reward_value_loss    | 0.011         |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 12           |
|    mean_reward          | 1            |
|    true_cost            | 0.00439      |
| infos/                  |              |
|    cost                 | 0.0374       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0593      |
|    ep_len_mean          | 14.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 729          |
|    iterations           | 165          |
|    time_elapsed         | 463          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.007445797  |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.17        |
|    cost_value_loss      | 0.0661       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.378       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00188      |
|    mean_cost_advantages | 0.0038316427 |
|    mean_reward_advan... | 0.046055317  |
|    n_updates            | 1640         |
|    nu                   | 7.75         |
|    nu_loss              | -0.0113      |
|    policy_gradient_loss | -0.0011      |
|    reward_explained_... | 0.716        |
|    reward_value_loss    | 0.0101       |
|    total_cost           | 3.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0351       |
|    ep_len_mean          | 20.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 166          |
|    time_elapsed         | 465          |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.019770771  |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.302        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.11        |
|    cost_value_loss      | 0.199        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.365       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0721       |
|    mean_cost_advantages | 0.043246835  |
|    mean_reward_advan... | 0.023480434  |
|    n_updates            | 1650         |
|    nu                   | 7.77         |
|    nu_loss              | -0.034       |
|    policy_gradient_loss | -0.00399     |
|    reward_explained_... | 0.746        |
|    reward_value_loss    | 0.00838      |
|    total_cost           | 9.0          |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 1             |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.032         |
|    ep_len_mean          | 16.7          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 167           |
|    time_elapsed         | 467           |
|    total_timesteps      | 342016        |
| train/                  |               |
|    approx_kl            | 0.019945016   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.166         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.4          |
|    cost_value_loss      | 0.0228        |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.397        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000754      |
|    mean_cost_advantages | -0.033137474  |
|    mean_reward_advan... | -0.07611102   |
|    n_updates            | 1660          |
|    nu                   | 7.78          |
|    nu_loss              | -0.00379      |
|    policy_gradient_loss | -0.00243      |
|    reward_explained_... | 0.557         |
|    reward_value_loss    | 0.0122        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 0.6           |
|    true_cost            | 0.00781       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.155        |
|    ep_len_mean          | 13.1          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 731           |
|    iterations           | 168           |
|    time_elapsed         | 470           |
|    total_timesteps      | 344064        |
| train/                  |               |
|    approx_kl            | 0.015915917   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.118         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.13         |
|    cost_value_loss      | 0.045         |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.372        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00163       |
|    mean_cost_advantages | -0.0030749857 |
|    mean_reward_advan... | 0.033411145   |
|    n_updates            | 1670          |
|    nu                   | 7.8           |
|    nu_loss              | -0.0076       |
|    policy_gradient_loss | -0.00163      |
|    reward_explained_... | 0.742         |
|    reward_value_loss    | 0.0111        |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.13
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 33.8        |
|    mean_reward          | 1           |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0288      |
|    ep_len_mean          | 26          |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 169         |
|    time_elapsed         | 472         |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.13303086  |
|    average_cost         | 0.0078125   |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.24       |
|    cost_value_loss      | 0.344       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.324      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0999      |
|    mean_cost_advantages | 0.097587705 |
|    mean_reward_advan... | 0.06291185  |
|    n_updates            | 1680        |
|    nu                   | 7.82        |
|    nu_loss              | -0.0609     |
|    policy_gradient_loss | -0.0187     |
|    reward_explained_... | 0.783       |
|    reward_value_loss    | 0.00735     |
|    total_cost           | 16.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 19.8        |
|    mean_reward          | 1           |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.0464      |
|    ep_len_mean          | 23.1        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 170         |
|    time_elapsed         | 474         |
|    total_timesteps      | 348160      |
| train/                  |             |
|    approx_kl            | 0.018975817 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.0544      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.365       |
|    cost_value_loss      | 0.00264     |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.365      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00414    |
|    mean_cost_advantages | -0.08187541 |
|    mean_reward_advan... | -0.22513928 |
|    n_updates            | 1690        |
|    nu                   | 7.84        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00304    |
|    reward_explained_... | 0.0908      |
|    reward_value_loss    | 0.0172      |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 1           |
|    true_cost            | 0.000488    |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.047       |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 171         |
|    time_elapsed         | 476         |
|    total_timesteps      | 350208      |
| train/                  |             |
|    approx_kl            | 0.021417268 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.35        |
|    cost_value_loss      | 0.000918    |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.397      |
|    learning_rate        | 0.0003      |
|    loss                 | 5.11e-06    |
|    mean_cost_advantages | -0.0357371  |
|    mean_reward_advan... | 0.037134126 |
|    n_updates            | 1700        |
|    nu                   | 7.86        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00372    |
|    reward_explained_... | 0.78        |
|    reward_value_loss    | 0.014       |
|    total_cost           | 0.0         |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 9.2           |
|    mean_reward          | 0.2           |
|    true_cost            | 0.0083        |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.175        |
|    ep_len_mean          | 13.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 733           |
|    iterations           | 172           |
|    time_elapsed         | 479           |
|    total_timesteps      | 352256        |
| train/                  |               |
|    approx_kl            | 0.010285219   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.173         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.91         |
|    cost_value_loss      | 0.0231        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.362        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00115       |
|    mean_cost_advantages | -0.0050909477 |
|    mean_reward_advan... | 0.09471473    |
|    n_updates            | 1710          |
|    nu                   | 7.87          |
|    nu_loss              | -0.00384      |
|    policy_gradient_loss | -0.00288      |
|    reward_explained_... | 0.813         |
|    reward_value_loss    | 0.00898       |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1           |
|    mean_ep_length       | 23.6        |
|    mean_reward          | 1           |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
|    info                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 0.041       |
|    ep_len_mean          | 22.8        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 173         |
|    time_elapsed         | 482         |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.028625768 |
|    average_cost         | 0.008300781 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.17       |
|    cost_value_loss      | 0.375       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.349      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.219       |
|    mean_cost_advantages | 0.11275085  |
|    mean_reward_advan... | 0.066214465 |
|    n_updates            | 1720        |
|    nu                   | 7.89        |
|    nu_loss              | -0.0653     |
|    policy_gradient_loss | -0.0132     |
|    reward_explained_... | 0.712       |
|    reward_value_loss    | 0.00984     |
|    total_cost           | 17.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.03         |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 174          |
|    time_elapsed         | 484          |
|    total_timesteps      | 356352       |
| train/                  |              |
|    approx_kl            | 0.020570349  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.278        |
|    cost_value_loss      | 0.00209      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.38        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00103     |
|    mean_cost_advantages | -0.062216565 |
|    mean_reward_advan... | -0.13498697  |
|    n_updates            | 1730         |
|    nu                   | 7.91         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0026      |
|    reward_explained_... | 0.584        |
|    reward_value_loss    | 0.0137       |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.00684      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.135       |
|    ep_len_mean          | 13.8         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 175          |
|    time_elapsed         | 487          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.009715706  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.98        |
|    cost_value_loss      | 0.0465       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.369       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.85e-05     |
|    mean_cost_advantages | -0.011873944 |
|    mean_reward_advan... | 0.06139791   |
|    n_updates            | 1740         |
|    nu                   | 7.93         |
|    nu_loss              | -0.00772     |
|    policy_gradient_loss | -0.00184     |
|    reward_explained_... | 0.747        |
|    reward_value_loss    | 0.0107       |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0276       |
|    ep_len_mean          | 22.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 176          |
|    time_elapsed         | 489          |
|    total_timesteps      | 360448       |
| train/                  |              |
|    approx_kl            | 0.018110273  |
|    average_cost         | 0.0068359375 |
|    clip_fraction        | 0.213        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.25        |
|    cost_value_loss      | 0.322        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.343       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0972       |
|    mean_cost_advantages | 0.08598095   |
|    mean_reward_advan... | 0.071904466  |
|    n_updates            | 1750         |
|    nu                   | 7.95         |
|    nu_loss              | -0.0542      |
|    policy_gradient_loss | -0.0101      |
|    reward_explained_... | 0.779        |
|    reward_value_loss    | 0.00806      |
|    total_cost           | 14.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 19            |
|    mean_reward          | 1             |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0605        |
|    ep_len_mean          | 16.3          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 737           |
|    iterations           | 177           |
|    time_elapsed         | 491           |
|    total_timesteps      | 362496        |
| train/                  |               |
|    approx_kl            | 0.022307873   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.129         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.961        |
|    cost_value_loss      | 0.025         |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.378        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0039        |
|    mean_cost_advantages | -0.05183998   |
|    mean_reward_advan... | -0.12793064   |
|    n_updates            | 1760          |
|    nu                   | 7.97          |
|    nu_loss              | -0.00388      |
|    policy_gradient_loss | -0.00163      |
|    reward_explained_... | 0.448         |
|    reward_value_loss    | 0.0136        |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.00488      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0796      |
|    ep_len_mean          | 14.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 178          |
|    time_elapsed         | 494          |
|    total_timesteps      | 364544       |
| train/                  |              |
|    approx_kl            | 0.010734798  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.33         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.255        |
|    cost_value_loss      | 0.000184     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.359       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00215      |
|    mean_cost_advantages | -0.021642176 |
|    mean_reward_advan... | 0.06837273   |
|    n_updates            | 1770         |
|    nu                   | 7.99         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00514     |
|    reward_explained_... | 0.729        |
|    reward_value_loss    | 0.0111       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 24.2         |
|    mean_reward          | 1            |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.041        |
|    ep_len_mean          | 22.7         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 738          |
|    iterations           | 179          |
|    time_elapsed         | 496          |
|    total_timesteps      | 366592       |
| train/                  |              |
|    approx_kl            | 0.056158774  |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.335        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.44        |
|    cost_value_loss      | 0.235        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.33        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0256       |
|    mean_cost_advantages | 0.07020984   |
|    mean_reward_advan... | 0.058739826  |
|    n_updates            | 1780         |
|    nu                   | 8.01         |
|    nu_loss              | -0.039       |
|    policy_gradient_loss | -0.0075      |
|    reward_explained_... | 0.787        |
|    reward_value_loss    | 0.00716      |
|    total_cost           | 10.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0356       |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 738          |
|    iterations           | 180          |
|    time_elapsed         | 498          |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.029098764  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0874       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.28         |
|    cost_value_loss      | 0.00242      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.363       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000884     |
|    mean_cost_advantages | -0.044752743 |
|    mean_reward_advan... | -0.13268821  |
|    n_updates            | 1790         |
|    nu                   | 8.02         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00221     |
|    reward_explained_... | 0.39         |
|    reward_value_loss    | 0.0159       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0123       |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 739           |
|    iterations           | 181           |
|    time_elapsed         | 501           |
|    total_timesteps      | 370688        |
| train/                  |               |
|    approx_kl            | 0.017629594   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.154         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.15         |
|    cost_value_loss      | 0.0242        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.37         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0153        |
|    mean_cost_advantages | -0.014842283  |
|    mean_reward_advan... | 0.025469985   |
|    n_updates            | 1800          |
|    nu                   | 8.04          |
|    nu_loss              | -0.00392      |
|    policy_gradient_loss | -0.0025       |
|    reward_explained_... | 0.753         |
|    reward_value_loss    | 0.0132        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 22.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.034        |
|    ep_len_mean          | 20.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 182          |
|    time_elapsed         | 503          |
|    total_timesteps      | 372736       |
| train/                  |              |
|    approx_kl            | 0.016776007  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0861       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.3         |
|    cost_value_loss      | 0.119        |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.345       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.109        |
|    mean_cost_advantages | 0.022064844  |
|    mean_reward_advan... | 0.0767836    |
|    n_updates            | 1810         |
|    nu                   | 8.06         |
|    nu_loss              | -0.0196      |
|    policy_gradient_loss | -0.000272    |
|    reward_explained_... | 0.755        |
|    reward_value_loss    | 0.0109       |
|    total_cost           | 5.0          |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00342       |
| infos/                  |               |
|    cost                 | 0.0391        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0462       |
|    ep_len_mean          | 16.4          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 740           |
|    iterations           | 183           |
|    time_elapsed         | 506           |
|    total_timesteps      | 374784        |
| train/                  |               |
|    approx_kl            | 0.016155262   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.171         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.94         |
|    cost_value_loss      | 0.0243        |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.376        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000936     |
|    mean_cost_advantages | -0.016891543  |
|    mean_reward_advan... | -0.04777605   |
|    n_updates            | 1820          |
|    nu                   | 8.07          |
|    nu_loss              | -0.00393      |
|    policy_gradient_loss | -0.00188      |
|    reward_explained_... | 0.643         |
|    reward_value_loss    | 0.0115        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 20           |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0275       |
|    ep_len_mean          | 22           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 741          |
|    iterations           | 184          |
|    time_elapsed         | 508          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.02140062   |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.245        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.08        |
|    cost_value_loss      | 0.167        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.33        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0132       |
|    mean_cost_advantages | 0.03306258   |
|    mean_reward_advan... | 0.04240041   |
|    n_updates            | 1830         |
|    nu                   | 8.09         |
|    nu_loss              | -0.0276      |
|    policy_gradient_loss | -0.00308     |
|    reward_explained_... | 0.676        |
|    reward_value_loss    | 0.0124       |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 1             |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.027         |
|    ep_len_mean          | 17.2          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 742           |
|    iterations           | 185           |
|    time_elapsed         | 510           |
|    total_timesteps      | 378880        |
| train/                  |               |
|    approx_kl            | 0.018050326   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.103         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.78         |
|    cost_value_loss      | 0.0248        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.376        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00151       |
|    mean_cost_advantages | -0.027785886  |
|    mean_reward_advan... | -0.07132054   |
|    n_updates            | 1840          |
|    nu                   | 8.11          |
|    nu_loss              | -0.00395      |
|    policy_gradient_loss | -0.00141      |
|    reward_explained_... | 0.607         |
|    reward_value_loss    | 0.0126        |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 1             |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.00719      |
|    ep_len_mean          | 14.2          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 742           |
|    iterations           | 186           |
|    time_elapsed         | 513           |
|    total_timesteps      | 380928        |
| train/                  |               |
|    approx_kl            | 0.0110889785  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.129         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.63         |
|    cost_value_loss      | 0.049         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.359        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00292       |
|    mean_cost_advantages | -0.0053682625 |
|    mean_reward_advan... | 0.055491153   |
|    n_updates            | 1850          |
|    nu                   | 8.12          |
|    nu_loss              | -0.00792      |
|    policy_gradient_loss | -0.00167      |
|    reward_explained_... | 0.723         |
|    reward_value_loss    | 0.0133        |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16           |
|    mean_reward          | 1            |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0282       |
|    ep_len_mean          | 17.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 741          |
|    iterations           | 187          |
|    time_elapsed         | 516          |
|    total_timesteps      | 382976       |
| train/                  |              |
|    approx_kl            | 0.011195287  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0639       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.07        |
|    cost_value_loss      | 0.122        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.347       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0797       |
|    mean_cost_advantages | 0.023264958  |
|    mean_reward_advan... | 0.07000993   |
|    n_updates            | 1860         |
|    nu                   | 8.14         |
|    nu_loss              | -0.0198      |
|    policy_gradient_loss | -0.000377    |
|    reward_explained_... | 0.773        |
|    reward_value_loss    | 0.00867      |
|    total_cost           | 5.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.00439      |
| infos/                  |              |
|    cost                 | 0.0395       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0737      |
|    ep_len_mean          | 14.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 741          |
|    iterations           | 188          |
|    time_elapsed         | 519          |
|    total_timesteps      | 385024       |
| train/                  |              |
|    approx_kl            | 0.007858096  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.11        |
|    cost_value_loss      | 0.0494       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.362       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00708      |
|    mean_cost_advantages | -0.015530375 |
|    mean_reward_advan... | -0.017254928 |
|    n_updates            | 1870         |
|    nu                   | 8.15         |
|    nu_loss              | -0.00795     |
|    policy_gradient_loss | -0.00193     |
|    reward_explained_... | 0.668        |
|    reward_value_loss    | 0.0106       |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0311       |
|    ep_len_mean          | 21           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 189          |
|    time_elapsed         | 521          |
|    total_timesteps      | 387072       |
| train/                  |              |
|    approx_kl            | 0.021517307  |
|    average_cost         | 0.0043945312 |
|    clip_fraction        | 0.261        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.14        |
|    cost_value_loss      | 0.222        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.342       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0197       |
|    mean_cost_advantages | 0.051726967  |
|    mean_reward_advan... | 0.026123434  |
|    n_updates            | 1880         |
|    nu                   | 8.17         |
|    nu_loss              | -0.0358      |
|    policy_gradient_loss | -0.00557     |
|    reward_explained_... | 0.704        |
|    reward_value_loss    | 0.00872      |
|    total_cost           | 9.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0164       |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 742           |
|    iterations           | 190           |
|    time_elapsed         | 523           |
|    total_timesteps      | 389120        |
| train/                  |               |
|    approx_kl            | 0.019383311   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.12          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.65         |
|    cost_value_loss      | 0.0256        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.359        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0058        |
|    mean_cost_advantages | -0.038673393  |
|    mean_reward_advan... | -0.09336805   |
|    n_updates            | 1890          |
|    nu                   | 8.19          |
|    nu_loss              | -0.00399      |
|    policy_gradient_loss | -0.00167      |
|    reward_explained_... | 0.551         |
|    reward_value_loss    | 0.0138        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15.8         |
|    mean_reward          | 1            |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0            |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.00394      |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 191          |
|    time_elapsed         | 526          |
|    total_timesteps      | 391168       |
| train/                  |              |
|    approx_kl            | 0.015277909  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.05        |
|    cost_value_loss      | 0.124        |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.342       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.015        |
|    mean_cost_advantages | 0.0187142    |
|    mean_reward_advan... | 0.058368817  |
|    n_updates            | 1900         |
|    nu                   | 8.2          |
|    nu_loss              | -0.02        |
|    policy_gradient_loss | -0.000633    |
|    reward_explained_... | 0.711        |
|    reward_value_loss    | 0.011        |
|    total_cost           | 5.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0233       |
|    ep_len_mean          | 17.7          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 741           |
|    iterations           | 192           |
|    time_elapsed         | 530           |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 0.0028101108  |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0.0399        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.79         |
|    cost_value_loss      | 0.0748        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.356        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00396       |
|    mean_cost_advantages | -0.0027973284 |
|    mean_reward_advan... | -0.027842278  |
|    n_updates            | 1910          |
|    nu                   | 8.22          |
|    nu_loss              | -0.012        |
|    policy_gradient_loss | 1.2e-07       |
|    reward_explained_... | 0.72          |
|    reward_value_loss    | 0.0114        |
|    total_cost           | 3.0           |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 27.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0.04         |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0264       |
|    ep_len_mean          | 23.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 193          |
|    time_elapsed         | 532          |
|    total_timesteps      | 395264       |
| train/                  |              |
|    approx_kl            | 0.020040482  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.53        |
|    cost_value_loss      | 0.124        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.339       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.109        |
|    mean_cost_advantages | 0.010126848  |
|    mean_reward_advan... | 0.012058891  |
|    n_updates            | 1920         |
|    nu                   | 8.24         |
|    nu_loss              | -0.0201      |
|    policy_gradient_loss | -0.0011      |
|    reward_explained_... | 0.634        |
|    reward_value_loss    | 0.0132       |
|    total_cost           | 5.0          |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 0.6           |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0395        |
|    ep_len_mean          | 17.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 742           |
|    iterations           | 194           |
|    time_elapsed         | 535           |
|    total_timesteps      | 397312        |
| train/                  |               |
|    approx_kl            | 0.020376522   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.147         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.83         |
|    cost_value_loss      | 0.0256        |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.384        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00664       |
|    mean_cost_advantages | -0.026021361  |
|    mean_reward_advan... | -0.067300536  |
|    n_updates            | 1930          |
|    nu                   | 8.25          |
|    nu_loss              | -0.00402      |
|    policy_gradient_loss | -0.00179      |
|    reward_explained_... | 0.526         |
|    reward_value_loss    | 0.0166        |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 1             |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0.0402        |
|    info                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.0151       |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 741           |
|    iterations           | 195           |
|    time_elapsed         | 538           |
|    total_timesteps      | 399360        |
| train/                  |               |
|    approx_kl            | 0.0083781     |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.145         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.17         |
|    cost_value_loss      | 0.0257        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.368        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000204     |
|    mean_cost_advantages | -0.008787389  |
|    mean_reward_advan... | 0.058831822   |
|    n_updates            | 1940          |
|    nu                   | 8.26          |
|    nu_loss              | -0.00403      |
|    policy_gradient_loss | -0.00252      |
|    reward_explained_... | 0.757         |
|    reward_value_loss    | 0.0137        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 22.4         |
|    mean_reward          | 1            |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0.0403       |
|    info                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.033        |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 741          |
|    iterations           | 196          |
|    time_elapsed         | 541          |
|    total_timesteps      | 401408       |
| train/                  |              |
|    approx_kl            | 0.018925905  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.179        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.74        |
|    cost_value_loss      | 0.127        |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.338       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00609      |
|    mean_cost_advantages | 0.026888445  |
|    mean_reward_advan... | 0.06789951   |
|    n_updates            | 1950         |
|    nu                   | 8.28         |
|    nu_loss              | -0.0202      |
|    policy_gradient_loss | -0.00104     |
|    reward_explained_... | 0.801        |
|    reward_value_loss    | 0.0084       |
|    total_cost           | 5.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Mean reward: 1.000000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 09.24 minutes[0m
