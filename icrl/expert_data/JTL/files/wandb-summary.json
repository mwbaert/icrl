{"eval/mean_reward": 9994.5, "eval/mean_ep_length": 12.0, "eval/best_mean_reward": 9994.5, "rollout/adjusted_reward": 829.716064453125, "eval/true_cost": 0.0, "time/iterations": 98, "time/fps": 434, "time/time_elapsed": 462, "time/total_timesteps": 200704, "infos/cost": 0.0, "rollout/ep_rew_mean": 9994.5, "rollout/ep_len_mean": 12.0, "_step": 200704, "_runtime": 476, "_timestamp": 1654678930, "train/learning_rate": 0.0003, "train/entropy_loss": -0.07261856670374982, "train/policy_gradient_loss": 0.00042704290613011786, "train/reward_value_loss": 2.159901695275579e-06, "train/cost_value_loss": 3.7920373713307145e-08, "train/approx_kl": 0.0008687932277098298, "train/clip_fraction": 0.026904296875, "train/loss": -0.0007503289962187409, "train/mean_reward_advantages": -0.0005004768026992679, "train/mean_cost_advantages": -0.000159888862981461, "train/reward_explained_variance": 0.9999493262330361, "train/cost_explained_variance": 0.015034317970275879, "train/nu": 2.641254186630249, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 970, "train/clip_range": 0.2}