{"eval/mean_reward": 9996.1, "eval/mean_ep_length": 11.6, "eval/best_mean_reward": 9996.833333333332, "rollout/adjusted_reward": 880.5275268554688, "eval/true_cost": 0.0, "time/iterations": 489, "time/fps": 1277, "time/time_elapsed": 3919, "time/total_timesteps": 5007360, "infos/cost": 0.0, "rollout/ep_rew_mean": 9996.146666730001, "rollout/ep_len_mean": 11.59, "_timestamp": 1656364307, "_runtime": 3924, "_step": 5007360, "train/learning_rate": 0.0003, "train/entropy_loss": -0.0028757336457056228, "train/policy_gradient_loss": -1.1719642028468862e-05, "train/reward_value_loss": 5955.750675201416, "train/cost_value_loss": 7.801191617176074e-06, "train/approx_kl": 4.211980922264047e-05, "train/clip_fraction": 4.8828125e-05, "train/loss": 2793.213623046875, "train/mean_reward_advantages": -2.055785894393921, "train/mean_cost_advantages": 0.0002043996937572956, "train/reward_explained_variance": 0.9427384324371815, "train/cost_explained_variance": -0.051538944244384766, "train/nu": 3.1697092056274414, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 4880, "train/clip_range": 0.2, "_wandb": {"runtime": 3930}}