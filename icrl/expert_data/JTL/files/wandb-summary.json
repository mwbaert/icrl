{"eval/mean_reward": 9994.5, "eval/mean_ep_length": 12.0, "eval/best_mean_reward": 9994.5, "rollout/adjusted_reward": 834.6175537109375, "eval/true_cost": 0.0009765625, "time/iterations": 98, "time/fps": 445, "time/time_elapsed": 450, "time/total_timesteps": 200704, "infos/cost": 0.0, "rollout/ep_rew_mean": 9994.5, "rollout/ep_len_mean": 12.0, "_step": 200704, "_runtime": 465, "_timestamp": 1654586322, "train/learning_rate": 0.0003, "train/entropy_loss": -0.12131902221590281, "train/policy_gradient_loss": 2.1691995279854436e-05, "train/reward_value_loss": 5.827683001591311e-06, "train/cost_value_loss": 7.349233602571559e-07, "train/approx_kl": 0.002087702276185155, "train/clip_fraction": 0.0224609375, "train/loss": 0.0021325331181287766, "train/mean_reward_advantages": -0.0014909330056980252, "train/mean_cost_advantages": -0.0006674014730378985, "train/reward_explained_variance": 0.9997060724999756, "train/cost_explained_variance": 0.13254010677337646, "train/nu": 6.530193328857422, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 970, "train/clip_range": 0.2}