{"eval/mean_reward": 9994.5, "eval/mean_ep_length": 12.0, "eval/best_mean_reward": 9994.5, "rollout/adjusted_reward": 835.451904296875, "eval/true_cost": 0.0009765625, "time/iterations": 40, "time/fps": 464, "time/time_elapsed": 176, "time/total_timesteps": 81920, "infos/cost": 0.0, "rollout/ep_rew_mean": 9994.4975, "rollout/ep_len_mean": 12.02, "_step": 81920, "_runtime": 190, "_timestamp": 1653903784, "train/learning_rate": 0.0003, "train/entropy_loss": -0.11861242523882538, "train/policy_gradient_loss": -0.0003683661874605337, "train/reward_value_loss": 5.991637496691737e-05, "train/cost_value_loss": 9.870961605003003e-05, "train/approx_kl": 0.0015161707997322083, "train/clip_fraction": 0.033349609375, "train/loss": 0.0005764374509453773, "train/mean_reward_advantages": -0.010527016595005989, "train/mean_cost_advantages": 0.0009053447283804417, "train/reward_explained_variance": 0.996360067743808, "train/cost_explained_variance": -10.569452285766602, "train/nu": 3.8097586631774902, "train/nu_loss": -0.003693667007610202, "train/average_cost": 0.0009765625, "train/total_cost": 2.0, "train/early_stop_epoch": 10, "train/n_updates": 390, "train/clip_range": 0.2}