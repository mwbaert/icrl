{"eval/mean_reward": 9991.733333333334, "eval/mean_ep_length": 18.0, "eval/best_mean_reward": 9992.75, "rollout/adjusted_reward": 573.8162841796875, "eval/true_cost": 0.0029296875, "time/iterations": 10, "time/fps": 997, "time/time_elapsed": 102, "time/total_timesteps": 102400, "infos/cost": 0.0, "rollout/ep_rew_mean": 9991.815833339999, "rollout/ep_len_mean": 18.87, "_step": 102400, "_runtime": 126, "_timestamp": 1655800537, "train/learning_rate": 0.0003, "train/entropy_loss": -0.8293265202082694, "train/policy_gradient_loss": -0.00441034113248381, "train/reward_value_loss": 0.11455284464464058, "train/cost_value_loss": 0.009539343113101496, "train/approx_kl": 0.016019154340028763, "train/clip_fraction": 0.1400390625, "train/loss": 0.041290827095508575, "train/mean_reward_advantages": -0.543535053730011, "train/mean_cost_advantages": -0.022675611078739166, "train/reward_explained_variance": 0.6334092020988464, "train/cost_explained_variance": -0.5803769826889038, "train/nu": 1.5427680015563965, "train/nu_loss": -0.013435501605272293, "train/average_cost": 0.008984374813735485, "train/total_cost": 92.0, "train/early_stop_epoch": 3, "train/n_updates": 90, "train/clip_range": 0.2}