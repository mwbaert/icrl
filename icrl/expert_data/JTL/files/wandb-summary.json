{"eval/mean_reward": 9992.033333333333, "eval/mean_ep_length": 19.4, "eval/best_mean_reward": 9993.8, "rollout/adjusted_reward": 543.5402221679688, "eval/true_cost": 0.000390625, "time/iterations": 293, "time/fps": 954, "time/time_elapsed": 3144, "time/total_timesteps": 3000320, "infos/cost": 0.0, "rollout/ep_rew_mean": 9992.50083328, "rollout/ep_len_mean": 18.53, "_step": 3000320, "_runtime": 3215, "_timestamp": 1656327901, "train/learning_rate": 0.0003, "train/entropy_loss": -0.14552927373442798, "train/policy_gradient_loss": 0.0008777829301519549, "train/reward_value_loss": 293608.66650390625, "train/cost_value_loss": 0.0078253932653638, "train/approx_kl": 0.062115781009197235, "train/clip_fraction": 0.0535400390625, "train/loss": 146121.359375, "train/mean_reward_advantages": 106.46134948730469, "train/mean_cost_advantages": -0.0013247668975964189, "train/reward_explained_variance": NaN, "train/cost_explained_variance": -1.165665626525879, "train/nu": 9.682635307312012, "train/nu_loss": -0.0037812322843819857, "train/average_cost": 0.0003906250058207661, "train/total_cost": 4.0, "train/early_stop_epoch": 3, "train/n_updates": 2920, "train/clip_range": 0.2}