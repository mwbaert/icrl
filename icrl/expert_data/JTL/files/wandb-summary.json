{"eval/mean_reward": -3.4333333333333327, "eval/mean_ep_length": 10.6, "eval/best_mean_reward": -3.0833333333333335, "rollout/adjusted_reward": -0.3327880799770355, "eval/true_cost": 0.0, "time/iterations": 98, "time/fps": 750, "time/time_elapsed": 1336, "time/total_timesteps": 1003520, "infos/cost": 0.0, "rollout/ep_rew_mean": -3.82083329, "rollout/ep_len_mean": 11.47, "_timestamp": 1656566107, "_runtime": 1344, "_step": 1003520, "train/learning_rate": 0.0003, "train/entropy_loss": -0.0005656106799960981, "train/policy_gradient_loss": 4.818734083607973e-07, "train/reward_value_loss": 0.09068863951892127, "train/cost_value_loss": 5.726644785408741e-07, "train/approx_kl": -4.0585546230431646e-05, "train/clip_fraction": 0.0, "train/loss": 0.027294181287288666, "train/mean_reward_advantages": 0.020231787115335464, "train/mean_cost_advantages": -0.00017178682901430875, "train/reward_explained_variance": 0.9395602159202099, "train/cost_explained_variance": -0.00899958610534668, "train/nu": 3.7643213272094727, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 970, "train/clip_range": 0.2, "_wandb": {"runtime": 1355}}