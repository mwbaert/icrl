{"eval/mean_reward": -3.616666666666667, "eval/mean_ep_length": 10.8, "eval/best_mean_reward": -3.1833333333333336, "rollout/adjusted_reward": -0.44055700302124023, "eval/true_cost": 0.04755859375, "time/iterations": 40, "time/fps": 584, "time/time_elapsed": 701, "time/total_timesteps": 409600, "infos/cost": 0.05, "rollout/ep_rew_mean": -3.39666649, "rollout/ep_len_mean": 10.47, "_timestamp": 1656506987, "_runtime": 712, "_step": 409600, "train/learning_rate": 0.0003, "train/entropy_loss": -0.00343233148829313, "train/policy_gradient_loss": -0.003138201962842067, "train/reward_value_loss": 0.004396535314317589, "train/cost_value_loss": 0.07404001103714108, "train/approx_kl": 0.016076266765594482, "train/clip_fraction": 0.0055078125, "train/loss": 0.025051385164260864, "train/mean_reward_advantages": 0.03470522165298462, "train/mean_cost_advantages": -0.003013220150023699, "train/reward_explained_variance": 0.9958330634981394, "train/cost_explained_variance": -0.2928839921951294, "train/nu": 2.425161838531494, "train/nu_loss": -0.10880397260189056, "train/average_cost": 0.04541015625, "train/total_cost": 465.0, "train/early_stop_epoch": 4, "train/n_updates": 390, "train/clip_range": 0.2, "_wandb": {"runtime": 727}}