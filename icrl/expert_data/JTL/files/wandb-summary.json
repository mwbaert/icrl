{"eval/mean_reward": -2.583333333333333, "eval/mean_ep_length": 10.8, "eval/best_mean_reward": -2.3, "rollout/adjusted_reward": -0.2971302270889282, "eval/true_cost": 0.0259765625, "time/iterations": 98, "time/fps": 892, "time/time_elapsed": 1124, "time/total_timesteps": 1003520, "infos/cost": 0.02, "rollout/ep_rew_mean": -2.4358333500000002, "rollout/ep_len_mean": 10.39, "_timestamp": 1657095856, "_runtime": 1133, "_step": 1003520, "train/learning_rate": 0.0003, "train/entropy_loss": -0.10969126340700314, "train/policy_gradient_loss": -0.0012326170117608623, "train/reward_value_loss": 0.010607935476764396, "train/cost_value_loss": 0.04169213529472472, "train/approx_kl": 0.013204175047576427, "train/clip_fraction": 0.03189453125, "train/loss": 0.044754114001989365, "train/mean_reward_advantages": 0.034897077828645706, "train/mean_cost_advantages": 0.06701807677745819, "train/reward_explained_variance": 0.9887264976277947, "train/cost_explained_variance": -286.6682434082031, "train/nu": 2.450928211212158, "train/nu_loss": -0.04642515629529953, "train/average_cost": 0.01904296875, "train/total_cost": 195.0, "train/early_stop_epoch": 10, "train/n_updates": 970, "train/clip_range": 0.2, "_wandb": {"runtime": 1143}}