{"eval/mean_reward": 9994.5, "eval/mean_ep_length": 12.0, "eval/best_mean_reward": 9994.5, "rollout/adjusted_reward": 835.4722900390625, "eval/true_cost": 0.0009765625, "time/iterations": 40, "time/fps": 451, "time/time_elapsed": 181, "time/total_timesteps": 81920, "infos/cost": 0.0, "rollout/ep_rew_mean": 9994.49583333, "rollout/ep_len_mean": 12.02, "_step": 81920, "_runtime": 197, "_timestamp": 1653998684, "train/learning_rate": 0.0003, "train/entropy_loss": -0.11626442857086658, "train/policy_gradient_loss": -0.0014894515189384588, "train/reward_value_loss": 0.00030416661539902636, "train/cost_value_loss": 0.00025289092883866714, "train/approx_kl": 0.008270015008747578, "train/clip_fraction": 0.02314453125, "train/loss": -0.0004599975945893675, "train/mean_reward_advantages": -0.002174483612179756, "train/mean_cost_advantages": -0.0015318171354010701, "train/reward_explained_variance": 0.983689583837986, "train/cost_explained_variance": -22.27228546142578, "train/nu": 3.768355131149292, "train/nu_loss": -0.007306802086532116, "train/average_cost": 0.001953125, "train/total_cost": 4.0, "train/early_stop_epoch": 10, "train/n_updates": 390, "train/clip_range": 0.2}