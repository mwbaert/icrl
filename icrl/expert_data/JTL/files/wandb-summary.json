{"eval/mean_reward": 9993.816666666666, "eval/mean_ep_length": 13.8, "eval/best_mean_reward": 9993.816666666666, "rollout/adjusted_reward": 632.059326171875, "eval/true_cost": 0.0125, "time/iterations": 10, "time/fps": 1453, "time/time_elapsed": 70, "time/total_timesteps": 102400, "infos/cost": 0.005319927926465497, "rollout/ep_rew_mean": 9992.41583341, "rollout/ep_len_mean": 15.76, "_step": 102400, "_runtime": 92, "_timestamp": 1655120382, "train/learning_rate": 0.0003, "train/entropy_loss": -0.8150080507621169, "train/policy_gradient_loss": -0.005102528914876097, "train/reward_value_loss": 0.11147936742054299, "train/cost_value_loss": 0.016570481441885932, "train/approx_kl": 0.016275351867079735, "train/clip_fraction": 0.138818359375, "train/loss": 0.03898238763213158, "train/mean_reward_advantages": -0.4996921420097351, "train/mean_cost_advantages": -0.03641500324010849, "train/reward_explained_variance": 0.47587138414382935, "train/cost_explained_variance": -0.11857044696807861, "train/nu": 1.5808473825454712, "train/nu_loss": -0.040428031235933304, "train/average_cost": 0.02656250074505806, "train/total_cost": 272.0, "train/early_stop_epoch": 1, "train/n_updates": 90, "train/clip_range": 0.2}