[32;1mConfigured folder ./cpg/wandb/run-20220613_133810-32ratmhy/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -4.44e+05 |
|    mean_ep_length   | 175       |
|    mean_reward      | -4.44e+05 |
|    true_cost        | 0.212     |
| infos/              |           |
|    cost             | 0.0301    |
| rollout/            |           |
|    adjusted_reward  | 7.32      |
|    ep_len_mean      | 159       |
|    ep_rew_mean      | 3.58e+03  |
| time/               |           |
|    fps              | 2219      |
|    iterations       | 1         |
|    time_elapsed     | 4         |
|    total_timesteps  | 10240     |
-----------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.3e+05    |
|    mean_ep_length       | 91.8        |
|    mean_reward          | -2.3e+05    |
|    true_cost            | 0.167       |
| infos/                  |             |
|    cost                 | 0.0417      |
| rollout/                |             |
|    adjusted_reward      | 18.1        |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | 4.18e+03    |
| time/                   |             |
|    fps                  | 1650        |
|    iterations           | 2           |
|    time_elapsed         | 12          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.015652072 |
|    average_cost         | 0.21191406  |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.553      |
|    cost_value_loss      | 0.246       |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.611       |
|    mean_cost_advantages | 0.49167585  |
|    mean_reward_advan... | 0.31638095  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.212      |
|    policy_gradient_loss | -0.00539    |
|    reward_explained_... | -16.2       |
|    reward_value_loss    | 1.73        |
|    total_cost           | 2170.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.5e+05    |
|    mean_ep_length       | 111         |
|    mean_reward          | -1.5e+05    |
|    true_cost            | 0.143       |
| infos/                  |             |
|    cost                 | 0.0207      |
| rollout/                |             |
|    adjusted_reward      | 63.9        |
|    ep_len_mean          | 94.4        |
|    ep_rew_mean          | 8.84e+03    |
| time/                   |             |
|    fps                  | 1686        |
|    iterations           | 3           |
|    time_elapsed         | 18          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.015280704 |
|    average_cost         | 0.16679688  |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.61       |
|    cost_value_loss      | 0.108       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.88        |
|    mean_cost_advantages | 0.1935357   |
|    mean_reward_advan... | 0.43995175  |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.178      |
|    policy_gradient_loss | -0.00582    |
|    reward_explained_... | -9.43       |
|    reward_value_loss    | 2.29        |
|    total_cost           | 1708.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.8e+04    |
|    mean_ep_length       | 40.6        |
|    mean_reward          | -1.8e+04    |
|    true_cost            | 0.113       |
| infos/                  |             |
|    cost                 | 0.0101      |
| rollout/                |             |
|    adjusted_reward      | 105         |
|    ep_len_mean          | 77.3        |
|    ep_rew_mean          | 9.35e+03    |
| time/                   |             |
|    fps                  | 1684        |
|    iterations           | 4           |
|    time_elapsed         | 24          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.016364248 |
|    average_cost         | 0.14257812  |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.46       |
|    cost_value_loss      | 0.116       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.3        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91        |
|    mean_cost_advantages | 0.07078963  |
|    mean_reward_advan... | 1.1853496   |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.161      |
|    policy_gradient_loss | -0.00822    |
|    reward_explained_... | -6.34       |
|    reward_value_loss    | 3.95        |
|    total_cost           | 1460.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.8e+04     |
|    mean_ep_length       | 38.4         |
|    mean_reward          | -5.6e+04     |
|    true_cost            | 0.0715       |
| infos/                  |              |
|    cost                 | 0.0119       |
| rollout/                |              |
|    adjusted_reward      | 202          |
|    ep_len_mean          | 50.1         |
|    ep_rew_mean          | 9.77e+03     |
| time/                   |              |
|    fps                  | 1686         |
|    iterations           | 5            |
|    time_elapsed         | 30           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.016828129  |
|    average_cost         | 0.11259766   |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.14        |
|    cost_value_loss      | 0.107        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -1.23        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.57         |
|    mean_cost_advantages | -0.019448558 |
|    mean_reward_advan... | 1.3021691    |
|    n_updates            | 40           |
|    nu                   | 1.27         |
|    nu_loss              | -0.135       |
|    policy_gradient_loss | -0.00814     |
|    reward_explained_... | -1.58        |
|    reward_value_loss    | 3.79         |
|    total_cost           | 1153.0       |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.8e+04    |
|    mean_ep_length       | 39.2        |
|    mean_reward          | -7.8e+04    |
|    true_cost            | 0.0543      |
| infos/                  |             |
|    cost                 | 0.0201      |
| rollout/                |             |
|    adjusted_reward      | 292         |
|    ep_len_mean          | 33.4        |
|    ep_rew_mean          | 9.98e+03    |
| time/                   |             |
|    fps                  | 1689        |
|    iterations           | 6           |
|    time_elapsed         | 36          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.015637742 |
|    average_cost         | 0.07148437  |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.77       |
|    cost_value_loss      | 0.076       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.39        |
|    mean_cost_advantages | -0.15882543 |
|    mean_reward_advan... | 1.6587547   |
|    n_updates            | 50          |
|    nu                   | 1.33        |
|    nu_loss              | -0.0905     |
|    policy_gradient_loss | -0.00894    |
|    reward_explained_... | -0.215      |
|    reward_value_loss    | 3.01        |
|    total_cost           | 732.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.2e+04    |
|    mean_ep_length       | 24.4        |
|    mean_reward          | -1.2e+04    |
|    true_cost            | 0.0481      |
| infos/                  |             |
|    cost                 | 0.0032      |
| rollout/                |             |
|    adjusted_reward      | 381         |
|    ep_len_mean          | 27.9        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 1643        |
|    iterations           | 7           |
|    time_elapsed         | 43          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.01536724  |
|    average_cost         | 0.054296874 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.505      |
|    cost_value_loss      | 0.045       |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -1.04       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.641       |
|    mean_cost_advantages | -0.16106138 |
|    mean_reward_advan... | 1.0840248   |
|    n_updates            | 60          |
|    nu                   | 1.4         |
|    nu_loss              | -0.0723     |
|    policy_gradient_loss | -0.00927    |
|    reward_explained_... | 0.316       |
|    reward_value_loss    | 1.72        |
|    total_cost           | 556.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.01e+03   |
|    mean_ep_length       | 21.2        |
|    mean_reward          | -4.01e+03   |
|    true_cost            | 0.0402      |
| infos/                  |             |
|    cost                 | 0.00664     |
| rollout/                |             |
|    adjusted_reward      | 498         |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 1541        |
|    iterations           | 8           |
|    time_elapsed         | 53          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.015870083 |
|    average_cost         | 0.04814453  |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0376     |
|    cost_value_loss      | 0.0311      |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.972      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.391       |
|    mean_cost_advantages | -0.11083098 |
|    mean_reward_advan... | 0.17991135  |
|    n_updates            | 70          |
|    nu                   | 1.46        |
|    nu_loss              | -0.0673     |
|    policy_gradient_loss | -0.00761    |
|    reward_explained_... | 0.481       |
|    reward_value_loss    | 0.683       |
|    total_cost           | 493.0       |
-----------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0266      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 551         |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 1449        |
|    iterations           | 9           |
|    time_elapsed         | 63          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.016745834 |
|    average_cost         | 0.040234376 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.222      |
|    cost_value_loss      | 0.0252      |
|    early_stop_epoch     | 7           |
|    entropy_loss         | -0.907      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0573      |
|    mean_cost_advantages | -0.07054953 |
|    mean_reward_advan... | -0.3141953  |
|    n_updates            | 80          |
|    nu                   | 1.52        |
|    nu_loss              | -0.0588     |
|    policy_gradient_loss | -0.00715    |
|    reward_explained_... | 0.572       |
|    reward_value_loss    | 0.217       |
|    total_cost           | 412.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 13.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0125       |
| infos/                  |              |
|    cost                 | 0.00532      |
| rollout/                |              |
|    adjusted_reward      | 632          |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1453         |
|    iterations           | 10           |
|    time_elapsed         | 70           |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.016275352  |
|    average_cost         | 0.0265625    |
|    clip_fraction        | 0.139        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.119       |
|    cost_value_loss      | 0.0166       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.815       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.039        |
|    mean_cost_advantages | -0.036415003 |
|    mean_reward_advan... | -0.49969214  |
|    n_updates            | 90           |
|    nu                   | 1.58         |
|    nu_loss              | -0.0404      |
|    policy_gradient_loss | -0.0051      |
|    reward_explained_... | 0.476        |
|    reward_value_loss    | 0.111        |
|    total_cost           | 272.0        |
------------------------------------------
Mean reward: 9993.305556 +/- 0.877532.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 01.55 minutes[0m
