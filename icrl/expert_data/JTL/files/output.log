[32;1mConfigured folder ./cpg/wandb/run-20220608_105414-1chccibb/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -1.12e+06 |
|    mean_ep_length   | 173       |
|    mean_reward      | -1.12e+06 |
|    true_cost        | 0.662     |
| infos/              |           |
|    cost             | 0.0331    |
| rollout/            |           |
|    adjusted_reward  | 1.52      |
|    ep_len_mean      | 168       |
|    ep_rew_mean      | 3.18e+03  |
| time/               |           |
|    fps              | 530       |
|    iterations       | 1         |
|    time_elapsed     | 3         |
|    total_timesteps  | 2048      |
-----------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.54e+05    |
|    mean_ep_length       | 124          |
|    mean_reward          | -8.54e+05    |
|    true_cost            | 0.604        |
| infos/                  |              |
|    cost                 | 0.0255       |
| rollout/                |              |
|    adjusted_reward      | 40.1         |
|    ep_len_mean          | 141          |
|    ep_rew_mean          | 5.96e+03     |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 2            |
|    time_elapsed         | 8            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0100068925 |
|    average_cost         | 0.6616211    |
|    clip_fraction        | 0.0858       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.74        |
|    cost_value_loss      | 0.407        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.38        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.552        |
|    mean_cost_advantages | 0.61752117   |
|    mean_reward_advan... | 0.13346106   |
|    n_updates            | 10           |
|    nu                   | 1.06         |
|    nu_loss              | -0.662       |
|    policy_gradient_loss | -0.00679     |
|    reward_explained_... | -20.5        |
|    reward_value_loss    | 1.68         |
|    total_cost           | 1355.0       |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.82e+05   |
|    mean_ep_length       | 99.6        |
|    mean_reward          | -3.82e+05   |
|    true_cost            | 0.552       |
| infos/                  |             |
|    cost                 | 0.0162      |
| rollout/                |             |
|    adjusted_reward      | 174         |
|    ep_len_mean          | 89.1        |
|    ep_rew_mean          | 8.02e+03    |
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 3           |
|    time_elapsed         | 12          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.015874377 |
|    average_cost         | 0.6035156   |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.21       |
|    cost_value_loss      | 0.0986      |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -1.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.19        |
|    mean_cost_advantages | 0.3026318   |
|    mean_reward_advan... | 0.9626397   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.642      |
|    policy_gradient_loss | -0.00871    |
|    reward_explained_... | -10.8       |
|    reward_value_loss    | 3.46        |
|    total_cost           | 1236.0      |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.56e+05   |
|    mean_ep_length       | 67.6        |
|    mean_reward          | -2.56e+05   |
|    true_cost            | 0.33        |
| infos/                  |             |
|    cost                 | 0.0153      |
| rollout/                |             |
|    adjusted_reward      | 264         |
|    ep_len_mean          | 55.2        |
|    ep_rew_mean          | 9.57e+03    |
| time/                   |             |
|    fps                  | 487         |
|    iterations           | 4           |
|    time_elapsed         | 16          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.018279295 |
|    average_cost         | 0.5522461   |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.48       |
|    cost_value_loss      | 0.121       |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -1.29       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.94        |
|    mean_cost_advantages | 0.002600029 |
|    mean_reward_advan... | 2.3467956   |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.625      |
|    policy_gradient_loss | -0.014      |
|    reward_explained_... | -2.64       |
|    reward_value_loss    | 4.89        |
|    total_cost           | 1131.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -6e+04      |
|    mean_ep_length       | 24.4        |
|    mean_reward          | -6e+04      |
|    true_cost            | 0.303       |
| infos/                  |             |
|    cost                 | 0.0244      |
| rollout/                |             |
|    adjusted_reward      | 438         |
|    ep_len_mean          | 28.7        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 5           |
|    time_elapsed         | 20          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.018445607 |
|    average_cost         | 0.33007812  |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.232       |
|    cost_value_loss      | 0.0911      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.21       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.92        |
|    mean_cost_advantages | -0.19666187 |
|    mean_reward_advan... | 1.9021521   |
|    n_updates            | 40          |
|    nu                   | 1.27        |
|    nu_loss              | -0.396      |
|    policy_gradient_loss | -0.0119     |
|    reward_explained_... | 0.563       |
|    reward_value_loss    | 3.06        |
|    total_cost           | 676.0       |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.2e+04    |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -4.2e+04    |
|    true_cost            | 0.282       |
| infos/                  |             |
|    cost                 | 0.0174      |
| rollout/                |             |
|    adjusted_reward      | 508         |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 500         |
|    iterations           | 6           |
|    time_elapsed         | 24          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.0177912   |
|    average_cost         | 0.30322266  |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.344       |
|    cost_value_loss      | 0.0545      |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.06       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.322       |
|    mean_cost_advantages | -0.21419892 |
|    mean_reward_advan... | 0.9232082   |
|    n_updates            | 50          |
|    nu                   | 1.34        |
|    nu_loss              | -0.384      |
|    policy_gradient_loss | -0.0126     |
|    reward_explained_... | 0.704       |
|    reward_value_loss    | 1.02        |
|    total_cost           | 621.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.4e+04    |
|    mean_ep_length       | 17.4        |
|    mean_reward          | -3.4e+04    |
|    true_cost            | 0.129       |
| infos/                  |             |
|    cost                 | 0.0058      |
| rollout/                |             |
|    adjusted_reward      | 585         |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 485         |
|    iterations           | 7           |
|    time_elapsed         | 29          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.024479682 |
|    average_cost         | 0.28222656  |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.369       |
|    cost_value_loss      | 0.0276      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.939      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0559      |
|    mean_cost_advantages | -0.1294063  |
|    mean_reward_advan... | -0.11612703 |
|    n_updates            | 60          |
|    nu                   | 1.4         |
|    nu_loss              | -0.377      |
|    policy_gradient_loss | -0.00696    |
|    reward_explained_... | 0.493       |
|    reward_value_loss    | 0.315       |
|    total_cost           | 578.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1e+04       |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -1e+04       |
|    true_cost            | 0.102        |
| infos/                  |              |
|    cost                 | 0.0012       |
| rollout/                |              |
|    adjusted_reward      | 671          |
|    ep_len_mean          | 15.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 8            |
|    time_elapsed         | 35           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.014480308  |
|    average_cost         | 0.12890625   |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.466        |
|    cost_value_loss      | 0.00681      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.797       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00976      |
|    mean_cost_advantages | -0.104647465 |
|    mean_reward_advan... | -0.48840976  |
|    n_updates            | 70           |
|    nu                   | 1.47         |
|    nu_loss              | -0.181       |
|    policy_gradient_loss | -0.00533     |
|    reward_explained_... | 0.554        |
|    reward_value_loss    | 0.0372       |
|    total_cost           | 264.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 3.99e+03    |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 3.99e+03    |
|    true_cost            | 0.0547      |
| infos/                  |             |
|    cost                 | 0.00431     |
| rollout/                |             |
|    adjusted_reward      | 711         |
|    ep_len_mean          | 14.4        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 462         |
|    iterations           | 9           |
|    time_elapsed         | 39          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014186072 |
|    average_cost         | 0.10205078  |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0763     |
|    cost_value_loss      | 0.00483     |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.661      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00733    |
|    mean_cost_advantages | -0.03700517 |
|    mean_reward_advan... | -0.4877427  |
|    n_updates            | 80          |
|    nu                   | 1.53        |
|    nu_loss              | -0.15       |
|    policy_gradient_loss | -0.00678    |
|    reward_explained_... | 0.425       |
|    reward_value_loss    | 0.0209      |
|    total_cost           | 209.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 3.99e+03     |
|    mean_ep_length       | 13.2         |
|    mean_reward          | -2.01e+03    |
|    true_cost            | 0.0698       |
| infos/                  |              |
|    cost                 | 0.00255      |
| rollout/                |              |
|    adjusted_reward      | 742          |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 10           |
|    time_elapsed         | 43           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.016846536  |
|    average_cost         | 0.0546875    |
|    clip_fraction        | 0.0898       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.953       |
|    cost_value_loss      | 0.00245      |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.571       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00345     |
|    mean_cost_advantages | -0.025068436 |
|    mean_reward_advan... | -0.36404127  |
|    n_updates            | 90           |
|    nu                   | 1.59         |
|    nu_loss              | -0.0837      |
|    policy_gradient_loss | -0.0055      |
|    reward_explained_... | 0.519        |
|    reward_value_loss    | 0.0185       |
|    total_cost           | 112.0        |
------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | -5.07          |
|    true_cost            | 0.0518         |
| infos/                  |                |
|    cost                 | 0.00656        |
| rollout/                |                |
|    adjusted_reward      | 785            |
|    ep_len_mean          | 12.8           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 466            |
|    iterations           | 11             |
|    time_elapsed         | 48             |
|    total_timesteps      | 22528          |
| train/                  |                |
|    approx_kl            | 0.015698815    |
|    average_cost         | 0.06982422     |
|    clip_fraction        | 0.127          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -3.06          |
|    cost_value_loss      | 0.00308        |
|    early_stop_epoch     | 9              |
|    entropy_loss         | -0.489         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.0073        |
|    mean_cost_advantages | -0.00049804104 |
|    mean_reward_advan... | -0.28516376    |
|    n_updates            | 100            |
|    nu                   | 1.64           |
|    nu_loss              | -0.111         |
|    policy_gradient_loss | -0.00789       |
|    reward_explained_... | 0.626          |
|    reward_value_loss    | 0.00901        |
|    total_cost           | 143.0          |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 3.99e+03     |
|    mean_ep_length       | 13.6         |
|    mean_reward          | -1.6e+04     |
|    true_cost            | 0.0518       |
| infos/                  |              |
|    cost                 | 0.00406      |
| rollout/                |              |
|    adjusted_reward      | 823          |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 465          |
|    iterations           | 12           |
|    time_elapsed         | 52           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.014559504  |
|    average_cost         | 0.051757812  |
|    clip_fraction        | 0.0792       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.62        |
|    cost_value_loss      | 0.00208      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.41        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00388     |
|    mean_cost_advantages | -0.012198301 |
|    mean_reward_advan... | -0.19852589  |
|    n_updates            | 110          |
|    nu                   | 1.7          |
|    nu_loss              | -0.0851      |
|    policy_gradient_loss | -0.00548     |
|    reward_explained_... | 0.764        |
|    reward_value_loss    | 0.0044       |
|    total_cost           | 106.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.99e+03      |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -8e+03        |
|    true_cost            | 0.0527        |
| infos/                  |               |
|    cost                 | 0.00139       |
| rollout/                |               |
|    adjusted_reward      | 825           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 465           |
|    iterations           | 13            |
|    time_elapsed         | 57            |
|    total_timesteps      | 26624         |
| train/                  |               |
|    approx_kl            | 0.004757371   |
|    average_cost         | 0.051757812   |
|    clip_fraction        | 0.0619        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.19         |
|    cost_value_loss      | 0.00213       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.348        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00603      |
|    mean_cost_advantages | -0.0038352872 |
|    mean_reward_advan... | -0.16190784   |
|    n_updates            | 120           |
|    nu                   | 1.74          |
|    nu_loss              | -0.0877       |
|    policy_gradient_loss | -0.00313      |
|    reward_explained_... | 0.898         |
|    reward_value_loss    | 0.00242       |
|    total_cost           | 106.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 3.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 3.99e+03     |
|    true_cost            | 0.0498       |
| infos/                  |              |
|    cost                 | 0.00429      |
| rollout/                |              |
|    adjusted_reward      | 828          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 457          |
|    iterations           | 14           |
|    time_elapsed         | 62           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0043760473 |
|    average_cost         | 0.052734375  |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.05        |
|    cost_value_loss      | 0.0022       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.312       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00085      |
|    mean_cost_advantages | -0.006268626 |
|    mean_reward_advan... | -0.13993248  |
|    n_updates            | 130          |
|    nu                   | 1.79         |
|    nu_loss              | -0.092       |
|    policy_gradient_loss | -0.00168     |
|    reward_explained_... | 0.935        |
|    reward_value_loss    | 0.00155      |
|    total_cost           | 108.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0552        |
| infos/                  |               |
|    cost                 | 0.00441       |
| rollout/                |               |
|    adjusted_reward      | 827           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 441           |
|    iterations           | 15            |
|    time_elapsed         | 69            |
|    total_timesteps      | 30720         |
| train/                  |               |
|    approx_kl            | 0.0047398307  |
|    average_cost         | 0.049804688   |
|    clip_fraction        | 0.0462        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.25         |
|    cost_value_loss      | 0.00205       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.292        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00382      |
|    mean_cost_advantages | -0.0033954503 |
|    mean_reward_advan... | -0.115572624  |
|    n_updates            | 140           |
|    nu                   | 1.84          |
|    nu_loss              | -0.0892       |
|    policy_gradient_loss | -0.00211      |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 0.00108       |
|    total_cost           | 102.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 1.99e+03      |
|    true_cost            | 0.061         |
| infos/                  |               |
|    cost                 | 0.00753       |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 439           |
|    iterations           | 16            |
|    time_elapsed         | 74            |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 0.0023198512  |
|    average_cost         | 0.05517578    |
|    clip_fraction        | 0.0329        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.99         |
|    cost_value_loss      | 0.00211       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.274        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00665      |
|    mean_cost_advantages | -0.0007564643 |
|    mean_reward_advan... | -0.097347066  |
|    n_updates            | 150           |
|    nu                   | 1.88          |
|    nu_loss              | -0.101        |
|    policy_gradient_loss | -0.0014       |
|    reward_explained_... | 0.969         |
|    reward_value_loss    | 0.000742      |
|    total_cost           | 113.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0381       |
| infos/                  |              |
|    cost                 | 0.00309      |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 436          |
|    iterations           | 17           |
|    time_elapsed         | 79           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.007038669  |
|    average_cost         | 0.061035156  |
|    clip_fraction        | 0.079        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.51        |
|    cost_value_loss      | 0.00261      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.26        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0113       |
|    mean_cost_advantages | 0.002180068  |
|    mean_reward_advan... | -0.078059584 |
|    n_updates            | 160          |
|    nu                   | 1.92         |
|    nu_loss              | -0.115       |
|    policy_gradient_loss | -0.0028      |
|    reward_explained_... | 0.974        |
|    reward_value_loss    | 0.000578     |
|    total_cost           | 125.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 5.99e+03      |
|    true_cost            | 0.0352        |
| infos/                  |               |
|    cost                 | 0.00158       |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 435           |
|    iterations           | 18            |
|    time_elapsed         | 84            |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.0048409887  |
|    average_cost         | 0.038085938   |
|    clip_fraction        | 0.0319        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.64         |
|    cost_value_loss      | 0.00181       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.229        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00283       |
|    mean_cost_advantages | -0.0065975264 |
|    mean_reward_advan... | -0.062002815  |
|    n_updates            | 170           |
|    nu                   | 1.96          |
|    nu_loss              | -0.0732       |
|    policy_gradient_loss | -0.0015       |
|    reward_explained_... | 0.98          |
|    reward_value_loss    | 0.000433      |
|    total_cost           | 78.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.0308         |
| infos/                  |                |
|    cost                 | 0.00161        |
| rollout/                |                |
|    adjusted_reward      | 839            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 433            |
|    iterations           | 19             |
|    time_elapsed         | 89             |
|    total_timesteps      | 38912          |
| train/                  |                |
|    approx_kl            | 0.0011195084   |
|    average_cost         | 0.03515625     |
|    clip_fraction        | 0.00957        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -6.55          |
|    cost_value_loss      | 0.00171        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.201         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00119       |
|    mean_cost_advantages | -0.00029741484 |
|    mean_reward_advan... | -0.05311544    |
|    n_updates            | 180            |
|    nu                   | 2              |
|    nu_loss              | -0.069         |
|    policy_gradient_loss | -0.000549      |
|    reward_explained_... | 0.987          |
|    reward_value_loss    | 0.000291       |
|    total_cost           | 72.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0366        |
| infos/                  |               |
|    cost                 | 0.00495       |
| rollout/                |               |
|    adjusted_reward      | 828           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 427           |
|    iterations           | 20            |
|    time_elapsed         | 95            |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00096482097 |
|    average_cost         | 0.030761719   |
|    clip_fraction        | 0.018         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.17         |
|    cost_value_loss      | 0.00152       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.195        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000172      |
|    mean_cost_advantages | -0.0040662573 |
|    mean_reward_advan... | -0.047016606  |
|    n_updates            | 190           |
|    nu                   | 2.04          |
|    nu_loss              | -0.0615       |
|    policy_gradient_loss | -0.000323     |
|    reward_explained_... | 0.992         |
|    reward_value_loss    | 0.000185      |
|    total_cost           | 63.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1.99e+03      |
|    true_cost            | 0.0249        |
| infos/                  |               |
|    cost                 | 0.00252       |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 424           |
|    iterations           | 21            |
|    time_elapsed         | 101           |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | 0.0037085642  |
|    average_cost         | 0.036621094   |
|    clip_fraction        | 0.0417        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.68         |
|    cost_value_loss      | 0.00188       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.186        |
|    learning_rate        | 0.0003        |
|    loss                 | -5.26e-07     |
|    mean_cost_advantages | -0.0014826611 |
|    mean_reward_advan... | -0.042776123  |
|    n_updates            | 200           |
|    nu                   | 2.07          |
|    nu_loss              | -0.0746       |
|    policy_gradient_loss | -0.00179      |
|    reward_explained_... | 0.987         |
|    reward_value_loss    | 0.000219      |
|    total_cost           | 75.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0308        |
| infos/                  |               |
|    cost                 | 0.00687       |
| rollout/                |               |
|    adjusted_reward      | 837           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 423           |
|    iterations           | 22            |
|    time_elapsed         | 106           |
|    total_timesteps      | 45056         |
| train/                  |               |
|    approx_kl            | 0.0036384037  |
|    average_cost         | 0.024902344   |
|    clip_fraction        | 0.0251        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.69         |
|    cost_value_loss      | 0.00139       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.187        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00183       |
|    mean_cost_advantages | -0.0090070665 |
|    mean_reward_advan... | -0.03535012   |
|    n_updates            | 210           |
|    nu                   | 2.11          |
|    nu_loss              | -0.0516       |
|    policy_gradient_loss | -0.000463     |
|    reward_explained_... | 0.991         |
|    reward_value_loss    | 0.00016       |
|    total_cost           | 51.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0234        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 422           |
|    iterations           | 23            |
|    time_elapsed         | 111           |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.0012956356  |
|    average_cost         | 0.030761719   |
|    clip_fraction        | 0.0228        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -28.9         |
|    cost_value_loss      | 0.00169       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.176        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00369       |
|    mean_cost_advantages | 0.00048004003 |
|    mean_reward_advan... | -0.03219235   |
|    n_updates            | 220           |
|    nu                   | 2.14          |
|    nu_loss              | -0.0648       |
|    policy_gradient_loss | -0.000608     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 0.000101      |
|    total_cost           | 63.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0205       |
| infos/                  |              |
|    cost                 | 0.00178      |
| rollout/                |              |
|    adjusted_reward      | 837          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 421          |
|    iterations           | 24           |
|    time_elapsed         | 116          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0004912356 |
|    average_cost         | 0.0234375    |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.74        |
|    cost_value_loss      | 0.00141      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.167       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00365      |
|    mean_cost_advantages | -0.008119367 |
|    mean_reward_advan... | -0.028054168 |
|    n_updates            | 230          |
|    nu                   | 2.17         |
|    nu_loss              | -0.0501      |
|    policy_gradient_loss | -0.000371    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 9.33e-05     |
|    total_cost           | 48.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 5.99e+03     |
|    true_cost            | 0.00781      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 420          |
|    iterations           | 25           |
|    time_elapsed         | 121          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0018183496 |
|    average_cost         | 0.020507812  |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -7.35        |
|    cost_value_loss      | 0.00131      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.162       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000996     |
|    mean_cost_advantages | -0.008345356 |
|    mean_reward_advan... | -0.023633257 |
|    n_updates            | 240          |
|    nu                   | 2.2          |
|    nu_loss              | -0.0445      |
|    policy_gradient_loss | -0.000409    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 7.41e-05     |
|    total_cost           | 42.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0117       |
| infos/                  |              |
|    cost                 | 0.00184      |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 419          |
|    iterations           | 26           |
|    time_elapsed         | 126          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0029798755 |
|    average_cost         | 0.0078125    |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.7         |
|    cost_value_loss      | 0.000575     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.154       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000169     |
|    mean_cost_advantages | -0.005343277 |
|    mean_reward_advan... | -0.021459855 |
|    n_updates            | 250          |
|    nu                   | 2.22         |
|    nu_loss              | -0.0172      |
|    policy_gradient_loss | -6.14e-05    |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 5.18e-05     |
|    total_cost           | 16.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0156        |
| infos/                  |               |
|    cost                 | 0.00188       |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 418           |
|    iterations           | 27            |
|    time_elapsed         | 132           |
|    total_timesteps      | 55296         |
| train/                  |               |
|    approx_kl            | 0.0014456506  |
|    average_cost         | 0.01171875    |
|    clip_fraction        | 0.0153        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.15         |
|    cost_value_loss      | 0.000838      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.142        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00306      |
|    mean_cost_advantages | -0.0017970924 |
|    mean_reward_advan... | -0.018224522  |
|    n_updates            | 260           |
|    nu                   | 2.25          |
|    nu_loss              | -0.0261       |
|    policy_gradient_loss | -0.000423     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 4.73e-05      |
|    total_cost           | 24.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00586      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 417          |
|    iterations           | 28           |
|    time_elapsed         | 137          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.004383419  |
|    average_cost         | 0.015625     |
|    clip_fraction        | 0.0448       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -11.9        |
|    cost_value_loss      | 0.00112      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.14        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00048     |
|    mean_cost_advantages | 0.003901316  |
|    mean_reward_advan... | -0.017148273 |
|    n_updates            | 270          |
|    nu                   | 2.27         |
|    nu_loss              | -0.0351      |
|    policy_gradient_loss | -0.000617    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 3.35e-05     |
|    total_cost           | 32.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 417           |
|    iterations           | 29            |
|    time_elapsed         | 142           |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00044811622 |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0.0108        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.11         |
|    cost_value_loss      | 0.000518      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.132        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000187     |
|    mean_cost_advantages | -0.0058414578 |
|    mean_reward_advan... | -0.015486151  |
|    n_updates            | 280           |
|    nu                   | 2.29          |
|    nu_loss              | -0.0133       |
|    policy_gradient_loss | -0.00028      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.66e-05      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 417           |
|    iterations           | 30            |
|    time_elapsed         | 147           |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00023761595 |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0.0102        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -10.5         |
|    cost_value_loss      | 0.000496      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.124        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00215       |
|    mean_cost_advantages | -0.004994416  |
|    mean_reward_advan... | -0.013194458  |
|    n_updates            | 290           |
|    nu                   | 2.32          |
|    nu_loss              | -0.0134       |
|    policy_gradient_loss | 0.000175      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.48e-05      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 5.99e+03      |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 417           |
|    iterations           | 31            |
|    time_elapsed         | 152           |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.0027731084  |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0.014         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -15.2         |
|    cost_value_loss      | 0.000504      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.109        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00177      |
|    mean_cost_advantages | -0.0017401793 |
|    mean_reward_advan... | -0.013714442  |
|    n_updates            | 300           |
|    nu                   | 2.33          |
|    nu_loss              | -0.0136       |
|    policy_gradient_loss | -0.000162     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 4e-05         |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00391       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 416           |
|    iterations           | 32            |
|    time_elapsed         | 157           |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.0008259593  |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0.00552       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -16.3         |
|    cost_value_loss      | 0.000525      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.106        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000304      |
|    mean_cost_advantages | -0.0023522591 |
|    mean_reward_advan... | -0.012092626  |
|    n_updates            | 310           |
|    nu                   | 2.35          |
|    nu_loss              | -0.0137       |
|    policy_gradient_loss | 0.000152      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.85e-05      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0137        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 416           |
|    iterations           | 33            |
|    time_elapsed         | 162           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.0015323723  |
|    average_cost         | 0.00390625    |
|    clip_fraction        | 0.0112        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -12.9         |
|    cost_value_loss      | 0.00037       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.103        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00322       |
|    mean_cost_advantages | -0.0005446453 |
|    mean_reward_advan... | -0.011901158  |
|    n_updates            | 320           |
|    nu                   | 2.37          |
|    nu_loss              | -0.00919      |
|    policy_gradient_loss | 5.05e-06      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.87e-05      |
|    total_cost           | 8.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00586      |
| infos/                  |              |
|    cost                 | 0.00208      |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 417          |
|    iterations           | 34           |
|    time_elapsed         | 166          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.013552581  |
|    average_cost         | 0.013671875  |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -46.5        |
|    cost_value_loss      | 0.0013       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.116       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000146    |
|    mean_cost_advantages | 0.002362681  |
|    mean_reward_advan... | -0.010978406 |
|    n_updates            | 330          |
|    nu                   | 2.38         |
|    nu_loss              | -0.0324      |
|    policy_gradient_loss | -0.000933    |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 7.08e-05     |
|    total_cost           | 28.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 417           |
|    iterations           | 35            |
|    time_elapsed         | 171           |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.0010610924  |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0.0174        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.74         |
|    cost_value_loss      | 0.000572      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.115        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00204      |
|    mean_cost_advantages | -0.0057651075 |
|    mean_reward_advan... | -0.008684434  |
|    n_updates            | 340           |
|    nu                   | 2.4           |
|    nu_loss              | -0.014        |
|    policy_gradient_loss | 6.03e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.61e-05      |
|    total_cost           | 12.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00586      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 417          |
|    iterations           | 36           |
|    time_elapsed         | 176          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0014096354 |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.2         |
|    cost_value_loss      | 0.000205     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.11        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000798    |
|    mean_cost_advantages | -0.004388715 |
|    mean_reward_advan... | -0.008982522 |
|    n_updates            | 350          |
|    nu                   | 2.41         |
|    nu_loss              | -0.00469     |
|    policy_gradient_loss | 3.51e-05     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.31e-05     |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 418           |
|    iterations           | 37            |
|    time_elapsed         | 181           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00033577136 |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0.0124        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -60.6         |
|    cost_value_loss      | 0.000605      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.106        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000831      |
|    mean_cost_advantages | 0.0008796426  |
|    mean_reward_advan... | -0.007462308  |
|    n_updates            | 360           |
|    nu                   | 2.43          |
|    nu_loss              | -0.0141       |
|    policy_gradient_loss | 0.000288      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.3e-05       |
|    total_cost           | 12.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00391      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 418          |
|    iterations           | 38           |
|    time_elapsed         | 185          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0008020107 |
|    average_cost         | 0.005859375  |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -12.4        |
|    cost_value_loss      | 0.000604     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.107       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000274     |
|    mean_cost_advantages | 0.005237511  |
|    mean_reward_advan... | -0.008286441 |
|    n_updates            | 370          |
|    nu                   | 2.44         |
|    nu_loss              | -0.0142      |
|    policy_gradient_loss | 0.0005       |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.43e-05     |
|    total_cost           | 12.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0117       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 419          |
|    iterations           | 39           |
|    time_elapsed         | 190          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0025173316 |
|    average_cost         | 0.00390625   |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -14.1        |
|    cost_value_loss      | 0.000423     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.106       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00598      |
|    mean_cost_advantages | -0.001878085 |
|    mean_reward_advan... | -0.007933571 |
|    n_updates            | 380          |
|    nu                   | 2.45         |
|    nu_loss              | -0.00952     |
|    policy_gradient_loss | -0.000394    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.29e-05     |
|    total_cost           | 8.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 419           |
|    iterations           | 40            |
|    time_elapsed         | 195           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.0037554023  |
|    average_cost         | 0.01171875    |
|    clip_fraction        | 0.0296        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -44.6         |
|    cost_value_loss      | 0.00122       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.11         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00106       |
|    mean_cost_advantages | 0.0034096406  |
|    mean_reward_advan... | -0.0073882504 |
|    n_updates            | 390           |
|    nu                   | 2.46          |
|    nu_loss              | -0.0287       |
|    policy_gradient_loss | -0.000219     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.67e-05      |
|    total_cost           | 24.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00391       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 418           |
|    iterations           | 41            |
|    time_elapsed         | 200           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.0007401346  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0168        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.04         |
|    cost_value_loss      | 0.000121      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.104        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00814       |
|    mean_cost_advantages | -0.0035919175 |
|    mean_reward_advan... | -0.0070349947 |
|    n_updates            | 400           |
|    nu                   | 2.47          |
|    nu_loss              | -0.0024       |
|    policy_gradient_loss | -0.000146     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.01e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00684       |
| infos/                  |               |
|    cost                 | 0.00229       |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 419           |
|    iterations           | 42            |
|    time_elapsed         | 204           |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.003480466   |
|    average_cost         | 0.00390625    |
|    clip_fraction        | 0.0286        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -23.7         |
|    cost_value_loss      | 0.000455      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.104        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000915      |
|    mean_cost_advantages | 0.0017071902  |
|    mean_reward_advan... | -0.0071818875 |
|    n_updates            | 410           |
|    nu                   | 2.48          |
|    nu_loss              | -0.00965      |
|    policy_gradient_loss | -0.000744     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.89e-05      |
|    total_cost           | 8.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00684      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 420          |
|    iterations           | 43           |
|    time_elapsed         | 209          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0023460933 |
|    average_cost         | 0.0068359375 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -67.4        |
|    cost_value_loss      | 0.000843     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.107       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000557    |
|    mean_cost_advantages | 0.0049515767 |
|    mean_reward_advan... | -0.006202836 |
|    n_updates            | 420          |
|    nu                   | 2.49         |
|    nu_loss              | -0.0169      |
|    policy_gradient_loss | -0.000973    |
|    reward_explained_... | 0.995        |
|    reward_value_loss    | 0.000144     |
|    total_cost           | 14.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00391       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 420           |
|    iterations           | 44            |
|    time_elapsed         | 214           |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.0021344996  |
|    average_cost         | 0.0068359375  |
|    clip_fraction        | 0.0228        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -28.6         |
|    cost_value_loss      | 0.000815      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.105        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.9e-05       |
|    mean_cost_advantages | 0.0039514825  |
|    mean_reward_advan... | -0.0051699523 |
|    n_updates            | 430           |
|    nu                   | 2.5           |
|    nu_loss              | -0.017        |
|    policy_gradient_loss | -0.000605     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 8.25e-05      |
|    total_cost           | 14.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 421           |
|    iterations           | 45            |
|    time_elapsed         | 218           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.0025474906  |
|    average_cost         | 0.00390625    |
|    clip_fraction        | 0.025         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.43         |
|    cost_value_loss      | 0.000488      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.103        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000956      |
|    mean_cost_advantages | -0.002624609  |
|    mean_reward_advan... | -0.0027249483 |
|    n_updates            | 440           |
|    nu                   | 2.51          |
|    nu_loss              | -0.00975      |
|    policy_gradient_loss | -0.000182     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 1.42e-05      |
|    total_cost           | 8.0           |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0.00239      |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 422          |
|    iterations           | 46           |
|    time_elapsed         | 223          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.01532452   |
|    average_cost         | 0.005859375  |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -43.3        |
|    cost_value_loss      | 0.000768     |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.108       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000937    |
|    mean_cost_advantages | 0.0025344228 |
|    mean_reward_advan... | -0.0058218   |
|    n_updates            | 450          |
|    nu                   | 2.51         |
|    nu_loss              | -0.0147      |
|    policy_gradient_loss | -0.00102     |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 6.83e-05     |
|    total_cost           | 12.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 422           |
|    iterations           | 47            |
|    time_elapsed         | 227           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.0006643728  |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0173        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.1          |
|    cost_value_loss      | 0.00026       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.109        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000234     |
|    mean_cost_advantages | -0.0047180406 |
|    mean_reward_advan... | -0.003082261  |
|    n_updates            | 460           |
|    nu                   | 2.52          |
|    nu_loss              | -0.00491      |
|    policy_gradient_loss | -6.16e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.99e-05      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 422           |
|    iterations           | 48            |
|    time_elapsed         | 232           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.003666495   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0417        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.11         |
|    cost_value_loss      | 0.000134      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.101        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000237      |
|    mean_cost_advantages | -0.0028401783 |
|    mean_reward_advan... | -0.0035381825 |
|    n_updates            | 470           |
|    nu                   | 2.53          |
|    nu_loss              | -0.00246      |
|    policy_gradient_loss | 3.23e-06      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.04e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 423          |
|    iterations           | 49           |
|    time_elapsed         | 236          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0031346744 |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -42.5        |
|    cost_value_loss      | 0.000136     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.105       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00355      |
|    mean_cost_advantages | -0.00269556  |
|    mean_reward_advan... | -0.005827746 |
|    n_updates            | 480          |
|    nu                   | 2.53         |
|    nu_loss              | -0.00247     |
|    policy_gradient_loss | 6.42e-05     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 5.51e-06     |
|    total_cost           | 2.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1.99e+03      |
|    true_cost            | 0.0117        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 423           |
|    iterations           | 50            |
|    time_elapsed         | 241           |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.013470165   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0388        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.227         |
|    cost_value_loss      | 1.63e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0924       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000562     |
|    mean_cost_advantages | -0.0023999657 |
|    mean_reward_advan... | -0.0037636836 |
|    n_updates            | 490           |
|    nu                   | 2.54          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000173     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 9.11e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 424           |
|    iterations           | 51            |
|    time_elapsed         | 246           |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 0.017520498   |
|    average_cost         | 0.01171875    |
|    clip_fraction        | 0.0207        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -796          |
|    cost_value_loss      | 0.00159       |
|    early_stop_epoch     | 8             |
|    entropy_loss         | -0.0935       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00213      |
|    mean_cost_advantages | 0.007151287   |
|    mean_reward_advan... | -0.0034774235 |
|    n_updates            | 500           |
|    nu                   | 2.54          |
|    nu_loss              | -0.0298       |
|    policy_gradient_loss | -0.000929     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 8.42e-05      |
|    total_cost           | 24.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00293        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 424            |
|    iterations           | 52             |
|    time_elapsed         | 250            |
|    total_timesteps      | 106496         |
| train/                  |                |
|    approx_kl            | 0.009681694    |
|    average_cost         | 0.0009765625   |
|    clip_fraction        | 0.028          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2             |
|    cost_value_loss      | 0.000147       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0858        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00489        |
|    mean_cost_advantages | -0.00019779889 |
|    mean_reward_advan... | -0.004287909   |
|    n_updates            | 510            |
|    nu                   | 2.55           |
|    nu_loss              | -0.00249       |
|    policy_gradient_loss | -4.83e-05      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 1.24e-05       |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 424           |
|    iterations           | 53            |
|    time_elapsed         | 255           |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.0017122432  |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.0156        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -63.5         |
|    cost_value_loss      | 0.000423      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0756       |
|    learning_rate        | 0.0003        |
|    loss                 | 9.87e-05      |
|    mean_cost_advantages | 0.0016821116  |
|    mean_reward_advan... | -0.0052180947 |
|    n_updates            | 520           |
|    nu                   | 2.56          |
|    nu_loss              | -0.00747      |
|    policy_gradient_loss | 0.000448      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 5.28e-06      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 425           |
|    iterations           | 54            |
|    time_elapsed         | 260           |
|    total_timesteps      | 110592        |
| train/                  |               |
|    approx_kl            | 0.0011690852  |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.06         |
|    cost_value_loss      | 0.000356      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0708       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.9e-05       |
|    mean_cost_advantages | -0.0017581915 |
|    mean_reward_advan... | -0.005062404  |
|    n_updates            | 530           |
|    nu                   | 2.56          |
|    nu_loss              | -0.00499      |
|    policy_gradient_loss | -0.000335     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 8.49e-05      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 424           |
|    iterations           | 55            |
|    time_elapsed         | 265           |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.0008365028  |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.00757       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -105          |
|    cost_value_loss      | 0.000442      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.059        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00106      |
|    mean_cost_advantages | 0.0018876395  |
|    mean_reward_advan... | -0.0019038095 |
|    n_updates            | 540           |
|    nu                   | 2.57          |
|    nu_loss              | -0.0075       |
|    policy_gradient_loss | -4.32e-05     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 9.32e-06      |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0.00261       |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 425           |
|    iterations           | 56            |
|    time_elapsed         | 269           |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.015078962   |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -47.9         |
|    cost_value_loss      | 0.000451      |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.0441       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00194      |
|    mean_cost_advantages | 0.00061393826 |
|    mean_reward_advan... | -0.0018440536 |
|    n_updates            | 550           |
|    nu                   | 2.57          |
|    nu_loss              | -0.00752      |
|    policy_gradient_loss | -0.000165     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 4.98e-05      |
|    total_cost           | 6.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 425            |
|    iterations           | 57             |
|    time_elapsed         | 274            |
|    total_timesteps      | 116736         |
| train/                  |                |
|    approx_kl            | 0.0019166556   |
|    average_cost         | 0.005859375    |
|    clip_fraction        | 0.00371        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -42            |
|    cost_value_loss      | 0.000896       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0323        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000923       |
|    mean_cost_advantages | 0.004803351    |
|    mean_reward_advan... | -0.00021460844 |
|    n_updates            | 560            |
|    nu                   | 2.57           |
|    nu_loss              | -0.0151        |
|    policy_gradient_loss | -0.000267      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.06e-05       |
|    total_cost           | 12.0           |
--------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 427           |
|    iterations           | 58            |
|    time_elapsed         | 278           |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 0.019530375   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0383        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.24         |
|    cost_value_loss      | 0.000164      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0482       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0122       |
|    mean_cost_advantages | 9.559415e-05  |
|    mean_reward_advan... | -0.0022584717 |
|    n_updates            | 570           |
|    nu                   | 2.58          |
|    nu_loss              | -0.00251      |
|    policy_gradient_loss | -0.000251     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 6.95e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 427           |
|    iterations           | 59            |
|    time_elapsed         | 282           |
|    total_timesteps      | 120832        |
| train/                  |               |
|    approx_kl            | 0.004139274   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0245        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -15.2         |
|    cost_value_loss      | 0.000164      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0635       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000893      |
|    mean_cost_advantages | 0.00039067055 |
|    mean_reward_advan... | -0.0037706234 |
|    n_updates            | 580           |
|    nu                   | 2.58          |
|    nu_loss              | -0.00252      |
|    policy_gradient_loss | 0.000208      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.84e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 426            |
|    iterations           | 60             |
|    time_elapsed         | 288            |
|    total_timesteps      | 122880         |
| train/                  |                |
|    approx_kl            | 0.001242979    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0278         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0309        |
|    cost_value_loss      | 1.49e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0682        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00062        |
|    mean_cost_advantages | -0.00029360503 |
|    mean_reward_advan... | -0.0024174913  |
|    n_updates            | 590            |
|    nu                   | 2.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000328       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 3.38e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 426           |
|    iterations           | 61            |
|    time_elapsed         | 292           |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.0028724233  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0258        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -178          |
|    cost_value_loss      | 0.000169      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.064        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00102       |
|    mean_cost_advantages | 0.00051544025 |
|    mean_reward_advan... | -0.0015450994 |
|    n_updates            | 600           |
|    nu                   | 2.59          |
|    nu_loss              | -0.00253      |
|    policy_gradient_loss | -7.45e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 6.31e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 426           |
|    iterations           | 62            |
|    time_elapsed         | 297           |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 0.005236076   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0302        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -87.1         |
|    cost_value_loss      | 0.000172      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0621       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000358      |
|    mean_cost_advantages | -0.00151847   |
|    mean_reward_advan... | -0.0014918104 |
|    n_updates            | 610           |
|    nu                   | 2.59          |
|    nu_loss              | -0.00253      |
|    policy_gradient_loss | 0.00049       |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 3.22e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 426           |
|    iterations           | 63            |
|    time_elapsed         | 302           |
|    total_timesteps      | 129024        |
| train/                  |               |
|    approx_kl            | 0.0005330487  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0156        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -12.9         |
|    cost_value_loss      | 0.000171      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0624       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000421      |
|    mean_cost_advantages | -0.0013318753 |
|    mean_reward_advan... | -0.0017839248 |
|    n_updates            | 620           |
|    nu                   | 2.6           |
|    nu_loss              | -0.00253      |
|    policy_gradient_loss | 0.000351      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 8.78e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 426           |
|    iterations           | 64            |
|    time_elapsed         | 307           |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 0.0013587627  |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0299        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -94.8         |
|    cost_value_loss      | 0.000349      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0702       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000213      |
|    mean_cost_advantages | -0.0009002609 |
|    mean_reward_advan... | -0.0021359539 |
|    n_updates            | 630           |
|    nu                   | 2.6           |
|    nu_loss              | -0.00507      |
|    policy_gradient_loss | 0.000454      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.16e-06      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0.00281       |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 426           |
|    iterations           | 65            |
|    time_elapsed         | 311           |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.0014940924  |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0289        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -49.3         |
|    cost_value_loss      | 0.000355      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0666       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00104      |
|    mean_cost_advantages | -0.002324928  |
|    mean_reward_advan... | -0.0022757621 |
|    n_updates            | 640           |
|    nu                   | 2.6           |
|    nu_loss              | -0.00507      |
|    policy_gradient_loss | 0.000473      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.95e-05      |
|    total_cost           | 4.0           |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 427           |
|    iterations           | 66            |
|    time_elapsed         | 316           |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 0.026568372   |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.0347        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -35.2         |
|    cost_value_loss      | 0.000685      |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.0846       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.002         |
|    mean_cost_advantages | 0.0013013801  |
|    mean_reward_advan... | -0.0030355742 |
|    n_updates            | 650           |
|    nu                   | 2.6           |
|    nu_loss              | -0.00762      |
|    policy_gradient_loss | -0.000491     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 9.98e-05      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 428           |
|    iterations           | 67            |
|    time_elapsed         | 320           |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 0.005583262   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.026         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0343       |
|    cost_value_loss      | 1.24e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0754       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00288      |
|    mean_cost_advantages | 2.9658331e-05 |
|    mean_reward_advan... | 0.0007769795  |
|    n_updates            | 660           |
|    nu                   | 2.61          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000478     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 2.56e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 428            |
|    iterations           | 68             |
|    time_elapsed         | 324            |
|    total_timesteps      | 139264         |
| train/                  |                |
|    approx_kl            | 0.015856903    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0357         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00594       |
|    cost_value_loss      | 3.72e-07       |
|    early_stop_epoch     | 5              |
|    entropy_loss         | -0.0802        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000629      |
|    mean_cost_advantages | -0.00049999997 |
|    mean_reward_advan... | 0.0029248612   |
|    n_updates            | 670            |
|    nu                   | 2.61           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000827      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.8e-05        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00391        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 825            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 429            |
|    iterations           | 69             |
|    time_elapsed         | 329            |
|    total_timesteps      | 141312         |
| train/                  |                |
|    approx_kl            | 0.0006061324   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0225         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.047         |
|    cost_value_loss      | 2.59e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0916        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000444       |
|    mean_cost_advantages | -0.00042292842 |
|    mean_reward_advan... | -0.0002106592  |
|    n_updates            | 680            |
|    nu                   | 2.61           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000311       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.26e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 429           |
|    iterations           | 70            |
|    time_elapsed         | 333           |
|    total_timesteps      | 143360        |
| train/                  |               |
|    approx_kl            | 0.015925078   |
|    average_cost         | 0.00390625    |
|    clip_fraction        | 0.0328        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -31.3         |
|    cost_value_loss      | 0.00107       |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.0914       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0006        |
|    mean_cost_advantages | 0.0043032337  |
|    mean_reward_advan... | -0.0026763668 |
|    n_updates            | 690           |
|    nu                   | 2.61          |
|    nu_loss              | -0.0102       |
|    policy_gradient_loss | -0.00131      |
|    reward_explained_... | 0.875         |
|    reward_value_loss    | 0.000549      |
|    total_cost           | 8.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 71           |
|    time_elapsed         | 338          |
|    total_timesteps      | 145408       |
| train/                  |              |
|    approx_kl            | 0.0034981868 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0213       |
|    cost_value_loss      | 9.08e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0741      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00277     |
|    mean_cost_advantages | -0.000992454 |
|    mean_reward_advan... | 0.0012644932 |
|    n_updates            | 700          |
|    nu                   | 2.61         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.000105     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.44e-05     |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 430           |
|    iterations           | 72            |
|    time_elapsed         | 342           |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 0.015756315   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0266        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0758       |
|    cost_value_loss      | 1.28e-07      |
|    early_stop_epoch     | 8             |
|    entropy_loss         | -0.0793       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00414      |
|    mean_cost_advantages | 0.0002403307  |
|    mean_reward_advan... | -0.0003202006 |
|    n_updates            | 710           |
|    nu                   | 2.62          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -7.27e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.56e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 430           |
|    iterations           | 73            |
|    time_elapsed         | 347           |
|    total_timesteps      | 149504        |
| train/                  |               |
|    approx_kl            | 0.003611097   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0343        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.23e+03     |
|    cost_value_loss      | 0.000201      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0843       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00313       |
|    mean_cost_advantages | 0.0006144836  |
|    mean_reward_advan... | -0.0004502065 |
|    n_updates            | 720           |
|    nu                   | 2.62          |
|    nu_loss              | -0.00256      |
|    policy_gradient_loss | -0.000106     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.85e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 430           |
|    iterations           | 74            |
|    time_elapsed         | 351           |
|    total_timesteps      | 151552        |
| train/                  |               |
|    approx_kl            | 0.001789612   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0167        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -11.3         |
|    cost_value_loss      | 0.000286      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0776       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00022       |
|    mean_cost_advantages | 0.0006573157  |
|    mean_reward_advan... | -0.0012141964 |
|    n_updates            | 730           |
|    nu                   | 2.62          |
|    nu_loss              | -0.00256      |
|    policy_gradient_loss | -0.00027      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.81e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 431           |
|    iterations           | 75            |
|    time_elapsed         | 356           |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 0.00024719478 |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.00889       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -69.5         |
|    cost_value_loss      | 0.000407      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0737       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000331     |
|    mean_cost_advantages | 0.0015489059  |
|    mean_reward_advan... | -0.0031621696 |
|    n_updates            | 740           |
|    nu                   | 2.62          |
|    nu_loss              | -0.00512      |
|    policy_gradient_loss | 0.00015       |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.83e-06      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 431           |
|    iterations           | 76            |
|    time_elapsed         | 361           |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 0.003470887   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0182        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -12.5         |
|    cost_value_loss      | 0.00021       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0646       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000885      |
|    mean_cost_advantages | -0.0016499038 |
|    mean_reward_advan... | -0.0009800022 |
|    n_updates            | 750           |
|    nu                   | 2.62          |
|    nu_loss              | -0.00256      |
|    policy_gradient_loss | 0.000164      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 1.18e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 77            |
|    time_elapsed         | 364           |
|    total_timesteps      | 157696        |
| train/                  |               |
|    approx_kl            | 0.021375101   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0477        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -310          |
|    cost_value_loss      | 0.00021       |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.0801       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000425      |
|    mean_cost_advantages | -0.0002782006 |
|    mean_reward_advan... | -0.001052945  |
|    n_updates            | 760           |
|    nu                   | 2.62          |
|    nu_loss              | -0.00256      |
|    policy_gradient_loss | 7.92e-07      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 5.78e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 78            |
|    time_elapsed         | 369           |
|    total_timesteps      | 159744        |
| train/                  |               |
|    approx_kl            | 0.0014064616  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.033         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.23          |
|    cost_value_loss      | 5.44e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0973       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000133      |
|    mean_cost_advantages | -0.001832192  |
|    mean_reward_advan... | -0.0006513594 |
|    n_updates            | 770           |
|    nu                   | 2.63          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000277     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.86e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 79            |
|    time_elapsed         | 374           |
|    total_timesteps      | 161792        |
| train/                  |               |
|    approx_kl            | 0.012289718   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0271        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.119         |
|    cost_value_loss      | 1.61e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0735       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.002        |
|    mean_cost_advantages | -0.0005374697 |
|    mean_reward_advan... | -0.0028579957 |
|    n_updates            | 780           |
|    nu                   | 2.63          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 3.57e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 7.92e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 432            |
|    iterations           | 80             |
|    time_elapsed         | 378            |
|    total_timesteps      | 163840         |
| train/                  |                |
|    approx_kl            | 0.032196015    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.04           |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0418        |
|    cost_value_loss      | 1.14e-07       |
|    early_stop_epoch     | 3              |
|    entropy_loss         | -0.0836        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00257       |
|    mean_cost_advantages | -0.00026864687 |
|    mean_reward_advan... | -0.0012147638  |
|    n_updates            | 790            |
|    nu                   | 2.63           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000655      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.02e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 81            |
|    time_elapsed         | 383           |
|    total_timesteps      | 165888        |
| train/                  |               |
|    approx_kl            | 0.0019528698  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0207        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -26.6         |
|    cost_value_loss      | 0.000323      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0835       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00104      |
|    mean_cost_advantages | 0.0010373185  |
|    mean_reward_advan... | -0.0025398144 |
|    n_updates            | 800           |
|    nu                   | 2.63          |
|    nu_loss              | -0.00257      |
|    policy_gradient_loss | -0.000661     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.67e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 82            |
|    time_elapsed         | 388           |
|    total_timesteps      | 167936        |
| train/                  |               |
|    approx_kl            | 0.008947141   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0464        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0603       |
|    cost_value_loss      | 8.85e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0976       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000191     |
|    mean_cost_advantages | -0.0029910726 |
|    mean_reward_advan... | -0.0028960642 |
|    n_updates            | 810           |
|    nu                   | 2.63          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000363     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.14e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 83            |
|    time_elapsed         | 393           |
|    total_timesteps      | 169984        |
| train/                  |               |
|    approx_kl            | 0.013437351   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0235        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -32.9         |
|    cost_value_loss      | 0.000319      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0678       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00175      |
|    mean_cost_advantages | 0.00041326584 |
|    mean_reward_advan... | -0.0045706667 |
|    n_updates            | 820           |
|    nu                   | 2.63          |
|    nu_loss              | -0.00257      |
|    policy_gradient_loss | -0.000575     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 6.52e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 84            |
|    time_elapsed         | 397           |
|    total_timesteps      | 172032        |
| train/                  |               |
|    approx_kl            | 0.0022639504  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0225        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -104          |
|    cost_value_loss      | 0.000229      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0696       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00112       |
|    mean_cost_advantages | -0.0039479723 |
|    mean_reward_advan... | 0.0011788392  |
|    n_updates            | 830           |
|    nu                   | 2.63          |
|    nu_loss              | -0.00257      |
|    policy_gradient_loss | -0.000282     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 2.49e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 85            |
|    time_elapsed         | 402           |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | -2.64775e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0215        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.178         |
|    cost_value_loss      | 5.49e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0711       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000183      |
|    mean_cost_advantages | -0.0018414522 |
|    mean_reward_advan... | 0.00060496863 |
|    n_updates            | 840           |
|    nu                   | 2.63          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000171     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 2.09e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 432            |
|    iterations           | 86             |
|    time_elapsed         | 407            |
|    total_timesteps      | 176128         |
| train/                  |                |
|    approx_kl            | -0.00058603333 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0153         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0804         |
|    cost_value_loss      | 9.36e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0704        |
|    learning_rate        | 0.0003         |
|    loss                 | 5.58e-05       |
|    mean_cost_advantages | -0.0003050096  |
|    mean_reward_advan... | 0.0012301597   |
|    n_updates            | 850            |
|    nu                   | 2.63           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 9.05e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.57e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 433           |
|    iterations           | 87            |
|    time_elapsed         | 411           |
|    total_timesteps      | 178176        |
| train/                  |               |
|    approx_kl            | 0.015928183   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0352        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -27.2         |
|    cost_value_loss      | 0.00035       |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0876       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0409        |
|    mean_cost_advantages | 0.001141197   |
|    mean_reward_advan... | -0.0008614036 |
|    n_updates            | 860           |
|    nu                   | 2.64          |
|    nu_loss              | -0.00257      |
|    policy_gradient_loss | 0.00249       |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 6.35e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 433           |
|    iterations           | 88            |
|    time_elapsed         | 415           |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 0.004166531   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0334        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0152       |
|    cost_value_loss      | 1.08e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0982       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000103     |
|    mean_cost_advantages | -0.0059112688 |
|    mean_reward_advan... | 0.002494226   |
|    n_updates            | 870           |
|    nu                   | 2.64          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000137      |
|    reward_explained_... | 0.988         |
|    reward_value_loss    | 1.73e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 433           |
|    iterations           | 89            |
|    time_elapsed         | 420           |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 0.0006250362  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0279        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.35          |
|    cost_value_loss      | 1.4e-07       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0904       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000306     |
|    mean_cost_advantages | -0.0015457829 |
|    mean_reward_advan... | -0.001664381  |
|    n_updates            | 880           |
|    nu                   | 2.64          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 1.81e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.41e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 433            |
|    iterations           | 90             |
|    time_elapsed         | 425            |
|    total_timesteps      | 184320         |
| train/                  |                |
|    approx_kl            | 0.0074343835   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0417         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.184          |
|    cost_value_loss      | 5.22e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0989        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00123        |
|    mean_cost_advantages | -0.00035980585 |
|    mean_reward_advan... | -0.0014693921  |
|    n_updates            | 890            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -7.79e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 7.5e-07        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 433            |
|    iterations           | 91             |
|    time_elapsed         | 429            |
|    total_timesteps      | 186368         |
| train/                  |                |
|    approx_kl            | 0.000538044    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0236         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0854         |
|    cost_value_loss      | 6.09e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.102         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00269        |
|    mean_cost_advantages | -0.00026436648 |
|    mean_reward_advan... | -0.000817762   |
|    n_updates            | 900            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.18e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.83e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 434            |
|    iterations           | 92             |
|    time_elapsed         | 434            |
|    total_timesteps      | 188416         |
| train/                  |                |
|    approx_kl            | 0.009408854    |
|    average_cost         | 0.0009765625   |
|    clip_fraction        | 0.0472         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -43.1          |
|    cost_value_loss      | 0.00037        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.105         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00355        |
|    mean_cost_advantages | 0.0011975219   |
|    mean_reward_advan... | -0.00033261802 |
|    n_updates            | 910            |
|    nu                   | 2.64           |
|    nu_loss              | -0.00258       |
|    policy_gradient_loss | -0.00051       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 5.74e-05       |
|    total_cost           | 2.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 433            |
|    iterations           | 93             |
|    time_elapsed         | 438            |
|    total_timesteps      | 190464         |
| train/                  |                |
|    approx_kl            | 0.006303532    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0544         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.114          |
|    cost_value_loss      | 1.67e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0945        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00625       |
|    mean_cost_advantages | 0.0034632243   |
|    mean_reward_advan... | -0.00070348557 |
|    n_updates            | 920            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0002        |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 6.59e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 434            |
|    iterations           | 94             |
|    time_elapsed         | 443            |
|    total_timesteps      | 192512         |
| train/                  |                |
|    approx_kl            | 0.0013808089   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0119         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0815         |
|    cost_value_loss      | 4.97e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0954        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00147        |
|    mean_cost_advantages | 0.00025077845  |
|    mean_reward_advan... | 0.000119447315 |
|    n_updates            | 930            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000152       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.59e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 433            |
|    iterations           | 95             |
|    time_elapsed         | 448            |
|    total_timesteps      | 194560         |
| train/                  |                |
|    approx_kl            | 0.008509349    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0436         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0397        |
|    cost_value_loss      | 5.26e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.085         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000479      |
|    mean_cost_advantages | -9.3240844e-05 |
|    mean_reward_advan... | -0.0005924149  |
|    n_updates            | 940            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000131      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.05e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 434            |
|    iterations           | 96             |
|    time_elapsed         | 452            |
|    total_timesteps      | 196608         |
| train/                  |                |
|    approx_kl            | 0.0031743732   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0166         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0365        |
|    cost_value_loss      | 2.86e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0792        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000502      |
|    mean_cost_advantages | -0.00011365609 |
|    mean_reward_advan... | -0.00069184834 |
|    n_updates            | 950            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000271       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 5.05e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 434            |
|    iterations           | 97             |
|    time_elapsed         | 457            |
|    total_timesteps      | 198656         |
| train/                  |                |
|    approx_kl            | 0.0013458848   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0238         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0149        |
|    cost_value_loss      | 3.62e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0742        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00318        |
|    mean_cost_advantages | -0.00016135935 |
|    mean_reward_advan... | -0.001427846   |
|    n_updates            | 960            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.58e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.1e-06        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 434            |
|    iterations           | 98             |
|    time_elapsed         | 462            |
|    total_timesteps      | 200704         |
| train/                  |                |
|    approx_kl            | 0.0008687932   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0269         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.015          |
|    cost_value_loss      | 3.79e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0726        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00075       |
|    mean_cost_advantages | -0.00015988886 |
|    mean_reward_advan... | -0.0005004768  |
|    n_updates            | 970            |
|    nu                   | 2.64           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000427       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 2.16e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Mean reward: 9994.500000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 07.95 minutes[0m
