[32mConfigured folder /tmp/wandb/run-20220629_123755-1ouxm0ba/files for saving
[32mName: JTL-v0_CJTL-v0_dnc_True_dno_True_dnr_True_goal_0_ws_True_s_20_sid_-1
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
Wrapping eval env in a VecNormalize.
Using cpu device
----------------------------------
| eval/               |          |
|    best_mean_reward | -77.2    |
|    mean_ep_length   | 114      |
|    mean_reward      | -77.2    |
|    true_cost        | 0.505    |
| infos/              |          |
|    cost             | 0.56     |
| rollout/            |          |
|    adjusted_reward  | -1.23    |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | -95.4    |
| time/               |          |
|    fps              | 965      |
|    iterations       | 1        |
|    time_elapsed     | 10       |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -77.2       |
|    mean_ep_length       | 129         |
|    mean_reward          | -86.5       |
|    true_cost            | 0.289       |
| infos/                  |             |
|    cost                 | 0.28        |
| rollout/                |             |
|    adjusted_reward      | -0.968      |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | -88.1       |
| time/                   |             |
|    fps                  | 806         |
|    iterations           | 2           |
|    time_elapsed         | 25          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.026682992 |
|    average_cost         | 0.5053711   |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -291        |
|    cost_value_loss      | 26.6        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.2        |
|    mean_cost_advantages | 7.819995    |
|    mean_reward_advan... | -10.869412  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.505      |
|    policy_gradient_loss | -0.0247     |
|    reward_explained_... | -455        |
|    reward_value_loss    | 45.4        |
|    total_cost           | 5175.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -77.2       |
|    mean_ep_length       | 155         |
|    mean_reward          | -97.8       |
|    true_cost            | 0.241       |
| infos/                  |             |
|    cost                 | 0.19        |
| rollout/                |             |
|    adjusted_reward      | -0.901      |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | -89.7       |
| time/                   |             |
|    fps                  | 789         |
|    iterations           | 3           |
|    time_elapsed         | 38          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.020927932 |
|    average_cost         | 0.28935546  |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.64       |
|    cost_value_loss      | 23.7        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 26.6        |
|    mean_cost_advantages | 2.669936    |
|    mean_reward_advan... | -7.1137342  |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.308      |
|    policy_gradient_loss | -0.0195     |
|    reward_explained_... | -3.41       |
|    reward_value_loss    | 44          |
|    total_cost           | 2963.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -77.2       |
|    mean_ep_length       | 148         |
|    mean_reward          | -98.9       |
|    true_cost            | 0.168       |
| infos/                  |             |
|    cost                 | 0.21        |
| rollout/                |             |
|    adjusted_reward      | -0.882      |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | -103        |
| time/                   |             |
|    fps                  | 780         |
|    iterations           | 4           |
|    time_elapsed         | 52          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.018837612 |
|    average_cost         | 0.24091797  |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2          |
|    cost_value_loss      | 23.5        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.26       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.4        |
|    mean_cost_advantages | 1.464394    |
|    mean_reward_advan... | -5.094546   |
|    n_updates            | 30          |
|    nu                   | 1.19        |
|    nu_loss              | -0.272      |
|    policy_gradient_loss | -0.0158     |
|    reward_explained_... | -1.59       |
|    reward_value_loss    | 45.5        |
|    total_cost           | 2467.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -77.2       |
|    mean_ep_length       | 135         |
|    mean_reward          | -82.4       |
|    true_cost            | 0.117       |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.764      |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -106        |
| time/                   |             |
|    fps                  | 762         |
|    iterations           | 5           |
|    time_elapsed         | 67          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.019151198 |
|    average_cost         | 0.16845703  |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.68       |
|    cost_value_loss      | 19.6        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 29.6        |
|    mean_cost_advantages | 0.33117324  |
|    mean_reward_advan... | -4.8546557  |
|    n_updates            | 40          |
|    nu                   | 1.26        |
|    nu_loss              | -0.201      |
|    policy_gradient_loss | -0.0138     |
|    reward_explained_... | -1.39       |
|    reward_value_loss    | 51.2        |
|    total_cost           | 1725.0      |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -77.2       |
|    mean_ep_length       | 150         |
|    mean_reward          | -95         |
|    true_cost            | 0.0846      |
| infos/                  |             |
|    cost                 | 0.1         |
| rollout/                |             |
|    adjusted_reward      | -0.75       |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -101        |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 6           |
|    time_elapsed         | 83          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.016683672 |
|    average_cost         | 0.116601564 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.05       |
|    cost_value_loss      | 15.2        |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.12       |
|    learning_rate        | 0.0003      |
|    loss                 | 29.1        |
|    mean_cost_advantages | -0.5116782  |
|    mean_reward_advan... | -2.7386975  |
|    n_updates            | 50          |
|    nu                   | 1.32        |
|    nu_loss              | -0.146      |
|    policy_gradient_loss | -0.00568    |
|    reward_explained_... | -0.414      |
|    reward_value_loss    | 54.4        |
|    total_cost           | 1194.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -37.5       |
|    mean_ep_length       | 77          |
|    mean_reward          | -37.5       |
|    true_cost            | 0.0565      |
| infos/                  |             |
|    cost                 | 0.09        |
| rollout/                |             |
|    adjusted_reward      | -0.715      |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -105        |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 7           |
|    time_elapsed         | 97          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.016417926 |
|    average_cost         | 0.08457031  |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.92       |
|    cost_value_loss      | 11.3        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 40          |
|    mean_cost_advantages | -0.79724103 |
|    mean_reward_advan... | -2.097652   |
|    n_updates            | 60          |
|    nu                   | 1.38        |
|    nu_loss              | -0.111      |
|    policy_gradient_loss | -0.00568    |
|    reward_explained_... | -0.161      |
|    reward_value_loss    | 58.5        |
|    total_cost           | 866.0       |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -37.5      |
|    mean_ep_length       | 200        |
|    mean_reward          | -148       |
|    true_cost            | 0.0363     |
| infos/                  |            |
|    cost                 | 0.01       |
| rollout/                |            |
|    adjusted_reward      | -0.694     |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | -110       |
| time/                   |            |
|    fps                  | 694        |
|    iterations           | 8          |
|    time_elapsed         | 117        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.02067427 |
|    average_cost         | 0.05654297 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    cost_explained_va... | -0.6       |
|    cost_value_loss      | 6.27       |
|    early_stop_epoch     | 6          |
|    entropy_loss         | -0.983     |
|    learning_rate        | 0.0003     |
|    loss                 | 39.4       |
|    mean_cost_advantages | -0.9912733 |
|    mean_reward_advan... | -1.6478882 |
|    n_updates            | 70         |
|    nu                   | 1.43       |
|    nu_loss              | -0.0778    |
|    policy_gradient_loss | -0.00714   |
|    reward_explained_... | -0.101     |
|    reward_value_loss    | 67.8       |
|    total_cost           | 579.0      |
----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -37.5       |
|    mean_ep_length       | 115         |
|    mean_reward          | -84.4       |
|    true_cost            | 0.0382      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.693      |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -101        |
| time/                   |             |
|    fps                  | 650         |
|    iterations           | 9           |
|    time_elapsed         | 141         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.012864019 |
|    average_cost         | 0.036328126 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.291      |
|    cost_value_loss      | 3.06        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.943      |
|    learning_rate        | 0.0003      |
|    loss                 | 40.4        |
|    mean_cost_advantages | -1.1289308  |
|    mean_reward_advan... | -1.1454854  |
|    n_updates            | 80          |
|    nu                   | 1.48        |
|    nu_loss              | -0.052      |
|    policy_gradient_loss | -0.00549    |
|    reward_explained_... | 0.0743      |
|    reward_value_loss    | 65.2        |
|    total_cost           | 372.0       |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -37.5       |
|    mean_ep_length       | 184         |
|    mean_reward          | -139        |
|    true_cost            | 0.0391      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.721      |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | -99.1       |
| time/                   |             |
|    fps                  | 647         |
|    iterations           | 10          |
|    time_elapsed         | 158         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.015862396 |
|    average_cost         | 0.038183592 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.412      |
|    cost_value_loss      | 3.19        |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -0.935      |
|    learning_rate        | 0.0003      |
|    loss                 | 24.1        |
|    mean_cost_advantages | -0.7177849  |
|    mean_reward_advan... | -0.8998491  |
|    n_updates            | 90          |
|    nu                   | 1.53        |
|    nu_loss              | -0.0567     |
|    policy_gradient_loss | -0.00747    |
|    reward_explained_... | 0.362       |
|    reward_value_loss    | 69.4        |
|    total_cost           | 391.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -37.5       |
|    mean_ep_length       | 114         |
|    mean_reward          | -60.4       |
|    true_cost            | 0.0589      |
| infos/                  |             |
|    cost                 | 0.14        |
| rollout/                |             |
|    adjusted_reward      | -0.67       |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | -72.2       |
| time/                   |             |
|    fps                  | 619         |
|    iterations           | 11          |
|    time_elapsed         | 181         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.014118302 |
|    average_cost         | 0.0390625   |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.54       |
|    cost_value_loss      | 3.29        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.89       |
|    learning_rate        | 0.0003      |
|    loss                 | 67.1        |
|    mean_cost_advantages | -0.5332807  |
|    mean_reward_advan... | -0.61716473 |
|    n_updates            | 100         |
|    nu                   | 1.58        |
|    nu_loss              | -0.0599     |
|    policy_gradient_loss | -0.00528    |
|    reward_explained_... | 0.355       |
|    reward_value_loss    | 87.8        |
|    total_cost           | 400.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -37.5        |
|    mean_ep_length       | 83.8         |
|    mean_reward          | -48.6        |
|    true_cost            | 0.0371       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.626       |
|    ep_len_mean          | 98           |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 630          |
|    iterations           | 12           |
|    time_elapsed         | 194          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.032471918  |
|    average_cost         | 0.058886718  |
|    clip_fraction        | 0.189        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.109       |
|    cost_value_loss      | 7.76         |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.884       |
|    learning_rate        | 0.0003       |
|    loss                 | 38.4         |
|    mean_cost_advantages | -0.115985915 |
|    mean_reward_advan... | 0.6508125    |
|    n_updates            | 110          |
|    nu                   | 1.63         |
|    nu_loss              | -0.0932      |
|    policy_gradient_loss | -0.0273      |
|    reward_explained_... | 0.598        |
|    reward_value_loss    | 75.7         |
|    total_cost           | 603.0        |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -37.5       |
|    mean_ep_length       | 79.6        |
|    mean_reward          | -51.1       |
|    true_cost            | 0.0368      |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | -0.592      |
|    ep_len_mean          | 67          |
|    ep_rew_mean          | -34.9       |
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 13          |
|    time_elapsed         | 211         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.0168288   |
|    average_cost         | 0.037109375 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.266       |
|    cost_value_loss      | 4.33        |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.872      |
|    learning_rate        | 0.0003      |
|    loss                 | 36.7        |
|    mean_cost_advantages | -0.45817977 |
|    mean_reward_advan... | 1.4802126   |
|    n_updates            | 120         |
|    nu                   | 1.67        |
|    nu_loss              | -0.0605     |
|    policy_gradient_loss | -0.0091     |
|    reward_explained_... | 0.605       |
|    reward_value_loss    | 81.1        |
|    total_cost           | 380.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -15.2        |
|    mean_ep_length       | 36.2         |
|    mean_reward          | -15.2        |
|    true_cost            | 0.0288       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.55        |
|    ep_len_mean          | 60.9         |
|    ep_rew_mean          | -31.4        |
| time/                   |              |
|    fps                  | 634          |
|    iterations           | 14           |
|    time_elapsed         | 225          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0154751865 |
|    average_cost         | 0.036816407  |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.108        |
|    cost_value_loss      | 2.72         |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.853       |
|    learning_rate        | 0.0003       |
|    loss                 | 41           |
|    mean_cost_advantages | -0.48903626  |
|    mean_reward_advan... | 2.7166712    |
|    n_updates            | 130          |
|    nu                   | 1.72         |
|    nu_loss              | -0.0616      |
|    policy_gradient_loss | -0.0057      |
|    reward_explained_... | 0.632        |
|    reward_value_loss    | 76.2         |
|    total_cost           | 377.0        |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -15.2       |
|    mean_ep_length       | 98.4        |
|    mean_reward          | -76.7       |
|    true_cost            | 0.0268      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.606      |
|    ep_len_mean          | 51.6        |
|    ep_rew_mean          | -28.8       |
| time/                   |             |
|    fps                  | 638         |
|    iterations           | 15          |
|    time_elapsed         | 240         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.016251644 |
|    average_cost         | 0.028808594 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.432       |
|    cost_value_loss      | 0.788       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 28.6        |
|    mean_cost_advantages | -0.48053274 |
|    mean_reward_advan... | 2.363435    |
|    n_updates            | 140         |
|    nu                   | 1.76        |
|    nu_loss              | -0.0495     |
|    policy_gradient_loss | -0.00573    |
|    reward_explained_... | 0.735       |
|    reward_value_loss    | 62.7        |
|    total_cost           | 295.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -14.2       |
|    mean_ep_length       | 37.4        |
|    mean_reward          | -14.2       |
|    true_cost            | 0.0445      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.542      |
|    ep_len_mean          | 27.4        |
|    ep_rew_mean          | -10.2       |
| time/                   |             |
|    fps                  | 637         |
|    iterations           | 16          |
|    time_elapsed         | 257         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.015251656 |
|    average_cost         | 0.026757812 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.209       |
|    cost_value_loss      | 0.553       |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.767      |
|    learning_rate        | 0.0003      |
|    loss                 | 44.3        |
|    mean_cost_advantages | -0.30421013 |
|    mean_reward_advan... | 0.5264884   |
|    n_updates            | 150         |
|    nu                   | 1.8         |
|    nu_loss              | -0.0471     |
|    policy_gradient_loss | -0.00514    |
|    reward_explained_... | 0.766       |
|    reward_value_loss    | 76          |
|    total_cost           | 274.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -8.72       |
|    mean_ep_length       | 24          |
|    mean_reward          | -8.72       |
|    true_cost            | 0.0343      |
| infos/                  |             |
|    cost                 | 0.09        |
| rollout/                |             |
|    adjusted_reward      | -0.454      |
|    ep_len_mean          | 23.9        |
|    ep_rew_mean          | -9.81       |
| time/                   |             |
|    fps                  | 642         |
|    iterations           | 17          |
|    time_elapsed         | 271         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.018941337 |
|    average_cost         | 0.04453125  |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.59       |
|    cost_value_loss      | 1.6         |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.719      |
|    learning_rate        | 0.0003      |
|    loss                 | 22.7        |
|    mean_cost_advantages | -0.08177527 |
|    mean_reward_advan... | 1.9783204   |
|    n_updates            | 160         |
|    nu                   | 1.84        |
|    nu_loss              | -0.0802     |
|    policy_gradient_loss | -0.00226    |
|    reward_explained_... | 0.814       |
|    reward_value_loss    | 49.8        |
|    total_cost           | 456.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.45       |
|    mean_ep_length       | 20.6        |
|    mean_reward          | -7.45       |
|    true_cost            | 0.0437      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.462      |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | -6.49       |
| time/                   |             |
|    fps                  | 652         |
|    iterations           | 18          |
|    time_elapsed         | 282         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.018525723 |
|    average_cost         | 0.034277342 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.114      |
|    cost_value_loss      | 0.552       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.654      |
|    learning_rate        | 0.0003      |
|    loss                 | 6.94        |
|    mean_cost_advantages | -0.2432578  |
|    mean_reward_advan... | 1.3804909   |
|    n_updates            | 170         |
|    nu                   | 1.88        |
|    nu_loss              | -0.0631     |
|    policy_gradient_loss | -0.00375    |
|    reward_explained_... | 0.841       |
|    reward_value_loss    | 16.6        |
|    total_cost           | 351.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.07        |
|    mean_ep_length       | 17.8         |
|    mean_reward          | -6.07        |
|    true_cost            | 0.029        |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.486       |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | -6.31        |
| time/                   |              |
|    fps                  | 661          |
|    iterations           | 19           |
|    time_elapsed         | 293          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.04760359   |
|    average_cost         | 0.04375      |
|    clip_fraction        | 0.311        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.672       |
|    cost_value_loss      | 0.421        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.532       |
|    learning_rate        | 0.0003       |
|    loss                 | 22.9         |
|    mean_cost_advantages | -0.096241616 |
|    mean_reward_advan... | 0.87961495   |
|    n_updates            | 180          |
|    nu                   | 1.92         |
|    nu_loss              | -0.0823      |
|    policy_gradient_loss | -0.0107      |
|    reward_explained_... | 0.895        |
|    reward_value_loss    | 10.9         |
|    total_cost           | 448.0        |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.47       |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -5.47       |
|    true_cost            | 0.0244      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | -0.435      |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | -7.11       |
| time/                   |             |
|    fps                  | 664         |
|    iterations           | 20          |
|    time_elapsed         | 308         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.018328069 |
|    average_cost         | 0.029003907 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.139      |
|    cost_value_loss      | 0.267       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.488      |
|    learning_rate        | 0.0003      |
|    loss                 | 10.5        |
|    mean_cost_advantages | -0.13946286 |
|    mean_reward_advan... | -0.49371657 |
|    n_updates            | 190         |
|    nu                   | 1.96        |
|    nu_loss              | -0.0557     |
|    policy_gradient_loss | -0.00314    |
|    reward_explained_... | 0.889       |
|    reward_value_loss    | 32.7        |
|    total_cost           | 297.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.03       |
|    mean_ep_length       | 14.4        |
|    mean_reward          | -5.03       |
|    true_cost            | 0.00615     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.363      |
|    ep_len_mean          | 15.2        |
|    ep_rew_mean          | -5.11       |
| time/                   |             |
|    fps                  | 670         |
|    iterations           | 21          |
|    time_elapsed         | 320         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.049265914 |
|    average_cost         | 0.024414062 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.153      |
|    cost_value_loss      | 0.136       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.422      |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    mean_cost_advantages | -0.09440233 |
|    mean_reward_advan... | 0.2150077   |
|    n_updates            | 200         |
|    nu                   | 2           |
|    nu_loss              | -0.0478     |
|    policy_gradient_loss | -0.0051     |
|    reward_explained_... | 0.904       |
|    reward_value_loss    | 20.7        |
|    total_cost           | 250.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.27       |
|    mean_ep_length       | 12.8        |
|    mean_reward          | -4.27       |
|    true_cost            | 0.00498     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.35       |
|    ep_len_mean          | 13.5        |
|    ep_rew_mean          | -4.51       |
| time/                   |             |
|    fps                  | 678         |
|    iterations           | 22          |
|    time_elapsed         | 332         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.015247902 |
|    average_cost         | 0.006152344 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.545      |
|    cost_value_loss      | 0.1         |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.296      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.216       |
|    mean_cost_advantages | -0.11418662 |
|    mean_reward_advan... | 0.2452366   |
|    n_updates            | 210         |
|    nu                   | 2.03        |
|    nu_loss              | -0.0123     |
|    policy_gradient_loss | -0.00211    |
|    reward_explained_... | 0.888       |
|    reward_value_loss    | 7.29        |
|    total_cost           | 63.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.95        |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -3.95        |
|    true_cost            | 0.00107      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.344       |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | -4.31        |
| time/                   |              |
|    fps                  | 682          |
|    iterations           | 23           |
|    time_elapsed         | 344          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.029044438  |
|    average_cost         | 0.0049804687 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.11        |
|    cost_value_loss      | 0.0969       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.203       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.193        |
|    mean_cost_advantages | -0.06165292  |
|    mean_reward_advan... | 0.12973014   |
|    n_updates            | 220          |
|    nu                   | 2.06         |
|    nu_loss              | -0.0101      |
|    policy_gradient_loss | -0.00389     |
|    reward_explained_... | 0.875        |
|    reward_value_loss    | 2.77         |
|    total_cost           | 51.0         |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.95        |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -3.98        |
|    true_cost            | 0.00205      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.339       |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | -4.24        |
| time/                   |              |
|    fps                  | 676          |
|    iterations           | 24           |
|    time_elapsed         | 363          |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.01751008   |
|    average_cost         | 0.0010742188 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.944       |
|    cost_value_loss      | 0.00655      |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.15        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.123        |
|    mean_cost_advantages | -0.041574586 |
|    mean_reward_advan... | 0.22595584   |
|    n_updates            | 230          |
|    nu                   | 2.09         |
|    nu_loss              | -0.00221     |
|    policy_gradient_loss | -0.00443     |
|    reward_explained_... | 0.926        |
|    reward_value_loss    | 2.8          |
|    total_cost           | 11.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.82        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -3.82        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.332       |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | -3.96        |
| time/                   |              |
|    fps                  | 678          |
|    iterations           | 25           |
|    time_elapsed         | 377          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.021011492  |
|    average_cost         | 0.0020507812 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.721       |
|    cost_value_loss      | 0.0371       |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.0855      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.112        |
|    mean_cost_advantages | 0.004188282  |
|    mean_reward_advan... | 0.12615089   |
|    n_updates            | 240          |
|    nu                   | 2.12         |
|    nu_loss              | -0.00429     |
|    policy_gradient_loss | -0.0034      |
|    reward_explained_... | 0.893        |
|    reward_value_loss    | 0.206        |
|    total_cost           | 21.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.82       |
|    mean_ep_length       | 12.8        |
|    mean_reward          | -4.45       |
|    true_cost            | 0.000391    |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.333      |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | -3.9        |
| time/                   |             |
|    fps                  | 664         |
|    iterations           | 26          |
|    time_elapsed         | 400         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.004029872 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.105      |
|    cost_value_loss      | 0.000124    |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.0564     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0874      |
|    mean_cost_advantages | 0.014045653 |
|    mean_reward_advan... | 0.08558057  |
|    n_updates            | 250         |
|    nu                   | 2.14        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000986   |
|    reward_explained_... | 0.922       |
|    reward_value_loss    | 0.136       |
|    total_cost           | 0.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.47        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | -3.47        |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.333       |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | -3.89        |
| time/                   |              |
|    fps                  | 652          |
|    iterations           | 27           |
|    time_elapsed         | 423          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.004002981  |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0165      |
|    cost_value_loss      | 0.0019       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0394      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0357       |
|    mean_cost_advantages | 0.0036562055 |
|    mean_reward_advan... | 0.047881864  |
|    n_updates            | 260          |
|    nu                   | 2.17         |
|    nu_loss              | -0.000837    |
|    policy_gradient_loss | -0.00096     |
|    reward_explained_... | 0.92         |
|    reward_value_loss    | 0.134        |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.47         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -4.2          |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.334        |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | -3.86         |
| time/                   |               |
|    fps                  | 641           |
|    iterations           | 28            |
|    time_elapsed         | 447           |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 0.002356112   |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0074        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.456        |
|    cost_value_loss      | 0.003         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0298       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0353        |
|    mean_cost_advantages | 0.0073399     |
|    mean_reward_advan... | 0.05232901    |
|    n_updates            | 270           |
|    nu                   | 2.19          |
|    nu_loss              | -0.000634     |
|    policy_gradient_loss | -0.000156     |
|    reward_explained_... | 0.932         |
|    reward_value_loss    | 0.108         |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.43        |
|    mean_ep_length       | 10.6         |
|    mean_reward          | -3.43        |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.334       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -3.84        |
| time/                   |              |
|    fps                  | 631          |
|    iterations           | 29           |
|    time_elapsed         | 470          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0008812427 |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0.00343      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.7         |
|    cost_value_loss      | 0.00548      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0173      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0661       |
|    mean_cost_advantages | 0.00535297   |
|    mean_reward_advan... | 0.027191639  |
|    n_updates            | 280          |
|    nu                   | 2.21         |
|    nu_loss              | -0.00171     |
|    policy_gradient_loss | -0.000461    |
|    reward_explained_... | 0.938        |
|    reward_value_loss    | 0.096        |
|    total_cost           | 8.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.43        |
|    mean_ep_length       | 12           |
|    mean_reward          | -4.07        |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.334       |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | -3.75        |
| time/                   |              |
|    fps                  | 621          |
|    iterations           | 30           |
|    time_elapsed         | 493          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0017956629 |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.00216      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -20.2        |
|    cost_value_loss      | 0.000462     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0054      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0593       |
|    mean_cost_advantages | 0.006259921  |
|    mean_reward_advan... | 0.0046082097 |
|    n_updates            | 290          |
|    nu                   | 2.22         |
|    nu_loss              | -0.000215    |
|    policy_gradient_loss | -0.000117    |
|    reward_explained_... | 0.934        |
|    reward_value_loss    | 0.1          |
|    total_cost           | 1.0          |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.43         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -4.1          |
|    true_cost            | 0.0143        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.363        |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | -3.7          |
| time/                   |               |
|    fps                  | 623           |
|    iterations           | 31            |
|    time_elapsed         | 509           |
|    total_timesteps      | 317440        |
| train/                  |               |
|    approx_kl            | 0.019701226   |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.0113        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.26         |
|    cost_value_loss      | 0.0131        |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.0187       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0213        |
|    mean_cost_advantages | 0.004957085   |
|    mean_reward_advan... | 0.032635275   |
|    n_updates            | 300           |
|    nu                   | 2.24          |
|    nu_loss              | -0.00152      |
|    policy_gradient_loss | -0.000753     |
|    reward_explained_... | 0.921         |
|    reward_value_loss    | 0.118         |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.43         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | -3.48         |
|    true_cost            | 0.0159        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | -0.367        |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | -3.76         |
| time/                   |               |
|    fps                  | 615           |
|    iterations           | 32            |
|    time_elapsed         | 532           |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.00024877093 |
|    average_cost         | 0.014257813   |
|    clip_fraction        | 0.00574       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -7.92e+03     |
|    cost_value_loss      | 0.0464        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0379       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0342        |
|    mean_cost_advantages | 0.05682645    |
|    mean_reward_advan... | 0.08526667    |
|    n_updates            | 310           |
|    nu                   | 2.25          |
|    nu_loss              | -0.0319       |
|    policy_gradient_loss | 0.000185      |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 0.0698        |
|    total_cost           | 146.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.43        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -3.77        |
|    true_cost            | 0.0208       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.377       |
|    ep_len_mean          | 10.9         |
|    ep_rew_mean          | -3.6         |
| time/                   |              |
|    fps                  | 607          |
|    iterations           | 33           |
|    time_elapsed         | 556          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0006687938 |
|    average_cost         | 0.015917968  |
|    clip_fraction        | 0.00683      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.69        |
|    cost_value_loss      | 0.0509       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0435      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0491       |
|    mean_cost_advantages | 0.023990411  |
|    mean_reward_advan... | 0.004589846  |
|    n_updates            | 320          |
|    nu                   | 2.27         |
|    nu_loss              | -0.0359      |
|    policy_gradient_loss | -0.000577    |
|    reward_explained_... | 0.961        |
|    reward_value_loss    | 0.0553       |
|    total_cost           | 163.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.25         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | -3.25         |
|    true_cost            | 0.0214        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | -0.379        |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | -3.65         |
| time/                   |               |
|    fps                  | 600           |
|    iterations           | 34            |
|    time_elapsed         | 579           |
|    total_timesteps      | 348160        |
| train/                  |               |
|    approx_kl            | 0.00046280576 |
|    average_cost         | 0.02080078    |
|    clip_fraction        | 0.00796       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.97         |
|    cost_value_loss      | 0.061         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0429       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0294        |
|    mean_cost_advantages | 0.005291871   |
|    mean_reward_advan... | 0.022767935   |
|    n_updates            | 330           |
|    nu                   | 2.29          |
|    nu_loss              | -0.0472       |
|    policy_gradient_loss | -0.00105      |
|    reward_explained_... | 0.969         |
|    reward_value_loss    | 0.0409        |
|    total_cost           | 213.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.25        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | -3.62        |
|    true_cost            | 0.0284       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.393       |
|    ep_len_mean          | 10.7         |
|    ep_rew_mean          | -3.49        |
| time/                   |              |
|    fps                  | 594          |
|    iterations           | 35           |
|    time_elapsed         | 602          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.0008968335 |
|    average_cost         | 0.021386718  |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.72        |
|    cost_value_loss      | 0.0598       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0456      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0603       |
|    mean_cost_advantages | 0.017298821  |
|    mean_reward_advan... | -0.022526238 |
|    n_updates            | 340          |
|    nu                   | 2.31         |
|    nu_loss              | -0.0489      |
|    policy_gradient_loss | -0.00155     |
|    reward_explained_... | 0.968        |
|    reward_value_loss    | 0.0426       |
|    total_cost           | 219.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.25       |
|    mean_ep_length       | 11.4        |
|    mean_reward          | -3.8        |
|    true_cost            | 0.0327      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | -0.406      |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | -3.69       |
| time/                   |             |
|    fps                  | 588         |
|    iterations           | 36          |
|    time_elapsed         | 626         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.013426589 |
|    average_cost         | 0.02841797  |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.96       |
|    cost_value_loss      | 0.0691      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.0475     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0417      |
|    mean_cost_advantages | 0.02926595  |
|    mean_reward_advan... | 0.064281605 |
|    n_updates            | 350         |
|    nu                   | 2.32        |
|    nu_loss              | -0.0655     |
|    policy_gradient_loss | -0.00276    |
|    reward_explained_... | 0.978       |
|    reward_value_loss    | 0.0293      |
|    total_cost           | 291.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.18        |
|    mean_ep_length       | 10.2         |
|    mean_reward          | -3.18        |
|    true_cost            | 0.0387       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.417       |
|    ep_len_mean          | 10.8         |
|    ep_rew_mean          | -3.58        |
| time/                   |              |
|    fps                  | 594          |
|    iterations           | 37           |
|    time_elapsed         | 637          |
|    total_timesteps      | 378880       |
| train/                  |              |
|    approx_kl            | 0.062279057  |
|    average_cost         | 0.032714844  |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.32        |
|    cost_value_loss      | 0.0773       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0476      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0622       |
|    mean_cost_advantages | 0.030431742  |
|    mean_reward_advan... | -0.041427184 |
|    n_updates            | 360          |
|    nu                   | 2.35         |
|    nu_loss              | -0.0761      |
|    policy_gradient_loss | -0.00374     |
|    reward_explained_... | 0.943        |
|    reward_value_loss    | 0.0717       |
|    total_cost           | 335.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.18        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | -3.57        |
|    true_cost            | 0.0413       |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | -0.424       |
|    ep_len_mean          | 10.6         |
|    ep_rew_mean          | -3.44        |
| time/                   |              |
|    fps                  | 588          |
|    iterations           | 38           |
|    time_elapsed         | 661          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.0044843033 |
|    average_cost         | 0.038671874  |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.761       |
|    cost_value_loss      | 0.109        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0344      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0527       |
|    mean_cost_advantages | 0.0021415495 |
|    mean_reward_advan... | 0.10960988   |
|    n_updates            | 370          |
|    nu                   | 2.37         |
|    nu_loss              | -0.0907      |
|    policy_gradient_loss | -0.00307     |
|    reward_explained_... | 0.985        |
|    reward_value_loss    | 0.0156       |
|    total_cost           | 396.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.18       |
|    mean_ep_length       | 10.6        |
|    mean_reward          | -3.48       |
|    true_cost            | 0.0454      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.434      |
|    ep_len_mean          | 10.6        |
|    ep_rew_mean          | -3.45       |
| time/                   |             |
|    fps                  | 583         |
|    iterations           | 39          |
|    time_elapsed         | 684         |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.004680665 |
|    average_cost         | 0.041308593 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.297      |
|    cost_value_loss      | 0.0759      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.0241     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0433      |
|    mean_cost_advantages | 0.030851964 |
|    mean_reward_advan... | 0.027772522 |
|    n_updates            | 380         |
|    nu                   | 2.4         |
|    nu_loss              | -0.0979     |
|    policy_gradient_loss | -0.00343    |
|    reward_explained_... | 0.99        |
|    reward_value_loss    | 0.0125      |
|    total_cost           | 423.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.18         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | -3.62         |
|    true_cost            | 0.0476        |
| infos/                  |               |
|    cost                 | 0.05          |
| rollout/                |               |
|    adjusted_reward      | -0.441        |
|    ep_len_mean          | 10.5          |
|    ep_rew_mean          | -3.4          |
| time/                   |               |
|    fps                  | 584           |
|    iterations           | 40            |
|    time_elapsed         | 701           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 0.016076267   |
|    average_cost         | 0.045410156   |
|    clip_fraction        | 0.00551       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.293        |
|    cost_value_loss      | 0.074         |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.00343      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0251        |
|    mean_cost_advantages | -0.0030132202 |
|    mean_reward_advan... | 0.03470522    |
|    n_updates            | 390           |
|    nu                   | 2.43          |
|    nu_loss              | -0.109        |
|    policy_gradient_loss | -0.00314      |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 0.0044        |
|    total_cost           | 465.0         |
-------------------------------------------
[32mTime taken: 12.10 minutes
/home/jovyan/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)