[32;1mConfigured folder ./cpg/wandb/run-20220531_140127-1bpf4m0q/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
Using cpu device/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))

/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -1.17e+06 |
|    mean_ep_length   | 166       |
|    mean_reward      | -1.17e+06 |
|    true_cost        | 0.589     |
| infos/              |           |
|    cost             | 0.0329    |
| rollout/            |           |
|    adjusted_reward  | -7.93     |
|    ep_len_mean      | 194       |
|    ep_rew_mean      | 839       |
| time/               |           |
|    fps              | 538       |
|    iterations       | 1         |
|    time_elapsed     | 3         |
|    total_timesteps  | 2048      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.17e+06   |
|    mean_ep_length       | 171         |
|    mean_reward          | -1.18e+06   |
|    true_cost            | 0.486       |
| infos/                  |             |
|    cost                 | 0.0189      |
| rollout/                |             |
|    adjusted_reward      | 10.9        |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 2.78e+03    |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 2           |
|    time_elapsed         | 9           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.006757022 |
|    average_cost         | 0.58935547  |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.38       |
|    cost_value_loss      | 0.311       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.24        |
|    mean_cost_advantages | 0.6401646   |
|    mean_reward_advan... | -0.4912989  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.589      |
|    policy_gradient_loss | -0.00418    |
|    reward_explained_... | -12.3       |
|    reward_value_loss    | 0.66        |
|    total_cost           | 1207.0      |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.56e+05   |
|    mean_ep_length       | 53.2        |
|    mean_reward          | -3.56e+05   |
|    true_cost            | 0.469       |
| infos/                  |             |
|    cost                 | 0.0302      |
| rollout/                |             |
|    adjusted_reward      | 68.5        |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | 5.74e+03    |
| time/                   |             |
|    fps                  | 452         |
|    iterations           | 3           |
|    time_elapsed         | 13          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012899802 |
|    average_cost         | 0.48632812  |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.02       |
|    cost_value_loss      | 0.0957      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.388       |
|    mean_cost_advantages | 0.2899875   |
|    mean_reward_advan... | 0.5321475   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.518      |
|    policy_gradient_loss | -0.00701    |
|    reward_explained_... | -134        |
|    reward_value_loss    | 2.04        |
|    total_cost           | 996.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.9e+05    |
|    mean_ep_length       | 32          |
|    mean_reward          | -1.9e+05    |
|    true_cost            | 0.507       |
| infos/                  |             |
|    cost                 | 0.0156      |
| rollout/                |             |
|    adjusted_reward      | 138         |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | 7.46e+03    |
| time/                   |             |
|    fps                  | 467         |
|    iterations           | 4           |
|    time_elapsed         | 17          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.016250363 |
|    average_cost         | 0.46875     |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.08       |
|    cost_value_loss      | 0.132       |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -1.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91        |
|    mean_cost_advantages | 0.14622372  |
|    mean_reward_advan... | 1.3418211   |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.53       |
|    policy_gradient_loss | -0.012      |
|    reward_explained_... | -5.9        |
|    reward_value_loss    | 4.8         |
|    total_cost           | 960.0       |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.6e+05    |
|    mean_ep_length       | 37.6        |
|    mean_reward          | -1.6e+05    |
|    true_cost            | 0.526       |
| infos/                  |             |
|    cost                 | 0.033       |
| rollout/                |             |
|    adjusted_reward      | 293         |
|    ep_len_mean          | 49.6        |
|    ep_rew_mean          | 9.88e+03    |
| time/                   |             |
|    fps                  | 482         |
|    iterations           | 5           |
|    time_elapsed         | 21          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.019144457 |
|    average_cost         | 0.50683594  |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.793      |
|    cost_value_loss      | 0.149       |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.29       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.92        |
|    mean_cost_advantages | 0.030600425 |
|    mean_reward_advan... | 1.9621143   |
|    n_updates            | 40          |
|    nu                   | 1.27        |
|    nu_loss              | -0.608      |
|    policy_gradient_loss | -0.0106     |
|    reward_explained_... | -0.337      |
|    reward_value_loss    | 4.73        |
|    total_cost           | 1038.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.14e+05   |
|    mean_ep_length       | 24.8        |
|    mean_reward          | -1.14e+05   |
|    true_cost            | 0.67        |
| infos/                  |             |
|    cost                 | 0.0315      |
| rollout/                |             |
|    adjusted_reward      | 391         |
|    ep_len_mean          | 28.7        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 502         |
|    iterations           | 6           |
|    time_elapsed         | 24          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.025610447 |
|    average_cost         | 0.5258789   |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.38       |
|    cost_value_loss      | 0.142       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.968       |
|    mean_cost_advantages | -0.15185316 |
|    mean_reward_advan... | 1.9526948   |
|    n_updates            | 50          |
|    nu                   | 1.34        |
|    nu_loss              | -0.667      |
|    policy_gradient_loss | -0.0114     |
|    reward_explained_... | 0.179       |
|    reward_value_loss    | 3.65        |
|    total_cost           | 1077.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.6e+04    |
|    mean_ep_length       | 22.4        |
|    mean_reward          | -7.6e+04    |
|    true_cost            | 0.54        |
| infos/                  |             |
|    cost                 | 0.0206      |
| rollout/                |             |
|    adjusted_reward      | 517         |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 518         |
|    iterations           | 7           |
|    time_elapsed         | 27          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.022916442 |
|    average_cost         | 0.67041016  |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.22       |
|    cost_value_loss      | 0.0998      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.08       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.249       |
|    mean_cost_advantages | -0.09055167 |
|    mean_reward_advan... | 0.74811745  |
|    n_updates            | 60          |
|    nu                   | 1.42        |
|    nu_loss              | -0.9        |
|    policy_gradient_loss | -0.00874    |
|    reward_explained_... | 0.568       |
|    reward_value_loss    | 0.809       |
|    total_cost           | 1373.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.6e+04    |
|    mean_ep_length       | 19.2        |
|    mean_reward          | -1.1e+05    |
|    true_cost            | 0.59        |
| infos/                  |             |
|    cost                 | 0.039       |
| rollout/                |             |
|    adjusted_reward      | 565         |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 8           |
|    time_elapsed         | 31          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.018204875 |
|    average_cost         | 0.54003906  |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.568       |
|    cost_value_loss      | 0.046       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.01       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0494      |
|    mean_cost_advantages | -0.23826389 |
|    mean_reward_advan... | 0.01948157  |
|    n_updates            | 70          |
|    nu                   | 1.5         |
|    nu_loss              | -0.766      |
|    policy_gradient_loss | -0.00818    |
|    reward_explained_... | 0.482       |
|    reward_value_loss    | 0.215       |
|    total_cost           | 1106.0      |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.6e+04    |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -9e+04      |
|    true_cost            | 0.585       |
| infos/                  |             |
|    cost                 | 0.0429      |
| rollout/                |             |
|    adjusted_reward      | 636         |
|    ep_len_mean          | 16.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 516         |
|    iterations           | 9           |
|    time_elapsed         | 35          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.010266781 |
|    average_cost         | 0.58984375  |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.611       |
|    cost_value_loss      | 0.0247      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.918      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0322      |
|    mean_cost_advantages | -0.07523693 |
|    mean_reward_advan... | -0.47994834 |
|    n_updates            | 80          |
|    nu                   | 1.57        |
|    nu_loss              | -0.882      |
|    policy_gradient_loss | -0.00683    |
|    reward_explained_... | 0.511       |
|    reward_value_loss    | 0.0404      |
|    total_cost           | 1208.0      |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.6e+04    |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -9.6e+04    |
|    true_cost            | 0.598       |
| infos/                  |             |
|    cost                 | 0.0427      |
| rollout/                |             |
|    adjusted_reward      | 651         |
|    ep_len_mean          | 15.7        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 510         |
|    iterations           | 10          |
|    time_elapsed         | 40          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.008651613 |
|    average_cost         | 0.5854492   |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.747       |
|    cost_value_loss      | 0.0148      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.848      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00331    |
|    mean_cost_advantages | -0.05473011 |
|    mean_reward_advan... | -0.48236763 |
|    n_updates            | 90          |
|    nu                   | 1.66        |
|    nu_loss              | -0.922      |
|    policy_gradient_loss | -0.0101     |
|    reward_explained_... | 0.406       |
|    reward_value_loss    | 0.0275      |
|    total_cost           | 1199.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5e+04       |
|    mean_ep_length       | 13.2         |
|    mean_reward          | -5e+04       |
|    true_cost            | 0.551        |
| infos/                  |              |
|    cost                 | 0.0402       |
| rollout/                |              |
|    adjusted_reward      | 729          |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 518          |
|    iterations           | 11           |
|    time_elapsed         | 43           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.03776426   |
|    average_cost         | 0.59765625   |
|    clip_fraction        | 0.225        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.679        |
|    cost_value_loss      | 0.0168       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.798       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00527      |
|    mean_cost_advantages | 0.0005389906 |
|    mean_reward_advan... | -0.38354763  |
|    n_updates            | 100          |
|    nu                   | 1.74         |
|    nu_loss              | -0.99        |
|    policy_gradient_loss | -0.0101      |
|    reward_explained_... | 0.439        |
|    reward_value_loss    | 0.031        |
|    total_cost           | 1224.0       |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5e+04      |
|    mean_ep_length       | 13          |
|    mean_reward          | -7.4e+04    |
|    true_cost            | 0.525       |
| infos/                  |             |
|    cost                 | 0.0374      |
| rollout/                |             |
|    adjusted_reward      | 777         |
|    ep_len_mean          | 13.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 12          |
|    time_elapsed         | 47          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016404968 |
|    average_cost         | 0.55078125  |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.808       |
|    cost_value_loss      | 0.00911     |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.72       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00361     |
|    mean_cost_advantages | -0.05281714 |
|    mean_reward_advan... | -0.25509912 |
|    n_updates            | 110         |
|    nu                   | 1.83        |
|    nu_loss              | -0.959      |
|    policy_gradient_loss | -0.00737    |
|    reward_explained_... | 0.669       |
|    reward_value_loss    | 0.011       |
|    total_cost           | 1128.0      |
-----------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5e+04      |
|    mean_ep_length       | 12.4        |
|    mean_reward          | -6.6e+04    |
|    true_cost            | 0.478       |
| infos/                  |             |
|    cost                 | 0.0375      |
| rollout/                |             |
|    adjusted_reward      | 789         |
|    ep_len_mean          | 13          |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 13          |
|    time_elapsed         | 51          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.015574618 |
|    average_cost         | 0.5253906   |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.809       |
|    cost_value_loss      | 0.00735     |
|    early_stop_epoch     | 8           |
|    entropy_loss         | -0.645      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00771    |
|    mean_cost_advantages | -0.02597178 |
|    mean_reward_advan... | -0.21861589 |
|    n_updates            | 120         |
|    nu                   | 1.91        |
|    nu_loss              | -0.959      |
|    policy_gradient_loss | -0.00638    |
|    reward_explained_... | 0.768       |
|    reward_value_loss    | 0.00652     |
|    total_cost           | 1076.0      |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5e+04       |
|    mean_ep_length       | 12.8         |
|    mean_reward          | -5.4e+04     |
|    true_cost            | 0.454        |
| infos/                  |              |
|    cost                 | 0.0383       |
| rollout/                |              |
|    adjusted_reward      | 801          |
|    ep_len_mean          | 12.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 517          |
|    iterations           | 14           |
|    time_elapsed         | 55           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.015662797  |
|    average_cost         | 0.47802734   |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.86         |
|    cost_value_loss      | 0.00483      |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.605       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00467     |
|    mean_cost_advantages | -0.013420349 |
|    mean_reward_advan... | -0.18825507  |
|    n_updates            | 130          |
|    nu                   | 2            |
|    nu_loss              | -0.914       |
|    policy_gradient_loss | -0.00369     |
|    reward_explained_... | 0.852        |
|    reward_value_loss    | 0.00505      |
|    total_cost           | 979.0        |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.2e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -4.2e+04     |
|    true_cost            | 0.429        |
| infos/                  |              |
|    cost                 | 0.0352       |
| rollout/                |              |
|    adjusted_reward      | 804          |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 510          |
|    iterations           | 15           |
|    time_elapsed         | 60           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.015170213  |
|    average_cost         | 0.45410156   |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.832        |
|    cost_value_loss      | 0.00535      |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.553       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00504      |
|    mean_cost_advantages | -0.012027909 |
|    mean_reward_advan... | -0.16168204  |
|    n_updates            | 140          |
|    nu                   | 2.09         |
|    nu_loss              | -0.908       |
|    policy_gradient_loss | -0.00538     |
|    reward_explained_... | 0.86         |
|    reward_value_loss    | 0.00333      |
|    total_cost           | 930.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.2e+04     |
|    mean_ep_length       | 12.6         |
|    mean_reward          | -4.8e+04     |
|    true_cost            | 0.426        |
| infos/                  |              |
|    cost                 | 0.0295       |
| rollout/                |              |
|    adjusted_reward      | 818          |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 498          |
|    iterations           | 16           |
|    time_elapsed         | 65           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.010219837  |
|    average_cost         | 0.42919922   |
|    clip_fraction        | 0.0761       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.818        |
|    cost_value_loss      | 0.00477      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.504       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000709    |
|    mean_cost_advantages | -0.002290653 |
|    mean_reward_advan... | -0.13055605  |
|    n_updates            | 150          |
|    nu                   | 2.18         |
|    nu_loss              | -0.896       |
|    policy_gradient_loss | -0.00294     |
|    reward_explained_... | 0.881        |
|    reward_value_loss    | 0.00251      |
|    total_cost           | 879.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.2e+04     |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -4.4e+04     |
|    true_cost            | 0.401        |
| infos/                  |              |
|    cost                 | 0.0317       |
| rollout/                |              |
|    adjusted_reward      | 827          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 492          |
|    iterations           | 17           |
|    time_elapsed         | 70           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.013712564  |
|    average_cost         | 0.42626953   |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.859        |
|    cost_value_loss      | 0.00395      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.474       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00603      |
|    mean_cost_advantages | 9.952241e-05 |
|    mean_reward_advan... | -0.105829105 |
|    n_updates            | 160          |
|    nu                   | 2.27         |
|    nu_loss              | -0.928       |
|    policy_gradient_loss | -0.00454     |
|    reward_explained_... | 0.926        |
|    reward_value_loss    | 0.00156      |
|    total_cost           | 873.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.4e+04     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -3.4e+04     |
|    true_cost            | 0.403        |
| infos/                  |              |
|    cost                 | 0.0315       |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 488          |
|    iterations           | 18           |
|    time_elapsed         | 75           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.010954091  |
|    average_cost         | 0.4008789    |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.833        |
|    cost_value_loss      | 0.00419      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.45        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00324      |
|    mean_cost_advantages | -0.005958947 |
|    mean_reward_advan... | -0.09431802  |
|    n_updates            | 170          |
|    nu                   | 2.36         |
|    nu_loss              | -0.909       |
|    policy_gradient_loss | -0.00278     |
|    reward_explained_... | 0.936        |
|    reward_value_loss    | 0.00137      |
|    total_cost           | 821.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -3e+04       |
|    true_cost            | 0.365        |
| infos/                  |              |
|    cost                 | 0.027        |
| rollout/                |              |
|    adjusted_reward      | 810          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 484          |
|    iterations           | 19           |
|    time_elapsed         | 80           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.010508381  |
|    average_cost         | 0.4033203    |
|    clip_fraction        | 0.0872       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.876        |
|    cost_value_loss      | 0.003        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.421       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000629    |
|    mean_cost_advantages | 0.0023562717 |
|    mean_reward_advan... | -0.08067681  |
|    n_updates            | 180          |
|    nu                   | 2.45         |
|    nu_loss              | -0.95        |
|    policy_gradient_loss | -0.00251     |
|    reward_explained_... | 0.958        |
|    reward_value_loss    | 0.000796     |
|    total_cost           | 826.0        |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.6e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -2.6e+04     |
|    true_cost            | 0.343        |
| infos/                  |              |
|    cost                 | 0.0309       |
| rollout/                |              |
|    adjusted_reward      | 834          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 483          |
|    iterations           | 20           |
|    time_elapsed         | 84           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.016052047  |
|    average_cost         | 0.3647461    |
|    clip_fraction        | 0.144        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.902        |
|    cost_value_loss      | 0.00257      |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.402       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00458     |
|    mean_cost_advantages | -0.005554647 |
|    mean_reward_advan... | -0.07925639  |
|    n_updates            | 190          |
|    nu                   | 2.54         |
|    nu_loss              | -0.892       |
|    policy_gradient_loss | -0.00463     |
|    reward_explained_... | 0.753        |
|    reward_value_loss    | 0.00427      |
|    total_cost           | 747.0        |
------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.2e+04      |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -2.2e+04      |
|    true_cost            | 0.299         |
| infos/                  |               |
|    cost                 | 0.0244        |
| rollout/                |               |
|    adjusted_reward      | 829           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 21            |
|    time_elapsed         | 89            |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | 0.016750522   |
|    average_cost         | 0.34326172    |
|    clip_fraction        | 0.127         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.824         |
|    cost_value_loss      | 0.00362       |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.362        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00559       |
|    mean_cost_advantages | -0.0026240288 |
|    mean_reward_advan... | -0.058373276  |
|    n_updates            | 200           |
|    nu                   | 2.63          |
|    nu_loss              | -0.87         |
|    policy_gradient_loss | -0.00225      |
|    reward_explained_... | 0.954         |
|    reward_value_loss    | 0.000581      |
|    total_cost           | 703.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.2e+04      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | -2.8e+04      |
|    true_cost            | 0.288         |
| infos/                  |               |
|    cost                 | 0.0292        |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 478           |
|    iterations           | 22            |
|    time_elapsed         | 94            |
|    total_timesteps      | 45056         |
| train/                  |               |
|    approx_kl            | 0.005659217   |
|    average_cost         | 0.29882812    |
|    clip_fraction        | 0.0515        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.854         |
|    cost_value_loss      | 0.00297       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.323        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.002         |
|    mean_cost_advantages | -0.0123618655 |
|    mean_reward_advan... | -0.054550886  |
|    n_updates            | 210           |
|    nu                   | 2.71          |
|    nu_loss              | -0.784        |
|    policy_gradient_loss | -0.00106      |
|    reward_explained_... | 0.982         |
|    reward_value_loss    | 0.000375      |
|    total_cost           | 612.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -1.6e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -1.6e+04      |
|    true_cost            | 0.263         |
| infos/                  |               |
|    cost                 | 0.0207        |
| rollout/                |               |
|    adjusted_reward      | 838           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 475           |
|    iterations           | 23            |
|    time_elapsed         | 99            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.0035640695  |
|    average_cost         | 0.28808594    |
|    clip_fraction        | 0.0268        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.871         |
|    cost_value_loss      | 0.00224       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.297        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00157      |
|    mean_cost_advantages | -0.0016230692 |
|    mean_reward_advan... | -0.04993889   |
|    n_updates            | 220           |
|    nu                   | 2.8           |
|    nu_loss              | -0.782        |
|    policy_gradient_loss | -0.000601     |
|    reward_explained_... | 0.985         |
|    reward_value_loss    | 0.000308      |
|    total_cost           | 590.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.4e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -1.4e+04     |
|    true_cost            | 0.257        |
| infos/                  |              |
|    cost                 | 0.0228       |
| rollout/                |              |
|    adjusted_reward      | 823          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 24           |
|    time_elapsed         | 104          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0043170415 |
|    average_cost         | 0.2631836    |
|    clip_fraction        | 0.0534       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.795        |
|    cost_value_loss      | 0.00351      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.274       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000468    |
|    mean_cost_advantages | -0.006763236 |
|    mean_reward_advan... | -0.042044498 |
|    n_updates            | 230          |
|    nu                   | 2.89         |
|    nu_loss              | -0.737       |
|    policy_gradient_loss | -0.00102     |
|    reward_explained_... | 0.99         |
|    reward_value_loss    | 0.0002       |
|    total_cost           | 539.0        |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.4e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -2.2e+04     |
|    true_cost            | 0.212        |
| infos/                  |              |
|    cost                 | 0.0231       |
| rollout/                |              |
|    adjusted_reward      | 837          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 25           |
|    time_elapsed         | 108          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.020046365  |
|    average_cost         | 0.25683594   |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.833        |
|    cost_value_loss      | 0.00276      |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.271       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000143    |
|    mean_cost_advantages | -0.004229504 |
|    mean_reward_advan... | -0.042177718 |
|    n_updates            | 240          |
|    nu                   | 2.97         |
|    nu_loss              | -0.741       |
|    policy_gradient_loss | -0.00201     |
|    reward_explained_... | 0.937        |
|    reward_value_loss    | 0.00122      |
|    total_cost           | 526.0        |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8e+03       |
|    mean_ep_length       | 12           |
|    mean_reward          | -8e+03       |
|    true_cost            | 0.182        |
| infos/                  |              |
|    cost                 | 0.0103       |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 26           |
|    time_elapsed         | 112          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.01598503   |
|    average_cost         | 0.21240234   |
|    clip_fraction        | 0.0986       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.799        |
|    cost_value_loss      | 0.00293      |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.269       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00162      |
|    mean_cost_advantages | -0.027878672 |
|    mean_reward_advan... | -0.03851959  |
|    n_updates            | 250          |
|    nu                   | 3.05         |
|    nu_loss              | -0.631       |
|    policy_gradient_loss | -0.00188     |
|    reward_explained_... | 0.987        |
|    reward_value_loss    | 0.000202     |
|    total_cost           | 435.0        |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8e+03       |
|    mean_ep_length       | 12           |
|    mean_reward          | -8e+03       |
|    true_cost            | 0.111        |
| infos/                  |              |
|    cost                 | 0.0095       |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 27           |
|    time_elapsed         | 117          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.023391038  |
|    average_cost         | 0.18164062   |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.561        |
|    cost_value_loss      | 0.00543      |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.276       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00128     |
|    mean_cost_advantages | -0.018771369 |
|    mean_reward_advan... | -0.0335515   |
|    n_updates            | 260          |
|    nu                   | 3.13         |
|    nu_loss              | -0.554       |
|    policy_gradient_loss | -0.00204     |
|    reward_explained_... | 0.991        |
|    reward_value_loss    | 0.000208     |
|    total_cost           | 372.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 1.99e+03     |
|    true_cost            | 0.062        |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 837          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 28           |
|    time_elapsed         | 122          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0072174966 |
|    average_cost         | 0.110839844  |
|    clip_fraction        | 0.0896       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.283        |
|    cost_value_loss      | 0.00526      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.245       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00504      |
|    mean_cost_advantages | -0.02755823  |
|    mean_reward_advan... | -0.031505685 |
|    n_updates            | 270          |
|    nu                   | 3.21         |
|    nu_loss              | -0.347       |
|    policy_gradient_loss | -0.00144     |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 0.000141     |
|    total_cost           | 227.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 5.99e+03     |
|    true_cost            | 0.0386       |
| infos/                  |              |
|    cost                 | 0.00389      |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 29           |
|    time_elapsed         | 128          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.006102207  |
|    average_cost         | 0.06201172   |
|    clip_fraction        | 0.094        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.13        |
|    cost_value_loss      | 0.00401      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.225       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00304     |
|    mean_cost_advantages | -0.029432606 |
|    mean_reward_advan... | -0.028960098 |
|    n_updates            | 280          |
|    nu                   | 3.28         |
|    nu_loss              | -0.199       |
|    policy_gradient_loss | -0.00137     |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 9.84e-05     |
|    total_cost           | 127.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 5.99e+03      |
|    true_cost            | 0.0415        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 460           |
|    iterations           | 30            |
|    time_elapsed         | 133           |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00016188121 |
|    average_cost         | 0.03857422    |
|    clip_fraction        | 0.0331        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.451        |
|    cost_value_loss      | 0.00285       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.199        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0011        |
|    mean_cost_advantages | -0.020920971  |
|    mean_reward_advan... | -0.026001371  |
|    n_updates            | 290           |
|    nu                   | 3.34          |
|    nu_loss              | -0.126        |
|    policy_gradient_loss | -0.000365     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 7.05e-05      |
|    total_cost           | 79.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0103        |
| infos/                  |               |
|    cost                 | 0.00199       |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 459           |
|    iterations           | 31            |
|    time_elapsed         | 138           |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.013280276   |
|    average_cost         | 0.041503906   |
|    clip_fraction        | 0.0831        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.85         |
|    cost_value_loss      | 0.00287       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.182        |
|    learning_rate        | 0.0003        |
|    loss                 | -8.37e-05     |
|    mean_cost_advantages | -0.0063875224 |
|    mean_reward_advan... | -0.025491673  |
|    n_updates            | 300           |
|    nu                   | 3.41          |
|    nu_loss              | -0.139        |
|    policy_gradient_loss | -0.0014       |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 6.75e-05      |
|    total_cost           | 85.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00488      |
| infos/                  |              |
|    cost                 | 0.00101      |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 457          |
|    iterations           | 32           |
|    time_elapsed         | 143          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0052137347 |
|    average_cost         | 0.010253906  |
|    clip_fraction        | 0.0427       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.73        |
|    cost_value_loss      | 0.00102      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.172       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00134      |
|    mean_cost_advantages | -0.01219284  |
|    mean_reward_advan... | -0.023419583 |
|    n_updates            | 310          |
|    nu                   | 3.46         |
|    nu_loss              | -0.0349      |
|    policy_gradient_loss | -0.000656    |
|    reward_explained_... | 0.994        |
|    reward_value_loss    | 0.00012      |
|    total_cost           | 21.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 456           |
|    iterations           | 33            |
|    time_elapsed         | 148           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.0011292051  |
|    average_cost         | 0.0048828125  |
|    clip_fraction        | 0.0102        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.74         |
|    cost_value_loss      | 0.000479      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.158        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000544      |
|    mean_cost_advantages | -0.0062934253 |
|    mean_reward_advan... | -0.020144217  |
|    n_updates            | 320           |
|    nu                   | 3.51          |
|    nu_loss              | -0.0169       |
|    policy_gradient_loss | -0.000119     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.84e-05      |
|    total_cost           | 10.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00293        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 836            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 455            |
|    iterations           | 34             |
|    time_elapsed         | 152            |
|    total_timesteps      | 69632          |
| train/                  |                |
|    approx_kl            | 0.0011231141   |
|    average_cost         | 0.0029296875   |
|    clip_fraction        | 0.012          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -3.52          |
|    cost_value_loss      | 0.000307       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.149         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000766       |
|    mean_cost_advantages | -0.00048069854 |
|    mean_reward_advan... | -0.019017618   |
|    n_updates            | 330            |
|    nu                   | 3.56           |
|    nu_loss              | -0.0103        |
|    policy_gradient_loss | 2.93e-07       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 3.52e-05       |
|    total_cost           | 6.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 454           |
|    iterations           | 35            |
|    time_elapsed         | 157           |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | -0.0011949054 |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.00459       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.1          |
|    cost_value_loss      | 0.000291      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.142        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0002        |
|    mean_cost_advantages | 0.00046337885 |
|    mean_reward_advan... | -0.017494485  |
|    n_updates            | 340           |
|    nu                   | 3.6           |
|    nu_loss              | -0.0104       |
|    policy_gradient_loss | 2.13e-05      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.15e-05      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 452           |
|    iterations           | 36            |
|    time_elapsed         | 162           |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.0016363262  |
|    average_cost         | 0.0024414062  |
|    clip_fraction        | 0.00742       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -8.32         |
|    cost_value_loss      | 0.000327      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.136        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000592     |
|    mean_cost_advantages | -0.0007001067 |
|    mean_reward_advan... | -0.0150397895 |
|    n_updates            | 350           |
|    nu                   | 3.64          |
|    nu_loss              | -0.0088       |
|    policy_gradient_loss | 1.25e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.73e-05      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 37            |
|    time_elapsed         | 168           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.000893333   |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.0192        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -13.6         |
|    cost_value_loss      | 0.000335      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.126        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000812      |
|    mean_cost_advantages | 0.00035021393 |
|    mean_reward_advan... | -0.015693953  |
|    n_updates            | 360           |
|    nu                   | 3.68          |
|    nu_loss              | -0.0107       |
|    policy_gradient_loss | -0.000488     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 7.26e-05      |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 776           |
|    ep_len_mean          | 13.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 38            |
|    time_elapsed         | 172           |
|    total_timesteps      | 77824         |
| train/                  |               |
|    approx_kl            | 0.016262032   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0197        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.103         |
|    cost_value_loss      | 1.25e-05      |
|    early_stop_epoch     | 7             |
|    entropy_loss         | -0.123        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00431      |
|    mean_cost_advantages | -0.0028284488 |
|    mean_reward_advan... | -0.011390616  |
|    n_updates            | 370           |
|    nu                   | 3.71          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000198     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.12e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 821           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 452           |
|    iterations           | 39            |
|    time_elapsed         | 176           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.01859117    |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.11          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -29.3         |
|    cost_value_loss      | 0.000278      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.158        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00475      |
|    mean_cost_advantages | 0.00076659466 |
|    mean_reward_advan... | -0.036168747  |
|    n_updates            | 380           |
|    nu                   | 3.74          |
|    nu_loss              | -0.00725      |
|    policy_gradient_loss | -0.00442      |
|    reward_explained_... | 0.889         |
|    reward_value_loss    | 0.00219       |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 451           |
|    iterations           | 40            |
|    time_elapsed         | 181           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.008270015   |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0231        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -22.3         |
|    cost_value_loss      | 0.000253      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.116        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00046      |
|    mean_cost_advantages | -0.0015318171 |
|    mean_reward_advan... | -0.0021744836 |
|    n_updates            | 390           |
|    nu                   | 3.77          |
|    nu_loss              | -0.00731      |
|    policy_gradient_loss | -0.00149      |
|    reward_explained_... | 0.984         |
|    reward_value_loss    | 0.000304      |
|    total_cost           | 4.0           |
-------------------------------------------
Mean reward: 9994.500000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[0. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[1. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[2. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[3. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[4. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[4. 5.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 5.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 4.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 3.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 2.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[6. 2.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[6. 1.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[0. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[1. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[2. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[3. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[4. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 5.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 4.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 3.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 2.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 1.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[6. 1.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[0. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[1. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[2. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[3. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[4. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 6.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 5.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 4.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 3.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 2.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[5. 1.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[6. 1.]
[ 0  1  2  3  4  5  6  7  8  9 10 11]
[6 0]
[0. 6.]
[32;1mTime taken: 03.28 minutes[0m
