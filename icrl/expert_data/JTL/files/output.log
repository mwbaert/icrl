[32mConfigured folder /tmp/wandb/run-20220706_080523-157hc4m4/files for saving
[32mName: JTL-v0_CJTL-v0_dnc_True_dno_True_dnr_True_goal_0_ws_True_s_20_sid_-1
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
Wrapping eval env in a VecNormalize.
Using cpu device
----------------------------------
| eval/               |          |
|    best_mean_reward | -6e+03   |
|    mean_ep_length   | 114      |
|    mean_reward      | -6e+03   |
|    true_cost        | 0.49     |
| infos/              |          |
|    cost             | 0.61     |
| rollout/            |          |
|    adjusted_reward  | -1.21    |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | -99.6    |
| time/               |          |
|    fps              | 1359     |
|    iterations       | 1        |
|    time_elapsed     | 7        |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -3.81e+03  |
|    mean_ep_length       | 83         |
|    mean_reward          | -3.81e+03  |
|    true_cost            | 0.381      |
| infos/                  |            |
|    cost                 | 0.35       |
| rollout/                |            |
|    adjusted_reward      | -1.07      |
|    ep_len_mean          | 144        |
|    ep_rew_mean          | -98.4      |
| time/                   |            |
|    fps                  | 1200       |
|    iterations           | 2          |
|    time_elapsed         | 17         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.01839782 |
|    average_cost         | 0.48984376 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -294       |
|    cost_value_loss      | 30.1       |
|    early_stop_epoch     | 1          |
|    entropy_loss         | -1.38      |
|    learning_rate        | 0.0003     |
|    loss                 | 19.4       |
|    mean_cost_advantages | 7.513114   |
|    mean_reward_advan... | -10.966848 |
|    n_updates            | 10         |
|    nu                   | 1.06       |
|    nu_loss              | -0.49      |
|    policy_gradient_loss | -0.0182    |
|    reward_explained_... | -445       |
|    reward_value_loss    | 58.4       |
|    total_cost           | 5016.0     |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.81e+03   |
|    mean_ep_length       | 165         |
|    mean_reward          | -4.17e+03   |
|    true_cost            | 0.234       |
| infos/                  |             |
|    cost                 | 0.13        |
| rollout/                |             |
|    adjusted_reward      | -0.893      |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | -89.8       |
| time/                   |             |
|    fps                  | 1097        |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.017742487 |
|    average_cost         | 0.3807617   |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -7.3        |
|    cost_value_loss      | 29.3        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 34.5        |
|    mean_cost_advantages | 4.113931    |
|    mean_reward_advan... | -7.3170905  |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.405      |
|    policy_gradient_loss | -0.0226     |
|    reward_explained_... | -14.3       |
|    reward_value_loss    | 54.1        |
|    total_cost           | 3899.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.73e+03   |
|    mean_ep_length       | 103         |
|    mean_reward          | -1.73e+03   |
|    true_cost            | 0.157       |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.702      |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | -64.5       |
| time/                   |             |
|    fps                  | 1076        |
|    iterations           | 4           |
|    time_elapsed         | 38          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.035018105 |
|    average_cost         | 0.23408203  |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.02       |
|    cost_value_loss      | 21.6        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 30          |
|    mean_cost_advantages | 1.053278    |
|    mean_reward_advan... | -5.0078554  |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.265      |
|    policy_gradient_loss | -0.0266     |
|    reward_explained_... | -8.95       |
|    reward_value_loss    | 48.2        |
|    total_cost           | 2397.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -457        |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -457        |
|    true_cost            | 0.142       |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.676      |
|    ep_len_mean          | 87.9        |
|    ep_rew_mean          | -44.1       |
| time/                   |             |
|    fps                  | 1078        |
|    iterations           | 5           |
|    time_elapsed         | 47          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.0208874   |
|    average_cost         | 0.1571289   |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.79       |
|    cost_value_loss      | 18.1        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.11       |
|    learning_rate        | 0.0003      |
|    loss                 | 36.9        |
|    mean_cost_advantages | -0.65158725 |
|    mean_reward_advan... | -1.3252696  |
|    n_updates            | 40          |
|    nu                   | 1.26        |
|    nu_loss              | -0.188      |
|    policy_gradient_loss | -0.0187     |
|    reward_explained_... | -2.14       |
|    reward_value_loss    | 44.5        |
|    total_cost           | 1609.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -457       |
|    mean_ep_length       | 48.2       |
|    mean_reward          | -1.35e+03  |
|    true_cost            | 0.0799     |
| infos/                  |            |
|    cost                 | 0.09       |
| rollout/                |            |
|    adjusted_reward      | -0.59      |
|    ep_len_mean          | 75.3       |
|    ep_rew_mean          | -35.4      |
| time/                   |            |
|    fps                  | 1067       |
|    iterations           | 6          |
|    time_elapsed         | 57         |
|    total_timesteps      | 61440      |
| train/                  |            |
|    approx_kl            | 0.0213981  |
|    average_cost         | 0.1415039  |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -1.82      |
|    cost_value_loss      | 19.4       |
|    early_stop_epoch     | 2          |
|    entropy_loss         | -1.01      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.8       |
|    mean_cost_advantages | -0.7592213 |
|    mean_reward_advan... | 0.2515127  |
|    n_updates            | 50         |
|    nu                   | 1.32       |
|    nu_loss              | -0.178     |
|    policy_gradient_loss | -0.0184    |
|    reward_explained_... | -0.785     |
|    reward_value_loss    | 48.8       |
|    total_cost           | 1449.0     |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -60.2       |
|    mean_ep_length       | 54.4        |
|    mean_reward          | -60.2       |
|    true_cost            | 0.0781      |
| infos/                  |             |
|    cost                 | 0.07        |
| rollout/                |             |
|    adjusted_reward      | -0.491      |
|    ep_len_mean          | 44.7        |
|    ep_rew_mean          | -17.5       |
| time/                   |             |
|    fps                  | 1069        |
|    iterations           | 7           |
|    time_elapsed         | 67          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.021446316 |
|    average_cost         | 0.079882815 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.827      |
|    cost_value_loss      | 10.5        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.941      |
|    learning_rate        | 0.0003      |
|    loss                 | 38          |
|    mean_cost_advantages | -1.5317435  |
|    mean_reward_advan... | 1.0574358   |
|    n_updates            | 60          |
|    nu                   | 1.38        |
|    nu_loss              | -0.106      |
|    policy_gradient_loss | -0.0202     |
|    reward_explained_... | 0.0217      |
|    reward_value_loss    | 53.1        |
|    total_cost           | 818.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -60.2       |
|    mean_ep_length       | 32.4        |
|    mean_reward          | -170        |
|    true_cost            | 0.0649      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.443      |
|    ep_len_mean          | 37.9        |
|    ep_rew_mean          | -13         |
| time/                   |             |
|    fps                  | 1078        |
|    iterations           | 8           |
|    time_elapsed         | 75          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.032821946 |
|    average_cost         | 0.078125    |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.569      |
|    cost_value_loss      | 5.36        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.925      |
|    learning_rate        | 0.0003      |
|    loss                 | 12          |
|    mean_cost_advantages | -1.8420731  |
|    mean_reward_advan... | 4.8652725   |
|    n_updates            | 70          |
|    nu                   | 1.44        |
|    nu_loss              | -0.108      |
|    policy_gradient_loss | -0.0178     |
|    reward_explained_... | 0.525       |
|    reward_value_loss    | 26.5        |
|    total_cost           | 800.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -60.2       |
|    mean_ep_length       | 24          |
|    mean_reward          | -107        |
|    true_cost            | 0.0587      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.43       |
|    ep_len_mean          | 27.7        |
|    ep_rew_mean          | -9.34       |
| time/                   |             |
|    fps                  | 1085        |
|    iterations           | 9           |
|    time_elapsed         | 84          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.022149414 |
|    average_cost         | 0.064941406 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.456      |
|    cost_value_loss      | 3.89        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.827      |
|    learning_rate        | 0.0003      |
|    loss                 | 6.07        |
|    mean_cost_advantages | -1.3879765  |
|    mean_reward_advan... | 3.6370063   |
|    n_updates            | 80          |
|    nu                   | 1.49        |
|    nu_loss              | -0.0934     |
|    policy_gradient_loss | -0.0145     |
|    reward_explained_... | 0.649       |
|    reward_value_loss    | 16.4        |
|    total_cost           | 665.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -50.4       |
|    mean_ep_length       | 26.8        |
|    mean_reward          | -50.4       |
|    true_cost            | 0.0475      |
| infos/                  |             |
|    cost                 | 0.07        |
| rollout/                |             |
|    adjusted_reward      | -0.402      |
|    ep_len_mean          | 24.7        |
|    ep_rew_mean          | -7.84       |
| time/                   |             |
|    fps                  | 1097        |
|    iterations           | 10          |
|    time_elapsed         | 93          |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.017507384 |
|    average_cost         | 0.058691405 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.203       |
|    cost_value_loss      | 1.63        |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.762      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.39        |
|    mean_cost_advantages | -1.0211903  |
|    mean_reward_advan... | 2.8949084   |
|    n_updates            | 90          |
|    nu                   | 1.55        |
|    nu_loss              | -0.0876     |
|    policy_gradient_loss | -0.00891    |
|    reward_explained_... | 0.713       |
|    reward_value_loss    | 7.35        |
|    total_cost           | 601.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.4        |
|    mean_ep_length       | 23.2        |
|    mean_reward          | -7.4        |
|    true_cost            | 0.0425      |
| infos/                  |             |
|    cost                 | 0.07        |
| rollout/                |             |
|    adjusted_reward      | -0.384      |
|    ep_len_mean          | 24.6        |
|    ep_rew_mean          | -7.43       |
| time/                   |             |
|    fps                  | 1110        |
|    iterations           | 11          |
|    time_elapsed         | 101         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.015314044 |
|    average_cost         | 0.047460936 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.402       |
|    cost_value_loss      | 0.629       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.716      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    mean_cost_advantages | -0.5772257  |
|    mean_reward_advan... | 1.3143852   |
|    n_updates            | 100         |
|    nu                   | 1.6         |
|    nu_loss              | -0.0734     |
|    policy_gradient_loss | -0.00535    |
|    reward_explained_... | 0.671       |
|    reward_value_loss    | 4.86        |
|    total_cost           | 486.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -6.1        |
|    mean_ep_length       | 20          |
|    mean_reward          | -6.1        |
|    true_cost            | 0.0466      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.38       |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | -6.11       |
| time/                   |             |
|    fps                  | 1115        |
|    iterations           | 12          |
|    time_elapsed         | 110         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.020223225 |
|    average_cost         | 0.04248047  |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.335       |
|    cost_value_loss      | 0.329       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.619      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.796       |
|    mean_cost_advantages | -0.20078728 |
|    mean_reward_advan... | 0.123883866 |
|    n_updates            | 110         |
|    nu                   | 1.65        |
|    nu_loss              | -0.0679     |
|    policy_gradient_loss | -0.00892    |
|    reward_explained_... | 0.515       |
|    reward_value_loss    | 3.9         |
|    total_cost           | 435.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.62       |
|    mean_ep_length       | 18.2        |
|    mean_reward          | -5.62       |
|    true_cost            | 0.0247      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.341      |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | -5.45       |
| time/                   |             |
|    fps                  | 1121        |
|    iterations           | 13          |
|    time_elapsed         | 118         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.033729456 |
|    average_cost         | 0.046582032 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.306      |
|    cost_value_loss      | 0.334       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.553      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.487       |
|    mean_cost_advantages | -0.06708034 |
|    mean_reward_advan... | 0.6016488   |
|    n_updates            | 120         |
|    nu                   | 1.69        |
|    nu_loss              | -0.0767     |
|    policy_gradient_loss | -0.0129     |
|    reward_explained_... | 0.799       |
|    reward_value_loss    | 0.964       |
|    total_cost           | 477.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.07       |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -4.07       |
|    true_cost            | 0.00957     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.292      |
|    ep_len_mean          | 15.7        |
|    ep_rew_mean          | -4.34       |
| time/                   |             |
|    fps                  | 1134        |
|    iterations           | 14          |
|    time_elapsed         | 126         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.02825901  |
|    average_cost         | 0.02470703  |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0207      |
|    cost_value_loss      | 0.192       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.455      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.2         |
|    mean_cost_advantages | -0.18029495 |
|    mean_reward_advan... | 0.39949325  |
|    n_updates            | 130         |
|    nu                   | 1.74        |
|    nu_loss              | -0.0419     |
|    policy_gradient_loss | -0.0161     |
|    reward_explained_... | 0.878       |
|    reward_value_loss    | 0.615       |
|    total_cost           | 253.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.17       |
|    mean_ep_length       | 13          |
|    mean_reward          | -3.17       |
|    true_cost            | 0.00791     |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.271      |
|    ep_len_mean          | 13.6        |
|    ep_rew_mean          | -3.47       |
| time/                   |             |
|    fps                  | 1144        |
|    iterations           | 15          |
|    time_elapsed         | 134         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.020085959 |
|    average_cost         | 0.009570313 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.845      |
|    cost_value_loss      | 0.082       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.348      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0944      |
|    mean_cost_advantages | -0.07986795 |
|    mean_reward_advan... | 0.43143147  |
|    n_updates            | 140         |
|    nu                   | 1.78        |
|    nu_loss              | -0.0166     |
|    policy_gradient_loss | -0.0133     |
|    reward_explained_... | 0.905       |
|    reward_value_loss    | 0.311       |
|    total_cost           | 98.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.17        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | -3.37        |
|    true_cost            | 0.00264      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.256       |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | -3.26        |
| time/                   |              |
|    fps                  | 1145         |
|    iterations           | 16           |
|    time_elapsed         | 143          |
|    total_timesteps      | 163840       |
| train/                  |              |
|    approx_kl            | 0.016728075  |
|    average_cost         | 0.007910157  |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.3         |
|    cost_value_loss      | 0.066        |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.235       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.159        |
|    mean_cost_advantages | -0.013890323 |
|    mean_reward_advan... | 0.27706864   |
|    n_updates            | 150          |
|    nu                   | 1.82         |
|    nu_loss              | -0.0141      |
|    policy_gradient_loss | -0.00924     |
|    reward_explained_... | 0.926        |
|    reward_value_loss    | 0.164        |
|    total_cost           | 81.0         |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.1         |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -3.1         |
|    true_cost            | 0.000879     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.25        |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | -3.12        |
| time/                   |              |
|    fps                  | 1098         |
|    iterations           | 17           |
|    time_elapsed         | 158          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.016471993  |
|    average_cost         | 0.0026367188 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.74        |
|    cost_value_loss      | 0.0313       |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.17        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0319       |
|    mean_cost_advantages | -0.002813962 |
|    mean_reward_advan... | 0.18168092   |
|    n_updates            | 160          |
|    nu                   | 1.86         |
|    nu_loss              | -0.0048      |
|    policy_gradient_loss | -0.00903     |
|    reward_explained_... | 0.955        |
|    reward_value_loss    | 0.0845       |
|    total_cost           | 27.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.77        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -2.77        |
|    true_cost            | 0.00117      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.248       |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | -2.95        |
| time/                   |              |
|    fps                  | 1107         |
|    iterations           | 18           |
|    time_elapsed         | 166          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.024145171  |
|    average_cost         | 0.0008789062 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.87        |
|    cost_value_loss      | 0.00904      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.109       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0124       |
|    mean_cost_advantages | -0.026657408 |
|    mean_reward_advan... | 0.1320065    |
|    n_updates            | 170          |
|    nu                   | 1.89         |
|    nu_loss              | -0.00163     |
|    policy_gradient_loss | -0.00751     |
|    reward_explained_... | 0.966        |
|    reward_value_loss    | 0.0626       |
|    total_cost           | 9.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -2.77          |
|    mean_ep_length       | 11.6           |
|    mean_reward          | -2.9           |
|    true_cost            | 0.00107        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.247         |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | -2.88          |
| time/                   |                |
|    fps                  | 1062           |
|    iterations           | 19             |
|    time_elapsed         | 183            |
|    total_timesteps      | 194560         |
| train/                  |                |
|    approx_kl            | 0.012679407    |
|    average_cost         | 0.001171875    |
|    clip_fraction        | 0.0292         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.15          |
|    cost_value_loss      | 0.012          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0851        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00964        |
|    mean_cost_advantages | -0.00081518217 |
|    mean_reward_advan... | 0.10807017     |
|    n_updates            | 180            |
|    nu                   | 1.92           |
|    nu_loss              | -0.00221       |
|    policy_gradient_loss | -0.00412       |
|    reward_explained_... | 0.979          |
|    reward_value_loss    | 0.0315         |
|    total_cost           | 12.0           |
--------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.10
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.77         |
|    mean_ep_length       | 13.6          |
|    mean_reward          | -3.33         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.233        |
|    ep_len_mean          | 13.5          |
|    ep_rew_mean          | -3.12         |
| time/                   |               |
|    fps                  | 1025          |
|    iterations           | 20            |
|    time_elapsed         | 199           |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 0.095536694   |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0.0329        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00869      |
|    cost_value_loss      | 0.00786       |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.0883       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00401      |
|    mean_cost_advantages | -0.0040511666 |
|    mean_reward_advan... | 0.075118676   |
|    n_updates            | 190           |
|    nu                   | 1.94          |
|    nu_loss              | -0.00206      |
|    policy_gradient_loss | -0.00263      |
|    reward_explained_... | 0.991         |
|    reward_value_loss    | 0.0124        |
|    total_cost           | 11.0          |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.77         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -2.9          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.239        |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | -2.95         |
| time/                   |               |
|    fps                  | 1036          |
|    iterations           | 21            |
|    time_elapsed         | 207           |
|    total_timesteps      | 215040        |
| train/                  |               |
|    approx_kl            | 0.022698456   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.23          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00308      |
|    cost_value_loss      | 1.81e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.242        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00292       |
|    mean_cost_advantages | -0.0003895418 |
|    mean_reward_advan... | -0.14222065   |
|    n_updates            | 200           |
|    nu                   | 1.97          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0109       |
|    reward_explained_... | 0.975         |
|    reward_value_loss    | 0.0395        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.77         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | -2.9          |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.245        |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | -2.91         |
| time/                   |               |
|    fps                  | 1044          |
|    iterations           | 22            |
|    time_elapsed         | 215           |
|    total_timesteps      | 225280        |
| train/                  |               |
|    approx_kl            | 0.07982807    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.212         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0541       |
|    cost_value_loss      | 1.15e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0914       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00327       |
|    mean_cost_advantages | 0.00035933164 |
|    mean_reward_advan... | 0.03443911    |
|    n_updates            | 210           |
|    nu                   | 1.99          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00908      |
|    reward_explained_... | 0.988         |
|    reward_value_loss    | 0.0202        |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.77        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -2.77        |
|    true_cost            | 0.00127      |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.249       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.86        |
| time/                   |              |
|    fps                  | 1008         |
|    iterations           | 23           |
|    time_elapsed         | 233          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0049890443 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.74        |
|    cost_value_loss      | 0.00556      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0652      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00438      |
|    mean_cost_advantages | 0.0017936568 |
|    mean_reward_advan... | 0.114178695  |
|    n_updates            | 220          |
|    nu                   | 2.01         |
|    nu_loss              | -0.000778    |
|    policy_gradient_loss | -0.00256     |
|    reward_explained_... | 0.995        |
|    reward_value_loss    | 0.00769      |
|    total_cost           | 4.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.77        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -2.9         |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.246       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.82        |
| time/                   |              |
|    fps                  | 972          |
|    iterations           | 24           |
|    time_elapsed         | 252          |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0032792247 |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.69        |
|    cost_value_loss      | 0.0117       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0426      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00922      |
|    mean_cost_advantages | 0.004207271  |
|    mean_reward_advan... | 0.027695369  |
|    n_updates            | 230          |
|    nu                   | 2.03         |
|    nu_loss              | -0.00256     |
|    policy_gradient_loss | -0.00105     |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 0.00471      |
|    total_cost           | 13.0         |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.77        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -3.3         |
|    true_cost            | 0.00156      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.258       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.89        |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 25           |
|    time_elapsed         | 266          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.016666269  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.66        |
|    cost_value_loss      | 0.000406     |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.0164      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00226      |
|    mean_cost_advantages | -0.015263468 |
|    mean_reward_advan... | 0.016598346  |
|    n_updates            | 240          |
|    nu                   | 2.05         |
|    nu_loss              | -0.000198    |
|    policy_gradient_loss | -0.00127     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00173      |
|    total_cost           | 1.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.77        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -2.77        |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.248       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -2.82        |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 26           |
|    time_elapsed         | 275          |
|    total_timesteps      | 266240       |
| train/                  |              |
|    approx_kl            | 0.016577518  |
|    average_cost         | 0.0015625    |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.282       |
|    cost_value_loss      | 0.0177       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0155      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0365       |
|    mean_cost_advantages | 0.004022775  |
|    mean_reward_advan... | -0.039654378 |
|    n_updates            | 250          |
|    nu                   | 2.07         |
|    nu_loss              | -0.0032      |
|    policy_gradient_loss | -0.000204    |
|    reward_explained_... | 0.94         |
|    reward_value_loss    | 0.0933       |
|    total_cost           | 16.0         |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.23
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.77         |
|    mean_ep_length       | 14.6          |
|    mean_reward          | -3.65         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.243        |
|    ep_len_mean          | 13.9          |
|    ep_rew_mean          | -3.35         |
| time/                   |               |
|    fps                  | 963           |
|    iterations           | 27            |
|    time_elapsed         | 286           |
|    total_timesteps      | 276480        |
| train/                  |               |
|    approx_kl            | 0.23403876    |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0488        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.43         |
|    cost_value_loss      | 0.00872       |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.0411       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00844      |
|    mean_cost_advantages | 0.019217124   |
|    mean_reward_advan... | 0.07469861    |
|    n_updates            | 260           |
|    nu                   | 2.08          |
|    nu_loss              | -0.00101      |
|    policy_gradient_loss | -0.00122      |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 0.00312       |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.77         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | -3            |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.238        |
|    ep_len_mean          | 13.4          |
|    ep_rew_mean          | -3.18         |
| time/                   |               |
|    fps                  | 971           |
|    iterations           | 28            |
|    time_elapsed         | 294           |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 0.06307821    |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.127         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.45         |
|    cost_value_loss      | 0.00474       |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0719       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0136        |
|    mean_cost_advantages | -0.0039643957 |
|    mean_reward_advan... | -0.27342188   |
|    n_updates            | 270           |
|    nu                   | 2.09          |
|    nu_loss              | -0.00122      |
|    policy_gradient_loss | -0.00665      |
|    reward_explained_... | 0.954         |
|    reward_value_loss    | 0.0547        |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.73        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | -2.73        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.237       |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | -2.98        |
| time/                   |              |
|    fps                  | 979          |
|    iterations           | 29           |
|    time_elapsed         | 303          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.055639267  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.139        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.47         |
|    cost_value_loss      | 9.92e-06     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0787      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00541      |
|    mean_cost_advantages | -0.012120055 |
|    mean_reward_advan... | 0.09419997   |
|    n_updates            | 280          |
|    nu                   | 2.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00762     |
|    reward_explained_... | 0.988        |
|    reward_value_loss    | 0.0185       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.73         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | -3.17         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.242        |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | -2.86         |
| time/                   |               |
|    fps                  | 986           |
|    iterations           | 30            |
|    time_elapsed         | 311           |
|    total_timesteps      | 307200        |
| train/                  |               |
|    approx_kl            | 0.029442957   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.112         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.264         |
|    cost_value_loss      | 3.69e-06      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0794       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00284       |
|    mean_cost_advantages | -0.0029259878 |
|    mean_reward_advan... | 0.0775123     |
|    n_updates            | 290           |
|    nu                   | 2.12          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00752      |
|    reward_explained_... | 0.99          |
|    reward_value_loss    | 0.0155        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.73        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -2.77        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.245       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.83        |
| time/                   |              |
|    fps                  | 972          |
|    iterations           | 31           |
|    time_elapsed         | 326          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.015393393  |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.36        |
|    cost_value_loss      | 0.00569      |
|    early_stop_epoch     | 8            |
|    entropy_loss         | -0.0722      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00218      |
|    mean_cost_advantages | 0.0010057057 |
|    mean_reward_advan... | 0.101886794  |
|    n_updates            | 300          |
|    nu                   | 2.13         |
|    nu_loss              | -0.000827    |
|    policy_gradient_loss | -0.00396     |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 0.00978      |
|    total_cost           | 4.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.73        |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -3.03        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.246       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -2.83        |
| time/                   |              |
|    fps                  | 953          |
|    iterations           | 32           |
|    time_elapsed         | 343          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0026198686 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.139       |
|    cost_value_loss      | 3.07e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0678      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00242      |
|    mean_cost_advantages | 0.0023419042 |
|    mean_reward_advan... | 0.065174595  |
|    n_updates            | 310          |
|    nu                   | 2.14         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00118     |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.00272      |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.63        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -2.63        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.246       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.85        |
| time/                   |              |
|    fps                  | 937          |
|    iterations           | 33           |
|    time_elapsed         | 360          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0024766831 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.00709      |
|    cost_value_loss      | 4.17e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0661      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0057       |
|    mean_cost_advantages | 0.000910031  |
|    mean_reward_advan... | 0.022961961  |
|    n_updates            | 320          |
|    nu                   | 2.15         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000477    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00137      |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.06
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 14            |
|    mean_reward          | -2.98         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.196        |
|    ep_len_mean          | 16.8          |
|    ep_rew_mean          | -3.28         |
| time/                   |               |
|    fps                  | 928           |
|    iterations           | 34            |
|    time_elapsed         | 374           |
|    total_timesteps      | 348160        |
| train/                  |               |
|    approx_kl            | 0.060264803   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0275        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.033        |
|    cost_value_loss      | 2.7e-07       |
|    early_stop_epoch     | 8             |
|    entropy_loss         | -0.0662       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000287      |
|    mean_cost_advantages | 0.00014615535 |
|    mean_reward_advan... | 0.010294543   |
|    n_updates            | 330           |
|    nu                   | 2.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00036      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 0.00104       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 15            |
|    mean_reward          | -3.22         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.22         |
|    ep_len_mean          | 13.6          |
|    ep_rew_mean          | -3.01         |
| time/                   |               |
|    fps                  | 936           |
|    iterations           | 35            |
|    time_elapsed         | 382           |
|    total_timesteps      | 358400        |
| train/                  |               |
|    approx_kl            | 0.033618934   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.26          |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0148        |
|    cost_value_loss      | 2.02e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.345        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0154        |
|    mean_cost_advantages | -5.474903e-05 |
|    mean_reward_advan... | -0.31056827   |
|    n_updates            | 340           |
|    nu                   | 2.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0117       |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 0.0706        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -2.63          |
|    mean_ep_length       | 13.6           |
|    mean_reward          | -3.07          |
|    true_cost            | 0.000684       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.232         |
|    ep_len_mean          | 13             |
|    ep_rew_mean          | -2.99          |
| time/                   |                |
|    fps                  | 927            |
|    iterations           | 36             |
|    time_elapsed         | 397            |
|    total_timesteps      | 368640         |
| train/                  |                |
|    approx_kl            | 0.015425546    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.153          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0407        |
|    cost_value_loss      | 2.41e-07       |
|    early_stop_epoch     | 8              |
|    entropy_loss         | -0.243         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00107       |
|    mean_cost_advantages | -0.00015681048 |
|    mean_reward_advan... | 0.040512964    |
|    n_updates            | 350            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00855       |
|    reward_explained_... | 0.977          |
|    reward_value_loss    | 0.0295         |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | -2.78         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.237        |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | -2.93         |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 37            |
|    time_elapsed         | 405           |
|    total_timesteps      | 378880        |
| train/                  |               |
|    approx_kl            | 0.021510273   |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.162         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.42         |
|    cost_value_loss      | 0.0117        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.195        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000856      |
|    mean_cost_advantages | 0.0032694614  |
|    mean_reward_advan... | 0.09251477    |
|    n_updates            | 360           |
|    nu                   | 2.17          |
|    nu_loss              | -0.00148      |
|    policy_gradient_loss | -0.00853      |
|    reward_explained_... | 0.99          |
|    reward_value_loss    | 0.0163        |
|    total_cost           | 7.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | -2.77         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.245        |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | -2.84         |
| time/                   |               |
|    fps                  | 940           |
|    iterations           | 38            |
|    time_elapsed         | 413           |
|    total_timesteps      | 389120        |
| train/                  |               |
|    approx_kl            | 0.037627302   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.135         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0942        |
|    cost_value_loss      | 4.24e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.145        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00063      |
|    mean_cost_advantages | -0.0063609094 |
|    mean_reward_advan... | 0.05958314    |
|    n_updates            | 370           |
|    nu                   | 2.18          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00835      |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 0.00683       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -2.63          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | -2.93          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.245         |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | -2.8           |
| time/                   |                |
|    fps                  | 930            |
|    iterations           | 39             |
|    time_elapsed         | 429            |
|    total_timesteps      | 399360         |
| train/                  |                |
|    approx_kl            | 0.0022576617   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0285         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0309        |
|    cost_value_loss      | 3.64e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0862        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00192        |
|    mean_cost_advantages | -0.00014635253 |
|    mean_reward_advan... | 0.07060996     |
|    n_updates            | 380            |
|    nu                   | 2.18           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.002         |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 0.0013         |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | -2.63         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.246        |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | -2.84         |
| time/                   |               |
|    fps                  | 920           |
|    iterations           | 40            |
|    time_elapsed         | 445           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 0.003541796   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0351        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0391       |
|    cost_value_loss      | 1.01e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0721       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000282      |
|    mean_cost_advantages | 0.00014351275 |
|    mean_reward_advan... | 0.015766462   |
|    n_updates            | 390           |
|    nu                   | 2.19          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00158      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.00071       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 200           |
|    mean_reward          | -38.6         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.193        |
|    ep_len_mean          | 103           |
|    ep_rew_mean          | -20.1         |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 41            |
|    time_elapsed         | 454           |
|    total_timesteps      | 419840        |
| train/                  |               |
|    approx_kl            | 0.03575262    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0536        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0767       |
|    cost_value_loss      | 7.87e-07      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0715       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00906      |
|    mean_cost_advantages | -0.00076247   |
|    mean_reward_advan... | 0.00034684353 |
|    n_updates            | 400           |
|    nu                   | 2.19          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00137      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.000461      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 98.4          |
|    mean_reward          | -18.9         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.19         |
|    ep_len_mean          | 126           |
|    ep_rew_mean          | -24           |
| time/                   |               |
|    fps                  | 912           |
|    iterations           | 42            |
|    time_elapsed         | 471           |
|    total_timesteps      | 430080        |
| train/                  |               |
|    approx_kl            | 0.0109203765  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.132         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0467       |
|    cost_value_loss      | 6.5e-06       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.184        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.216         |
|    mean_cost_advantages | 0.00015582128 |
|    mean_reward_advan... | -2.9029584    |
|    n_updates            | 410           |
|    nu                   | 2.2           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00339      |
|    reward_explained_... | -0.269        |
|    reward_value_loss    | 0.354         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 26.6          |
|    mean_reward          | -5.68         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.2          |
|    ep_len_mean          | 35.5          |
|    ep_rew_mean          | -7.21         |
| time/                   |               |
|    fps                  | 904           |
|    iterations           | 43            |
|    time_elapsed         | 486           |
|    total_timesteps      | 440320        |
| train/                  |               |
|    approx_kl            | 0.01285658    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.101         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0304       |
|    cost_value_loss      | 9.4e-06       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.197        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.542         |
|    mean_cost_advantages | 0.00015937748 |
|    mean_reward_advan... | -2.0060287    |
|    n_updates            | 420           |
|    nu                   | 2.2           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00302      |
|    reward_explained_... | -5.17         |
|    reward_value_loss    | 1.45          |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.63        |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -3.03        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.23        |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | -4.28        |
| time/                   |              |
|    fps                  | 895          |
|    iterations           | 44           |
|    time_elapsed         | 503          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.015313585  |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0.155        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.43        |
|    cost_value_loss      | 0.0147       |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.21        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.64         |
|    mean_cost_advantages | 0.010180733  |
|    mean_reward_advan... | -0.3683452   |
|    n_updates            | 430          |
|    nu                   | 2.2          |
|    nu_loss              | -0.00129     |
|    policy_gradient_loss | -0.0068      |
|    reward_explained_... | -3.14        |
|    reward_value_loss    | 4.22         |
|    total_cost           | 6.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.63        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -2.77        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.244       |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | -3.19        |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 45           |
|    time_elapsed         | 521          |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 0.011184616  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.169        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.272        |
|    cost_value_loss      | 9.01e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.157       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.69         |
|    mean_cost_advantages | 0.0033996452 |
|    mean_reward_advan... | 1.1179678    |
|    n_updates            | 440          |
|    nu                   | 2.21         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00843     |
|    reward_explained_... | -0.0194      |
|    reward_value_loss    | 3.62         |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.63       |
|    mean_ep_length       | 11.8        |
|    mean_reward          | -3.03       |
|    true_cost            | 0.00166     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.25       |
|    ep_len_mean          | 11.6        |
|    ep_rew_mean          | -2.88       |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 46          |
|    time_elapsed         | 529         |
|    total_timesteps      | 471040      |
| train/                  |             |
|    approx_kl            | 0.019170694 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.143       |
|    cost_value_loss      | 2.2e-06     |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0818     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.464       |
|    mean_cost_advantages | 0.001295086 |
|    mean_reward_advan... | 0.85969985  |
|    n_updates            | 450         |
|    nu                   | 2.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00519    |
|    reward_explained_... | 0.712       |
|    reward_value_loss    | 0.854       |
|    total_cost           | 0.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.63        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | -2.92        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.245       |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | -2.87        |
| time/                   |              |
|    fps                  | 874          |
|    iterations           | 47           |
|    time_elapsed         | 550          |
|    total_timesteps      | 481280       |
| train/                  |              |
|    approx_kl            | 0.013560435  |
|    average_cost         | 0.0016601563 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.238       |
|    cost_value_loss      | 0.0175       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0705      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0355       |
|    mean_cost_advantages | 0.0049225343 |
|    mean_reward_advan... | 0.28681058   |
|    n_updates            | 460          |
|    nu                   | 2.21         |
|    nu_loss              | -0.00367     |
|    policy_gradient_loss | -0.00193     |
|    reward_explained_... | 0.95         |
|    reward_value_loss    | 0.0231       |
|    total_cost           | 17.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.63        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -2.9         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.244       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.84        |
| time/                   |              |
|    fps                  | 868          |
|    iterations           | 48           |
|    time_elapsed         | 565          |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0037903718 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.035       |
|    cost_value_loss      | 1.47e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0735      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00296     |
|    mean_cost_advantages | 0.0012137259 |
|    mean_reward_advan... | 0.011386764  |
|    n_updates            | 470          |
|    nu                   | 2.21         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00175     |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 0.0082       |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -2.63          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | -2.77          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.247         |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | -2.88          |
| time/                   |                |
|    fps                  | 862            |
|    iterations           | 49             |
|    time_elapsed         | 581            |
|    total_timesteps      | 501760         |
| train/                  |                |
|    approx_kl            | 0.010782644    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0307         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00482       |
|    cost_value_loss      | 0.000419       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0625        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000784       |
|    mean_cost_advantages | 0.0004779268   |
|    mean_reward_advan... | -1.4979579e-05 |
|    n_updates            | 480            |
|    nu                   | 2.22           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00246       |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 0.00404        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | -2.63         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.246        |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | -2.82         |
| time/                   |               |
|    fps                  | 856           |
|    iterations           | 50            |
|    time_elapsed         | 597           |
|    total_timesteps      | 512000        |
| train/                  |               |
|    approx_kl            | 0.0012295155  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0289        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0268       |
|    cost_value_loss      | 1.91e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0599       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000428      |
|    mean_cost_advantages | 3.0229712e-05 |
|    mean_reward_advan... | 0.01414907    |
|    n_updates            | 490           |
|    nu                   | 2.22          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000215      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.000435      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -3.03         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.252        |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | -3.05         |
| time/                   |               |
|    fps                  | 856           |
|    iterations           | 51            |
|    time_elapsed         | 610           |
|    total_timesteps      | 522240        |
| train/                  |               |
|    approx_kl            | 0.041970674   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.038         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00688      |
|    cost_value_loss      | 1.9e-05       |
|    early_stop_epoch     | 5             |
|    entropy_loss         | -0.073        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00564      |
|    mean_cost_advantages | 0.00031039218 |
|    mean_reward_advan... | -0.0022299706 |
|    n_updates            | 500           |
|    nu                   | 2.22          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000318     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 0.00306       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.63        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -2.9         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.246       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -2.85        |
| time/                   |              |
|    fps                  | 860          |
|    iterations           | 52           |
|    time_elapsed         | 618          |
|    total_timesteps      | 532480       |
| train/                  |              |
|    approx_kl            | 0.08601679   |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.0929       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -426         |
|    cost_value_loss      | 0.000475     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0585      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0122       |
|    mean_cost_advantages | 0.0009861854 |
|    mean_reward_advan... | -0.12409562  |
|    n_updates            | 510          |
|    nu                   | 2.22         |
|    nu_loss              | -0.000217    |
|    policy_gradient_loss | -0.00601     |
|    reward_explained_... | 0.949        |
|    reward_value_loss    | 0.0714       |
|    total_cost           | 1.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.63        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -2.77        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.246       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -2.85        |
| time/                   |              |
|    fps                  | 855          |
|    iterations           | 53           |
|    time_elapsed         | 634          |
|    total_timesteps      | 542720       |
| train/                  |              |
|    approx_kl            | 0.001725443  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.324        |
|    cost_value_loss      | 4.09e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0616      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00211      |
|    mean_cost_advantages | -0.002898664 |
|    mean_reward_advan... | 0.09901402   |
|    n_updates            | 520          |
|    nu                   | 2.22         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 6.7e-05      |
|    reward_explained_... | 0.995        |
|    reward_value_loss    | 0.00114      |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.28
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -2.63          |
|    mean_ep_length       | 119            |
|    mean_reward          | -15.7          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.124         |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -12.7          |
| time/                   |                |
|    fps                  | 856            |
|    iterations           | 54             |
|    time_elapsed         | 645            |
|    total_timesteps      | 552960         |
| train/                  |                |
|    approx_kl            | 0.28318632     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.049          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.12           |
|    cost_value_loss      | 3.72e-07       |
|    early_stop_epoch     | 4              |
|    entropy_loss         | -0.073         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.0117        |
|    mean_cost_advantages | -0.00047418528 |
|    mean_reward_advan... | 0.026135668    |
|    n_updates            | 530            |
|    nu                   | 2.22           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00168       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 0.000796       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 81.6          |
|    mean_reward          | -9.03         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.116        |
|    ep_len_mean          | 62.4          |
|    ep_rew_mean          | -7.38         |
| time/                   |               |
|    fps                  | 859           |
|    iterations           | 55            |
|    time_elapsed         | 655           |
|    total_timesteps      | 563200        |
| train/                  |               |
|    approx_kl            | 0.061758827   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.208         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.162         |
|    cost_value_loss      | 1.72e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.209        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0923        |
|    mean_cost_advantages | 5.8959035e-05 |
|    mean_reward_advan... | -1.6396358    |
|    n_updates            | 540           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0132       |
|    reward_explained_... | -0.101        |
|    reward_value_loss    | 0.35          |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 17            |
|    mean_reward          | -3.5          |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.203        |
|    ep_len_mean          | 17.8          |
|    ep_rew_mean          | -3.55         |
| time/                   |               |
|    fps                  | 865           |
|    iterations           | 56            |
|    time_elapsed         | 662           |
|    total_timesteps      | 573440        |
| train/                  |               |
|    approx_kl            | 0.039359614   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0587        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.134        |
|    cost_value_loss      | 0.000154      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.263        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.283         |
|    mean_cost_advantages | 0.00013132997 |
|    mean_reward_advan... | -0.5459651    |
|    n_updates            | 550           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00419      |
|    reward_explained_... | 0.103         |
|    reward_value_loss    | 0.649         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -3.23         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.21         |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | -3.34         |
| time/                   |               |
|    fps                  | 859           |
|    iterations           | 57            |
|    time_elapsed         | 679           |
|    total_timesteps      | 583680        |
| train/                  |               |
|    approx_kl            | 0.010650152   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.122         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.27         |
|    cost_value_loss      | 0.00402       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.322        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.209         |
|    mean_cost_advantages | 0.0027505043  |
|    mean_reward_advan... | 1.1357727     |
|    n_updates            | 560           |
|    nu                   | 2.23          |
|    nu_loss              | -0.00109      |
|    policy_gradient_loss | -0.00736      |
|    reward_explained_... | 0.602         |
|    reward_value_loss    | 0.414         |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | -2.92         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.227        |
|    ep_len_mean          | 13.6          |
|    ep_rew_mean          | -3.06         |
| time/                   |               |
|    fps                  | 864           |
|    iterations           | 58            |
|    time_elapsed         | 687           |
|    total_timesteps      | 593920        |
| train/                  |               |
|    approx_kl            | 0.038278818   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.307         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.153         |
|    cost_value_loss      | 3.28e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.318        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0324        |
|    mean_cost_advantages | -0.0047243773 |
|    mean_reward_advan... | 0.623076      |
|    n_updates            | 570           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0137       |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.163         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 13.6          |
|    mean_reward          | -3.13         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.236        |
|    ep_len_mean          | 13.6          |
|    ep_rew_mean          | -3.2          |
| time/                   |               |
|    fps                  | 859           |
|    iterations           | 59            |
|    time_elapsed         | 702           |
|    total_timesteps      | 604160        |
| train/                  |               |
|    approx_kl            | 0.011896087   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.228         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0857        |
|    cost_value_loss      | 1.12e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.233        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0129        |
|    mean_cost_advantages | -0.0017562574 |
|    mean_reward_advan... | 0.34740138    |
|    n_updates            | 580           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00935      |
|    reward_explained_... | 0.97          |
|    reward_value_loss    | 0.0471        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.63         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -2.8          |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.237        |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | -2.98         |
| time/                   |               |
|    fps                  | 864           |
|    iterations           | 60            |
|    time_elapsed         | 710           |
|    total_timesteps      | 614400        |
| train/                  |               |
|    approx_kl            | 0.033544615   |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.168         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -671          |
|    cost_value_loss      | 0.000478      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.161        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0237        |
|    mean_cost_advantages | 0.00025881262 |
|    mean_reward_advan... | 0.07588957    |
|    n_updates            | 590           |
|    nu                   | 2.23          |
|    nu_loss              | -0.000218     |
|    policy_gradient_loss | -0.00731      |
|    reward_explained_... | 0.961         |
|    reward_value_loss    | 0.0632        |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -2.52        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.24        |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | -2.99        |
| time/                   |              |
|    fps                  | 868          |
|    iterations           | 61           |
|    time_elapsed         | 718          |
|    total_timesteps      | 624640       |
| train/                  |              |
|    approx_kl            | 0.024735097  |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.45        |
|    cost_value_loss      | 0.0068       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.125       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0199       |
|    mean_cost_advantages | 0.0055485633 |
|    mean_reward_advan... | 0.11708182   |
|    n_updates            | 600          |
|    nu                   | 2.23         |
|    nu_loss              | -0.000871    |
|    policy_gradient_loss | -0.00533     |
|    reward_explained_... | 0.985        |
|    reward_value_loss    | 0.0243       |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.52       |
|    mean_ep_length       | 12          |
|    mean_reward          | -2.93       |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.244      |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | -2.86       |
| time/                   |             |
|    fps                  | 873         |
|    iterations           | 62          |
|    time_elapsed         | 726         |
|    total_timesteps      | 634880      |
| train/                  |             |
|    approx_kl            | 0.033514164 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.12       |
|    cost_value_loss      | 6.37e-06    |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.108      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000631    |
|    mean_cost_advantages | 0.010584013 |
|    mean_reward_advan... | 0.0689313   |
|    n_updates            | 610         |
|    nu                   | 2.23        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00569    |
|    reward_explained_... | 0.994       |
|    reward_value_loss    | 0.00871     |
|    total_cost           | 0.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -2.8         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.245       |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | -2.87        |
| time/                   |              |
|    fps                  | 867          |
|    iterations           | 63           |
|    time_elapsed         | 743          |
|    total_timesteps      | 645120       |
| train/                  |              |
|    approx_kl            | 0.0038216044 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.396        |
|    cost_value_loss      | 2.4e-07      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0902      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00483     |
|    mean_cost_advantages | 0.002469113  |
|    mean_reward_advan... | 0.05396087   |
|    n_updates            | 620          |
|    nu                   | 2.23         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00266     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00176      |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -2.77        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.245       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.83        |
| time/                   |              |
|    fps                  | 863          |
|    iterations           | 64           |
|    time_elapsed         | 759          |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0030028713 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.128        |
|    cost_value_loss      | 1.45e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0861      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00299     |
|    mean_cost_advantages | 0.0003999873 |
|    mean_reward_advan... | 0.013914563  |
|    n_updates            | 630          |
|    nu                   | 2.23         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00191     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00109      |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -2.63        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.245       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -2.83        |
| time/                   |              |
|    fps                  | 859          |
|    iterations           | 65           |
|    time_elapsed         | 774          |
|    total_timesteps      | 665600       |
| train/                  |              |
|    approx_kl            | 0.015856279  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0273      |
|    cost_value_loss      | 1.29e-07     |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.0715      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00195     |
|    mean_cost_advantages | 8.231342e-05 |
|    mean_reward_advan... | 0.0006698533 |
|    n_updates            | 640          |
|    nu                   | 2.23         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0022      |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.0009       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.37
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.52         |
|    mean_ep_length       | 94.4          |
|    mean_reward          | -18.4         |
|    true_cost            | 0.00186       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.189        |
|    ep_len_mean          | 97.8          |
|    ep_rew_mean          | -18.2         |
| time/                   |               |
|    fps                  | 861           |
|    iterations           | 66            |
|    time_elapsed         | 784           |
|    total_timesteps      | 675840        |
| train/                  |               |
|    approx_kl            | 0.36576653    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0535        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0542       |
|    cost_value_loss      | 2.41e-07      |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.0594       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00555      |
|    mean_cost_advantages | -7.427113e-05 |
|    mean_reward_advan... | 0.004599173   |
|    n_updates            | 650           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000896     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.000467      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 21.2         |
|    mean_reward          | -4.62        |
|    true_cost            | 0.00322      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | -0.215       |
|    ep_len_mean          | 30.2         |
|    ep_rew_mean          | -6.28        |
| time/                   |              |
|    fps                  | 865          |
|    iterations           | 67           |
|    time_elapsed         | 792          |
|    total_timesteps      | 686080       |
| train/                  |              |
|    approx_kl            | 0.016967088  |
|    average_cost         | 0.0018554687 |
|    clip_fraction        | 0.178        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -870         |
|    cost_value_loss      | 0.00553      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.217       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.348        |
|    mean_cost_advantages | 0.0066599576 |
|    mean_reward_advan... | -2.4276671   |
|    n_updates            | 660          |
|    nu                   | 2.23         |
|    nu_loss              | -0.00414     |
|    policy_gradient_loss | -0.00583     |
|    reward_explained_... | -0.972       |
|    reward_value_loss    | 1.02         |
|    total_cost           | 19.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -3.55        |
|    true_cost            | 0.00186      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | -0.236       |
|    ep_len_mean          | 18.3         |
|    ep_rew_mean          | -4.25        |
| time/                   |              |
|    fps                  | 870          |
|    iterations           | 68           |
|    time_elapsed         | 800          |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.018607607  |
|    average_cost         | 0.0032226562 |
|    clip_fraction        | 0.187        |
|    cost_explained_va... | -1.01        |
|    cost_value_loss      | 0.0097       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.224       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13         |
|    mean_cost_advantages | -0.01037194  |
|    mean_reward_advan... | -0.24316278  |
|    n_updates            | 670          |
|    nu                   | 2.24         |
|    nu_loss              | -0.0072      |
|    policy_gradient_loss | -0.00788     |
|    reward_explained_... | -1.81        |
|    reward_value_loss    | 2.46         |
|    total_cost           | 33.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -3.03        |
|    true_cost            | 0.00137      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.255       |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | -3.38        |
| time/                   |              |
|    fps                  | 874          |
|    iterations           | 69           |
|    time_elapsed         | 808          |
|    total_timesteps      | 706560       |
| train/                  |              |
|    approx_kl            | 0.018009404  |
|    average_cost         | 0.0018554687 |
|    clip_fraction        | 0.174        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.367       |
|    cost_value_loss      | 0.0114       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.198       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.944        |
|    mean_cost_advantages | -0.010085678 |
|    mean_reward_advan... | 0.74966085   |
|    n_updates            | 680          |
|    nu                   | 2.24         |
|    nu_loss              | -0.00415     |
|    policy_gradient_loss | -0.00818     |
|    reward_explained_... | 0.0343       |
|    reward_value_loss    | 1.85         |
|    total_cost           | 19.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.52         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -2.95         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.254        |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | -3.18         |
| time/                   |               |
|    fps                  | 876           |
|    iterations           | 70            |
|    time_elapsed         | 817           |
|    total_timesteps      | 716800        |
| train/                  |               |
|    approx_kl            | 0.021553893   |
|    average_cost         | 0.0013671875  |
|    clip_fraction        | 0.147         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.87         |
|    cost_value_loss      | 0.0083        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.154        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.182         |
|    mean_cost_advantages | -0.0023883495 |
|    mean_reward_advan... | 0.80568296    |
|    n_updates            | 690           |
|    nu                   | 2.24          |
|    nu_loss              | -0.00306      |
|    policy_gradient_loss | -0.00599      |
|    reward_explained_... | 0.811         |
|    reward_value_loss    | 0.49          |
|    total_cost           | 14.0          |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.52         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -3.1          |
|    true_cost            | 0.00488       |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | -0.263        |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | -2.99         |
| time/                   |               |
|    fps                  | 880           |
|    iterations           | 71            |
|    time_elapsed         | 825           |
|    total_timesteps      | 727040        |
| train/                  |               |
|    approx_kl            | 0.03771607    |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.131         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.15         |
|    cost_value_loss      | 0.0026        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.125        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0589        |
|    mean_cost_advantages | -0.0063241897 |
|    mean_reward_advan... | 0.35645664    |
|    n_updates            | 700           |
|    nu                   | 2.24          |
|    nu_loss              | -0.000874     |
|    policy_gradient_loss | -0.00387      |
|    reward_explained_... | 0.932         |
|    reward_value_loss    | 0.128         |
|    total_cost           | 4.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -2.65        |
|    true_cost            | 0.00488      |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.26        |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -2.89        |
| time/                   |              |
|    fps                  | 881          |
|    iterations           | 72           |
|    time_elapsed         | 836          |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.018585686  |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.08        |
|    cost_value_loss      | 0.0515       |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.119       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0378       |
|    mean_cost_advantages | 0.02196232   |
|    mean_reward_advan... | 0.15098432   |
|    n_updates            | 710          |
|    nu                   | 2.24         |
|    nu_loss              | -0.0109      |
|    policy_gradient_loss | -0.00244     |
|    reward_explained_... | 0.978        |
|    reward_value_loss    | 0.0304       |
|    total_cost           | 50.0         |
------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -3.1         |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.248       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -2.83        |
| time/                   |              |
|    fps                  | 875          |
|    iterations           | 73           |
|    time_elapsed         | 854          |
|    total_timesteps      | 747520       |
| train/                  |              |
|    approx_kl            | 0.01713508   |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.0639       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.71        |
|    cost_value_loss      | 0.0413       |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.108       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0111       |
|    mean_cost_advantages | 0.015772993  |
|    mean_reward_advan... | 0.04962098   |
|    n_updates            | 720          |
|    nu                   | 2.24         |
|    nu_loss              | -0.0109      |
|    policy_gradient_loss | -0.00213     |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 0.00996      |
|    total_cost           | 50.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.52         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | -2.88         |
|    true_cost            | 0.0084        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.262        |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | -2.76         |
| time/                   |               |
|    fps                  | 878           |
|    iterations           | 74            |
|    time_elapsed         | 862           |
|    total_timesteps      | 757760        |
| train/                  |               |
|    approx_kl            | 0.032042556   |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.0629        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.16         |
|    cost_value_loss      | 0.00539       |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.102        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000616      |
|    mean_cost_advantages | -0.0052599884 |
|    mean_reward_advan... | 0.0013416766  |
|    n_updates            | 730           |
|    nu                   | 2.25          |
|    nu_loss              | -0.00131      |
|    policy_gradient_loss | 0.00591       |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 0.00575       |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.52       |
|    mean_ep_length       | 11.2        |
|    mean_reward          | -2.65       |
|    true_cost            | 0.00117     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.245      |
|    ep_len_mean          | 11.3        |
|    ep_rew_mean          | -2.75       |
| time/                   |             |
|    fps                  | 880         |
|    iterations           | 75          |
|    time_elapsed         | 872         |
|    total_timesteps      | 768000      |
| train/                  |             |
|    approx_kl            | 0.017401729 |
|    average_cost         | 0.008398438 |
|    clip_fraction        | 0.0508      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.37       |
|    cost_value_loss      | 0.101       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.118      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00432     |
|    mean_cost_advantages | 0.04732918  |
|    mean_reward_advan... | 0.035519682 |
|    n_updates            | 740         |
|    nu                   | 2.25        |
|    nu_loss              | -0.0189     |
|    policy_gradient_loss | -0.00665    |
|    reward_explained_... | 0.989       |
|    reward_value_loss    | 0.0153      |
|    total_cost           | 86.0        |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.52        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -2.73        |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.242       |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | -2.63        |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 76           |
|    time_elapsed         | 880          |
|    total_timesteps      | 778240       |
| train/                  |              |
|    approx_kl            | 0.01514585   |
|    average_cost         | 0.001171875  |
|    clip_fraction        | 0.0827       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.856       |
|    cost_value_loss      | 0.00895      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.12        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0104       |
|    mean_cost_advantages | -0.009365814 |
|    mean_reward_advan... | 0.03386425   |
|    n_updates            | 750          |
|    nu                   | 2.25         |
|    nu_loss              | -0.00264     |
|    policy_gradient_loss | -0.00395     |
|    reward_explained_... | 0.985        |
|    reward_value_loss    | 0.021        |
|    total_cost           | 12.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.52         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | -2.73         |
|    true_cost            | 0.00283       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.246        |
|    ep_len_mean          | 10.6          |
|    ep_rew_mean          | -2.54         |
| time/                   |               |
|    fps                  | 887           |
|    iterations           | 77            |
|    time_elapsed         | 888           |
|    total_timesteps      | 788480        |
| train/                  |               |
|    approx_kl            | 0.018742776   |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.0822        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.87         |
|    cost_value_loss      | 0.00364       |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.111        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00901       |
|    mean_cost_advantages | -0.0016768969 |
|    mean_reward_advan... | 0.004167174   |
|    n_updates            | 760           |
|    nu                   | 2.26          |
|    nu_loss              | -0.00132      |
|    policy_gradient_loss | -0.0037       |
|    reward_explained_... | 0.982         |
|    reward_value_loss    | 0.025         |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.4         |
|    mean_reward          | -2.5         |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.239       |
|    ep_len_mean          | 10.6         |
|    ep_rew_mean          | -2.53        |
| time/                   |              |
|    fps                  | 889          |
|    iterations           | 78           |
|    time_elapsed         | 897          |
|    total_timesteps      | 798720       |
| train/                  |              |
|    approx_kl            | 0.01705485   |
|    average_cost         | 0.0028320313 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.12        |
|    cost_value_loss      | 0.0221       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0984      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00219      |
|    mean_cost_advantages | 0.004871337  |
|    mean_reward_advan... | 0.050515283  |
|    n_updates            | 770          |
|    nu                   | 2.26         |
|    nu_loss              | -0.00639     |
|    policy_gradient_loss | -0.00396     |
|    reward_explained_... | 0.982        |
|    reward_value_loss    | 0.0168       |
|    total_cost           | 29.0         |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.8         |
|    mean_reward          | -2.5         |
|    true_cost            | 0.00449      |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.248       |
|    ep_len_mean          | 10.5         |
|    ep_rew_mean          | -2.51        |
| time/                   |              |
|    fps                  | 885          |
|    iterations           | 79           |
|    time_elapsed         | 913          |
|    total_timesteps      | 808960       |
| train/                  |              |
|    approx_kl            | 0.015658064  |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.0438       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.49        |
|    cost_value_loss      | 0.00211      |
|    early_stop_epoch     | 8            |
|    entropy_loss         | -0.108       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000779     |
|    mean_cost_advantages | -0.018240945 |
|    mean_reward_advan... | 0.035599142  |
|    n_updates            | 780          |
|    nu                   | 2.26         |
|    nu_loss              | -0.000882    |
|    policy_gradient_loss | 0.0136       |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 0.00769      |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.6         |
|    mean_reward          | -2.5         |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.241       |
|    ep_len_mean          | 10.8         |
|    ep_rew_mean          | -2.57        |
| time/                   |              |
|    fps                  | 883          |
|    iterations           | 80           |
|    time_elapsed         | 926          |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.02094167   |
|    average_cost         | 0.0044921874 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.32        |
|    cost_value_loss      | 0.0115       |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.103       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00329     |
|    mean_cost_advantages | 0.01420299   |
|    mean_reward_advan... | 0.0011888125 |
|    n_updates            | 790          |
|    nu                   | 2.26         |
|    nu_loss              | -0.0102      |
|    policy_gradient_loss | -0.00364     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00129      |
|    total_cost           | 46.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.5          |
|    mean_ep_length       | 10.2          |
|    mean_reward          | -2.5          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.239        |
|    ep_len_mean          | 10.6          |
|    ep_rew_mean          | -2.53         |
| time/                   |               |
|    fps                  | 887           |
|    iterations           | 81            |
|    time_elapsed         | 934           |
|    total_timesteps      | 829440        |
| train/                  |               |
|    approx_kl            | 0.015445356   |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.097         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.668        |
|    cost_value_loss      | 0.00202       |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0893       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.012         |
|    mean_cost_advantages | -0.0016201843 |
|    mean_reward_advan... | -0.03769123   |
|    n_updates            | 800           |
|    nu                   | 2.27          |
|    nu_loss              | -0.00155      |
|    policy_gradient_loss | -0.00192      |
|    reward_explained_... | 0.983         |
|    reward_value_loss    | 0.0199        |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.5          |
|    mean_ep_length       | 10.8          |
|    mean_reward          | -2.5          |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.239        |
|    ep_len_mean          | 10.6          |
|    ep_rew_mean          | -2.51         |
| time/                   |               |
|    fps                  | 879           |
|    iterations           | 82            |
|    time_elapsed         | 954           |
|    total_timesteps      | 839680        |
| train/                  |               |
|    approx_kl            | 0.010867826   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0401        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0822        |
|    cost_value_loss      | 1.76e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0915       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00494       |
|    mean_cost_advantages | 0.00054374046 |
|    mean_reward_advan... | 0.0011248871  |
|    n_updates            | 810           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00137      |
|    reward_explained_... | 0.993         |
|    reward_value_loss    | 0.00815       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.4         |
|    mean_reward          | -2.5         |
|    true_cost            | 0.0395       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.328       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -2.72        |
| time/                   |              |
|    fps                  | 876          |
|    iterations           | 83           |
|    time_elapsed         | 970          |
|    total_timesteps      | 849920       |
| train/                  |              |
|    approx_kl            | 0.016365696  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2           |
|    cost_value_loss      | 0.000521     |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.0892      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00524      |
|    mean_cost_advantages | 0.0013782    |
|    mean_reward_advan... | 0.008010785  |
|    n_updates            | 820          |
|    nu                   | 2.27         |
|    nu_loss              | -0.000443    |
|    policy_gradient_loss | -0.000125    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.00196      |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.5        |
|    mean_ep_length       | 10.2        |
|    mean_reward          | -2.5        |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.238      |
|    ep_len_mean          | 10.7        |
|    ep_rew_mean          | -2.51       |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 84          |
|    time_elapsed         | 979         |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.14158474  |
|    average_cost         | 0.039453126 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.32       |
|    cost_value_loss      | 0.521       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0552     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.268       |
|    mean_cost_advantages | 0.2670527   |
|    mean_reward_advan... | -0.08040635 |
|    n_updates            | 830         |
|    nu                   | 2.28        |
|    nu_loss              | -0.0897     |
|    policy_gradient_loss | -0.0471     |
|    reward_explained_... | 0.947       |
|    reward_value_loss    | 0.0568      |
|    total_cost           | 404.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.5        |
|    mean_ep_length       | 11          |
|    mean_reward          | -2.55       |
|    true_cost            | 0.0452      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.342      |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | -2.65       |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 85          |
|    time_elapsed         | 989         |
|    total_timesteps      | 870400      |
| train/                  |             |
|    approx_kl            | 0.035323616 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.136      |
|    cost_value_loss      | 0.00241     |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.0609     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    mean_cost_advantages | -0.23808756 |
|    mean_reward_advan... | 0.0729286   |
|    n_updates            | 840         |
|    nu                   | 2.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000897   |
|    reward_explained_... | 0.99        |
|    reward_value_loss    | 0.00355     |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.81
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.6         |
|    mean_reward          | -2.5         |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.24        |
|    ep_len_mean          | 10.4         |
|    ep_rew_mean          | -2.51        |
| time/                   |              |
|    fps                  | 882          |
|    iterations           | 86           |
|    time_elapsed         | 997          |
|    total_timesteps      | 880640       |
| train/                  |              |
|    approx_kl            | 0.80727977   |
|    average_cost         | 0.045214843  |
|    clip_fraction        | 0.136        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.21        |
|    cost_value_loss      | 0.0471       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0316      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00434      |
|    mean_cost_advantages | 0.075318515  |
|    mean_reward_advan... | -0.012681799 |
|    n_updates            | 850          |
|    nu                   | 2.3          |
|    nu_loss              | -0.104       |
|    policy_gradient_loss | -0.0334      |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 0.00202      |
|    total_cost           | 463.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.6         |
|    mean_reward          | -2.55        |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.238       |
|    ep_len_mean          | 10.5         |
|    ep_rew_mean          | -2.5         |
| time/                   |              |
|    fps                  | 885          |
|    iterations           | 87           |
|    time_elapsed         | 1005         |
|    total_timesteps      | 890880       |
| train/                  |              |
|    approx_kl            | 0.1364261    |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.072        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.188        |
|    cost_value_loss      | 0.00533      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0568      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000505    |
|    mean_cost_advantages | -0.2113353   |
|    mean_reward_advan... | 0.032056242  |
|    n_updates            | 860          |
|    nu                   | 2.32         |
|    nu_loss              | -0.00045     |
|    policy_gradient_loss | -0.00192     |
|    reward_explained_... | 0.991        |
|    reward_value_loss    | 0.00926      |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.5          |
|    mean_ep_length       | 10.6          |
|    mean_reward          | -2.6          |
|    true_cost            | 0.0104        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | -0.265        |
|    ep_len_mean          | 10.6          |
|    ep_rew_mean          | -2.56         |
| time/                   |               |
|    fps                  | 883           |
|    iterations           | 88            |
|    time_elapsed         | 1019          |
|    total_timesteps      | 901120        |
| train/                  |               |
|    approx_kl            | 0.016183073   |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0275        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.177        |
|    cost_value_loss      | 0.000826      |
|    early_stop_epoch     | 6             |
|    entropy_loss         | -0.0692       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00118      |
|    mean_cost_advantages | -0.019286985  |
|    mean_reward_advan... | 0.01164562    |
|    n_updates            | 870           |
|    nu                   | 2.33          |
|    nu_loss              | -0.000679     |
|    policy_gradient_loss | -0.00076      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 0.000833      |
|    total_cost           | 3.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.6         |
|    mean_reward          | -2.5         |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.239       |
|    ep_len_mean          | 10.4         |
|    ep_rew_mean          | -2.5         |
| time/                   |              |
|    fps                  | 885          |
|    iterations           | 89           |
|    time_elapsed         | 1028         |
|    total_timesteps      | 911360       |
| train/                  |              |
|    approx_kl            | 0.03535176   |
|    average_cost         | 0.010351563  |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.1e+03     |
|    cost_value_loss      | 0.0322       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0579      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00425      |
|    mean_cost_advantages | 0.034825895  |
|    mean_reward_advan... | -0.019027334 |
|    n_updates            | 880          |
|    nu                   | 2.34         |
|    nu_loss              | -0.0241      |
|    policy_gradient_loss | -0.0098      |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 0.00846      |
|    total_cost           | 106.0        |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.8         |
|    mean_reward          | -2.65        |
|    true_cost            | 0.0302       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.309       |
|    ep_len_mean          | 10.8         |
|    ep_rew_mean          | -2.58        |
| time/                   |              |
|    fps                  | 886          |
|    iterations           | 90           |
|    time_elapsed         | 1039         |
|    total_timesteps      | 921600       |
| train/                  |              |
|    approx_kl            | 0.034349333  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.186       |
|    cost_value_loss      | 0.000441     |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.0732      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00124     |
|    mean_cost_advantages | -0.023887435 |
|    mean_reward_advan... | 0.031320557  |
|    n_updates            | 890          |
|    nu                   | 2.35         |
|    nu_loss              | -0.000229    |
|    policy_gradient_loss | -0.000377    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.000753     |
|    total_cost           | 1.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.27
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.2         |
|    mean_reward          | -2.5         |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.238       |
|    ep_len_mean          | 10.5         |
|    ep_rew_mean          | -2.5         |
| time/                   |              |
|    fps                  | 889          |
|    iterations           | 91           |
|    time_elapsed         | 1047         |
|    total_timesteps      | 931840       |
| train/                  |              |
|    approx_kl            | 0.27411065   |
|    average_cost         | 0.03017578   |
|    clip_fraction        | 0.0979       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.76        |
|    cost_value_loss      | 0.0434       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0765      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0189      |
|    mean_cost_advantages | 0.07935701   |
|    mean_reward_advan... | -0.019715521 |
|    n_updates            | 900          |
|    nu                   | 2.37         |
|    nu_loss              | -0.071       |
|    policy_gradient_loss | -0.0243      |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 0.00239      |
|    total_cost           | 309.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.2         |
|    mean_reward          | -2.5         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.238       |
|    ep_len_mean          | 10.6         |
|    ep_rew_mean          | -2.51        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 92           |
|    time_elapsed         | 1056         |
|    total_timesteps      | 942080       |
| train/                  |              |
|    approx_kl            | 0.062154878  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.148        |
|    cost_value_loss      | 0.00175      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0641      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00504      |
|    mean_cost_advantages | -0.10868875  |
|    mean_reward_advan... | 0.021310065  |
|    n_updates            | 910          |
|    nu                   | 2.38         |
|    nu_loss              | -0.000231    |
|    policy_gradient_loss | 0.00263      |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 0.000936     |
|    total_cost           | 1.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.4         |
|    mean_reward          | -2.5         |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.238       |
|    ep_len_mean          | 10.4         |
|    ep_rew_mean          | -2.5         |
| time/                   |              |
|    fps                  | 887          |
|    iterations           | 93           |
|    time_elapsed         | 1073         |
|    total_timesteps      | 952320       |
| train/                  |              |
|    approx_kl            | 0.00419332   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.131        |
|    cost_value_loss      | 2.71e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0674      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00256      |
|    mean_cost_advantages | -0.006850651 |
|    mean_reward_advan... | 0.0019736122 |
|    n_updates            | 920          |
|    nu                   | 2.39         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000157    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.00252      |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.5          |
|    mean_ep_length       | 10.4          |
|    mean_reward          | -2.5          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.238        |
|    ep_len_mean          | 10.5          |
|    ep_rew_mean          | -2.5          |
| time/                   |               |
|    fps                  | 889           |
|    iterations           | 94            |
|    time_elapsed         | 1081          |
|    total_timesteps      | 962560        |
| train/                  |               |
|    approx_kl            | 0.017312722   |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.0348        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -40.7         |
|    cost_value_loss      | 0.000359      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.083        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00137      |
|    mean_cost_advantages | 0.00022385638 |
|    mean_reward_advan... | -0.011149373  |
|    n_updates            | 930           |
|    nu                   | 2.41          |
|    nu_loss              | -0.000234     |
|    policy_gradient_loss | 0.00044       |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.000347      |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.5          |
|    mean_ep_length       | 10.4          |
|    mean_reward          | -2.5          |
|    true_cost            | 0.0137        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | -0.271        |
|    ep_len_mean          | 10.6          |
|    ep_rew_mean          | -2.52         |
| time/                   |               |
|    fps                  | 891           |
|    iterations           | 95            |
|    time_elapsed         | 1091          |
|    total_timesteps      | 972800        |
| train/                  |               |
|    approx_kl            | 0.035364576   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0558        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0212       |
|    cost_value_loss      | 5.65e-06      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0764       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00212       |
|    mean_cost_advantages | 0.00015941651 |
|    mean_reward_advan... | -0.010820207  |
|    n_updates            | 940           |
|    nu                   | 2.42          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000173     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 0.0033        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.5         |
|    mean_ep_length       | 10.4         |
|    mean_reward          | -2.5         |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.238       |
|    ep_len_mean          | 10.5         |
|    ep_rew_mean          | -2.5         |
| time/                   |              |
|    fps                  | 894          |
|    iterations           | 96           |
|    time_elapsed         | 1099         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.08748761   |
|    average_cost         | 0.013671875  |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.26        |
|    cost_value_loss      | 0.0302       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.07        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0139       |
|    mean_cost_advantages | 0.037899353  |
|    mean_reward_advan... | 0.0051385215 |
|    n_updates            | 950          |
|    nu                   | 2.43         |
|    nu_loss              | -0.033       |
|    policy_gradient_loss | -0.0101      |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.00215      |
|    total_cost           | 140.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.3         |
|    mean_ep_length       | 10           |
|    mean_reward          | -2.3         |
|    true_cost            | 0.019        |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.279       |
|    ep_len_mean          | 10.4         |
|    ep_rew_mean          | -2.43        |
| time/                   |              |
|    fps                  | 895          |
|    iterations           | 97           |
|    time_elapsed         | 1108         |
|    total_timesteps      | 993280       |
| train/                  |              |
|    approx_kl            | 0.03894943   |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.133        |
|    cost_value_loss      | 0.000608     |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0748      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00125     |
|    mean_cost_advantages | -0.04079432  |
|    mean_reward_advan... | 0.0071194544 |
|    n_updates            | 960          |
|    nu                   | 2.44         |
|    nu_loss              | -0.000474    |
|    policy_gradient_loss | 0.00039      |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.000328     |
|    total_cost           | 2.0          |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.3        |
|    mean_ep_length       | 10.8        |
|    mean_reward          | -2.58       |
|    true_cost            | 0.026       |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.297      |
|    ep_len_mean          | 10.4        |
|    ep_rew_mean          | -2.44       |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 98          |
|    time_elapsed         | 1124        |
|    total_timesteps      | 1003520     |
| train/                  |             |
|    approx_kl            | 0.013204175 |
|    average_cost         | 0.019042969 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -287        |
|    cost_value_loss      | 0.0417      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.11       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0448      |
|    mean_cost_advantages | 0.06701808  |
|    mean_reward_advan... | 0.034897078 |
|    n_updates            | 970         |
|    nu                   | 2.45        |
|    nu_loss              | -0.0464     |
|    policy_gradient_loss | -0.00123    |
|    reward_explained_... | 0.989       |
|    reward_value_loss    | 0.0106      |
|    total_cost           | 195.0       |
-----------------------------------------
/home/jovyan/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
[32mTime taken: 19.03 minutes