[32;1mConfigured folder ./cpg/wandb/run-20220524_163240-3tjr4csh/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
----------------------------------
| eval/               |          |
|    best_mean_reward | -7.2e+04 |
|    mean_ep_length   | 86.4     |
|    mean_reward      | -7.2e+04 |
|    true_cost        | 0.84     |
| infos/              |          |
|    cost             | 0.0411   |
| rollout/            |          |
|    adjusted_reward  | -6.16    |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | -769     |
| time/               |          |
|    fps              | 634      |
|    iterations       | 1        |
|    time_elapsed     | 3        |
|    total_timesteps  | 2048     |
----------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.2e+04    |
|    mean_ep_length       | 108         |
|    mean_reward          | -9.38e+04   |
|    true_cost            | 0.861       |
| infos/                  |             |
|    cost                 | 0.0425      |
| rollout/                |             |
|    adjusted_reward      | -4.01       |
|    ep_len_mean          | 80.1        |
|    ep_rew_mean          | -372        |
| time/                   |             |
|    fps                  | 514         |
|    iterations           | 2           |
|    time_elapsed         | 7           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.01313418  |
|    average_cost         | 0.83984375  |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.42       |
|    cost_value_loss      | 0.228       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    mean_cost_advantages | 0.6332836   |
|    mean_reward_advan... | -0.56795067 |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.84       |
|    policy_gradient_loss | -0.00554    |
|    reward_explained_... | -2.23       |
|    reward_value_loss    | 0.201       |
|    total_cost           | 1720.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.2e+04    |
|    mean_ep_length       | 45.4        |
|    mean_reward          | -4.2e+04    |
|    true_cost            | 0.877       |
| infos/                  |             |
|    cost                 | 0.041       |
| rollout/                |             |
|    adjusted_reward      | -2.38       |
|    ep_len_mean          | 35.1        |
|    ep_rew_mean          | -101        |
| time/                   |             |
|    fps                  | 544         |
|    iterations           | 3           |
|    time_elapsed         | 11          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.020304224 |
|    average_cost         | 0.8613281   |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.94       |
|    cost_value_loss      | 0.0981      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0731      |
|    mean_cost_advantages | 0.2986017   |
|    mean_reward_advan... | -0.13210282 |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.917      |
|    policy_gradient_loss | -0.0128     |
|    reward_explained_... | -38.3       |
|    reward_value_loss    | 0.13        |
|    total_cost           | 1764.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -2.02e+04   |
|    mean_ep_length       | 21.8        |
|    mean_reward          | -2.02e+04   |
|    true_cost            | 0.855       |
| infos/                  |             |
|    cost                 | 0.037       |
| rollout/                |             |
|    adjusted_reward      | -0.429      |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | -8.84       |
| time/                   |             |
|    fps                  | 568         |
|    iterations           | 4           |
|    time_elapsed         | 14          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.030198926 |
|    average_cost         | 0.8769531   |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.27       |
|    cost_value_loss      | 0.112       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.26       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0798      |
|    mean_cost_advantages | 0.02339562  |
|    mean_reward_advan... | 0.13971213  |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.992      |
|    policy_gradient_loss | -0.0175     |
|    reward_explained_... | -0.573      |
|    reward_value_loss    | 0.141       |
|    total_cost           | 1796.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.08e+04   |
|    mean_ep_length       | 12.2        |
|    mean_reward          | -1.08e+04   |
|    true_cost            | 0.854       |
| infos/                  |             |
|    cost                 | 0.0378      |
| rollout/                |             |
|    adjusted_reward      | 0.963       |
|    ep_len_mean          | 13.6        |
|    ep_rew_mean          | 13.8        |
| time/                   |             |
|    fps                  | 585         |
|    iterations           | 5           |
|    time_elapsed         | 17          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.016893134 |
|    average_cost         | 0.85546875  |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.444      |
|    cost_value_loss      | 0.116       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.16       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0855      |
|    mean_cost_advantages | -0.1656169  |
|    mean_reward_advan... | 0.2698015   |
|    n_updates            | 40          |
|    nu                   | 1.27        |
|    nu_loss              | -1.03       |
|    policy_gradient_loss | -0.0195     |
|    reward_explained_... | 0.319       |
|    reward_value_loss    | 0.111       |
|    total_cost           | 1752.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -1.04e+04  |
|    mean_ep_length       | 11.4       |
|    mean_reward          | -1.04e+04  |
|    true_cost            | 0.898      |
| infos/                  |            |
|    cost                 | 0.0429     |
| rollout/                |            |
|    adjusted_reward      | 1.84       |
|    ep_len_mean          | 12.2       |
|    ep_rew_mean          | 22.1       |
| time/                   |            |
|    fps                  | 596        |
|    iterations           | 6          |
|    time_elapsed         | 20         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01828374 |
|    average_cost         | 0.8540039  |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -0.244     |
|    cost_value_loss      | 0.0594     |
|    early_stop_epoch     | 2          |
|    entropy_loss         | -1         |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0246     |
|    mean_cost_advantages | -0.1614008 |
|    mean_reward_advan... | 0.19295835 |
|    n_updates            | 50         |
|    nu                   | 1.34       |
|    nu_loss              | -1.09      |
|    policy_gradient_loss | -0.0134    |
|    reward_explained_... | 0.443      |
|    reward_value_loss    | 0.0382     |
|    total_cost           | 1749.0     |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.04e+04    |
|    mean_ep_length       | 16           |
|    mean_reward          | -1.36e+04    |
|    true_cost            | 0.866        |
| infos/                  |              |
|    cost                 | 0.0483       |
| rollout/                |              |
|    adjusted_reward      | 2.76         |
|    ep_len_mean          | 9.38         |
|    ep_rew_mean          | 28           |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 7            |
|    time_elapsed         | 23           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.027024511  |
|    average_cost         | 0.8984375    |
|    clip_fraction        | 0.317        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.903       |
|    cost_value_loss      | 0.0348       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.921       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00254      |
|    mean_cost_advantages | -0.054833636 |
|    mean_reward_advan... | 0.077118434  |
|    n_updates            | 60           |
|    nu                   | 1.42         |
|    nu_loss              | -1.21        |
|    policy_gradient_loss | -0.015       |
|    reward_explained_... | 0.407        |
|    reward_value_loss    | 0.0104       |
|    total_cost           | 1840.0       |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -9.55e+03    |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -9.55e+03    |
|    true_cost            | 0.854        |
| infos/                  |              |
|    cost                 | 0.0413       |
| rollout/                |              |
|    adjusted_reward      | 3.6          |
|    ep_len_mean          | 8.69         |
|    ep_rew_mean          | 29.7         |
| time/                   |              |
|    fps                  | 620          |
|    iterations           | 8            |
|    time_elapsed         | 26           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.01849609   |
|    average_cost         | 0.86572266   |
|    clip_fraction        | 0.211        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.574       |
|    cost_value_loss      | 0.0217       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.875       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00449     |
|    mean_cost_advantages | -0.092678934 |
|    mean_reward_advan... | 0.07870102   |
|    n_updates            | 70           |
|    nu                   | 1.5          |
|    nu_loss              | -1.23        |
|    policy_gradient_loss | -0.0142      |
|    reward_explained_... | 0.434        |
|    reward_value_loss    | 0.00569      |
|    total_cost           | 1773.0       |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -9.55e+03   |
|    mean_ep_length       | 14          |
|    mean_reward          | -1.24e+04   |
|    true_cost            | 0.894       |
| infos/                  |             |
|    cost                 | 0.0502      |
| rollout/                |             |
|    adjusted_reward      | 4.36        |
|    ep_len_mean          | 7.3         |
|    ep_rew_mean          | 32.6        |
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 9           |
|    time_elapsed         | 29          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.01674124  |
|    average_cost         | 0.8540039   |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0443     |
|    cost_value_loss      | 0.0132      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.792      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0113     |
|    mean_cost_advantages | -0.05546615 |
|    mean_reward_advan... | 0.045120608 |
|    n_updates            | 80          |
|    nu                   | 1.58        |
|    nu_loss              | -1.28       |
|    policy_gradient_loss | -0.0148     |
|    reward_explained_... | 0.597       |
|    reward_value_loss    | 0.0023      |
|    total_cost           | 1749.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.75e+03    |
|    mean_ep_length       | 6.6          |
|    mean_reward          | -4.75e+03    |
|    true_cost            | 0.879        |
| infos/                  |              |
|    cost                 | 0.0483       |
| rollout/                |              |
|    adjusted_reward      | 4.72         |
|    ep_len_mean          | 6.72         |
|    ep_rew_mean          | 33.8         |
| time/                   |              |
|    fps                  | 642          |
|    iterations           | 10           |
|    time_elapsed         | 31           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.016143732  |
|    average_cost         | 0.8935547    |
|    clip_fraction        | 0.147        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.371        |
|    cost_value_loss      | 0.00636      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.71        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0224      |
|    mean_cost_advantages | -0.033544738 |
|    mean_reward_advan... | 0.028631508  |
|    n_updates            | 90           |
|    nu                   | 1.66         |
|    nu_loss              | -1.41        |
|    policy_gradient_loss | -0.0103      |
|    reward_explained_... | 0.745        |
|    reward_value_loss    | 0.00111      |
|    total_cost           | 1830.0       |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.75e+03     |
|    mean_ep_length       | 9             |
|    mean_reward          | -7.55e+03     |
|    true_cost            | 0.894         |
| infos/                  |               |
|    cost                 | 0.0548        |
| rollout/                |               |
|    adjusted_reward      | 5.08          |
|    ep_len_mean          | 6.44          |
|    ep_rew_mean          | 34.4          |
| time/                   |               |
|    fps                  | 639           |
|    iterations           | 11            |
|    time_elapsed         | 35            |
|    total_timesteps      | 22528         |
| train/                  |               |
|    approx_kl            | 0.016728532   |
|    average_cost         | 0.87890625    |
|    clip_fraction        | 0.118         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.233         |
|    cost_value_loss      | 0.00854       |
|    early_stop_epoch     | 6             |
|    entropy_loss         | -0.633        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.018        |
|    mean_cost_advantages | -0.0113232285 |
|    mean_reward_advan... | 0.020197786   |
|    n_updates            | 100           |
|    nu                   | 1.74          |
|    nu_loss              | -1.46         |
|    policy_gradient_loss | -0.0123       |
|    reward_explained_... | 0.71          |
|    reward_value_loss    | 0.000999      |
|    total_cost           | 1800.0        |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -4.75e+03      |
|    mean_ep_length       | 7.2            |
|    mean_reward          | -5.55e+03      |
|    true_cost            | 0.904          |
| infos/                  |                |
|    cost                 | 0.0596         |
| rollout/                |                |
|    adjusted_reward      | 5.27           |
|    ep_len_mean          | 6.31           |
|    ep_rew_mean          | 34.5           |
| time/                   |                |
|    fps                  | 636            |
|    iterations           | 12             |
|    time_elapsed         | 38             |
|    total_timesteps      | 24576          |
| train/                  |                |
|    approx_kl            | 0.014403428    |
|    average_cost         | 0.8935547      |
|    clip_fraction        | 0.0949         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.447          |
|    cost_value_loss      | 0.00374        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.539         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00509       |
|    mean_cost_advantages | -0.00085182046 |
|    mean_reward_advan... | 0.016256358    |
|    n_updates            | 110            |
|    nu                   | 1.83           |
|    nu_loss              | -1.56          |
|    policy_gradient_loss | -0.00827       |
|    reward_explained_... | 0.854          |
|    reward_value_loss    | 0.000389       |
|    total_cost           | 1830.0         |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.75e+03    |
|    mean_ep_length       | 8.4          |
|    mean_reward          | -6.95e+03    |
|    true_cost            | 0.897        |
| infos/                  |              |
|    cost                 | 0.0562       |
| rollout/                |              |
|    adjusted_reward      | 5.39         |
|    ep_len_mean          | 6.28         |
|    ep_rew_mean          | 34.7         |
| time/                   |              |
|    fps                  | 631          |
|    iterations           | 13           |
|    time_elapsed         | 42           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.008150218  |
|    average_cost         | 0.9042969    |
|    clip_fraction        | 0.0743       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.746        |
|    cost_value_loss      | 0.00228      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.502       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00797     |
|    mean_cost_advantages | -0.006803887 |
|    mean_reward_advan... | 0.016444456  |
|    n_updates            | 120          |
|    nu                   | 1.92         |
|    nu_loss              | -1.65        |
|    policy_gradient_loss | -0.00455     |
|    reward_explained_... | 0.896        |
|    reward_value_loss    | 0.000284     |
|    total_cost           | 1852.0       |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -4.75e+03      |
|    mean_ep_length       | 6.6            |
|    mean_reward          | -5.15e+03      |
|    true_cost            | 0.904          |
| infos/                  |                |
|    cost                 | 0.0592         |
| rollout/                |                |
|    adjusted_reward      | 5.43           |
|    ep_len_mean          | 6.22           |
|    ep_rew_mean          | 34.7           |
| time/                   |                |
|    fps                  | 629            |
|    iterations           | 14             |
|    time_elapsed         | 45             |
|    total_timesteps      | 28672          |
| train/                  |                |
|    approx_kl            | 0.0021932547   |
|    average_cost         | 0.89697266     |
|    clip_fraction        | 0.0535         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.806          |
|    cost_value_loss      | 0.002          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.467         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00579       |
|    mean_cost_advantages | -2.6694732e-05 |
|    mean_reward_advan... | 0.01312656     |
|    n_updates            | 130            |
|    nu                   | 2              |
|    nu_loss              | -1.72          |
|    policy_gradient_loss | -0.00404       |
|    reward_explained_... | 0.954          |
|    reward_value_loss    | 0.000146       |
|    total_cost           | 1837.0         |
--------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.75e+03   |
|    mean_ep_length       | 11.6        |
|    mean_reward          | -9.95e+03   |
|    true_cost            | 0.88        |
| infos/                  |             |
|    cost                 | 0.058       |
| rollout/                |             |
|    adjusted_reward      | 5.51        |
|    ep_len_mean          | 6.09        |
|    ep_rew_mean          | 34.8        |
| time/                   |             |
|    fps                  | 637         |
|    iterations           | 15          |
|    time_elapsed         | 48          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.033897348 |
|    average_cost         | 0.9042969   |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.856       |
|    cost_value_loss      | 0.00171     |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.435      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00929    |
|    mean_cost_advantages | 0.007314544 |
|    mean_reward_advan... | 0.011574028 |
|    n_updates            | 140         |
|    nu                   | 2.09        |
|    nu_loss              | -1.81       |
|    policy_gradient_loss | -0.00629    |
|    reward_explained_... | 0.968       |
|    reward_value_loss    | 0.000126    |
|    total_cost           | 1852.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.75e+03     |
|    mean_ep_length       | 7             |
|    mean_reward          | -5.55e+03     |
|    true_cost            | 0.853         |
| infos/                  |               |
|    cost                 | 0.0637        |
| rollout/                |               |
|    adjusted_reward      | 5.58          |
|    ep_len_mean          | 6.06          |
|    ep_rew_mean          | 34.9          |
| time/                   |               |
|    fps                  | 641           |
|    iterations           | 16            |
|    time_elapsed         | 51            |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 0.015413936   |
|    average_cost         | 0.8798828     |
|    clip_fraction        | 0.104         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.871         |
|    cost_value_loss      | 0.00143       |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.429        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00676       |
|    mean_cost_advantages | -0.0031858697 |
|    mean_reward_advan... | 0.012411454   |
|    n_updates            | 150           |
|    nu                   | 2.19          |
|    nu_loss              | -1.84         |
|    policy_gradient_loss | -0.00228      |
|    reward_explained_... | 0.963         |
|    reward_value_loss    | 0.000132      |
|    total_cost           | 1802.0        |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.75e+03    |
|    mean_ep_length       | 13.4         |
|    mean_reward          | -1.2e+04     |
|    true_cost            | 0.816        |
| infos/                  |              |
|    cost                 | 0.0588       |
| rollout/                |              |
|    adjusted_reward      | 5.6          |
|    ep_len_mean          | 6.02         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 635          |
|    iterations           | 17           |
|    time_elapsed         | 54           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.011082407  |
|    average_cost         | 0.85302734   |
|    clip_fraction        | 0.143        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.866        |
|    cost_value_loss      | 0.0015       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.401       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00369     |
|    mean_cost_advantages | -0.005857534 |
|    mean_reward_advan... | 0.012637413  |
|    n_updates            | 160          |
|    nu                   | 2.28         |
|    nu_loss              | -1.87        |
|    policy_gradient_loss | -0.0016      |
|    reward_explained_... | 0.965        |
|    reward_value_loss    | 0.000122     |
|    total_cost           | 1747.0       |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.75e+03     |
|    mean_ep_length       | 6.6           |
|    mean_reward          | -4.95e+03     |
|    true_cost            | 0.804         |
| infos/                  |               |
|    cost                 | 0.0631        |
| rollout/                |               |
|    adjusted_reward      | 5.62          |
|    ep_len_mean          | 6             |
|    ep_rew_mean          | 35            |
| time/                   |               |
|    fps                  | 632           |
|    iterations           | 18            |
|    time_elapsed         | 58            |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.013078283   |
|    average_cost         | 0.81591797    |
|    clip_fraction        | 0.117         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.867         |
|    cost_value_loss      | 0.00145       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.374        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00113       |
|    mean_cost_advantages | -0.0032125958 |
|    mean_reward_advan... | 0.012389972   |
|    n_updates            | 170           |
|    nu                   | 2.37          |
|    nu_loss              | -1.86         |
|    policy_gradient_loss | -0.00173      |
|    reward_explained_... | 0.977         |
|    reward_value_loss    | 8.25e-05      |
|    total_cost           | 1671.0        |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.16e+03    |
|    mean_ep_length       | 6.2          |
|    mean_reward          | -4.16e+03    |
|    true_cost            | 0.801        |
| infos/                  |              |
|    cost                 | 0.0629       |
| rollout/                |              |
|    adjusted_reward      | 5.62         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 626          |
|    iterations           | 19           |
|    time_elapsed         | 62           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0059939874 |
|    average_cost         | 0.8041992    |
|    clip_fraction        | 0.0505       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.887        |
|    cost_value_loss      | 0.00132      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.351       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000114     |
|    mean_cost_advantages | 0.0064153858 |
|    mean_reward_advan... | 0.010672722  |
|    n_updates            | 180          |
|    nu                   | 2.47         |
|    nu_loss              | -1.91        |
|    policy_gradient_loss | -0.0012      |
|    reward_explained_... | 0.984        |
|    reward_value_loss    | 5.46e-05     |
|    total_cost           | 1647.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.16e+03    |
|    mean_ep_length       | 8.4          |
|    mean_reward          | -6.75e+03    |
|    true_cost            | 0.777        |
| infos/                  |              |
|    cost                 | 0.0574       |
| rollout/                |              |
|    adjusted_reward      | 5.59         |
|    ep_len_mean          | 6.01         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 612          |
|    iterations           | 20           |
|    time_elapsed         | 66           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0017088844 |
|    average_cost         | 0.80126953   |
|    clip_fraction        | 0.0438       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.89         |
|    cost_value_loss      | 0.0013       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.321       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00157     |
|    mean_cost_advantages | 0.009288618  |
|    mean_reward_advan... | 0.010236742  |
|    n_updates            | 190          |
|    nu                   | 2.56         |
|    nu_loss              | -1.98        |
|    policy_gradient_loss | -0.000837    |
|    reward_explained_... | 0.987        |
|    reward_value_loss    | 5.75e-05     |
|    total_cost           | 1641.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.16e+03    |
|    mean_ep_length       | 57           |
|    mean_reward          | -5.5e+04     |
|    true_cost            | 0.759        |
| infos/                  |              |
|    cost                 | 0.0593       |
| rollout/                |              |
|    adjusted_reward      | 5.61         |
|    ep_len_mean          | 6.03         |
|    ep_rew_mean          | 34.9         |
| time/                   |              |
|    fps                  | 604          |
|    iterations           | 21           |
|    time_elapsed         | 71           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0013976858 |
|    average_cost         | 0.77734375   |
|    clip_fraction        | 0.0722       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.872        |
|    cost_value_loss      | 0.00152      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.316       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000426     |
|    mean_cost_advantages | 0.003005632  |
|    mean_reward_advan... | 0.009435482  |
|    n_updates            | 200          |
|    nu                   | 2.66         |
|    nu_loss              | -1.99        |
|    policy_gradient_loss | -0.000531    |
|    reward_explained_... | 0.988        |
|    reward_value_loss    | 3.77e-05     |
|    total_cost           | 1592.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.16e+03    |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -9.76e+03    |
|    true_cost            | 0.761        |
| infos/                  |              |
|    cost                 | 0.0652       |
| rollout/                |              |
|    adjusted_reward      | 5.56         |
|    ep_len_mean          | 6.02         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 596          |
|    iterations           | 22           |
|    time_elapsed         | 75           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0027288138 |
|    average_cost         | 0.75927734   |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.893        |
|    cost_value_loss      | 0.00148      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.3         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000351     |
|    mean_cost_advantages | -0.002362572 |
|    mean_reward_advan... | 0.01004567   |
|    n_updates            | 210          |
|    nu                   | 2.76         |
|    nu_loss              | -2.02        |
|    policy_gradient_loss | -0.00116     |
|    reward_explained_... | 0.99         |
|    reward_value_loss    | 4.72e-05     |
|    total_cost           | 1555.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.16e+03    |
|    mean_ep_length       | 45           |
|    mean_reward          | -4.32e+04    |
|    true_cost            | 0.742        |
| infos/                  |              |
|    cost                 | 0.0585       |
| rollout/                |              |
|    adjusted_reward      | 5.27         |
|    ep_len_mean          | 6.02         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 585          |
|    iterations           | 23           |
|    time_elapsed         | 80           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0058000204 |
|    average_cost         | 0.76123047   |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.887        |
|    cost_value_loss      | 0.00169      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.278       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000672     |
|    mean_cost_advantages | 0.0059687416 |
|    mean_reward_advan... | 0.009036705  |
|    n_updates            | 220          |
|    nu                   | 2.85         |
|    nu_loss              | -2.1         |
|    policy_gradient_loss | -0.00118     |
|    reward_explained_... | 0.989        |
|    reward_value_loss    | 5.72e-05     |
|    total_cost           | 1559.0       |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.16e+03     |
|    mean_ep_length       | 6.4           |
|    mean_reward          | -4.36e+03     |
|    true_cost            | 0.709         |
| infos/                  |               |
|    cost                 | 0.0538        |
| rollout/                |               |
|    adjusted_reward      | 5.37          |
|    ep_len_mean          | 6.18          |
|    ep_rew_mean          | 34.7          |
| time/                   |               |
|    fps                  | 587           |
|    iterations           | 24            |
|    time_elapsed         | 83            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.08340851    |
|    average_cost         | 0.7421875     |
|    clip_fraction        | 0.0984        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.21         |
|    cost_value_loss      | 0.0393        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.26         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0103       |
|    mean_cost_advantages | 0.048350736   |
|    mean_reward_advan... | -0.0009886539 |
|    n_updates            | 230           |
|    nu                   | 2.95          |
|    nu_loss              | -2.12         |
|    policy_gradient_loss | -0.0172       |
|    reward_explained_... | 0.336         |
|    reward_value_loss    | 0.00199       |
|    total_cost           | 1520.0        |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.95e+03    |
|    mean_ep_length       | 6            |
|    mean_reward          | -3.95e+03    |
|    true_cost            | 0.685        |
| infos/                  |              |
|    cost                 | 0.0589       |
| rollout/                |              |
|    adjusted_reward      | 5.58         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 587          |
|    iterations           | 25           |
|    time_elapsed         | 87           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.036445487  |
|    average_cost         | 0.70947266   |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.279       |
|    cost_value_loss      | 0.00746      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.231       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0114      |
|    mean_cost_advantages | -0.011816974 |
|    mean_reward_advan... | 0.01106279   |
|    n_updates            | 240          |
|    nu                   | 3.05         |
|    nu_loss              | -2.09        |
|    policy_gradient_loss | -0.0101      |
|    reward_explained_... | 0.81         |
|    reward_value_loss    | 0.000424     |
|    total_cost           | 1453.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.95e+03    |
|    mean_ep_length       | 7            |
|    mean_reward          | -4.76e+03    |
|    true_cost            | 0.686        |
| infos/                  |              |
|    cost                 | 0.0557       |
| rollout/                |              |
|    adjusted_reward      | 5.62         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 583          |
|    iterations           | 26           |
|    time_elapsed         | 91           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0027307398 |
|    average_cost         | 0.6850586    |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.784        |
|    cost_value_loss      | 0.00161      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.197       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.76e-05     |
|    mean_cost_advantages | -0.023981595 |
|    mean_reward_advan... | 0.016642157  |
|    n_updates            | 250          |
|    nu                   | 3.14         |
|    nu_loss              | -2.09        |
|    policy_gradient_loss | -0.000854    |
|    reward_explained_... | 0.961        |
|    reward_value_loss    | 0.000102     |
|    total_cost           | 1403.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.95e+03    |
|    mean_ep_length       | 7            |
|    mean_reward          | -4.76e+03    |
|    true_cost            | 0.672        |
| infos/                  |              |
|    cost                 | 0.0591       |
| rollout/                |              |
|    adjusted_reward      | 5.57         |
|    ep_len_mean          | 6.02         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 581          |
|    iterations           | 27           |
|    time_elapsed         | 95           |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0017726681 |
|    average_cost         | 0.6855469    |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.937        |
|    cost_value_loss      | 0.000906     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.169       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00252     |
|    mean_cost_advantages | 9.655906e-05 |
|    mean_reward_advan... | 0.009860134  |
|    n_updates            | 260          |
|    nu                   | 3.24         |
|    nu_loss              | -2.16        |
|    policy_gradient_loss | -0.000758    |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 3.05e-05     |
|    total_cost           | 1404.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.95e+03    |
|    mean_ep_length       | 6            |
|    mean_reward          | -3.96e+03    |
|    true_cost            | 0.668        |
| infos/                  |              |
|    cost                 | 0.0592       |
| rollout/                |              |
|    adjusted_reward      | 5.61         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 579          |
|    iterations           | 28           |
|    time_elapsed         | 99           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.001692947  |
|    average_cost         | 0.6723633    |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.92         |
|    cost_value_loss      | 0.000998     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.14        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00125     |
|    mean_cost_advantages | 0.0037759743 |
|    mean_reward_advan... | 0.008292563  |
|    n_updates            | 270          |
|    nu                   | 3.34         |
|    nu_loss              | -2.18        |
|    policy_gradient_loss | -0.000524    |
|    reward_explained_... | 0.994        |
|    reward_value_loss    | 3.53e-05     |
|    total_cost           | 1377.0       |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.95e+03    |
|    mean_ep_length       | 6            |
|    mean_reward          | -3.96e+03    |
|    true_cost            | 0.631        |
| infos/                  |              |
|    cost                 | 0.0548       |
| rollout/                |              |
|    adjusted_reward      | 5.59         |
|    ep_len_mean          | 6.02         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 578          |
|    iterations           | 29           |
|    time_elapsed         | 102          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.019415017  |
|    average_cost         | 0.66845703   |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.97         |
|    cost_value_loss      | 0.000467     |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.138       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00134     |
|    mean_cost_advantages | 0.0003434374 |
|    mean_reward_advan... | 0.007813014  |
|    n_updates            | 280          |
|    nu                   | 3.44         |
|    nu_loss              | -2.23        |
|    policy_gradient_loss | -0.00065     |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 1.3e-05      |
|    total_cost           | 1369.0       |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.96e+03     |
|    mean_ep_length       | 6             |
|    mean_reward          | -2.96e+03     |
|    true_cost            | 0.6           |
| infos/                  |               |
|    cost                 | 0.0511        |
| rollout/                |               |
|    adjusted_reward      | 5.59          |
|    ep_len_mean          | 6             |
|    ep_rew_mean          | 35            |
| time/                   |               |
|    fps                  | 577           |
|    iterations           | 30            |
|    time_elapsed         | 106           |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.012350311   |
|    average_cost         | 0.6308594     |
|    clip_fraction        | 0.114         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.95          |
|    cost_value_loss      | 0.000735      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.174        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000301      |
|    mean_cost_advantages | -0.0015489953 |
|    mean_reward_advan... | 0.0087248115  |
|    n_updates            | 290           |
|    nu                   | 3.53          |
|    nu_loss              | -2.17         |
|    policy_gradient_loss | -0.00131      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 1.96e-05      |
|    total_cost           | 1292.0        |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.96e+03     |
|    mean_ep_length       | 6             |
|    mean_reward          | -3.16e+03     |
|    true_cost            | 0.575         |
| infos/                  |               |
|    cost                 | 0.0509        |
| rollout/                |               |
|    adjusted_reward      | 5.53          |
|    ep_len_mean          | 6.04          |
|    ep_rew_mean          | 34.9          |
| time/                   |               |
|    fps                  | 573           |
|    iterations           | 31            |
|    time_elapsed         | 110           |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.008191833   |
|    average_cost         | 0.60009766    |
|    clip_fraction        | 0.115         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.934         |
|    cost_value_loss      | 0.000955      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.185        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00523      |
|    mean_cost_advantages | -0.0010454879 |
|    mean_reward_advan... | 0.00850464    |
|    n_updates            | 300           |
|    nu                   | 3.63          |
|    nu_loss              | -2.12         |
|    policy_gradient_loss | -0.00117      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 1.73e-05      |
|    total_cost           | 1229.0        |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.76e+03    |
|    mean_ep_length       | 6            |
|    mean_reward          | -2.76e+03    |
|    true_cost            | 0.51         |
| infos/                  |              |
|    cost                 | 0.0469       |
| rollout/                |              |
|    adjusted_reward      | 5.61         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 574          |
|    iterations           | 32           |
|    time_elapsed         | 114          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.024483379  |
|    average_cost         | 0.5751953    |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.7          |
|    cost_value_loss      | 0.00454      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.182       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00821     |
|    mean_cost_advantages | 0.0003680241 |
|    mean_reward_advan... | 0.006780485  |
|    n_updates            | 310          |
|    nu                   | 3.72         |
|    nu_loss              | -2.09        |
|    policy_gradient_loss | -0.00305     |
|    reward_explained_... | 0.972        |
|    reward_value_loss    | 0.000175     |
|    total_cost           | 1178.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.76e+03    |
|    mean_ep_length       | 6            |
|    mean_reward          | -2.76e+03    |
|    true_cost            | 0.468        |
| infos/                  |              |
|    cost                 | 0.0429       |
| rollout/                |              |
|    adjusted_reward      | 5.6          |
|    ep_len_mean          | 6.02         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 571          |
|    iterations           | 33           |
|    time_elapsed         | 118          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.01472812   |
|    average_cost         | 0.5102539    |
|    clip_fraction        | 0.0459       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.863        |
|    cost_value_loss      | 0.00129      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.191       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000526     |
|    mean_cost_advantages | -0.010723583 |
|    mean_reward_advan... | 0.008924642  |
|    n_updates            | 320          |
|    nu                   | 3.81         |
|    nu_loss              | -1.9         |
|    policy_gradient_loss | -0.000953    |
|    reward_explained_... | 0.983        |
|    reward_value_loss    | 2.69e-05     |
|    total_cost           | 1045.0       |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.56e+03    |
|    mean_ep_length       | 6            |
|    mean_reward          | -2.56e+03    |
|    true_cost            | 0.41         |
| infos/                  |              |
|    cost                 | 0.0359       |
| rollout/                |              |
|    adjusted_reward      | 5.61         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 571          |
|    iterations           | 34           |
|    time_elapsed         | 121          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.015610107  |
|    average_cost         | 0.46826172   |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.746        |
|    cost_value_loss      | 0.00301      |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.194       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00288     |
|    mean_cost_advantages | -0.012451857 |
|    mean_reward_advan... | 0.0071030706 |
|    n_updates            | 330          |
|    nu                   | 3.91         |
|    nu_loss              | -1.79        |
|    policy_gradient_loss | -0.00212     |
|    reward_explained_... | 0.989        |
|    reward_value_loss    | 8.41e-05     |
|    total_cost           | 959.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.16e+03     |
|    mean_ep_length       | 6             |
|    mean_reward          | -2.16e+03     |
|    true_cost            | 0.306         |
| infos/                  |               |
|    cost                 | 0.0296        |
| rollout/                |               |
|    adjusted_reward      | 5.68          |
|    ep_len_mean          | 6.02          |
|    ep_rew_mean          | 35            |
| time/                   |               |
|    fps                  | 570           |
|    iterations           | 35            |
|    time_elapsed         | 125           |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.01734526    |
|    average_cost         | 0.41015625    |
|    clip_fraction        | 0.0957        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.729         |
|    cost_value_loss      | 0.00289       |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.19         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000391     |
|    mean_cost_advantages | -0.0067027365 |
|    mean_reward_advan... | 0.010119429   |
|    n_updates            | 340           |
|    nu                   | 3.99          |
|    nu_loss              | -1.6          |
|    policy_gradient_loss | -0.00263      |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 4.38e-05      |
|    total_cost           | 840.0         |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.16e+03    |
|    mean_ep_length       | 6            |
|    mean_reward          | -2.56e+03    |
|    true_cost            | 0.205        |
| infos/                  |              |
|    cost                 | 0.0222       |
| rollout/                |              |
|    adjusted_reward      | 5.69         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 36           |
|    time_elapsed         | 133          |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.023885472  |
|    average_cost         | 0.30615234   |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.515        |
|    cost_value_loss      | 0.00453      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.193       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00216      |
|    mean_cost_advantages | -0.035418056 |
|    mean_reward_advan... | 0.007881569  |
|    n_updates            | 350          |
|    nu                   | 4.08         |
|    nu_loss              | -1.22        |
|    policy_gradient_loss | -0.0029      |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 3.43e-05     |
|    total_cost           | 627.0        |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -763         |
|    mean_ep_length       | 6            |
|    mean_reward          | -763         |
|    true_cost            | 0.1          |
| infos/                  |              |
|    cost                 | 0.00976      |
| rollout/                |              |
|    adjusted_reward      | 5.75         |
|    ep_len_mean          | 6.02         |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 37           |
|    time_elapsed         | 141          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.01581093   |
|    average_cost         | 0.20507812   |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.107       |
|    cost_value_loss      | 0.00574      |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.157       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00508     |
|    mean_cost_advantages | -0.028282365 |
|    mean_reward_advan... | 0.0075711915 |
|    n_updates            | 360          |
|    nu                   | 4.16         |
|    nu_loss              | -0.837       |
|    policy_gradient_loss | -0.00246     |
|    reward_explained_... | 0.995        |
|    reward_value_loss    | 4.62e-05     |
|    total_cost           | 420.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -364         |
|    mean_ep_length       | 6            |
|    mean_reward          | -364         |
|    true_cost            | 0.0503       |
| infos/                  |              |
|    cost                 | 0.00887      |
| rollout/                |              |
|    adjusted_reward      | 5.75         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 506          |
|    iterations           | 38           |
|    time_elapsed         | 153          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.005704945  |
|    average_cost         | 0.100097656  |
|    clip_fraction        | 0.0456       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.502       |
|    cost_value_loss      | 0.00421      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.111       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0028       |
|    mean_cost_advantages | -0.026533544 |
|    mean_reward_advan... | 0.0069336267 |
|    n_updates            | 370          |
|    nu                   | 4.24         |
|    nu_loss              | -0.417       |
|    policy_gradient_loss | -0.00141     |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 2.77e-05     |
|    total_cost           | 205.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 35           |
|    mean_ep_length       | 6            |
|    mean_reward          | 35           |
|    true_cost            | 0.0234       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 5.78         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 461          |
|    iterations           | 39           |
|    time_elapsed         | 172          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.010217313  |
|    average_cost         | 0.05029297   |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.9         |
|    cost_value_loss      | 0.00273      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0577      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000397    |
|    mean_cost_advantages | -0.014529639 |
|    mean_reward_advan... | 0.0069529596 |
|    n_updates            | 380          |
|    nu                   | 4.31         |
|    nu_loss              | -0.213       |
|    policy_gradient_loss | -0.00145     |
|    reward_explained_... | 0.994        |
|    reward_value_loss    | 4.48e-05     |
|    total_cost           | 103.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 35           |
|    mean_ep_length       | 6            |
|    mean_reward          | 35           |
|    true_cost            | 0.00586      |
| infos/                  |              |
|    cost                 | 0.00201      |
| rollout/                |              |
|    adjusted_reward      | 5.79         |
|    ep_len_mean          | 6            |
|    ep_rew_mean          | 35           |
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 40           |
|    time_elapsed         | 180          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.011297771  |
|    average_cost         | 0.0234375    |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.58        |
|    cost_value_loss      | 0.00129      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.02        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000925     |
|    mean_cost_advantages | -0.014336341 |
|    mean_reward_advan... | 0.007685349  |
|    n_updates            | 390          |
|    nu                   | 4.37         |
|    nu_loss              | -0.101       |
|    policy_gradient_loss | -0.00121     |
|    reward_explained_... | 0.994        |
|    reward_value_loss    | 4.57e-05     |
|    total_cost           | 48.0         |
------------------------------------------
Mean reward: 35.000000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 03.22 minutes[0m
