[32mConfigured folder /tmp/wandb/run-20220627_200623-3tk3hyba/files for saving
[32mName: JTL-v0_CJTL-v0_dnc_True_dno_True_dnr_True_ws_True_s_20_sid_-1
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
Wrapping eval env in a VecNormalize.
Using cpu device
----------------------------------
| eval/               |          |
|    best_mean_reward | 7.91e+03 |
|    mean_ep_length   | 121      |
|    mean_reward      | 7.91e+03 |
|    true_cost        | 0.514    |
| infos/              |          |
|    cost             | 0.43     |
| rollout/            |          |
|    adjusted_reward  | 28       |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 4.43e+03 |
| time/               |          |
|    fps              | 2835     |
|    iterations       | 1        |
|    time_elapsed     | 3        |
|    total_timesteps  | 10240    |
----------------------------------
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 151         |
|    mean_reward          | 3.89e+03    |
|    true_cost            | 0.401       |
| infos/                  |             |
|    cost                 | 0.38        |
| rollout/                |             |
|    adjusted_reward      | 27.2        |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | 3.99e+03    |
| time/                   |             |
|    fps                  | 2289        |
|    iterations           | 2           |
|    time_elapsed         | 8           |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.019092176 |
|    average_cost         | 0.51396483  |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -416        |
|    cost_value_loss      | 27.6        |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.56e+06    |
|    mean_cost_advantages | 7.9775987   |
|    mean_reward_advan... | 446.20294   |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.514      |
|    policy_gradient_loss | -0.0113     |
|    reward_explained_... | -2.03e+07   |
|    reward_value_loss    | 2.48e+06    |
|    total_cost           | 5263.0      |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 153         |
|    mean_reward          | 5.91e+03    |
|    true_cost            | 0.322       |
| infos/                  |             |
|    cost                 | 0.49        |
| rollout/                |             |
|    adjusted_reward      | 33.2        |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 4.7e+03     |
| time/                   |             |
|    fps                  | 1981        |
|    iterations           | 3           |
|    time_elapsed         | 15          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.016203009 |
|    average_cost         | 0.40078124  |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -13.8       |
|    cost_value_loss      | 27.1        |
|    early_stop_epoch     | 6           |
|    entropy_loss         | -1.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.61e+05    |
|    mean_cost_advantages | 4.3482933   |
|    mean_reward_advan... | 432.86554   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.427      |
|    policy_gradient_loss | -0.0208     |
|    reward_explained_... | -1.4e+07    |
|    reward_value_loss    | 2.41e+06    |
|    total_cost           | 4104.0      |
-----------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 7.91e+03   |
|    mean_ep_length       | 123        |
|    mean_reward          | 5.92e+03   |
|    true_cost            | 0.237      |
| infos/                  |            |
|    cost                 | 0.17       |
| rollout/                |            |
|    adjusted_reward      | 35.3       |
|    ep_len_mean          | 150        |
|    ep_rew_mean          | 5.01e+03   |
| time/                   |            |
|    fps                  | 1825       |
|    iterations           | 4          |
|    time_elapsed         | 22         |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.01646015 |
|    average_cost         | 0.3223633  |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -2.39      |
|    cost_value_loss      | 29.4       |
|    early_stop_epoch     | 7          |
|    entropy_loss         | -1.27      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.62e+06   |
|    mean_cost_advantages | 2.0922763  |
|    mean_reward_advan... | 522.5083   |
|    n_updates            | 30         |
|    nu                   | 1.2        |
|    nu_loss              | -0.364     |
|    policy_gradient_loss | -0.0187    |
|    reward_explained_... | -2.21e+08  |
|    reward_value_loss    | 2.88e+06   |
|    total_cost           | 3301.0     |
----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 7.91e+03   |
|    mean_ep_length       | 159        |
|    mean_reward          | 3.88e+03   |
|    true_cost            | 0.151      |
| infos/                  |            |
|    cost                 | 0.1        |
| rollout/                |            |
|    adjusted_reward      | 30.5       |
|    ep_len_mean          | 151        |
|    ep_rew_mean          | 4.91e+03   |
| time/                   |            |
|    fps                  | 1790       |
|    iterations           | 5          |
|    time_elapsed         | 28         |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.01760889 |
|    average_cost         | 0.2368164  |
|    clip_fraction        | 0.0999     |
|    clip_range           | 0.2        |
|    cost_explained_va... | -1.74      |
|    cost_value_loss      | 26.4       |
|    early_stop_epoch     | 5          |
|    entropy_loss         | -1.18      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.88e+06   |
|    mean_cost_advantages | 0.4605196  |
|    mean_reward_advan... | 558.3955   |
|    n_updates            | 40         |
|    nu                   | 1.26       |
|    nu_loss              | -0.284     |
|    policy_gradient_loss | -0.022     |
|    reward_explained_... | -3.49e+09  |
|    reward_value_loss    | 3.04e+06   |
|    total_cost           | 2425.0     |
----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 170         |
|    mean_reward          | 3.89e+03    |
|    true_cost            | 0.105       |
| infos/                  |             |
|    cost                 | 0.16        |
| rollout/                |             |
|    adjusted_reward      | 25.7        |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | 4.01e+03    |
| time/                   |             |
|    fps                  | 1771        |
|    iterations           | 6           |
|    time_elapsed         | 34          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.015359652 |
|    average_cost         | 0.15136719  |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.18       |
|    cost_value_loss      | 19.2        |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -1.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.03e+06    |
|    mean_cost_advantages | -0.89703846 |
|    mean_reward_advan... | 462.24234   |
|    n_updates            | 50          |
|    nu                   | 1.33        |
|    nu_loss              | -0.191      |
|    policy_gradient_loss | -0.0101     |
|    reward_explained_... | -7.78e+09   |
|    reward_value_loss    | 2.6e+06     |
|    total_cost           | 1550.0      |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 7.91e+03   |
|    mean_ep_length       | 183        |
|    mean_reward          | 1.89e+03   |
|    true_cost            | 0.0863     |
| infos/                  |            |
|    cost                 | 0.01       |
| rollout/                |            |
|    adjusted_reward      | 27.6       |
|    ep_len_mean          | 160        |
|    ep_rew_mean          | 4.51e+03   |
| time/                   |            |
|    fps                  | 1775       |
|    iterations           | 7          |
|    time_elapsed         | 40         |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.02371452 |
|    average_cost         | 0.10498047 |
|    clip_fraction        | 0.0997     |
|    clip_range           | 0.2        |
|    cost_explained_va... | -0.62      |
|    cost_value_loss      | 12.7       |
|    early_stop_epoch     | 4          |
|    entropy_loss         | -0.972     |
|    learning_rate        | 0.0003     |
|    loss                 | 9.35e+05   |
|    mean_cost_advantages | -1.4294889 |
|    mean_reward_advan... | 387.36914  |
|    n_updates            | 60         |
|    nu                   | 1.39       |
|    nu_loss              | -0.14      |
|    policy_gradient_loss | -0.0105    |
|    reward_explained_... | -4.05e+10  |
|    reward_value_loss    | 2.2e+06    |
|    total_cost           | 1075.0     |
----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 197         |
|    mean_reward          | 1.89e+03    |
|    true_cost            | 0.0522      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | 10.1        |
|    ep_len_mean          | 175         |
|    ep_rew_mean          | 3.2e+03     |
| time/                   |             |
|    fps                  | 1700        |
|    iterations           | 8           |
|    time_elapsed         | 48          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.009315409 |
|    average_cost         | 0.08632813  |
|    clip_fraction        | 0.0539      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.797      |
|    cost_value_loss      | 11.1        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.898      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.04e+05    |
|    mean_cost_advantages | -1.2731675  |
|    mean_reward_advan... | 429.64716   |
|    n_updates            | 70          |
|    nu                   | 1.45        |
|    nu_loss              | -0.12       |
|    policy_gradient_loss | -0.0062     |
|    reward_explained_... | -4.96e+10   |
|    reward_value_loss    | 2.37e+06    |
|    total_cost           | 884.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 200         |
|    mean_reward          | -120        |
|    true_cost            | 0.0271      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | 0.277       |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 877         |
| time/                   |             |
|    fps                  | 1752        |
|    iterations           | 9           |
|    time_elapsed         | 52          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.07665004  |
|    average_cost         | 0.052246094 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.122      |
|    cost_value_loss      | 6.79        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.77       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.71e+05    |
|    mean_cost_advantages | -1.2994833  |
|    mean_reward_advan... | 126.829956  |
|    n_updates            | 80          |
|    nu                   | 1.51        |
|    nu_loss              | -0.0759     |
|    policy_gradient_loss | -0.00192    |
|    reward_explained_... | -6.35e+10   |
|    reward_value_loss    | 8.86e+05    |
|    total_cost           | 535.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 200         |
|    mean_reward          | -127        |
|    true_cost            | 0.0225      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.668      |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | -29.6       |
| time/                   |             |
|    fps                  | 1696        |
|    iterations           | 10          |
|    time_elapsed         | 60          |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.014759868 |
|    average_cost         | 0.027050782 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.268      |
|    cost_value_loss      | 3.37        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.509      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.35e+05    |
|    mean_cost_advantages | -1.4543798  |
|    mean_reward_advan... | -30.260681  |
|    n_updates            | 90          |
|    nu                   | 1.56        |
|    nu_loss              | -0.0408     |
|    policy_gradient_loss | -0.00472    |
|    reward_explained_... | -9.94e+07   |
|    reward_value_loss    | 8.23e+04    |
|    total_cost           | 277.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 200         |
|    mean_reward          | -122        |
|    true_cost            | 0.00742     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.636      |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | -126        |
| time/                   |             |
|    fps                  | 1653        |
|    iterations           | 11          |
|    time_elapsed         | 68          |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.014932439 |
|    average_cost         | 0.022460938 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.193       |
|    cost_value_loss      | 2.74        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.453      |
|    learning_rate        | 0.0003      |
|    loss                 | 39.5        |
|    mean_cost_advantages | -1.1188446  |
|    mean_reward_advan... | -35.811256  |
|    n_updates            | 100         |
|    nu                   | 1.61        |
|    nu_loss              | -0.0351     |
|    policy_gradient_loss | -0.00528    |
|    reward_explained_... | -0.694      |
|    reward_value_loss    | 204         |
|    total_cost           | 230.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 200          |
|    mean_reward          | -123         |
|    true_cost            | 0.0118       |
| infos/                  |              |
|    cost                 | 0.11         |
| rollout/                |              |
|    adjusted_reward      | -0.607       |
|    ep_len_mean          | 200          |
|    ep_rew_mean          | -121         |
| time/                   |              |
|    fps                  | 1622         |
|    iterations           | 12           |
|    time_elapsed         | 75           |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0052039437 |
|    average_cost         | 0.007421875  |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.616        |
|    cost_value_loss      | 0.638        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.407       |
|    learning_rate        | 0.0003       |
|    loss                 | 29.1         |
|    mean_cost_advantages | -1.0482957   |
|    mean_reward_advan... | -27.102142   |
|    n_updates            | 110          |
|    nu                   | 1.66         |
|    nu_loss              | -0.012       |
|    policy_gradient_loss | -0.0021      |
|    reward_explained_... | 0.656        |
|    reward_value_loss    | 67.8         |
|    total_cost           | 76.0         |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 200         |
|    mean_reward          | -125        |
|    true_cost            | 0.00352     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.614      |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | -119        |
| time/                   |             |
|    fps                  | 1645        |
|    iterations           | 13          |
|    time_elapsed         | 80          |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.017033543 |
|    average_cost         | 0.011816407 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.211      |
|    cost_value_loss      | 1.04        |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -0.331      |
|    learning_rate        | 0.0003      |
|    loss                 | 11.9        |
|    mean_cost_advantages | -0.7327291  |
|    mean_reward_advan... | -20.47394   |
|    n_updates            | 120         |
|    nu                   | 1.7         |
|    nu_loss              | -0.0196     |
|    policy_gradient_loss | -0.00362    |
|    reward_explained_... | 0.807       |
|    reward_value_loss    | 44.2        |
|    total_cost           | 121.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 200         |
|    mean_reward          | -113        |
|    true_cost            | 0.00186     |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | -0.566      |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | -117        |
| time/                   |             |
|    fps                  | 1617        |
|    iterations           | 14          |
|    time_elapsed         | 88          |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.013269502 |
|    average_cost         | 0.003515625 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.374       |
|    cost_value_loss      | 0.299       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.294      |
|    learning_rate        | 0.0003      |
|    loss                 | 6.3         |
|    mean_cost_advantages | -0.7567922  |
|    mean_reward_advan... | -16.731586  |
|    n_updates            | 130         |
|    nu                   | 1.73        |
|    nu_loss              | -0.00596    |
|    policy_gradient_loss | -0.0155     |
|    reward_explained_... | 0.766       |
|    reward_value_loss    | 14.8        |
|    total_cost           | 36.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 200          |
|    mean_reward          | -108         |
|    true_cost            | 0.0105       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.518       |
|    ep_len_mean          | 200          |
|    ep_rew_mean          | -106         |
| time/                   |              |
|    fps                  | 1595         |
|    iterations           | 15           |
|    time_elapsed         | 96           |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.012714383  |
|    average_cost         | 0.0018554687 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.445        |
|    cost_value_loss      | 0.139        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.258       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.95         |
|    mean_cost_advantages | -0.5614431   |
|    mean_reward_advan... | -12.027086   |
|    n_updates            | 140          |
|    nu                   | 1.77         |
|    nu_loss              | -0.00322     |
|    policy_gradient_loss | -0.000819    |
|    reward_explained_... | 0.662        |
|    reward_value_loss    | 4.72         |
|    total_cost           | 19.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 200         |
|    mean_reward          | -93.2       |
|    true_cost            | 0.0135      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 1.49        |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 108         |
| time/                   |             |
|    fps                  | 1628        |
|    iterations           | 16          |
|    time_elapsed         | 100         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.021158418 |
|    average_cost         | 0.010546875 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.89       |
|    cost_value_loss      | 0.728       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.195      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.5         |
|    mean_cost_advantages | -0.21470702 |
|    mean_reward_advan... | -7.9400053  |
|    n_updates            | 150         |
|    nu                   | 1.8         |
|    nu_loss              | -0.0187     |
|    policy_gradient_loss | -0.0106     |
|    reward_explained_... | 0.361       |
|    reward_value_loss    | 7.59        |
|    total_cost           | 108.0       |
-----------------------------------------
Early stopping at step 7 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 7.91e+03    |
|    mean_ep_length       | 200         |
|    mean_reward          | -96.5       |
|    true_cost            | 9.77e-05    |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 1.49        |
|    ep_len_mean          | 196         |
|    ep_rew_mean          | 312         |
| time/                   |             |
|    fps                  | 1619        |
|    iterations           | 17          |
|    time_elapsed         | 107         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.0320207   |
|    average_cost         | 0.013476563 |
|    clip_fraction        | 0.0251      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0258     |
|    cost_value_loss      | 1.58        |
|    early_stop_epoch     | 7           |
|    entropy_loss         | -0.137      |
|    learning_rate        | 0.0003      |
|    loss                 | 78.5        |
|    mean_cost_advantages | -0.05108037 |
|    mean_reward_advan... | 25.286776   |
|    n_updates            | 160         |
|    nu                   | 1.83        |
|    nu_loss              | -0.0243     |
|    policy_gradient_loss | -0.00426    |
|    reward_explained_... | -1.71e+04   |
|    reward_value_loss    | 1.67e+05    |
|    total_cost           | 138.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 200          |
|    mean_reward          | -91.9        |
|    true_cost            | 0.00234      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.541        |
|    ep_len_mean          | 198          |
|    ep_rew_mean          | 211          |
| time/                   |              |
|    fps                  | 1600         |
|    iterations           | 18           |
|    time_elapsed         | 115          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0023367044 |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.00608      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.332        |
|    cost_value_loss      | 0.192        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.155       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+05     |
|    mean_cost_advantages | -0.22113883  |
|    mean_reward_advan... | 28.399097    |
|    n_updates            | 170          |
|    nu                   | 1.86         |
|    nu_loss              | -0.000179    |
|    policy_gradient_loss | -0.000486    |
|    reward_explained_... | -1.93e+04    |
|    reward_value_loss    | 1.67e+05     |
|    total_cost           | 1.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 200          |
|    mean_reward          | -84          |
|    true_cost            | 0.000781     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.435       |
|    ep_len_mean          | 199          |
|    ep_rew_mean          | 14.3         |
| time/                   |              |
|    fps                  | 1583         |
|    iterations           | 19           |
|    time_elapsed         | 122          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.0019636054 |
|    average_cost         | 0.00234375   |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.749        |
|    cost_value_loss      | 0.539        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.122       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.81e+05     |
|    mean_cost_advantages | -0.11792693  |
|    mean_reward_advan... | 11.091864    |
|    n_updates            | 180          |
|    nu                   | 1.88         |
|    nu_loss              | -0.00436     |
|    policy_gradient_loss | -0.00163     |
|    reward_explained_... | -288         |
|    reward_value_loss    | 8.42e+04     |
|    total_cost           | 24.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 176          |
|    mean_reward          | 1.93e+03     |
|    true_cost            | 0.000781     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 5.44         |
|    ep_len_mean          | 193          |
|    ep_rew_mean          | 518          |
| time/                   |              |
|    fps                  | 1569         |
|    iterations           | 20           |
|    time_elapsed         | 130          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.006053671  |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.338        |
|    cost_value_loss      | 0.121        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.139       |
|    learning_rate        | 0.0003       |
|    loss                 | 15.3         |
|    mean_cost_advantages | -0.119380906 |
|    mean_reward_advan... | -4.194503    |
|    n_updates            | 190          |
|    nu                   | 1.91         |
|    nu_loss              | -0.00147     |
|    policy_gradient_loss | -0.00107     |
|    reward_explained_... | 0.783        |
|    reward_value_loss    | 22.9         |
|    total_cost           | 8.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 200          |
|    mean_reward          | -81.7        |
|    true_cost            | 0.000781     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 1.55         |
|    ep_len_mean          | 192          |
|    ep_rew_mean          | 722          |
| time/                   |              |
|    fps                  | 1555         |
|    iterations           | 21           |
|    time_elapsed         | 138          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | -3.05376e-06 |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.184        |
|    cost_value_loss      | 0.0403       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.128       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.39e+03     |
|    mean_cost_advantages | -0.09289958  |
|    mean_reward_advan... | 90.17009     |
|    n_updates            | 200          |
|    nu                   | 1.93         |
|    nu_loss              | -0.00149     |
|    policy_gradient_loss | 0.000346     |
|    reward_explained_... | -4.96e+03    |
|    reward_value_loss    | 4.89e+05     |
|    total_cost           | 8.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 7.91e+03       |
|    mean_ep_length       | 160            |
|    mean_reward          | 5.94e+03       |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 3.5            |
|    ep_len_mean          | 193            |
|    ep_rew_mean          | 523            |
| time/                   |                |
|    fps                  | 1544           |
|    iterations           | 22             |
|    time_elapsed         | 145            |
|    total_timesteps      | 225280         |
| train/                  |                |
|    approx_kl            | 0.000116570154 |
|    average_cost         | 0.00078125     |
|    clip_fraction        | 0.00565        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.356          |
|    cost_value_loss      | 0.0471         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.129         |
|    learning_rate        | 0.0003         |
|    loss                 | 151            |
|    mean_cost_advantages | -0.06561092    |
|    mean_reward_advan... | 16.410374      |
|    n_updates            | 210            |
|    nu                   | 1.95           |
|    nu_loss              | -0.00151       |
|    policy_gradient_loss | -0.000488      |
|    reward_explained_... | -97.8          |
|    reward_value_loss    | 1.66e+05       |
|    total_cost           | 8.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 158          |
|    mean_reward          | 3.94e+03     |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 13.3         |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | 1.63e+03     |
| time/                   |              |
|    fps                  | 1533         |
|    iterations           | 23           |
|    time_elapsed         | 153          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.001173347  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.00304      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.493        |
|    cost_value_loss      | 0.00839      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.126       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.43e+04     |
|    mean_cost_advantages | -0.05754726  |
|    mean_reward_advan... | 49.52193     |
|    n_updates            | 220          |
|    nu                   | 1.97         |
|    nu_loss              | -0.00019     |
|    policy_gradient_loss | -0.000318    |
|    reward_explained_... | -369         |
|    reward_value_loss    | 3.28e+05     |
|    total_cost           | 1.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 7.91e+03      |
|    mean_ep_length       | 200           |
|    mean_reward          | -86.7         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 13.3          |
|    ep_len_mean          | 177           |
|    ep_rew_mean          | 2.53e+03      |
| time/                   |               |
|    fps                  | 1524          |
|    iterations           | 24            |
|    time_elapsed         | 161           |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.00018184292 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.00336       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.35          |
|    cost_value_loss      | 0.0259        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.147        |
|    learning_rate        | 0.0003        |
|    loss                 | 6.29e+05      |
|    mean_cost_advantages | -0.037924003  |
|    mean_reward_advan... | 204.26689     |
|    n_updates            | 230           |
|    nu                   | 1.98          |
|    nu_loss              | -0.00134      |
|    policy_gradient_loss | -0.000316     |
|    reward_explained_... | -455          |
|    reward_value_loss    | 1.13e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 7.91e+03      |
|    mean_ep_length       | 134           |
|    mean_reward          | 5.96e+03      |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 22.1          |
|    ep_len_mean          | 171           |
|    ep_rew_mean          | 3.03e+03      |
| time/                   |               |
|    fps                  | 1516          |
|    iterations           | 25            |
|    time_elapsed         | 168           |
|    total_timesteps      | 256000        |
| train/                  |               |
|    approx_kl            | 0.00070342806 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.0082        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.27          |
|    cost_value_loss      | 0.0141        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.152        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.82e+04      |
|    mean_cost_advantages | -0.028592005  |
|    mean_reward_advan... | 199.317       |
|    n_updates            | 240           |
|    nu                   | 2             |
|    nu_loss              | -0.00116      |
|    policy_gradient_loss | -0.000518     |
|    reward_explained_... | -152          |
|    reward_value_loss    | 1.12e+06      |
|    total_cost           | 6.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 7.91e+03       |
|    mean_ep_length       | 200            |
|    mean_reward          | -74.2          |
|    true_cost            | 0.00264        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 27.9           |
|    ep_len_mean          | 155            |
|    ep_rew_mean          | 4.14e+03       |
| time/                   |                |
|    fps                  | 1507           |
|    iterations           | 26             |
|    time_elapsed         | 176            |
|    total_timesteps      | 266240         |
| train/                  |                |
|    approx_kl            | -2.2500753e-06 |
|    average_cost         | 0.000390625    |
|    clip_fraction        | 0.0104         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.408          |
|    cost_value_loss      | 0.00738        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.181         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.25e+06       |
|    mean_cost_advantages | -0.028857535   |
|    mean_reward_advan... | 327.63483      |
|    n_updates            | 250            |
|    nu                   | 2.01           |
|    nu_loss              | -0.00078       |
|    policy_gradient_loss | -0.000971      |
|    reward_explained_... | -205           |
|    reward_value_loss    | 1.83e+06       |
|    total_cost           | 4.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 178          |
|    mean_reward          | 1.92e+03     |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 32.8         |
|    ep_len_mean          | 143          |
|    ep_rew_mean          | 4.85e+03     |
| time/                   |              |
|    fps                  | 1498         |
|    iterations           | 27           |
|    time_elapsed         | 184          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.0013799716 |
|    average_cost         | 0.0026367188 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.252        |
|    cost_value_loss      | 0.0758       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.232       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.8e+05      |
|    mean_cost_advantages | 0.0020265253 |
|    mean_reward_advan... | 401.37628    |
|    n_updates            | 260          |
|    nu                   | 2.02         |
|    nu_loss              | -0.0053      |
|    policy_gradient_loss | -0.000933    |
|    reward_explained_... | -180         |
|    reward_value_loss    | 2.28e+06     |
|    total_cost           | 27.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.91e+03     |
|    mean_ep_length       | 102          |
|    mean_reward          | 5.96e+03     |
|    true_cost            | 0.000879     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 48.4         |
|    ep_len_mean          | 128          |
|    ep_rew_mean          | 6.05e+03     |
| time/                   |              |
|    fps                  | 1491         |
|    iterations           | 28           |
|    time_elapsed         | 192          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.0011617839 |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.439        |
|    cost_value_loss      | 0.00422      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.214       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.56e+05     |
|    mean_cost_advantages | -0.023476463 |
|    mean_reward_advan... | 478.94962    |
|    n_updates            | 270          |
|    nu                   | 2.04         |
|    nu_loss              | -0.000395    |
|    policy_gradient_loss | -0.000665    |
|    reward_explained_... | -148         |
|    reward_value_loss    | 2.66e+06     |
|    total_cost           | 2.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 7.94e+03      |
|    mean_ep_length       | 142           |
|    mean_reward          | 7.94e+03      |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 39.7          |
|    ep_len_mean          | 137           |
|    ep_rew_mean          | 5.45e+03      |
| time/                   |               |
|    fps                  | 1484          |
|    iterations           | 29            |
|    time_elapsed         | 199           |
|    total_timesteps      | 296960        |
| train/                  |               |
|    approx_kl            | 0.0016709858  |
|    average_cost         | 0.0008789062  |
|    clip_fraction        | 0.0176        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.443         |
|    cost_value_loss      | 0.0188        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.27         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.75e+05      |
|    mean_cost_advantages | -0.0121127255 |
|    mean_reward_advan... | 707.0873      |
|    n_updates            | 280           |
|    nu                   | 2.05          |
|    nu_loss              | -0.00179      |
|    policy_gradient_loss | -0.00104      |
|    reward_explained_... | -183          |
|    reward_value_loss    | 3.88e+06      |
|    total_cost           | 9.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 7.94e+03      |
|    mean_ep_length       | 168           |
|    mean_reward          | 1.93e+03      |
|    true_cost            | 0.00117       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 61.1          |
|    ep_len_mean          | 111           |
|    ep_rew_mean          | 6.66e+03      |
| time/                   |               |
|    fps                  | 1478          |
|    iterations           | 30            |
|    time_elapsed         | 207           |
|    total_timesteps      | 307200        |
| train/                  |               |
|    approx_kl            | 0.00038065118 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.0176        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00872       |
|    cost_value_loss      | 0.0107        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.26         |
|    learning_rate        | 0.0003        |
|    loss                 | 2.96e+05      |
|    mean_cost_advantages | -0.010134053  |
|    mean_reward_advan... | 548.82947     |
|    n_updates            | 290           |
|    nu                   | 2.06          |
|    nu_loss              | -0.000799     |
|    policy_gradient_loss | -0.000956     |
|    reward_explained_... | -123          |
|    reward_value_loss    | 3.14e+06      |
|    total_cost           | 4.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 7.95e+03     |
|    mean_ep_length       | 120          |
|    mean_reward          | 7.95e+03     |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 44.5         |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | 5.95e+03     |
| time/                   |              |
|    fps                  | 1472         |
|    iterations           | 31           |
|    time_elapsed         | 215          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0014787822 |
|    average_cost         | 0.001171875  |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.685       |
|    cost_value_loss      | 0.0332       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.283       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.36e+06     |
|    mean_cost_advantages | 0.0021338828 |
|    mean_reward_advan... | 859.94885    |
|    n_updates            | 300          |
|    nu                   | 2.06         |
|    nu_loss              | -0.00241     |
|    policy_gradient_loss | -0.00111     |
|    reward_explained_... | -171         |
|    reward_value_loss    | 4.8e+06      |
|    total_cost           | 12.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 7.97e+03      |
|    mean_ep_length       | 102           |
|    mean_reward          | 7.97e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 72.9          |
|    ep_len_mean          | 97            |
|    ep_rew_mean          | 7.07e+03      |
| time/                   |               |
|    fps                  | 1467          |
|    iterations           | 32            |
|    time_elapsed         | 223           |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.000545845   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.155        |
|    cost_value_loss      | 0.0115        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.277        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.62e+06      |
|    mean_cost_advantages | -0.0030755163 |
|    mean_reward_advan... | 598.0353      |
|    n_updates            | 310           |
|    nu                   | 2.07          |
|    nu_loss              | -0.00101      |
|    policy_gradient_loss | -0.000798     |
|    reward_explained_... | -107          |
|    reward_value_loss    | 3.48e+06      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 7.97e+03      |
|    mean_ep_length       | 151           |
|    mean_reward          | 5.94e+03      |
|    true_cost            | 0.00449       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 78.7          |
|    ep_len_mean          | 94.2          |
|    ep_rew_mean          | 7.66e+03      |
| time/                   |               |
|    fps                  | 1460          |
|    iterations           | 33            |
|    time_elapsed         | 231           |
|    total_timesteps      | 337920        |
| train/                  |               |
|    approx_kl            | 0.00089883694 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0183        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.136        |
|    cost_value_loss      | 0.00809       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.324        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.35e+06      |
|    mean_cost_advantages | -0.0042893    |
|    mean_reward_advan... | 969.33997     |
|    n_updates            | 320           |
|    nu                   | 2.08          |
|    nu_loss              | -0.00101      |
|    policy_gradient_loss | -0.000985     |
|    reward_explained_... | -166          |
|    reward_value_loss    | 5.54e+06      |
|    total_cost           | 5.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.98e+03     |
|    mean_ep_length       | 40.4         |
|    mean_reward          | 9.98e+03     |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 123          |
|    ep_len_mean          | 72.4         |
|    ep_rew_mean          | 8.97e+03     |
| time/                   |              |
|    fps                  | 1456         |
|    iterations           | 34           |
|    time_elapsed         | 239          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0020373198 |
|    average_cost         | 0.0044921874 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.63        |
|    cost_value_loss      | 0.207        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.363       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.29e+06     |
|    mean_cost_advantages | 0.043576393  |
|    mean_reward_advan... | 1070.973     |
|    n_updates            | 330          |
|    nu                   | 2.09         |
|    nu_loss              | -0.00935     |
|    policy_gradient_loss | -0.00124     |
|    reward_explained_... | -173         |
|    reward_value_loss    | 5.99e+06     |
|    total_cost           | 46.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.98e+03     |
|    mean_ep_length       | 146          |
|    mean_reward          | 3.95e+03     |
|    true_cost            | 0.00215      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 120          |
|    ep_len_mean          | 74.9         |
|    ep_rew_mean          | 8.37e+03     |
| time/                   |              |
|    fps                  | 1450         |
|    iterations           | 35           |
|    time_elapsed         | 247          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.002568111  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.0403       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.459        |
|    cost_value_loss      | 0.0281       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.394       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.63e+06     |
|    mean_cost_advantages | -0.014900396 |
|    mean_reward_advan... | 1627.5807    |
|    n_updates            | 340          |
|    nu                   | 2.09         |
|    nu_loss              | -0.00204     |
|    policy_gradient_loss | -0.002       |
|    reward_explained_... | -226         |
|    reward_value_loss    | 9.09e+06     |
|    total_cost           | 10.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.98e+03     |
|    mean_ep_length       | 67.2         |
|    mean_reward          | 9.98e+03     |
|    true_cost            | 0.00391      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 141          |
|    ep_len_mean          | 57.7         |
|    ep_rew_mean          | 9.18e+03     |
| time/                   |              |
|    fps                  | 1446         |
|    iterations           | 36           |
|    time_elapsed         | 254          |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.001095213  |
|    average_cost         | 0.0021484375 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.233        |
|    cost_value_loss      | 0.0475       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.411       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.68e+06     |
|    mean_cost_advantages | 0.005113829  |
|    mean_reward_advan... | 1562.7955    |
|    n_updates            | 350          |
|    nu                   | 2.1          |
|    nu_loss              | -0.0045      |
|    policy_gradient_loss | -0.0012      |
|    reward_explained_... | -227         |
|    reward_value_loss    | 8.86e+06     |
|    total_cost           | 22.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.98e+03    |
|    mean_ep_length       | 45          |
|    mean_reward          | 9.98e+03    |
|    true_cost            | 0.00186     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 165         |
|    ep_len_mean          | 55          |
|    ep_rew_mean          | 9.28e+03    |
| time/                   |             |
|    fps                  | 1442        |
|    iterations           | 37          |
|    time_elapsed         | 262         |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.002284241 |
|    average_cost         | 0.00390625  |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.05       |
|    cost_value_loss      | 0.184       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.446      |
|    learning_rate        | 0.0003      |
|    loss                 | 6.8e+06     |
|    mean_cost_advantages | 0.016669244 |
|    mean_reward_advan... | 1797.5961   |
|    n_updates            | 360         |
|    nu                   | 2.11        |
|    nu_loss              | -0.00821    |
|    policy_gradient_loss | -0.00141    |
|    reward_explained_... | -321        |
|    reward_value_loss    | 1.02e+07    |
|    total_cost           | 40.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.98e+03     |
|    mean_ep_length       | 48           |
|    mean_reward          | 9.98e+03     |
|    true_cost            | 0.00361      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 172          |
|    ep_len_mean          | 59.5         |
|    ep_rew_mean          | 9.48e+03     |
| time/                   |              |
|    fps                  | 1438         |
|    iterations           | 38           |
|    time_elapsed         | 270          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.0036294356 |
|    average_cost         | 0.0018554687 |
|    clip_fraction        | 0.0438       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.228       |
|    cost_value_loss      | 0.0498       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.479       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.85e+06     |
|    mean_cost_advantages | -0.019076828 |
|    mean_reward_advan... | 2073.71      |
|    n_updates            | 370          |
|    nu                   | 2.11         |
|    nu_loss              | -0.00391     |
|    policy_gradient_loss | -0.00172     |
|    reward_explained_... | -337         |
|    reward_value_loss    | 1.18e+07     |
|    total_cost           | 19.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00166      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 188          |
|    ep_len_mean          | 51.6         |
|    ep_rew_mean          | 9.28e+03     |
| time/                   |              |
|    fps                  | 1435         |
|    iterations           | 39           |
|    time_elapsed         | 278          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.0032644935 |
|    average_cost         | 0.0036132813 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.31        |
|    cost_value_loss      | 0.151        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.51        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.94e+06     |
|    mean_cost_advantages | 0.017505053  |
|    mean_reward_advan... | 2147.9653    |
|    n_updates            | 380          |
|    nu                   | 2.12         |
|    nu_loss              | -0.00763     |
|    policy_gradient_loss | -0.00204     |
|    reward_explained_... | -478         |
|    reward_value_loss    | 1.22e+07     |
|    total_cost           | 37.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 43.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00479      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 266          |
|    ep_len_mean          | 37.4         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 1431         |
|    iterations           | 40           |
|    time_elapsed         | 286          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0041950857 |
|    average_cost         | 0.0016601563 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.434        |
|    cost_value_loss      | 0.0266       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.534       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.62e+06     |
|    mean_cost_advantages | -0.023380008 |
|    mean_reward_advan... | 2229.2385    |
|    n_updates            | 390          |
|    nu                   | 2.12         |
|    nu_loss              | -0.00352     |
|    policy_gradient_loss | -0.0022      |
|    reward_explained_... | -626         |
|    reward_value_loss    | 1.29e+07     |
|    total_cost           | 17.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 40.6         |
|    mean_reward          | 9.98e+03     |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 271          |
|    ep_len_mean          | 31.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1428         |
|    iterations           | 41           |
|    time_elapsed         | 293          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.0039323517 |
|    average_cost         | 0.0047851563 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.9         |
|    cost_value_loss      | 0.122        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.548       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.04e+06     |
|    mean_cost_advantages | 0.01275711   |
|    mean_reward_advan... | 3114.391     |
|    n_updates            | 400          |
|    nu                   | 2.13         |
|    nu_loss              | -0.0102      |
|    policy_gradient_loss | -0.00152     |
|    reward_explained_... | -1.1e+03     |
|    reward_value_loss    | 1.8e+07      |
|    total_cost           | 49.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00381      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 332          |
|    ep_len_mean          | 33.5         |
|    ep_rew_mean          | 9.58e+03     |
| time/                   |              |
|    fps                  | 1425         |
|    iterations           | 42           |
|    time_elapsed         | 301          |
|    total_timesteps      | 430080       |
| train/                  |              |
|    approx_kl            | 0.005684142  |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.206        |
|    cost_value_loss      | 0.0481       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.562       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.9e+06      |
|    mean_cost_advantages | -0.026345631 |
|    mean_reward_advan... | 3215.3823    |
|    n_updates            | 410          |
|    nu                   | 2.13         |
|    nu_loss              | -0.0052      |
|    policy_gradient_loss | -0.0022      |
|    reward_explained_... | -1.37e+03    |
|    reward_value_loss    | 1.83e+07     |
|    total_cost           | 25.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00684      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 394          |
|    ep_len_mean          | 26.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1422         |
|    iterations           | 43           |
|    time_elapsed         | 309          |
|    total_timesteps      | 440320       |
| train/                  |              |
|    approx_kl            | 0.00491314   |
|    average_cost         | 0.0038085938 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.747       |
|    cost_value_loss      | 0.0703       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.541       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02e+07     |
|    mean_cost_advantages | 0.0014642328 |
|    mean_reward_advan... | 3660.0032    |
|    n_updates            | 420          |
|    nu                   | 2.14         |
|    nu_loss              | -0.00813     |
|    policy_gradient_loss | -0.00157     |
|    reward_explained_... | -218         |
|    reward_value_loss    | 2.17e+07     |
|    total_cost           | 39.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00937      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 433          |
|    ep_len_mean          | 20.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1418         |
|    iterations           | 44           |
|    time_elapsed         | 317          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.006856364  |
|    average_cost         | 0.0068359375 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.22        |
|    cost_value_loss      | 0.327        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.522       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.3e+07      |
|    mean_cost_advantages | 0.032606363  |
|    mean_reward_advan... | 4206.68      |
|    n_updates            | 430          |
|    nu                   | 2.14         |
|    nu_loss              | -0.0146      |
|    policy_gradient_loss | -0.00223     |
|    reward_explained_... | -3.59e+03    |
|    reward_value_loss    | 2.52e+07     |
|    total_cost           | 70.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0104       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 417          |
|    ep_len_mean          | 23.9         |
|    ep_rew_mean          | 9.79e+03     |
| time/                   |              |
|    fps                  | 1415         |
|    iterations           | 45           |
|    time_elapsed         | 325          |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 0.0036486096 |
|    average_cost         | 0.009375     |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.58        |
|    cost_value_loss      | 0.342        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.488       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.21e+07     |
|    mean_cost_advantages | 0.03463678   |
|    mean_reward_advan... | 4401.6865    |
|    n_updates            | 440          |
|    nu                   | 2.15         |
|    nu_loss              | -0.0201      |
|    policy_gradient_loss | -0.00125     |
|    reward_explained_... | -991         |
|    reward_value_loss    | 2.68e+07     |
|    total_cost           | 96.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00762      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 454          |
|    ep_len_mean          | 22.3         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 1412         |
|    iterations           | 46           |
|    time_elapsed         | 333          |
|    total_timesteps      | 471040       |
| train/                  |              |
|    approx_kl            | 0.0018242424 |
|    average_cost         | 0.010449219  |
|    clip_fraction        | 7.81e-05     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.44        |
|    cost_value_loss      | 0.827        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.473       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.29e+07     |
|    mean_cost_advantages | 0.041880153  |
|    mean_reward_advan... | 4184.208     |
|    n_updates            | 450          |
|    nu                   | 2.16         |
|    nu_loss              | -0.0225      |
|    policy_gradient_loss | -0.000691    |
|    reward_explained_... | -204         |
|    reward_value_loss    | 2.57e+07     |
|    total_cost           | 107.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0124       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 471          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1409         |
|    iterations           | 47           |
|    time_elapsed         | 341          |
|    total_timesteps      | 481280       |
| train/                  |              |
|    approx_kl            | 0.0014383339 |
|    average_cost         | 0.0076171877 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.156        |
|    cost_value_loss      | 0.2          |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.438       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.72e+07     |
|    mean_cost_advantages | -0.010024213 |
|    mean_reward_advan... | 4506.501     |
|    n_updates            | 460          |
|    nu                   | 2.16         |
|    nu_loss              | -0.0164      |
|    policy_gradient_loss | -0.000781    |
|    reward_explained_... | -472         |
|    reward_value_loss    | 2.76e+07     |
|    total_cost           | 78.0         |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 15.8           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00898        |
| infos/                  |                |
|    cost                 | 0.01           |
| rollout/                |                |
|    adjusted_reward      | 467            |
|    ep_len_mean          | 23             |
|    ep_rew_mean          | 9.89e+03       |
| time/                   |                |
|    fps                  | 1406           |
|    iterations           | 48             |
|    time_elapsed         | 349            |
|    total_timesteps      | 491520         |
| train/                  |                |
|    approx_kl            | -3.3807206e-05 |
|    average_cost         | 0.012402344    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.543         |
|    cost_value_loss      | 0.691          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.43          |
|    learning_rate        | 0.0003         |
|    loss                 | 1.28e+07       |
|    mean_cost_advantages | 0.058724396    |
|    mean_reward_advan... | 4610.944       |
|    n_updates            | 470            |
|    nu                   | 2.17           |
|    nu_loss              | -0.0268        |
|    policy_gradient_loss | -0.000467      |
|    reward_explained_... | -312           |
|    reward_value_loss    | 2.83e+07       |
|    total_cost           | 127.0          |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00693       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 478           |
|    ep_len_mean          | 21.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1404          |
|    iterations           | 49            |
|    time_elapsed         | 357           |
|    total_timesteps      | 501760        |
| train/                  |               |
|    approx_kl            | 0.0006108042  |
|    average_cost         | 0.008984375   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.666        |
|    cost_value_loss      | 0.612         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.433        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.29e+07      |
|    mean_cost_advantages | -0.0016644873 |
|    mean_reward_advan... | 4601.914      |
|    n_updates            | 480           |
|    nu                   | 2.18          |
|    nu_loss              | -0.0195       |
|    policy_gradient_loss | -0.000791     |
|    reward_explained_... | -245          |
|    reward_value_loss    | 2.8e+07       |
|    total_cost           | 92.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 51.8         |
|    mean_reward          | 7.96e+03     |
|    true_cost            | 0.00518      |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 488          |
|    ep_len_mean          | 20.7         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 1401         |
|    iterations           | 50           |
|    time_elapsed         | 365          |
|    total_timesteps      | 512000       |
| train/                  |              |
|    approx_kl            | 0.004421251  |
|    average_cost         | 0.0069335937 |
|    clip_fraction        | 0.00847      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0659      |
|    cost_value_loss      | 0.233        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.422       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+07     |
|    mean_cost_advantages | -0.013114581 |
|    mean_reward_advan... | 4688.2764    |
|    n_updates            | 490          |
|    nu                   | 2.18         |
|    nu_loss              | -0.0151      |
|    policy_gradient_loss | -0.000895    |
|    reward_explained_... | -622         |
|    reward_value_loss    | 2.84e+07     |
|    total_cost           | 71.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00547      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 527          |
|    ep_len_mean          | 20.5         |
|    ep_rew_mean          | 9.79e+03     |
| time/                   |              |
|    fps                  | 1399         |
|    iterations           | 51           |
|    time_elapsed         | 373          |
|    total_timesteps      | 522240       |
| train/                  |              |
|    approx_kl            | 0.00306434   |
|    average_cost         | 0.0051757814 |
|    clip_fraction        | 0.00247      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0278      |
|    cost_value_loss      | 0.132        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.455       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+07     |
|    mean_cost_advantages | -0.027696947 |
|    mean_reward_advan... | 4651.652     |
|    n_updates            | 500          |
|    nu                   | 2.19         |
|    nu_loss              | -0.0113      |
|    policy_gradient_loss | -0.000699    |
|    reward_explained_... | -104         |
|    reward_value_loss    | 2.86e+07     |
|    total_cost           | 53.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0113       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 526          |
|    ep_len_mean          | 16.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1396         |
|    iterations           | 52           |
|    time_elapsed         | 381          |
|    total_timesteps      | 532480       |
| train/                  |              |
|    approx_kl            | 0.0019812856 |
|    average_cost         | 0.00546875   |
|    clip_fraction        | 0.000137     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.435       |
|    cost_value_loss      | 0.181        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.393       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.65e+07     |
|    mean_cost_advantages | -0.019340452 |
|    mean_reward_advan... | 4969.2344    |
|    n_updates            | 510          |
|    nu                   | 2.2          |
|    nu_loss              | -0.012       |
|    policy_gradient_loss | -0.000643    |
|    reward_explained_... | -129         |
|    reward_value_loss    | 3.06e+07     |
|    total_cost           | 56.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00615      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 559          |
|    ep_len_mean          | 18.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1394         |
|    iterations           | 53           |
|    time_elapsed         | 389          |
|    total_timesteps      | 542720       |
| train/                  |              |
|    approx_kl            | 0.0014721815 |
|    average_cost         | 0.011328125  |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.944       |
|    cost_value_loss      | 0.951        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.392       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.41e+07     |
|    mean_cost_advantages | 0.059889067  |
|    mean_reward_advan... | 4889.405     |
|    n_updates            | 520          |
|    nu                   | 2.21         |
|    nu_loss              | -0.0249      |
|    policy_gradient_loss | -0.00118     |
|    reward_explained_... | -112         |
|    reward_value_loss    | 3.01e+07     |
|    total_cost           | 116.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 25.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.012        |
| infos/                  |              |
|    cost                 | 0.23         |
| rollout/                |              |
|    adjusted_reward      | 548          |
|    ep_len_mean          | 17.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1392         |
|    iterations           | 54           |
|    time_elapsed         | 397          |
|    total_timesteps      | 552960       |
| train/                  |              |
|    approx_kl            | 0.0027604916 |
|    average_cost         | 0.006152344  |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.149       |
|    cost_value_loss      | 0.251        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.372       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.66e+07     |
|    mean_cost_advantages | -0.012629673 |
|    mean_reward_advan... | 5141.6377    |
|    n_updates            | 530          |
|    nu                   | 2.21         |
|    nu_loss              | -0.0136      |
|    policy_gradient_loss | -0.00104     |
|    reward_explained_... | -1.93e+10    |
|    reward_value_loss    | 3.16e+07     |
|    total_cost           | 63.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0107        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 574           |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1390          |
|    iterations           | 55            |
|    time_elapsed         | 405           |
|    total_timesteps      | 563200        |
| train/                  |               |
|    approx_kl            | 0.00055192236 |
|    average_cost         | 0.012011719   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00762       |
|    cost_value_loss      | 0.964         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.378        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.39e+07      |
|    mean_cost_advantages | 0.08529696    |
|    mean_reward_advan... | 4992.021      |
|    n_updates            | 540           |
|    nu                   | 2.22          |
|    nu_loss              | -0.0266       |
|    policy_gradient_loss | -0.000555     |
|    reward_explained_... | -137          |
|    reward_value_loss    | 3.06e+07      |
|    total_cost           | 123.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00684      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 591          |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1388         |
|    iterations           | 56           |
|    time_elapsed         | 412          |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.0034733736 |
|    average_cost         | 0.0107421875 |
|    clip_fraction        | 0.0086       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.537        |
|    cost_value_loss      | 0.47         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.372       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.56e+07     |
|    mean_cost_advantages | -0.021854393 |
|    mean_reward_advan... | 5177.6196    |
|    n_updates            | 550          |
|    nu                   | 2.23         |
|    nu_loss              | -0.0239      |
|    policy_gradient_loss | -0.00114     |
|    reward_explained_... | -209         |
|    reward_value_loss    | 3.18e+07     |
|    total_cost           | 110.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00889       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 602           |
|    ep_len_mean          | 15.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1386          |
|    iterations           | 57            |
|    time_elapsed         | 420           |
|    total_timesteps      | 583680        |
| train/                  |               |
|    approx_kl            | 0.00064787146 |
|    average_cost         | 0.0068359375  |
|    clip_fraction        | 0.000469      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.768        |
|    cost_value_loss      | 0.275         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.334        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.7e+07       |
|    mean_cost_advantages | -0.020986363  |
|    mean_reward_advan... | 5235.4463     |
|    n_updates            | 560           |
|    nu                   | 2.24          |
|    nu_loss              | -0.0153       |
|    policy_gradient_loss | -0.000588     |
|    reward_explained_... | -1.74e+12     |
|    reward_value_loss    | 3.22e+07      |
|    total_cost           | 70.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00273       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 585           |
|    ep_len_mean          | 16.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1384          |
|    iterations           | 58            |
|    time_elapsed         | 428           |
|    total_timesteps      | 593920        |
| train/                  |               |
|    approx_kl            | 0.00033260172 |
|    average_cost         | 0.008886719   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.385         |
|    cost_value_loss      | 0.29          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.343        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.52e+07      |
|    mean_cost_advantages | -0.0071351724 |
|    mean_reward_advan... | 5257.0103     |
|    n_updates            | 570           |
|    nu                   | 2.25          |
|    nu_loss              | -0.0199       |
|    policy_gradient_loss | -0.000473     |
|    reward_explained_... | -175          |
|    reward_value_loss    | 3.25e+07      |
|    total_cost           | 91.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00781       |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | 600           |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1382          |
|    iterations           | 59            |
|    time_elapsed         | 436           |
|    total_timesteps      | 604160        |
| train/                  |               |
|    approx_kl            | 0.00027480657 |
|    average_cost         | 0.002734375   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.368        |
|    cost_value_loss      | 0.0437        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.348        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.41e+07      |
|    mean_cost_advantages | -0.04492852   |
|    mean_reward_advan... | 5103.925      |
|    n_updates            | 580           |
|    nu                   | 2.26          |
|    nu_loss              | -0.00615      |
|    policy_gradient_loss | -0.000355     |
|    reward_explained_... | -101          |
|    reward_value_loss    | 3.14e+07      |
|    total_cost           | 28.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 13.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.0042         |
| infos/                  |                |
|    cost                 | 0.02           |
| rollout/                |                |
|    adjusted_reward      | 596            |
|    ep_len_mean          | 16.8           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 1381           |
|    iterations           | 60             |
|    time_elapsed         | 444            |
|    total_timesteps      | 614400         |
| train/                  |                |
|    approx_kl            | 0.000121996134 |
|    average_cost         | 0.0078125      |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.778         |
|    cost_value_loss      | 0.148          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.326         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.85e+07       |
|    mean_cost_advantages | 0.027480636    |
|    mean_reward_advan... | 5209.5806      |
|    n_updates            | 590            |
|    nu                   | 2.27           |
|    nu_loss              | -0.0176        |
|    policy_gradient_loss | -0.000332      |
|    reward_explained_... | -4.62e+12      |
|    reward_value_loss    | 3.18e+07       |
|    total_cost           | 80.0           |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 18             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00508        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 614            |
|    ep_len_mean          | 17.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 1379           |
|    iterations           | 61             |
|    time_elapsed         | 452            |
|    total_timesteps      | 624640         |
| train/                  |                |
|    approx_kl            | -5.0127077e-05 |
|    average_cost         | 0.004199219    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.656         |
|    cost_value_loss      | 0.0729         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.327         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.72e+07       |
|    mean_cost_advantages | -0.026888454   |
|    mean_reward_advan... | 5184.837       |
|    n_updates            | 600            |
|    nu                   | 2.28           |
|    nu_loss              | -0.00952       |
|    policy_gradient_loss | -0.000215      |
|    reward_explained_... | -173           |
|    reward_value_loss    | 3.16e+07       |
|    total_cost           | 43.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00762       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 612           |
|    ep_len_mean          | 16.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1377          |
|    iterations           | 62            |
|    time_elapsed         | 460           |
|    total_timesteps      | 634880        |
| train/                  |               |
|    approx_kl            | 0.0002702812  |
|    average_cost         | 0.005078125   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.588        |
|    cost_value_loss      | 0.146         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.323        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.52e+07      |
|    mean_cost_advantages | 0.00093512796 |
|    mean_reward_advan... | 5232.8765     |
|    n_updates            | 610           |
|    nu                   | 2.28          |
|    nu_loss              | -0.0116       |
|    policy_gradient_loss | -0.000345     |
|    reward_explained_... | -140          |
|    reward_value_loss    | 3.19e+07      |
|    total_cost           | 52.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00449      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 624          |
|    ep_len_mean          | 15.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1376         |
|    iterations           | 63           |
|    time_elapsed         | 468          |
|    total_timesteps      | 645120       |
| train/                  |              |
|    approx_kl            | 0.001757755  |
|    average_cost         | 0.0076171877 |
|    clip_fraction        | 0.00159      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.829       |
|    cost_value_loss      | 0.388        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.321       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.52e+07     |
|    mean_cost_advantages | 0.026054513  |
|    mean_reward_advan... | 5218.3457    |
|    n_updates            | 620          |
|    nu                   | 2.29         |
|    nu_loss              | -0.0174      |
|    policy_gradient_loss | -0.000645    |
|    reward_explained_... | -127         |
|    reward_value_loss    | 3.17e+07     |
|    total_cost           | 78.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00586      |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 671          |
|    ep_len_mean          | 15.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1375         |
|    iterations           | 64           |
|    time_elapsed         | 476          |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 3.422976e-05 |
|    average_cost         | 0.0044921874 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.573       |
|    cost_value_loss      | 0.0838       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.296       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.78e+07     |
|    mean_cost_advantages | -0.026582276 |
|    mean_reward_advan... | 5280.2983    |
|    n_updates            | 630          |
|    nu                   | 2.3          |
|    nu_loss              | -0.0103      |
|    policy_gradient_loss | -0.000305    |
|    reward_explained_... | -4.21e+11    |
|    reward_value_loss    | 3.2e+07      |
|    total_cost           | 46.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00508       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 664           |
|    ep_len_mean          | 15.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1373          |
|    iterations           | 65            |
|    time_elapsed         | 484           |
|    total_timesteps      | 665600        |
| train/                  |               |
|    approx_kl            | 9.921447e-05  |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.746        |
|    cost_value_loss      | 0.125         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.281        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.67e+07      |
|    mean_cost_advantages | 0.00080382527 |
|    mean_reward_advan... | 5507.335      |
|    n_updates            | 640           |
|    nu                   | 2.31          |
|    nu_loss              | -0.0135       |
|    policy_gradient_loss | -0.000224     |
|    reward_explained_... | -6.62e+11     |
|    reward_value_loss    | 3.37e+07      |
|    total_cost           | 60.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.004         |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 650           |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1372          |
|    iterations           | 66            |
|    time_elapsed         | 492           |
|    total_timesteps      | 675840        |
| train/                  |               |
|    approx_kl            | 0.00036915136 |
|    average_cost         | 0.005078125   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.53         |
|    cost_value_loss      | 0.122         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.283        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.43e+07      |
|    mean_cost_advantages | 0.00076844543 |
|    mean_reward_advan... | 5458.3994     |
|    n_updates            | 650           |
|    nu                   | 2.32          |
|    nu_loss              | -0.0117       |
|    policy_gradient_loss | -0.0004       |
|    reward_explained_... | -3.41e+13     |
|    reward_value_loss    | 3.32e+07      |
|    total_cost           | 52.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00508       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 649           |
|    ep_len_mean          | 16.8          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 1371          |
|    iterations           | 67            |
|    time_elapsed         | 500           |
|    total_timesteps      | 686080        |
| train/                  |               |
|    approx_kl            | 0.00032280968 |
|    average_cost         | 0.004003906   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.105         |
|    cost_value_loss      | 0.0974        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.294        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.6e+07       |
|    mean_cost_advantages | -0.016543826  |
|    mean_reward_advan... | 5327.7446     |
|    n_updates            | 660           |
|    nu                   | 2.33          |
|    nu_loss              | -0.00928      |
|    policy_gradient_loss | -0.000276     |
|    reward_explained_... | -98.7         |
|    reward_value_loss    | 3.23e+07      |
|    total_cost           | 41.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00547      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 664          |
|    ep_len_mean          | 14.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1369         |
|    iterations           | 68           |
|    time_elapsed         | 508          |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.0008020874 |
|    average_cost         | 0.005078125  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.031       |
|    cost_value_loss      | 0.133        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.281       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+07     |
|    mean_cost_advantages | 0.0028703467 |
|    mean_reward_advan... | 5276.573     |
|    n_updates            | 670          |
|    nu                   | 2.33         |
|    nu_loss              | -0.0118      |
|    policy_gradient_loss | -0.00025     |
|    reward_explained_... | -100         |
|    reward_value_loss    | 3.19e+07     |
|    total_cost           | 52.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00449      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 655          |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1368         |
|    iterations           | 69           |
|    time_elapsed         | 516          |
|    total_timesteps      | 706560       |
| train/                  |              |
|    approx_kl            | 0.0019015189 |
|    average_cost         | 0.00546875   |
|    clip_fraction        | 0.00315      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.55        |
|    cost_value_loss      | 0.0989       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.268       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+07      |
|    mean_cost_advantages | 0.008704704  |
|    mean_reward_advan... | 5363.4595    |
|    n_updates            | 680          |
|    nu                   | 2.34         |
|    nu_loss              | -0.0128      |
|    policy_gradient_loss | -0.000428    |
|    reward_explained_... | -5.68e+11    |
|    reward_value_loss    | 3.23e+07     |
|    total_cost           | 56.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00459       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 639           |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1367          |
|    iterations           | 70            |
|    time_elapsed         | 524           |
|    total_timesteps      | 716800        |
| train/                  |               |
|    approx_kl            | 0.00052993244 |
|    average_cost         | 0.0044921874  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.512        |
|    cost_value_loss      | 0.102         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.278        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.67e+07      |
|    mean_cost_advantages | -0.01432122   |
|    mean_reward_advan... | 5216.298      |
|    n_updates            | 690           |
|    nu                   | 2.35          |
|    nu_loss              | -0.0105       |
|    policy_gradient_loss | -0.000331     |
|    reward_explained_... | -46.7         |
|    reward_value_loss    | 3.15e+07      |
|    total_cost           | 46.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 13.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00693        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 678            |
|    ep_len_mean          | 14.5           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 1365           |
|    iterations           | 71             |
|    time_elapsed         | 532            |
|    total_timesteps      | 727040         |
| train/                  |                |
|    approx_kl            | -1.2476847e-05 |
|    average_cost         | 0.0045898436   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.915         |
|    cost_value_loss      | 0.0526         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.271         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.26e+07       |
|    mean_cost_advantages | -0.0060268594  |
|    mean_reward_advan... | 5144.4097      |
|    n_updates            | 700            |
|    nu                   | 2.36           |
|    nu_loss              | -0.0108        |
|    policy_gradient_loss | -0.000155      |
|    reward_explained_... | -49.8          |
|    reward_value_loss    | 3.07e+07       |
|    total_cost           | 47.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00391       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 680           |
|    ep_len_mean          | 13.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1364          |
|    iterations           | 72            |
|    time_elapsed         | 540           |
|    total_timesteps      | 737280        |
| train/                  |               |
|    approx_kl            | 0.00033447533 |
|    average_cost         | 0.0069335937  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.57         |
|    cost_value_loss      | 0.102         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.26         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.98e+07      |
|    mean_cost_advantages | 0.017681677   |
|    mean_reward_advan... | 5370.39       |
|    n_updates            | 710           |
|    nu                   | 2.36          |
|    nu_loss              | -0.0163       |
|    policy_gradient_loss | -0.000422     |
|    reward_explained_... | -6.73e+03     |
|    reward_value_loss    | 3.21e+07      |
|    total_cost           | 71.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 16            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00459       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 644           |
|    ep_len_mean          | 16.6          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 1363          |
|    iterations           | 73            |
|    time_elapsed         | 548           |
|    total_timesteps      | 747520        |
| train/                  |               |
|    approx_kl            | 0.00058976345 |
|    average_cost         | 0.00390625    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.36         |
|    cost_value_loss      | 0.0529        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.254        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.76e+07      |
|    mean_cost_advantages | -0.01421209   |
|    mean_reward_advan... | 5340.9946     |
|    n_updates            | 720           |
|    nu                   | 2.37          |
|    nu_loss              | -0.00924      |
|    policy_gradient_loss | -0.000224     |
|    reward_explained_... | -237          |
|    reward_value_loss    | 3.19e+07      |
|    total_cost           | 40.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00469      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 666          |
|    ep_len_mean          | 14.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1362         |
|    iterations           | 74           |
|    time_elapsed         | 556          |
|    total_timesteps      | 757760       |
| train/                  |              |
|    approx_kl            | 0.0002284133 |
|    average_cost         | 0.0045898436 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.355        |
|    cost_value_loss      | 0.143        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.263       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.44e+07     |
|    mean_cost_advantages | 0.0012152955 |
|    mean_reward_advan... | 5074.1523    |
|    n_updates            | 730          |
|    nu                   | 2.38         |
|    nu_loss              | -0.0109      |
|    policy_gradient_loss | -0.000148    |
|    reward_explained_... | -34.8        |
|    reward_value_loss    | 3.01e+07     |
|    total_cost           | 47.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0041        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 682           |
|    ep_len_mean          | 13.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1361          |
|    iterations           | 75            |
|    time_elapsed         | 564           |
|    total_timesteps      | 768000        |
| train/                  |               |
|    approx_kl            | 0.0008209469  |
|    average_cost         | 0.0046875     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.54         |
|    cost_value_loss      | 0.0739        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.25         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.49e+07      |
|    mean_cost_advantages | -0.0006683163 |
|    mean_reward_advan... | 5191.729      |
|    n_updates            | 740           |
|    nu                   | 2.39          |
|    nu_loss              | -0.0112       |
|    policy_gradient_loss | -0.000334     |
|    reward_explained_... | -71.7         |
|    reward_value_loss    | 3.07e+07      |
|    total_cost           | 48.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0042        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 691           |
|    ep_len_mean          | 13.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1360          |
|    iterations           | 76            |
|    time_elapsed         | 572           |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | -2.146922e-05 |
|    average_cost         | 0.0041015623  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.73         |
|    cost_value_loss      | 0.0564        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.241        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.34e+07      |
|    mean_cost_advantages | -0.0077881836 |
|    mean_reward_advan... | 5309.6553     |
|    n_updates            | 750           |
|    nu                   | 2.39          |
|    nu_loss              | -0.00979      |
|    policy_gradient_loss | -0.000148     |
|    reward_explained_... | -9.1e+10      |
|    reward_value_loss    | 3.13e+07      |
|    total_cost           | 42.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00254       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 690           |
|    ep_len_mean          | 13.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1359          |
|    iterations           | 77            |
|    time_elapsed         | 580           |
|    total_timesteps      | 788480        |
| train/                  |               |
|    approx_kl            | 7.1110204e-05 |
|    average_cost         | 0.004199219   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.55         |
|    cost_value_loss      | 0.0618        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.244        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.24e+07      |
|    mean_cost_advantages | -0.0022058648 |
|    mean_reward_advan... | 5312.4424     |
|    n_updates            | 760           |
|    nu                   | 2.4           |
|    nu_loss              | -0.0101       |
|    policy_gradient_loss | -0.000176     |
|    reward_explained_... | -2.04e+12     |
|    reward_value_loss    | 3.13e+07      |
|    total_cost           | 43.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00273       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 696           |
|    ep_len_mean          | 14.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1358          |
|    iterations           | 78            |
|    time_elapsed         | 587           |
|    total_timesteps      | 798720        |
| train/                  |               |
|    approx_kl            | 0.00022039088 |
|    average_cost         | 0.0025390624  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.64         |
|    cost_value_loss      | 0.0321        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.239        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.67e+07      |
|    mean_cost_advantages | -0.011384401  |
|    mean_reward_advan... | 5297.136      |
|    n_updates            | 770           |
|    nu                   | 2.41          |
|    nu_loss              | -0.0061       |
|    policy_gradient_loss | -0.000202     |
|    reward_explained_... | -1.18e+13     |
|    reward_value_loss    | 3.1e+07       |
|    total_cost           | 26.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00264        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 702            |
|    ep_len_mean          | 14.8           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 1357           |
|    iterations           | 79             |
|    time_elapsed         | 595            |
|    total_timesteps      | 808960         |
| train/                  |                |
|    approx_kl            | -5.6251884e-06 |
|    average_cost         | 0.002734375    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.42          |
|    cost_value_loss      | 0.0324         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.237         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.78e+07       |
|    mean_cost_advantages | -0.007827578   |
|    mean_reward_advan... | 5297.7534      |
|    n_updates            | 780            |
|    nu                   | 2.42           |
|    nu_loss              | -0.00659       |
|    policy_gradient_loss | -0.000188      |
|    reward_explained_... | -8.28e+12      |
|    reward_value_loss    | 3.1e+07        |
|    total_cost           | 28.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00527       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 708           |
|    ep_len_mean          | 14.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1356          |
|    iterations           | 80            |
|    time_elapsed         | 603           |
|    total_timesteps      | 819200        |
| train/                  |               |
|    approx_kl            | 0.0020613489  |
|    average_cost         | 0.0026367188  |
|    clip_fraction        | 0.00212       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0567       |
|    cost_value_loss      | 0.0451        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.236        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.69e+07      |
|    mean_cost_advantages | -0.0011245757 |
|    mean_reward_advan... | 5259.58       |
|    n_updates            | 790           |
|    nu                   | 2.42          |
|    nu_loss              | -0.00637      |
|    policy_gradient_loss | -0.000536     |
|    reward_explained_... | -54.8         |
|    reward_value_loss    | 3.08e+07      |
|    total_cost           | 27.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 15.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00254       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 717           |
|    ep_len_mean          | 14.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1355          |
|    iterations           | 81            |
|    time_elapsed         | 611           |
|    total_timesteps      | 829440        |
| train/                  |               |
|    approx_kl            | 0.00012798334 |
|    average_cost         | 0.0052734376  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.59         |
|    cost_value_loss      | 0.0688        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.218        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.5e+07       |
|    mean_cost_advantages | 0.016651845   |
|    mean_reward_advan... | 5315.021      |
|    n_updates            | 800           |
|    nu                   | 2.43          |
|    nu_loss              | -0.0128       |
|    policy_gradient_loss | -0.000178     |
|    reward_explained_... | -2.52e+11     |
|    reward_value_loss    | 3.09e+07      |
|    total_cost           | 54.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00479       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 713           |
|    ep_len_mean          | 14.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 1354          |
|    iterations           | 82            |
|    time_elapsed         | 619           |
|    total_timesteps      | 839680        |
| train/                  |               |
|    approx_kl            | -6.065994e-05 |
|    average_cost         | 0.0025390624  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.62         |
|    cost_value_loss      | 0.0232        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.209        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.51e+07      |
|    mean_cost_advantages | -0.019197168  |
|    mean_reward_advan... | 5332.618      |
|    n_updates            | 810           |
|    nu                   | 2.43          |
|    nu_loss              | -0.00617      |
|    policy_gradient_loss | -9.76e-05     |
|    reward_explained_... | -2.03e+13     |
|    reward_value_loss    | 3.1e+07       |
|    total_cost           | 26.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 730           |
|    ep_len_mean          | 13.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1353          |
|    iterations           | 83            |
|    time_elapsed         | 627           |
|    total_timesteps      | 849920        |
| train/                  |               |
|    approx_kl            | 0.00080740545 |
|    average_cost         | 0.0047851563  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.81         |
|    cost_value_loss      | 0.0809        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.212        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.57e+07      |
|    mean_cost_advantages | 0.015656415   |
|    mean_reward_advan... | 5272.7896     |
|    n_updates            | 820           |
|    nu                   | 2.44          |
|    nu_loss              | -0.0116       |
|    policy_gradient_loss | -0.000239     |
|    reward_explained_... | -2.09e+13     |
|    reward_value_loss    | 3.05e+07      |
|    total_cost           | 49.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00283      |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 712          |
|    ep_len_mean          | 14.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1352         |
|    iterations           | 84           |
|    time_elapsed         | 635          |
|    total_timesteps      | 860160       |
| train/                  |              |
|    approx_kl            | 0.0002578036 |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.86        |
|    cost_value_loss      | 0.045        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.212       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.56e+07     |
|    mean_cost_advantages | -0.011125235 |
|    mean_reward_advan... | 5334.8623    |
|    n_updates            | 830          |
|    nu                   | 2.45         |
|    nu_loss              | -0.00715     |
|    policy_gradient_loss | -0.000151    |
|    reward_explained_... | -3.55e+12    |
|    reward_value_loss    | 3.09e+07     |
|    total_cost           | 30.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00586      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 721          |
|    ep_len_mean          | 13.1         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1351         |
|    iterations           | 85           |
|    time_elapsed         | 643          |
|    total_timesteps      | 870400       |
| train/                  |              |
|    approx_kl            | 0.0001600066 |
|    average_cost         | 0.0028320313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.06        |
|    cost_value_loss      | 0.0311       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.209       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.31e+07     |
|    mean_cost_advantages | 0.0011518744 |
|    mean_reward_advan... | 5210.663     |
|    n_updates            | 840          |
|    nu                   | 2.45         |
|    nu_loss              | -0.00693     |
|    policy_gradient_loss | -0.000141    |
|    reward_explained_... | -4.94e+13    |
|    reward_value_loss    | 2.99e+07     |
|    total_cost           | 29.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00273       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 738           |
|    ep_len_mean          | 14.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1351          |
|    iterations           | 86            |
|    time_elapsed         | 651           |
|    total_timesteps      | 880640        |
| train/                  |               |
|    approx_kl            | 0.00040162698 |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.12         |
|    cost_value_loss      | 0.124         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.211        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.6e+07       |
|    mean_cost_advantages | 0.019421186   |
|    mean_reward_advan... | 5241.333      |
|    n_updates            | 850           |
|    nu                   | 2.46          |
|    nu_loss              | -0.0144       |
|    policy_gradient_loss | -0.00024      |
|    reward_explained_... | -4.57e+13     |
|    reward_value_loss    | 3e+07         |
|    total_cost           | 60.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00332      |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 717          |
|    ep_len_mean          | 14.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1350         |
|    iterations           | 87           |
|    time_elapsed         | 659          |
|    total_timesteps      | 890880       |
| train/                  |              |
|    approx_kl            | 0.0009893882 |
|    average_cost         | 0.002734375  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.8         |
|    cost_value_loss      | 0.0346       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.21        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.61e+07     |
|    mean_cost_advantages | -0.013132049 |
|    mean_reward_advan... | 5285.694     |
|    n_updates            | 860          |
|    nu                   | 2.47         |
|    nu_loss              | -0.00673     |
|    policy_gradient_loss | -0.000237    |
|    reward_explained_... | -2.51e+14    |
|    reward_value_loss    | 3.03e+07     |
|    total_cost           | 28.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00313       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 735           |
|    ep_len_mean          | 13.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1349          |
|    iterations           | 88            |
|    time_elapsed         | 667           |
|    total_timesteps      | 901120        |
| train/                  |               |
|    approx_kl            | 0.00021758917 |
|    average_cost         | 0.0033203126  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.03         |
|    cost_value_loss      | 0.0366        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.205        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.42e+07      |
|    mean_cost_advantages | 0.0006879132  |
|    mean_reward_advan... | 5175.0117     |
|    n_updates            | 870           |
|    nu                   | 2.47          |
|    nu_loss              | -0.00819      |
|    policy_gradient_loss | -0.000114     |
|    reward_explained_... | -4.57e+13     |
|    reward_value_loss    | 2.94e+07      |
|    total_cost           | 34.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00361      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 702          |
|    ep_len_mean          | 13.7         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1348         |
|    iterations           | 89           |
|    time_elapsed         | 675          |
|    total_timesteps      | 911360       |
| train/                  |              |
|    approx_kl            | 8.345291e-06 |
|    average_cost         | 0.003125     |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3           |
|    cost_value_loss      | 0.0486       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.194       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.55e+07     |
|    mean_cost_advantages | 0.0033899038 |
|    mean_reward_advan... | 5205.463     |
|    n_updates            | 880          |
|    nu                   | 2.48         |
|    nu_loss              | -0.00773     |
|    policy_gradient_loss | -0.000132    |
|    reward_explained_... | -4.53e+13    |
|    reward_value_loss    | 2.96e+07     |
|    total_cost           | 32.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.0042       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 729          |
|    ep_len_mean          | 13.6         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1347         |
|    iterations           | 90           |
|    time_elapsed         | 683          |
|    total_timesteps      | 921600       |
| train/                  |              |
|    approx_kl            | 8.187693e-05 |
|    average_cost         | 0.0036132813 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.447        |
|    cost_value_loss      | 0.0637       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.206       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+07      |
|    mean_cost_advantages | 0.005922881  |
|    mean_reward_advan... | 5005.3765    |
|    n_updates            | 890          |
|    nu                   | 2.48         |
|    nu_loss              | -0.00895     |
|    policy_gradient_loss | -0.000114    |
|    reward_explained_... | -35.2        |
|    reward_value_loss    | 2.82e+07     |
|    total_cost           | 37.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00303      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 737          |
|    ep_len_mean          | 13.7         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1347         |
|    iterations           | 91           |
|    time_elapsed         | 691          |
|    total_timesteps      | 931840       |
| train/                  |              |
|    approx_kl            | 8.453947e-05 |
|    average_cost         | 0.004199219  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.36        |
|    cost_value_loss      | 0.0728       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.195       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.36e+07     |
|    mean_cost_advantages | 0.0028099294 |
|    mean_reward_advan... | 5166.2266    |
|    n_updates            | 900          |
|    nu                   | 2.49         |
|    nu_loss              | -0.0104      |
|    policy_gradient_loss | -0.000145    |
|    reward_explained_... | -5.99e+10    |
|    reward_value_loss    | 2.91e+07     |
|    total_cost           | 43.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 14           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00381      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 758          |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1346         |
|    iterations           | 92           |
|    time_elapsed         | 699          |
|    total_timesteps      | 942080       |
| train/                  |              |
|    approx_kl            | 2.301935e-05 |
|    average_cost         | 0.0030273437 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.83        |
|    cost_value_loss      | 0.0368       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.191       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.39e+07     |
|    mean_cost_advantages | -0.013313828 |
|    mean_reward_advan... | 5166.4087    |
|    n_updates            | 910          |
|    nu                   | 2.5          |
|    nu_loss              | -0.00754     |
|    policy_gradient_loss | -0.000125    |
|    reward_explained_... | -9.56e+12    |
|    reward_value_loss    | 2.9e+07      |
|    total_cost           | 31.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00313      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 752          |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1345         |
|    iterations           | 93           |
|    time_elapsed         | 707          |
|    total_timesteps      | 952320       |
| train/                  |              |
|    approx_kl            | 0.0013708672 |
|    average_cost         | 0.0038085938 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.51        |
|    cost_value_loss      | 0.0463       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.187       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.57e+07     |
|    mean_cost_advantages | 0.0023215346 |
|    mean_reward_advan... | 5225.0146    |
|    n_updates            | 920          |
|    nu                   | 2.5          |
|    nu_loss              | -0.00951     |
|    policy_gradient_loss | -0.000267    |
|    reward_explained_... | -3.17e+13    |
|    reward_value_loss    | 2.94e+07     |
|    total_cost           | 39.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00479      |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 743          |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1345         |
|    iterations           | 94           |
|    time_elapsed         | 715          |
|    total_timesteps      | 962560       |
| train/                  |              |
|    approx_kl            | 0.0023664813 |
|    average_cost         | 0.003125     |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.7         |
|    cost_value_loss      | 0.0502       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.177       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.47e+07     |
|    mean_cost_advantages | 0.0008017408 |
|    mean_reward_advan... | 5169.212     |
|    n_updates            | 930          |
|    nu                   | 2.51         |
|    nu_loss              | -0.00782     |
|    policy_gradient_loss | -0.000369    |
|    reward_explained_... | -6.21e+13    |
|    reward_value_loss    | 2.89e+07     |
|    total_cost           | 32.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00547      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 721          |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1344         |
|    iterations           | 95           |
|    time_elapsed         | 723          |
|    total_timesteps      | 972800       |
| train/                  |              |
|    approx_kl            | 0.0008671127 |
|    average_cost         | 0.0047851563 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.01        |
|    cost_value_loss      | 0.0643       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.185       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.54e+07     |
|    mean_cost_advantages | 0.006553913  |
|    mean_reward_advan... | 5101.5234    |
|    n_updates            | 940          |
|    nu                   | 2.51         |
|    nu_loss              | -0.012       |
|    policy_gradient_loss | -0.00028     |
|    reward_explained_... | -147         |
|    reward_value_loss    | 2.84e+07     |
|    total_cost           | 49.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00205      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 751          |
|    ep_len_mean          | 13.2         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1343         |
|    iterations           | 96           |
|    time_elapsed         | 731          |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0012988169 |
|    average_cost         | 0.00546875   |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.83        |
|    cost_value_loss      | 0.0855       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.187       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.43e+07     |
|    mean_cost_advantages | 0.008013079  |
|    mean_reward_advan... | 4972.9893    |
|    n_updates            | 950          |
|    nu                   | 2.52         |
|    nu_loss              | -0.0138      |
|    policy_gradient_loss | -0.000414    |
|    reward_explained_... | -43.4        |
|    reward_value_loss    | 2.74e+07     |
|    total_cost           | 56.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00352       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 748           |
|    ep_len_mean          | 12.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1343          |
|    iterations           | 97            |
|    time_elapsed         | 739           |
|    total_timesteps      | 993280        |
| train/                  |               |
|    approx_kl            | 0.00044046086 |
|    average_cost         | 0.0020507812  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.29         |
|    cost_value_loss      | 0.0226        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.186        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.51e+07      |
|    mean_cost_advantages | -0.022867328  |
|    mean_reward_advan... | 5109.8022     |
|    n_updates            | 960           |
|    nu                   | 2.53          |
|    nu_loss              | -0.00517      |
|    policy_gradient_loss | -0.000171     |
|    reward_explained_... | -1.74e+13     |
|    reward_value_loss    | 2.82e+07      |
|    total_cost           | 21.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 13             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00283        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 747            |
|    ep_len_mean          | 14.6           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 1342           |
|    iterations           | 98             |
|    time_elapsed         | 747            |
|    total_timesteps      | 1003520        |
| train/                  |                |
|    approx_kl            | -1.2236449e-05 |
|    average_cost         | 0.003515625    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.81          |
|    cost_value_loss      | 0.0495         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.182         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.56e+07       |
|    mean_cost_advantages | 0.0055183857   |
|    mean_reward_advan... | 5049.652       |
|    n_updates            | 970            |
|    nu                   | 2.53           |
|    nu_loss              | -0.00888       |
|    policy_gradient_loss | -9.98e-05      |
|    reward_explained_... | -9.53e+11      |
|    reward_value_loss    | 2.78e+07       |
|    total_cost           | 36.0           |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00117      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 757          |
|    ep_len_mean          | 13.1         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1341         |
|    iterations           | 99           |
|    time_elapsed         | 755          |
|    total_timesteps      | 1013760      |
| train/                  |              |
|    approx_kl            | 5.44562e-05  |
|    average_cost         | 0.0028320313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.08        |
|    cost_value_loss      | 0.0481       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.178       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.34e+07     |
|    mean_cost_advantages | -0.00436683  |
|    mean_reward_advan... | 5029.064     |
|    n_updates            | 980          |
|    nu                   | 2.54         |
|    nu_loss              | -0.00717     |
|    policy_gradient_loss | -0.000108    |
|    reward_explained_... | -3.81e+14    |
|    reward_value_loss    | 2.75e+07     |
|    total_cost           | 29.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00322      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 754          |
|    ep_len_mean          | 13.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1341         |
|    iterations           | 100          |
|    time_elapsed         | 763          |
|    total_timesteps      | 1024000      |
| train/                  |              |
|    approx_kl            | 0.0007081056 |
|    average_cost         | 0.001171875  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.46        |
|    cost_value_loss      | 0.0079       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.177       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.4e+07      |
|    mean_cost_advantages | -0.013827759 |
|    mean_reward_advan... | 5054.9287    |
|    n_updates            | 990          |
|    nu                   | 2.54         |
|    nu_loss              | -0.00298     |
|    policy_gradient_loss | -0.000143    |
|    reward_explained_... | -1.26e+13    |
|    reward_value_loss    | 2.76e+07     |
|    total_cost           | 12.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00156      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 757          |
|    ep_len_mean          | 13.5         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1340         |
|    iterations           | 101          |
|    time_elapsed         | 771          |
|    total_timesteps      | 1034240      |
| train/                  |              |
|    approx_kl            | 0.0007456896 |
|    average_cost         | 0.0032226562 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3           |
|    cost_value_loss      | 0.0405       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.171       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.5e+07      |
|    mean_cost_advantages | 0.01664457   |
|    mean_reward_advan... | 4996.3857    |
|    n_updates            | 1000         |
|    nu                   | 2.55         |
|    nu_loss              | -0.0082      |
|    policy_gradient_loss | -0.000182    |
|    reward_explained_... | -1.56e+14    |
|    reward_value_loss    | 2.72e+07     |
|    total_cost           | 33.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00537       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 746           |
|    ep_len_mean          | 13.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1340          |
|    iterations           | 102           |
|    time_elapsed         | 779           |
|    total_timesteps      | 1044480       |
| train/                  |               |
|    approx_kl            | 0.0008023599  |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.56         |
|    cost_value_loss      | 0.0255        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.172        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.24e+07      |
|    mean_cost_advantages | -0.0030336017 |
|    mean_reward_advan... | 5005.836      |
|    n_updates            | 1010          |
|    nu                   | 2.56          |
|    nu_loss              | -0.00399      |
|    policy_gradient_loss | -0.000177     |
|    reward_explained_... | -3.64e+13     |
|    reward_value_loss    | 2.71e+07      |
|    total_cost           | 16.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00313      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 753          |
|    ep_len_mean          | 12.7         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1339         |
|    iterations           | 103          |
|    time_elapsed         | 787          |
|    total_timesteps      | 1054720      |
| train/                  |              |
|    approx_kl            | 0.0010948075 |
|    average_cost         | 0.0053710938 |
|    clip_fraction        | 0.00831      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.82        |
|    cost_value_loss      | 0.0954       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.17        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.3e+07      |
|    mean_cost_advantages | 0.029867083  |
|    mean_reward_advan... | 4904.2295    |
|    n_updates            | 1020         |
|    nu                   | 2.56         |
|    nu_loss              | -0.0137      |
|    policy_gradient_loss | -0.00059     |
|    reward_explained_... | -1.06e+13    |
|    reward_value_loss    | 2.64e+07     |
|    total_cost           | 55.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00205      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 753          |
|    ep_len_mean          | 13.5         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1338         |
|    iterations           | 104          |
|    time_elapsed         | 795          |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0011434866 |
|    average_cost         | 0.003125     |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.89        |
|    cost_value_loss      | 0.04         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.176       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.23e+07     |
|    mean_cost_advantages | -0.02695097  |
|    mean_reward_advan... | 4940.5493    |
|    n_updates            | 1030         |
|    nu                   | 2.57         |
|    nu_loss              | -0.00801     |
|    policy_gradient_loss | -0.000233    |
|    reward_explained_... | -3.58e+13    |
|    reward_value_loss    | 2.64e+07     |
|    total_cost           | 32.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00361      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 756          |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1338         |
|    iterations           | 105          |
|    time_elapsed         | 803          |
|    total_timesteps      | 1075200      |
| train/                  |              |
|    approx_kl            | 0.0001909609 |
|    average_cost         | 0.0020507812 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.62        |
|    cost_value_loss      | 0.0308       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.166       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.23e+07     |
|    mean_cost_advantages | -0.009970982 |
|    mean_reward_advan... | 4920.2744    |
|    n_updates            | 1040         |
|    nu                   | 2.57         |
|    nu_loss              | -0.00527     |
|    policy_gradient_loss | -0.00012     |
|    reward_explained_... | -3.68e+13    |
|    reward_value_loss    | 2.62e+07     |
|    total_cost           | 21.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00264       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 772           |
|    ep_len_mean          | 13.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1337          |
|    iterations           | 106           |
|    time_elapsed         | 811           |
|    total_timesteps      | 1085440       |
| train/                  |               |
|    approx_kl            | 0.00018338149 |
|    average_cost         | 0.0036132813  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.41         |
|    cost_value_loss      | 0.0424        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.168        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.28e+07      |
|    mean_cost_advantages | 0.012779723   |
|    mean_reward_advan... | 4896.995      |
|    n_updates            | 1050          |
|    nu                   | 2.58          |
|    nu_loss              | -0.0093       |
|    policy_gradient_loss | -0.000182     |
|    reward_explained_... | -3.56e+13     |
|    reward_value_loss    | 2.6e+07       |
|    total_cost           | 37.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00225       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 759           |
|    ep_len_mean          | 13.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1337          |
|    iterations           | 107           |
|    time_elapsed         | 819           |
|    total_timesteps      | 1095680       |
| train/                  |               |
|    approx_kl            | 2.8654666e-05 |
|    average_cost         | 0.0026367188  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.43         |
|    cost_value_loss      | 0.0289        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.167        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.2e+07       |
|    mean_cost_advantages | -0.007849207  |
|    mean_reward_advan... | 4943.1025     |
|    n_updates            | 1060          |
|    nu                   | 2.58          |
|    nu_loss              | -0.0068       |
|    policy_gradient_loss | -0.000116     |
|    reward_explained_... | -9.78e+15     |
|    reward_value_loss    | 2.63e+07      |
|    total_cost           | 27.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00234       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 766           |
|    ep_len_mean          | 12.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1336          |
|    iterations           | 108           |
|    time_elapsed         | 827           |
|    total_timesteps      | 1105920       |
| train/                  |               |
|    approx_kl            | 2.3588107e-05 |
|    average_cost         | 0.0022460937  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.99         |
|    cost_value_loss      | 0.0326        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.161        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.42e+07      |
|    mean_cost_advantages | -0.0009669047 |
|    mean_reward_advan... | 4864.861      |
|    n_updates            | 1070          |
|    nu                   | 2.59          |
|    nu_loss              | -0.0058       |
|    policy_gradient_loss | -7.23e-05     |
|    reward_explained_... | -8.96e+12     |
|    reward_value_loss    | 2.56e+07      |
|    total_cost           | 23.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00273       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 759           |
|    ep_len_mean          | 13.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1336          |
|    iterations           | 109           |
|    time_elapsed         | 835           |
|    total_timesteps      | 1116160       |
| train/                  |               |
|    approx_kl            | 0.0008146589  |
|    average_cost         | 0.00234375    |
|    clip_fraction        | 0.00577       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.25         |
|    cost_value_loss      | 0.0439        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.162        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.27e+07      |
|    mean_cost_advantages | 0.00013416912 |
|    mean_reward_advan... | 4840.3413     |
|    n_updates            | 1080          |
|    nu                   | 2.59          |
|    nu_loss              | -0.00607      |
|    policy_gradient_loss | -0.000375     |
|    reward_explained_... | -17.9         |
|    reward_value_loss    | 2.55e+07      |
|    total_cost           | 24.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00361       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 760           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1335          |
|    iterations           | 110           |
|    time_elapsed         | 843           |
|    total_timesteps      | 1126400       |
| train/                  |               |
|    approx_kl            | 0.00032443283 |
|    average_cost         | 0.002734375   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.05         |
|    cost_value_loss      | 0.0334        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.153        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.2e+07       |
|    mean_cost_advantages | 0.0020478643  |
|    mean_reward_advan... | 4810.335      |
|    n_updates            | 1090          |
|    nu                   | 2.6           |
|    nu_loss              | -0.0071       |
|    policy_gradient_loss | -0.000146     |
|    reward_explained_... | -77.9         |
|    reward_value_loss    | 2.52e+07      |
|    total_cost           | 28.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00186       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 780           |
|    ep_len_mean          | 12.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1335          |
|    iterations           | 111           |
|    time_elapsed         | 851           |
|    total_timesteps      | 1136640       |
| train/                  |               |
|    approx_kl            | 4.8662525e-05 |
|    average_cost         | 0.0036132813  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2            |
|    cost_value_loss      | 0.0486        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.157        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.16e+07      |
|    mean_cost_advantages | 0.005034685   |
|    mean_reward_advan... | 4801.18       |
|    n_updates            | 1100          |
|    nu                   | 2.61          |
|    nu_loss              | -0.0094       |
|    policy_gradient_loss | -0.000104     |
|    reward_explained_... | -7.34e+04     |
|    reward_value_loss    | 2.5e+07       |
|    total_cost           | 37.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00352       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 769           |
|    ep_len_mean          | 12.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1334          |
|    iterations           | 112           |
|    time_elapsed         | 859           |
|    total_timesteps      | 1146880       |
| train/                  |               |
|    approx_kl            | 1.8400035e-05 |
|    average_cost         | 0.0018554687  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.98         |
|    cost_value_loss      | 0.0291        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.151        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.15e+07      |
|    mean_cost_advantages | -0.0063838675 |
|    mean_reward_advan... | 4837.8696     |
|    n_updates            | 1110          |
|    nu                   | 2.61          |
|    nu_loss              | -0.00483      |
|    policy_gradient_loss | -6.86e-05     |
|    reward_explained_... | -6.38e+03     |
|    reward_value_loss    | 2.52e+07      |
|    total_cost           | 19.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00215       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 774           |
|    ep_len_mean          | 13.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1334          |
|    iterations           | 113           |
|    time_elapsed         | 867           |
|    total_timesteps      | 1157120       |
| train/                  |               |
|    approx_kl            | 0.00013920825 |
|    average_cost         | 0.003515625   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.71         |
|    cost_value_loss      | 0.0428        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.153        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.17e+07      |
|    mean_cost_advantages | 0.0055082017  |
|    mean_reward_advan... | 4787.6885     |
|    n_updates            | 1120          |
|    nu                   | 2.62          |
|    nu_loss              | -0.00918      |
|    policy_gradient_loss | -0.000115     |
|    reward_explained_... | -3.34e+17     |
|    reward_value_loss    | 2.47e+07      |
|    total_cost           | 36.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 13.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00273        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 776            |
|    ep_len_mean          | 12.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1333           |
|    iterations           | 114            |
|    time_elapsed         | 875            |
|    total_timesteps      | 1167360        |
| train/                  |                |
|    approx_kl            | 0.000105484294 |
|    average_cost         | 0.0021484375   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.806         |
|    cost_value_loss      | 0.0285         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.148         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.32e+07       |
|    mean_cost_advantages | -0.005686646   |
|    mean_reward_advan... | 4781.4707      |
|    n_updates            | 1130           |
|    nu                   | 2.62           |
|    nu_loss              | -0.00562       |
|    policy_gradient_loss | -0.000114      |
|    reward_explained_... | -3.18e+13      |
|    reward_value_loss    | 2.46e+07       |
|    total_cost           | 22.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00225       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 793           |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1333          |
|    iterations           | 115           |
|    time_elapsed         | 883           |
|    total_timesteps      | 1177600       |
| train/                  |               |
|    approx_kl            | 5.9897848e-05 |
|    average_cost         | 0.002734375   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.26         |
|    cost_value_loss      | 0.0303        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.145        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.2e+07       |
|    mean_cost_advantages | 0.0035520017  |
|    mean_reward_advan... | 4752.962      |
|    n_updates            | 1140          |
|    nu                   | 2.63          |
|    nu_loss              | -0.00717      |
|    policy_gradient_loss | -8.1e-05      |
|    reward_explained_... | -3.26e+13     |
|    reward_value_loss    | 2.44e+07      |
|    total_cost           | 28.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00166      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 777          |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1332         |
|    iterations           | 116          |
|    time_elapsed         | 891          |
|    total_timesteps      | 1187840      |
| train/                  |              |
|    approx_kl            | 0.0002720893 |
|    average_cost         | 0.0022460937 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.18        |
|    cost_value_loss      | 0.0282       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.141       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.22e+07     |
|    mean_cost_advantages | 0.0003314439 |
|    mean_reward_advan... | 4783.7734    |
|    n_updates            | 1150         |
|    nu                   | 2.63         |
|    nu_loss              | -0.0059      |
|    policy_gradient_loss | -0.000124    |
|    reward_explained_... | -5.91e+12    |
|    reward_value_loss    | 2.45e+07     |
|    total_cost           | 23.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 793           |
|    ep_len_mean          | 13.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1332          |
|    iterations           | 117           |
|    time_elapsed         | 899           |
|    total_timesteps      | 1198080       |
| train/                  |               |
|    approx_kl            | 0.00030959194 |
|    average_cost         | 0.0016601563  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.46         |
|    cost_value_loss      | 0.0171        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.141        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.15e+07      |
|    mean_cost_advantages | -0.0066425824 |
|    mean_reward_advan... | 4718.126      |
|    n_updates            | 1160          |
|    nu                   | 2.64          |
|    nu_loss              | -0.00437      |
|    policy_gradient_loss | -7.17e-05     |
|    reward_explained_... | -3.11e+13     |
|    reward_value_loss    | 2.4e+07       |
|    total_cost           | 17.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00234       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 780           |
|    ep_len_mean          | 12.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1331          |
|    iterations           | 118           |
|    time_elapsed         | 907           |
|    total_timesteps      | 1208320       |
| train/                  |               |
|    approx_kl            | 0.00013317801 |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.43         |
|    cost_value_loss      | 0.0194        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.136        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.32e+07      |
|    mean_cost_advantages | -0.0036613592 |
|    mean_reward_advan... | 4746.878      |
|    n_updates            | 1170          |
|    nu                   | 2.64          |
|    nu_loss              | -0.00412      |
|    policy_gradient_loss | -9.59e-05     |
|    reward_explained_... | -2.9e+13      |
|    reward_value_loss    | 2.41e+07      |
|    total_cost           | 16.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00195        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 777            |
|    ep_len_mean          | 13             |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1331           |
|    iterations           | 119            |
|    time_elapsed         | 915            |
|    total_timesteps      | 1218560        |
| train/                  |                |
|    approx_kl            | 0.000117990676 |
|    average_cost         | 0.00234375     |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.37          |
|    cost_value_loss      | 0.0273         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.138         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.22e+07       |
|    mean_cost_advantages | 0.009447529    |
|    mean_reward_advan... | 4660.0786      |
|    n_updates            | 1180           |
|    nu                   | 2.65           |
|    nu_loss              | -0.00619       |
|    policy_gradient_loss | -9.92e-05      |
|    reward_explained_... | -4.13e+11      |
|    reward_value_loss    | 2.35e+07       |
|    total_cost           | 24.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00322       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 780           |
|    ep_len_mean          | 13.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1331          |
|    iterations           | 120           |
|    time_elapsed         | 923           |
|    total_timesteps      | 1228800       |
| train/                  |               |
|    approx_kl            | 0.00021530552 |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.97         |
|    cost_value_loss      | 0.0263        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.136        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.01e+07      |
|    mean_cost_advantages | -0.0022384147 |
|    mean_reward_advan... | 4648.006      |
|    n_updates            | 1190          |
|    nu                   | 2.65          |
|    nu_loss              | -0.00517      |
|    policy_gradient_loss | -0.000126     |
|    reward_explained_... | -1.68e+15     |
|    reward_value_loss    | 2.33e+07      |
|    total_cost           | 20.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00254      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 779          |
|    ep_len_mean          | 13.2         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1330         |
|    iterations           | 121          |
|    time_elapsed         | 931          |
|    total_timesteps      | 1239040      |
| train/                  |              |
|    approx_kl            | 0.0009733654 |
|    average_cost         | 0.0032226562 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.98        |
|    cost_value_loss      | 0.0464       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.133       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.14e+07     |
|    mean_cost_advantages | 0.011889075  |
|    mean_reward_advan... | 4632.953     |
|    n_updates            | 1200         |
|    nu                   | 2.66         |
|    nu_loss              | -0.00854     |
|    policy_gradient_loss | -0.000199    |
|    reward_explained_... | -7.49e+12    |
|    reward_value_loss    | 2.31e+07     |
|    total_cost           | 33.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00137      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 794          |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1330         |
|    iterations           | 122          |
|    time_elapsed         | 939          |
|    total_timesteps      | 1249280      |
| train/                  |              |
|    approx_kl            | 0.001061016  |
|    average_cost         | 0.0025390624 |
|    clip_fraction        | 0.00388      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.57        |
|    cost_value_loss      | 0.035        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.128       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.11e+07     |
|    mean_cost_advantages | -0.009398176 |
|    mean_reward_advan... | 4605.193     |
|    n_updates            | 1210         |
|    nu                   | 2.66         |
|    nu_loss              | -0.00674     |
|    policy_gradient_loss | -0.000251    |
|    reward_explained_... | -2.99e+13    |
|    reward_value_loss    | 2.29e+07     |
|    total_cost           | 26.0         |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 13.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00166        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 791            |
|    ep_len_mean          | 12.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1329           |
|    iterations           | 123            |
|    time_elapsed         | 947            |
|    total_timesteps      | 1259520        |
| train/                  |                |
|    approx_kl            | -1.1443393e-05 |
|    average_cost         | 0.0013671875   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.69          |
|    cost_value_loss      | 0.0164         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.132         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.06e+07       |
|    mean_cost_advantages | -0.007552768   |
|    mean_reward_advan... | 4619.677       |
|    n_updates            | 1220           |
|    nu                   | 2.67           |
|    nu_loss              | -0.00364       |
|    policy_gradient_loss | -3.97e-05      |
|    reward_explained_... | -2.88e+13      |
|    reward_value_loss    | 2.29e+07       |
|    total_cost           | 14.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00313       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 790           |
|    ep_len_mean          | 13            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1329          |
|    iterations           | 124           |
|    time_elapsed         | 955           |
|    total_timesteps      | 1269760       |
| train/                  |               |
|    approx_kl            | 1.4738971e-05 |
|    average_cost         | 0.0016601563  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.77         |
|    cost_value_loss      | 0.0205        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.127        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.22e+07      |
|    mean_cost_advantages | -0.001411252  |
|    mean_reward_advan... | 4597.364      |
|    n_updates            | 1230          |
|    nu                   | 2.67          |
|    nu_loss              | -0.00442      |
|    policy_gradient_loss | -5.09e-05     |
|    reward_explained_... | -7e+12        |
|    reward_value_loss    | 2.27e+07      |
|    total_cost           | 17.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 797           |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1329          |
|    iterations           | 125           |
|    time_elapsed         | 963           |
|    total_timesteps      | 1280000       |
| train/                  |               |
|    approx_kl            | 0.00010626453 |
|    average_cost         | 0.003125      |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.79         |
|    cost_value_loss      | 0.0344        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.126        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.28e+07      |
|    mean_cost_advantages | 0.0093062855  |
|    mean_reward_advan... | 4558.952      |
|    n_updates            | 1240          |
|    nu                   | 2.67          |
|    nu_loss              | -0.00834      |
|    policy_gradient_loss | -6.18e-05     |
|    reward_explained_... | -2.89e+13     |
|    reward_value_loss    | 2.24e+07      |
|    total_cost           | 32.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000879       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 793            |
|    ep_len_mean          | 12.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1328           |
|    iterations           | 126            |
|    time_elapsed         | 971            |
|    total_timesteps      | 1290240        |
| train/                  |                |
|    approx_kl            | -1.7496326e-05 |
|    average_cost         | 0.0015625      |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.12          |
|    cost_value_loss      | 0.025          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.126         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.07e+07       |
|    mean_cost_advantages | -0.004621557   |
|    mean_reward_advan... | 4553.2944      |
|    n_updates            | 1250           |
|    nu                   | 2.68           |
|    nu_loss              | -0.00418       |
|    policy_gradient_loss | -4.78e-05      |
|    reward_explained_... | -3.15e+12      |
|    reward_value_loss    | 2.23e+07       |
|    total_cost           | 16.0           |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00283      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 792          |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1328         |
|    iterations           | 127          |
|    time_elapsed         | 979          |
|    total_timesteps      | 1300480      |
| train/                  |              |
|    approx_kl            | 4.842619e-05 |
|    average_cost         | 0.0008789062 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.18        |
|    cost_value_loss      | 0.0111       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.125       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.05e+07     |
|    mean_cost_advantages | -0.009479133 |
|    mean_reward_advan... | 4520.3564    |
|    n_updates            | 1260         |
|    nu                   | 2.68         |
|    nu_loss              | -0.00235     |
|    policy_gradient_loss | -4.45e-05    |
|    reward_explained_... | -2.86e+13    |
|    reward_value_loss    | 2.2e+07      |
|    total_cost           | 9.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00117      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 789          |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1327         |
|    iterations           | 128          |
|    time_elapsed         | 987          |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 7.404633e-05 |
|    average_cost         | 0.0028320313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.84        |
|    cost_value_loss      | 0.0369       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.122       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.15e+07     |
|    mean_cost_advantages | 0.007377781  |
|    mean_reward_advan... | 4497.7173    |
|    n_updates            | 1270         |
|    nu                   | 2.69         |
|    nu_loss              | -0.0076      |
|    policy_gradient_loss | -7e-05       |
|    reward_explained_... | -6.96e+12    |
|    reward_value_loss    | 2.17e+07     |
|    total_cost           | 29.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 804           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1327          |
|    iterations           | 129           |
|    time_elapsed         | 995           |
|    total_timesteps      | 1320960       |
| train/                  |               |
|    approx_kl            | 8.303509e-06  |
|    average_cost         | 0.001171875   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.77         |
|    cost_value_loss      | 0.0134        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.123        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.11e+07      |
|    mean_cost_advantages | -0.0068563567 |
|    mean_reward_advan... | 4475.3164     |
|    n_updates            | 1280          |
|    nu                   | 2.69          |
|    nu_loss              | -0.00315      |
|    policy_gradient_loss | -4.21e-05     |
|    reward_explained_... | -2.72e+13     |
|    reward_value_loss    | 2.15e+07      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 794           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1327          |
|    iterations           | 130           |
|    time_elapsed         | 1003          |
|    total_timesteps      | 1331200       |
| train/                  |               |
|    approx_kl            | 7.140171e-06  |
|    average_cost         | 0.0012695312  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.66         |
|    cost_value_loss      | 0.0093        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.119        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.03e+07      |
|    mean_cost_advantages | -0.0045591155 |
|    mean_reward_advan... | 4493.112      |
|    n_updates            | 1290          |
|    nu                   | 2.7           |
|    nu_loss              | -0.00342      |
|    policy_gradient_loss | -3.94e-05     |
|    reward_explained_... | -2.88e+12     |
|    reward_value_loss    | 2.16e+07      |
|    total_cost           | 13.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 809          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1326         |
|    iterations           | 131          |
|    time_elapsed         | 1011         |
|    total_timesteps      | 1341440      |
| train/                  |              |
|    approx_kl            | 0.0004964009 |
|    average_cost         | 0.0013671875 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.27        |
|    cost_value_loss      | 0.0233       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.118       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.03e+07     |
|    mean_cost_advantages | 0.0036473304 |
|    mean_reward_advan... | 4433.6553    |
|    n_updates            | 1300         |
|    nu                   | 2.7          |
|    nu_loss              | -0.00369     |
|    policy_gradient_loss | -8.1e-05     |
|    reward_explained_... | -2.71e+13    |
|    reward_value_loss    | 2.11e+07     |
|    total_cost           | 14.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 815          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1326         |
|    iterations           | 132          |
|    time_elapsed         | 1019         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 8.087895e-05 |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.85        |
|    cost_value_loss      | 0.0279       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.113       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.11e+07     |
|    mean_cost_advantages | 0.0037522255 |
|    mean_reward_advan... | 4446.868     |
|    n_updates            | 1310         |
|    nu                   | 2.7          |
|    nu_loss              | -0.00527     |
|    policy_gradient_loss | -0.000106    |
|    reward_explained_... | -6.51e+12    |
|    reward_value_loss    | 2.12e+07     |
|    total_cost           | 20.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 794           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1325          |
|    iterations           | 133           |
|    time_elapsed         | 1027          |
|    total_timesteps      | 1361920       |
| train/                  |               |
|    approx_kl            | 0.00015875533 |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.7          |
|    cost_value_loss      | 0.01          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.115        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.21e+06      |
|    mean_cost_advantages | -0.007178484  |
|    mean_reward_advan... | 4446.6553     |
|    n_updates            | 1320          |
|    nu                   | 2.71          |
|    nu_loss              | -0.00264      |
|    policy_gradient_loss | -6.25e-05     |
|    reward_explained_... | -6.19e+12     |
|    reward_value_loss    | 2.11e+07      |
|    total_cost           | 10.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00166       |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 800           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1325          |
|    iterations           | 134           |
|    time_elapsed         | 1035          |
|    total_timesteps      | 1372160       |
| train/                  |               |
|    approx_kl            | 1.8001814e-05 |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.72         |
|    cost_value_loss      | 0.016         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.118        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.05e+07      |
|    mean_cost_advantages | 0.0052382685  |
|    mean_reward_advan... | 4364.5894     |
|    n_updates            | 1330          |
|    nu                   | 2.71          |
|    nu_loss              | -0.00529      |
|    policy_gradient_loss | -4.79e-05     |
|    reward_explained_... | -2.69e+16     |
|    reward_value_loss    | 2.05e+07      |
|    total_cost           | 20.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00273        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 794            |
|    ep_len_mean          | 13.1           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1325           |
|    iterations           | 135            |
|    time_elapsed         | 1043           |
|    total_timesteps      | 1382400        |
| train/                  |                |
|    approx_kl            | 0.000101007325 |
|    average_cost         | 0.0016601563   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.94          |
|    cost_value_loss      | 0.0177         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.113         |
|    learning_rate        | 0.0003         |
|    loss                 | 9.78e+06       |
|    mean_cost_advantages | 0.0014609848   |
|    mean_reward_advan... | 4355.5835      |
|    n_updates            | 1340           |
|    nu                   | 2.71           |
|    nu_loss              | -0.0045        |
|    policy_gradient_loss | -4.25e-05      |
|    reward_explained_... | -1.61e+12      |
|    reward_value_loss    | 2.04e+07       |
|    total_cost           | 17.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 810           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1324          |
|    iterations           | 136           |
|    time_elapsed         | 1051          |
|    total_timesteps      | 1392640       |
| train/                  |               |
|    approx_kl            | 0.00031652223 |
|    average_cost         | 0.002734375   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.15         |
|    cost_value_loss      | 0.0297        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.118        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.04e+07      |
|    mean_cost_advantages | 0.006637788   |
|    mean_reward_advan... | 4309.2197     |
|    n_updates            | 1350          |
|    nu                   | 2.72          |
|    nu_loss              | -0.00742      |
|    policy_gradient_loss | -0.000117     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 2e+07         |
|    total_cost           | 28.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 804          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1324         |
|    iterations           | 137          |
|    time_elapsed         | 1059         |
|    total_timesteps      | 1402880      |
| train/                  |              |
|    approx_kl            | 0.0007383068 |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 9.77e-06     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.37        |
|    cost_value_loss      | 0.0179       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.118       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.18e+06     |
|    mean_cost_advantages | -0.009405779 |
|    mean_reward_advan... | 4347.901     |
|    n_updates            | 1360         |
|    nu                   | 2.72         |
|    nu_loss              | -0.00345     |
|    policy_gradient_loss | -0.000145    |
|    reward_explained_... | -1.47e+12    |
|    reward_value_loss    | 2.02e+07     |
|    total_cost           | 13.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 806           |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1324          |
|    iterations           | 138           |
|    time_elapsed         | 1067          |
|    total_timesteps      | 1413120       |
| train/                  |               |
|    approx_kl            | 0.00010144651 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.66         |
|    cost_value_loss      | 0.00314       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.111        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.51e+06      |
|    mean_cost_advantages | -0.0046825027 |
|    mean_reward_advan... | 4303.386      |
|    n_updates            | 1370          |
|    nu                   | 2.73          |
|    nu_loss              | -0.00133      |
|    policy_gradient_loss | -2.81e-05     |
|    reward_explained_... | -1.24e+17     |
|    reward_value_loss    | 1.98e+07      |
|    total_cost           | 5.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 798          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1323         |
|    iterations           | 139          |
|    time_elapsed         | 1075         |
|    total_timesteps      | 1423360      |
| train/                  |              |
|    approx_kl            | 7.026452e-05 |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.54        |
|    cost_value_loss      | 0.0114       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.108       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.85e+06     |
|    mean_cost_advantages | 0.003686627  |
|    mean_reward_advan... | 4280.293     |
|    n_updates            | 1380         |
|    nu                   | 2.73         |
|    nu_loss              | -0.00399     |
|    policy_gradient_loss | -6.61e-05    |
|    reward_explained_... | -6.06e+12    |
|    reward_value_loss    | 1.96e+07     |
|    total_cost           | 15.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00166      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 808          |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1323         |
|    iterations           | 140          |
|    time_elapsed         | 1083         |
|    total_timesteps      | 1433600      |
| train/                  |              |
|    approx_kl            | 9.018557e-05 |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.9         |
|    cost_value_loss      | 0.0324       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.11        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.69e+06     |
|    mean_cost_advantages | 0.00765835   |
|    mean_reward_advan... | 4236.6045    |
|    n_updates            | 1390         |
|    nu                   | 2.73         |
|    nu_loss              | -0.00666     |
|    policy_gradient_loss | -0.000102    |
|    reward_explained_... | -6.2e+12     |
|    reward_value_loss    | 1.93e+07     |
|    total_cost           | 25.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 808           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1323          |
|    iterations           | 141           |
|    time_elapsed         | 1091          |
|    total_timesteps      | 1443840       |
| train/                  |               |
|    approx_kl            | 0.00019239647 |
|    average_cost         | 0.0016601563  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.51         |
|    cost_value_loss      | 0.0236        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.108        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.08e+06      |
|    mean_cost_advantages | -0.0020161807 |
|    mean_reward_advan... | 4240.9946     |
|    n_updates            | 1400          |
|    nu                   | 2.74          |
|    nu_loss              | -0.00454      |
|    policy_gradient_loss | -8.65e-05     |
|    reward_explained_... | -2.35e+13     |
|    reward_value_loss    | 1.93e+07      |
|    total_cost           | 17.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 811           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1322          |
|    iterations           | 142           |
|    time_elapsed         | 1099          |
|    total_timesteps      | 1454080       |
| train/                  |               |
|    approx_kl            | 6.9248854e-05 |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.03         |
|    cost_value_loss      | 0.0201        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.106        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.14e+06      |
|    mean_cost_advantages | -0.0008141965 |
|    mean_reward_advan... | 4218.259      |
|    n_updates            | 1410          |
|    nu                   | 2.74          |
|    nu_loss              | -0.00428      |
|    policy_gradient_loss | -4.66e-05     |
|    reward_explained_... | -2.58e+12     |
|    reward_value_loss    | 1.9e+07       |
|    total_cost           | 16.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00117       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 815           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1322          |
|    iterations           | 143           |
|    time_elapsed         | 1107          |
|    total_timesteps      | 1464320       |
| train/                  |               |
|    approx_kl            | 4.426624e-05  |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.61         |
|    cost_value_loss      | 0.0159        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.103        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.61e+06      |
|    mean_cost_advantages | -0.0015062271 |
|    mean_reward_advan... | 4197.8647     |
|    n_updates            | 1420          |
|    nu                   | 2.74          |
|    nu_loss              | -0.00428      |
|    policy_gradient_loss | -3.38e-05     |
|    reward_explained_... | -2.32e+13     |
|    reward_value_loss    | 1.89e+07      |
|    total_cost           | 16.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 808           |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1322          |
|    iterations           | 144           |
|    time_elapsed         | 1115          |
|    total_timesteps      | 1474560       |
| train/                  |               |
|    approx_kl            | 0.00039583142 |
|    average_cost         | 0.001171875   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.14         |
|    cost_value_loss      | 0.0082        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.101        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.04e+07      |
|    mean_cost_advantages | -0.0051315376 |
|    mean_reward_advan... | 4191.3867     |
|    n_updates            | 1430          |
|    nu                   | 2.75          |
|    nu_loss              | -0.00322      |
|    policy_gradient_loss | -5.33e-05     |
|    reward_explained_... | -8.85e+11     |
|    reward_value_loss    | 1.88e+07      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13            |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 816           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1321          |
|    iterations           | 145           |
|    time_elapsed         | 1123          |
|    total_timesteps      | 1484800       |
| train/                  |               |
|    approx_kl            | 3.4803605e-05 |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.03         |
|    cost_value_loss      | 0.0135        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.102        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.41e+06      |
|    mean_cost_advantages | 0.001080665   |
|    mean_reward_advan... | 4141.087      |
|    n_updates            | 1440          |
|    nu                   | 2.75          |
|    nu_loss              | -0.00429      |
|    policy_gradient_loss | -7.63e-05     |
|    reward_explained_... | -2.31e+13     |
|    reward_value_loss    | 1.84e+07      |
|    total_cost           | 16.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 812           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1321          |
|    iterations           | 146           |
|    time_elapsed         | 1131          |
|    total_timesteps      | 1495040       |
| train/                  |               |
|    approx_kl            | 0.0008057571  |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.00084       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.467        |
|    cost_value_loss      | 0.00118       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0982       |
|    learning_rate        | 0.0003        |
|    loss                 | 9.55e+06      |
|    mean_cost_advantages | -0.0057211947 |
|    mean_reward_advan... | 4131.3765     |
|    n_updates            | 1450          |
|    nu                   | 2.75          |
|    nu_loss              | -0.000269     |
|    policy_gradient_loss | -0.000131     |
|    reward_explained_... | -2.56e+12     |
|    reward_value_loss    | 1.83e+07      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 824           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1321          |
|    iterations           | 147           |
|    time_elapsed         | 1139          |
|    total_timesteps      | 1505280       |
| train/                  |               |
|    approx_kl            | 0.00010751172 |
|    average_cost         | 0.0024414062  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.54         |
|    cost_value_loss      | 0.0242        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.102        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.17e+06      |
|    mean_cost_advantages | 0.012147605   |
|    mean_reward_advan... | 4091.3313     |
|    n_updates            | 1460          |
|    nu                   | 2.76          |
|    nu_loss              | -0.00672      |
|    policy_gradient_loss | -6.72e-05     |
|    reward_explained_... | -2.39e+13     |
|    reward_value_loss    | 1.8e+07       |
|    total_cost           | 25.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 819           |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1320          |
|    iterations           | 148           |
|    time_elapsed         | 1147          |
|    total_timesteps      | 1515520       |
| train/                  |               |
|    approx_kl            | 2.3388315e-05 |
|    average_cost         | 0.0013671875  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.53         |
|    cost_value_loss      | 0.0168        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.1          |
|    learning_rate        | 0.0003        |
|    loss                 | 8.6e+06       |
|    mean_cost_advantages | -0.003738644  |
|    mean_reward_advan... | 4109.5693     |
|    n_updates            | 1470          |
|    nu                   | 2.76          |
|    nu_loss              | -0.00377      |
|    policy_gradient_loss | -2.95e-05     |
|    reward_explained_... | -5.34e+12     |
|    reward_value_loss    | 1.8e+07       |
|    total_cost           | 14.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 818           |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1320          |
|    iterations           | 149           |
|    time_elapsed         | 1155          |
|    total_timesteps      | 1525760       |
| train/                  |               |
|    approx_kl            | 5.9866697e-06 |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.65         |
|    cost_value_loss      | 0.0221        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0981       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.84e+06      |
|    mean_cost_advantages | 0.0013494187  |
|    mean_reward_advan... | 4079.037      |
|    n_updates            | 1480          |
|    nu                   | 2.76          |
|    nu_loss              | -0.00539      |
|    policy_gradient_loss | -3.5e-05      |
|    reward_explained_... | -5.29e+12     |
|    reward_value_loss    | 1.78e+07      |
|    total_cost           | 20.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00186       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 811           |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1320          |
|    iterations           | 150           |
|    time_elapsed         | 1163          |
|    total_timesteps      | 1536000       |
| train/                  |               |
|    approx_kl            | 0.00022773426 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.05         |
|    cost_value_loss      | 0.00561       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0973       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.15e+06      |
|    mean_cost_advantages | -0.009015357  |
|    mean_reward_advan... | 4038.9204     |
|    n_updates            | 1490          |
|    nu                   | 2.77          |
|    nu_loss              | -0.00162      |
|    policy_gradient_loss | -6.22e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.75e+07      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00234       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 809           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1319          |
|    iterations           | 151           |
|    time_elapsed         | 1171          |
|    total_timesteps      | 1546240       |
| train/                  |               |
|    approx_kl            | 1.0883261e-05 |
|    average_cost         | 0.0018554687  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.44         |
|    cost_value_loss      | 0.0228        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.101        |
|    learning_rate        | 0.0003        |
|    loss                 | 8.83e+06      |
|    mean_cost_advantages | 0.0027526377  |
|    mean_reward_advan... | 3999.0984     |
|    n_updates            | 1500          |
|    nu                   | 2.77          |
|    nu_loss              | -0.00513      |
|    policy_gradient_loss | -4.88e-05     |
|    reward_explained_... | -1.4e+12      |
|    reward_value_loss    | 1.72e+07      |
|    total_cost           | 19.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 814           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1319          |
|    iterations           | 152           |
|    time_elapsed         | 1179          |
|    total_timesteps      | 1556480       |
| train/                  |               |
|    approx_kl            | 0.00014774658 |
|    average_cost         | 0.00234375    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.08         |
|    cost_value_loss      | 0.0273        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.1          |
|    learning_rate        | 0.0003        |
|    loss                 | 7.96e+06      |
|    mean_cost_advantages | 0.004051864   |
|    mean_reward_advan... | 3972.0437     |
|    n_updates            | 1510          |
|    nu                   | 2.77          |
|    nu_loss              | -0.00649      |
|    policy_gradient_loss | -6.08e-05     |
|    reward_explained_... | -5.67e+16     |
|    reward_value_loss    | 1.7e+07       |
|    total_cost           | 24.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00137      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 802          |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1319         |
|    iterations           | 153          |
|    time_elapsed         | 1187         |
|    total_timesteps      | 1566720      |
| train/                  |              |
|    approx_kl            | 0.0010439157 |
|    average_cost         | 0.0013671875 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.77        |
|    cost_value_loss      | 0.0202       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0975      |
|    learning_rate        | 0.0003       |
|    loss                 | 8.64e+06     |
|    mean_cost_advantages | -0.003218637 |
|    mean_reward_advan... | 3959.1765    |
|    n_updates            | 1520         |
|    nu                   | 2.78         |
|    nu_loss              | -0.00379     |
|    policy_gradient_loss | -0.000126    |
|    reward_explained_... | -5.46e+12    |
|    reward_value_loss    | 1.69e+07     |
|    total_cost           | 14.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00205       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 813           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1318          |
|    iterations           | 154           |
|    time_elapsed         | 1195          |
|    total_timesteps      | 1576960       |
| train/                  |               |
|    approx_kl            | 0.0005367906  |
|    average_cost         | 0.0013671875  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.76         |
|    cost_value_loss      | 0.0202        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0958       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.93e+06      |
|    mean_cost_advantages | -0.0052569686 |
|    mean_reward_advan... | 3916.107      |
|    n_updates            | 1530          |
|    nu                   | 2.78          |
|    nu_loss              | -0.0038       |
|    policy_gradient_loss | -0.000108     |
|    reward_explained_... | -5.36e+12     |
|    reward_value_loss    | 1.65e+07      |
|    total_cost           | 14.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00117       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 825           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1318          |
|    iterations           | 155           |
|    time_elapsed         | 1203          |
|    total_timesteps      | 1587200       |
| train/                  |               |
|    approx_kl            | 7.5692005e-05 |
|    average_cost         | 0.0020507812  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.79         |
|    cost_value_loss      | 0.0206        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0964       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.33e+06      |
|    mean_cost_advantages | 0.0029083916  |
|    mean_reward_advan... | 3910.664      |
|    n_updates            | 1540          |
|    nu                   | 2.78          |
|    nu_loss              | -0.0057       |
|    policy_gradient_loss | -4.89e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.65e+07      |
|    total_cost           | 21.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 816           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1318          |
|    iterations           | 156           |
|    time_elapsed         | 1211          |
|    total_timesteps      | 1597440       |
| train/                  |               |
|    approx_kl            | 0.00015673549 |
|    average_cost         | 0.001171875   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.803        |
|    cost_value_loss      | 0.00902       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0926       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.93e+06      |
|    mean_cost_advantages | -0.0063153976 |
|    mean_reward_advan... | 3917.0996     |
|    n_updates            | 1550          |
|    nu                   | 2.79          |
|    nu_loss              | -0.00326      |
|    policy_gradient_loss | -6.19e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.64e+07      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 811           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1317          |
|    iterations           | 157           |
|    time_elapsed         | 1220          |
|    total_timesteps      | 1607680       |
| train/                  |               |
|    approx_kl            | 0.00017363789 |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.75         |
|    cost_value_loss      | 0.0266        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0933       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.23e+06      |
|    mean_cost_advantages | 0.0048727477  |
|    mean_reward_advan... | 3870.3406     |
|    n_updates            | 1560          |
|    nu                   | 2.79          |
|    nu_loss              | -0.00544      |
|    policy_gradient_loss | -9.06e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.61e+07      |
|    total_cost           | 20.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000879      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 829           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1317          |
|    iterations           | 158           |
|    time_elapsed         | 1228          |
|    total_timesteps      | 1617920       |
| train/                  |               |
|    approx_kl            | 6.4643194e-05 |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.18         |
|    cost_value_loss      | 0.0201        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0948       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.1e+06       |
|    mean_cost_advantages | -0.0040465067 |
|    mean_reward_advan... | 3843.022      |
|    n_updates            | 1570          |
|    nu                   | 2.79          |
|    nu_loss              | -0.003        |
|    policy_gradient_loss | -4.56e-05     |
|    reward_explained_... | -5.14e+12     |
|    reward_value_loss    | 1.59e+07      |
|    total_cost           | 11.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 15.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00166        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 818            |
|    ep_len_mean          | 12.2           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1317           |
|    iterations           | 159            |
|    time_elapsed         | 1236           |
|    total_timesteps      | 1628160        |
| train/                  |                |
|    approx_kl            | -3.8922368e-05 |
|    average_cost         | 0.0008789062   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.66          |
|    cost_value_loss      | 0.0127         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0916        |
|    learning_rate        | 0.0003         |
|    loss                 | 7.83e+06       |
|    mean_cost_advantages | -0.002748975   |
|    mean_reward_advan... | 3866.0         |
|    n_updates            | 1580           |
|    nu                   | 2.8            |
|    nu_loss              | -0.00246       |
|    policy_gradient_loss | -3.23e-05      |
|    reward_explained_... | -4.69e+12      |
|    reward_value_loss    | 1.59e+07       |
|    total_cost           | 9.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00117        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 819            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1316           |
|    iterations           | 160            |
|    time_elapsed         | 1244           |
|    total_timesteps      | 1638400        |
| train/                  |                |
|    approx_kl            | 9.7778604e-05  |
|    average_cost         | 0.0016601563   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.83          |
|    cost_value_loss      | 0.0183         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0912        |
|    learning_rate        | 0.0003         |
|    loss                 | 8.45e+06       |
|    mean_cost_advantages | -0.00027967227 |
|    mean_reward_advan... | 3816.2527      |
|    n_updates            | 1590           |
|    nu                   | 2.8            |
|    nu_loss              | -0.00464       |
|    policy_gradient_loss | -4.68e-05      |
|    reward_explained_... | -1.21e+12      |
|    reward_value_loss    | 1.56e+07       |
|    total_cost           | 17.0           |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00146        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 819            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1316           |
|    iterations           | 161            |
|    time_elapsed         | 1252           |
|    total_timesteps      | 1648640        |
| train/                  |                |
|    approx_kl            | 5.8451365e-05  |
|    average_cost         | 0.001171875    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -3.69          |
|    cost_value_loss      | 0.0131         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0886        |
|    learning_rate        | 0.0003         |
|    loss                 | 7.42e+06       |
|    mean_cost_advantages | -5.5135413e-05 |
|    mean_reward_advan... | 3783.145       |
|    n_updates            | 1600           |
|    nu                   | 2.8            |
|    nu_loss              | -0.00328       |
|    policy_gradient_loss | -2.04e-05      |
|    reward_explained_... | -5.06e+12      |
|    reward_value_loss    | 1.54e+07       |
|    total_cost           | 12.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 821           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1316          |
|    iterations           | 162           |
|    time_elapsed         | 1260          |
|    total_timesteps      | 1658880       |
| train/                  |               |
|    approx_kl            | 0.00032585912 |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.64         |
|    cost_value_loss      | 0.0164        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0901       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.81e+06      |
|    mean_cost_advantages | -0.002772507  |
|    mean_reward_advan... | 3762.8855     |
|    n_updates            | 1610          |
|    nu                   | 2.81          |
|    nu_loss              | -0.00411      |
|    policy_gradient_loss | -7.27e-05     |
|    reward_explained_... | -4.94e+12     |
|    reward_value_loss    | 1.52e+07      |
|    total_cost           | 15.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 823           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1315          |
|    iterations           | 163           |
|    time_elapsed         | 1268          |
|    total_timesteps      | 1669120       |
| train/                  |               |
|    approx_kl            | 0.00048570172 |
|    average_cost         | 0.0013671875  |
|    clip_fraction        | 0.000127      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.27         |
|    cost_value_loss      | 0.02          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0877       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.7e+06       |
|    mean_cost_advantages | 0.0019985032  |
|    mean_reward_advan... | 3743.1575     |
|    n_updates            | 1620          |
|    nu                   | 2.81          |
|    nu_loss              | -0.00384      |
|    policy_gradient_loss | -8.54e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.51e+07      |
|    total_cost           | 14.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 809           |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1315          |
|    iterations           | 164           |
|    time_elapsed         | 1276          |
|    total_timesteps      | 1679360       |
| train/                  |               |
|    approx_kl            | 3.5252167e-05 |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.96         |
|    cost_value_loss      | 0.0132        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0858       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.51e+06      |
|    mean_cost_advantages | -0.0026697111 |
|    mean_reward_advan... | 3727.9082     |
|    n_updates            | 1630          |
|    nu                   | 2.81          |
|    nu_loss              | -0.00412      |
|    policy_gradient_loss | -2.93e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.49e+07      |
|    total_cost           | 15.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00283       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 819           |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1315          |
|    iterations           | 165           |
|    time_elapsed         | 1284          |
|    total_timesteps      | 1689600       |
| train/                  |               |
|    approx_kl            | 0.00024073031 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.43         |
|    cost_value_loss      | 0.00719       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0877       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.73e+06      |
|    mean_cost_advantages | -0.0028205297 |
|    mean_reward_advan... | 3664.3516     |
|    n_updates            | 1640          |
|    nu                   | 2.82          |
|    nu_loss              | -0.00192      |
|    policy_gradient_loss | -7.43e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.45e+07      |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 819          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1314         |
|    iterations           | 166          |
|    time_elapsed         | 1292         |
|    total_timesteps      | 1699840      |
| train/                  |              |
|    approx_kl            | 0.0008255653 |
|    average_cost         | 0.0028320313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.81        |
|    cost_value_loss      | 0.0395       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0883      |
|    learning_rate        | 0.0003       |
|    loss                 | 7.59e+06     |
|    mean_cost_advantages | 0.009059617  |
|    mean_reward_advan... | 3661.7402    |
|    n_updates            | 1650         |
|    nu                   | 2.82         |
|    nu_loss              | -0.00798     |
|    policy_gradient_loss | -0.000131    |
|    reward_explained_... | -4.91e+12    |
|    reward_value_loss    | 1.45e+07     |
|    total_cost           | 29.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 822           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1314          |
|    iterations           | 167           |
|    time_elapsed         | 1300          |
|    total_timesteps      | 1710080       |
| train/                  |               |
|    approx_kl            | 0.00012301066 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.04         |
|    cost_value_loss      | 0.00492       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0875       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.41e+06      |
|    mean_cost_advantages | -0.011307428  |
|    mean_reward_advan... | 3648.3828     |
|    n_updates            | 1660          |
|    nu                   | 2.82          |
|    nu_loss              | -0.00193      |
|    policy_gradient_loss | -5.99e-05     |
|    reward_explained_... | -4.65e+12     |
|    reward_value_loss    | 1.43e+07      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00186       |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1314          |
|    iterations           | 168           |
|    time_elapsed         | 1308          |
|    total_timesteps      | 1720320       |
| train/                  |               |
|    approx_kl            | 7.731024e-05  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.23         |
|    cost_value_loss      | 0.00487       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0874       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.79e+06      |
|    mean_cost_advantages | -0.0033994995 |
|    mean_reward_advan... | 3624.0645     |
|    n_updates            | 1670          |
|    nu                   | 2.83          |
|    nu_loss              | -0.00193      |
|    policy_gradient_loss | -3.13e-05     |
|    reward_explained_... | -1.17e+12     |
|    reward_value_loss    | 1.41e+07      |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000781     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 821          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1313         |
|    iterations           | 169          |
|    time_elapsed         | 1317         |
|    total_timesteps      | 1730560      |
| train/                  |              |
|    approx_kl            | 8.286184e-05 |
|    average_cost         | 0.0018554687 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.08        |
|    cost_value_loss      | 0.0185       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0833      |
|    learning_rate        | 0.0003       |
|    loss                 | 7.75e+06     |
|    mean_cost_advantages | 0.0070994063 |
|    mean_reward_advan... | 3625.687     |
|    n_updates            | 1680         |
|    nu                   | 2.83         |
|    nu_loss              | -0.00525     |
|    policy_gradient_loss | -6.69e-05    |
|    reward_explained_... | -4.4e+12     |
|    reward_value_loss    | 1.41e+07     |
|    total_cost           | 19.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1313          |
|    iterations           | 170           |
|    time_elapsed         | 1325          |
|    total_timesteps      | 1740800       |
| train/                  |               |
|    approx_kl            | 0.00034014476 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.03         |
|    cost_value_loss      | 0.00932       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0815       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.82e+06      |
|    mean_cost_advantages | -0.0042016087 |
|    mean_reward_advan... | 3580.496      |
|    n_updates            | 1690          |
|    nu                   | 2.83          |
|    nu_loss              | -0.00221      |
|    policy_gradient_loss | -6.83e-05     |
|    reward_explained_... | -4.56e+12     |
|    reward_value_loss    | 1.38e+07      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1313          |
|    iterations           | 171           |
|    time_elapsed         | 1333          |
|    total_timesteps      | 1751040       |
| train/                  |               |
|    approx_kl            | 2.0573789e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.282        |
|    cost_value_loss      | 0.00246       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0794       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.75e+06      |
|    mean_cost_advantages | -0.005350559  |
|    mean_reward_advan... | 3574.4707     |
|    n_updates            | 1700          |
|    nu                   | 2.84          |
|    nu_loss              | -0.00083      |
|    policy_gradient_loss | -5.84e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.37e+07      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 826           |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1312          |
|    iterations           | 172           |
|    time_elapsed         | 1341          |
|    total_timesteps      | 1761280       |
| train/                  |               |
|    approx_kl            | -7.689232e-08 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.73         |
|    cost_value_loss      | 0.00736       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0805       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.88e+06      |
|    mean_cost_advantages | 0.004519284   |
|    mean_reward_advan... | 3543.4585     |
|    n_updates            | 1710          |
|    nu                   | 2.84          |
|    nu_loss              | -0.00138      |
|    policy_gradient_loss | -5.67e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.35e+07      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1312          |
|    iterations           | 173           |
|    time_elapsed         | 1349          |
|    total_timesteps      | 1771520       |
| train/                  |               |
|    approx_kl            | 0.00016112148 |
|    average_cost         | 0.0013671875  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.79         |
|    cost_value_loss      | 0.0138        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0811       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.32e+06      |
|    mean_cost_advantages | 0.003622248   |
|    mean_reward_advan... | 3519.7195     |
|    n_updates            | 1720          |
|    nu                   | 2.84          |
|    nu_loss              | -0.00388      |
|    policy_gradient_loss | -4.38e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.33e+07      |
|    total_cost           | 14.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00176       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1311          |
|    iterations           | 174           |
|    time_elapsed         | 1358          |
|    total_timesteps      | 1781760       |
| train/                  |               |
|    approx_kl            | 2.0376325e-05 |
|    average_cost         | 0.0012695312  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.88         |
|    cost_value_loss      | 0.0146        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0793       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.44e+06      |
|    mean_cost_advantages | -0.001537291  |
|    mean_reward_advan... | 3519.9531     |
|    n_updates            | 1730          |
|    nu                   | 2.84          |
|    nu_loss              | -0.00361      |
|    policy_gradient_loss | -3.81e-05     |
|    reward_explained_... | -4.11e+12     |
|    reward_value_loss    | 1.33e+07      |
|    total_cost           | 13.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1311          |
|    iterations           | 175           |
|    time_elapsed         | 1366          |
|    total_timesteps      | 1792000       |
| train/                  |               |
|    approx_kl            | 2.265434e-05  |
|    average_cost         | 0.0017578125  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.99         |
|    cost_value_loss      | 0.0231        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0795       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.44e+06      |
|    mean_cost_advantages | 8.8040346e-05 |
|    mean_reward_advan... | 3489.0151     |
|    n_updates            | 1740          |
|    nu                   | 2.85          |
|    nu_loss              | -0.005        |
|    policy_gradient_loss | -4.8e-05      |
|    reward_explained_... | -4.2e+12      |
|    reward_value_loss    | 1.31e+07      |
|    total_cost           | 18.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00283       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 822           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1311          |
|    iterations           | 176           |
|    time_elapsed         | 1374          |
|    total_timesteps      | 1802240       |
| train/                  |               |
|    approx_kl            | 0.00012792906 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.884        |
|    cost_value_loss      | 0.0056        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0769       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.34e+06      |
|    mean_cost_advantages | -0.008774096  |
|    mean_reward_advan... | 3475.9019     |
|    n_updates            | 1750          |
|    nu                   | 2.85          |
|    nu_loss              | -0.000834     |
|    policy_gradient_loss | -4.03e-05     |
|    reward_explained_... | -1e+12        |
|    reward_value_loss    | 1.29e+07      |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00156      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 829          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1310         |
|    iterations           | 177          |
|    time_elapsed         | 1382         |
|    total_timesteps      | 1812480      |
| train/                  |              |
|    approx_kl            | 0.0002740146 |
|    average_cost         | 0.0028320313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.75        |
|    cost_value_loss      | 0.0512       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.078       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.69e+06     |
|    mean_cost_advantages | 0.015098187  |
|    mean_reward_advan... | 3417.4875    |
|    n_updates            | 1760         |
|    nu                   | 2.85         |
|    nu_loss              | -0.00807     |
|    policy_gradient_loss | -0.000121    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 1.26e+07     |
|    total_cost           | 29.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 827           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1310          |
|    iterations           | 178           |
|    time_elapsed         | 1390          |
|    total_timesteps      | 1822720       |
| train/                  |               |
|    approx_kl            | 0.00026086075 |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.53         |
|    cost_value_loss      | 0.0211        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0785       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.36e+06      |
|    mean_cost_advantages | -0.0031001442 |
|    mean_reward_advan... | 3410.2817     |
|    n_updates            | 1770          |
|    nu                   | 2.86          |
|    nu_loss              | -0.00446      |
|    policy_gradient_loss | -7.33e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.25e+07      |
|    total_cost           | 16.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 838          |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1310         |
|    iterations           | 179          |
|    time_elapsed         | 1399         |
|    total_timesteps      | 1832960      |
| train/                  |              |
|    approx_kl            | 0.0003858626 |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.1         |
|    cost_value_loss      | 0.0127       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0774      |
|    learning_rate        | 0.0003       |
|    loss                 | 6.78e+06     |
|    mean_cost_advantages | -0.004199624 |
|    mean_reward_advan... | 3376.6282    |
|    n_updates            | 1780         |
|    nu                   | 2.86         |
|    nu_loss              | -0.00363     |
|    policy_gradient_loss | -8.36e-05    |
|    reward_explained_... | -4.18e+12    |
|    reward_value_loss    | 1.23e+07     |
|    total_cost           | 13.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00166       |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 840           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1309          |
|    iterations           | 180           |
|    time_elapsed         | 1407          |
|    total_timesteps      | 1843200       |
| train/                  |               |
|    approx_kl            | 0.00030101938 |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 5.86e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.28         |
|    cost_value_loss      | 0.00933       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0742       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.51e+06      |
|    mean_cost_advantages | -0.002072995  |
|    mean_reward_advan... | 3377.5652     |
|    n_updates            | 1790          |
|    nu                   | 2.86          |
|    nu_loss              | -0.00279      |
|    policy_gradient_loss | -8.21e-05     |
|    reward_explained_... | -4.08e+16     |
|    reward_value_loss    | 1.23e+07      |
|    total_cost           | 10.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 840          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1309         |
|    iterations           | 181          |
|    time_elapsed         | 1415         |
|    total_timesteps      | 1853440      |
| train/                  |              |
|    approx_kl            | 7.856259e-05 |
|    average_cost         | 0.0016601563 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.9         |
|    cost_value_loss      | 0.0174       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.073       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.5e+06      |
|    mean_cost_advantages | 0.0033172246 |
|    mean_reward_advan... | 3359.9211    |
|    n_updates            | 1800         |
|    nu                   | 2.87         |
|    nu_loss              | -0.00475     |
|    policy_gradient_loss | -2.83e-05    |
|    reward_explained_... | -3.85e+12    |
|    reward_value_loss    | 1.21e+07     |
|    total_cost           | 17.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 819           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1309          |
|    iterations           | 182           |
|    time_elapsed         | 1423          |
|    total_timesteps      | 1863680       |
| train/                  |               |
|    approx_kl            | 5.42545e-05   |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.16         |
|    cost_value_loss      | 0.00862       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0706       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.74e+06      |
|    mean_cost_advantages | -0.005903641  |
|    mean_reward_advan... | 3338.791      |
|    n_updates            | 1810          |
|    nu                   | 2.87          |
|    nu_loss              | -0.00196      |
|    policy_gradient_loss | -2.66e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.2e+07       |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1308          |
|    iterations           | 183           |
|    time_elapsed         | 1431          |
|    total_timesteps      | 1873920       |
| train/                  |               |
|    approx_kl            | 2.7073129e-05 |
|    average_cost         | 0.0012695312  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.87         |
|    cost_value_loss      | 0.0107        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0732       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.64e+06      |
|    mean_cost_advantages | 0.0029970095  |
|    mean_reward_advan... | 3266.9114     |
|    n_updates            | 1820          |
|    nu                   | 2.87          |
|    nu_loss              | -0.00364      |
|    policy_gradient_loss | -2.75e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.15e+07      |
|    total_cost           | 13.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1308         |
|    iterations           | 184          |
|    time_elapsed         | 1439         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 1.304819e-05 |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.9         |
|    cost_value_loss      | 0.0227       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0719      |
|    learning_rate        | 0.0003       |
|    loss                 | 5.93e+06     |
|    mean_cost_advantages | 0.002479544  |
|    mean_reward_advan... | 3282.2085    |
|    n_updates            | 1830         |
|    nu                   | 2.87         |
|    nu_loss              | -0.00421     |
|    policy_gradient_loss | -2.37e-05    |
|    reward_explained_... | -9.4e+11     |
|    reward_value_loss    | 1.16e+07     |
|    total_cost           | 15.0         |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000684       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 839            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1308           |
|    iterations           | 185            |
|    time_elapsed         | 1447           |
|    total_timesteps      | 1894400        |
| train/                  |                |
|    approx_kl            | -1.4171586e-06 |
|    average_cost         | 0.000390625    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.38          |
|    cost_value_loss      | 0.00262        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.07          |
|    learning_rate        | 0.0003         |
|    loss                 | 5.87e+06       |
|    mean_cost_advantages | -0.003796968   |
|    mean_reward_advan... | 3261.9749      |
|    n_updates            | 1840           |
|    nu                   | 2.88           |
|    nu_loss              | -0.00112       |
|    policy_gradient_loss | -1.69e-05      |
|    reward_explained_... | -3.69e+12      |
|    reward_value_loss    | 1.14e+07       |
|    total_cost           | 4.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 840           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1307          |
|    iterations           | 186           |
|    time_elapsed         | 1456          |
|    total_timesteps      | 1904640       |
| train/                  |               |
|    approx_kl            | 1.9823678e-05 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.75         |
|    cost_value_loss      | 0.00938       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0675       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.83e+06      |
|    mean_cost_advantages | 0.0016073711  |
|    mean_reward_advan... | 3240.6628     |
|    n_updates            | 1850          |
|    nu                   | 2.88          |
|    nu_loss              | -0.00197      |
|    policy_gradient_loss | -4.79e-05     |
|    reward_explained_... | -3.69e+12     |
|    reward_value_loss    | 1.13e+07      |
|    total_cost           | 7.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00137        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12.3           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1307           |
|    iterations           | 187            |
|    time_elapsed         | 1464           |
|    total_timesteps      | 1914880        |
| train/                  |                |
|    approx_kl            | -1.5703437e-05 |
|    average_cost         | 0.00078125     |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -5.02          |
|    cost_value_loss      | 0.0116         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0678        |
|    learning_rate        | 0.0003         |
|    loss                 | 4.63e+06       |
|    mean_cost_advantages | 0.0040058666   |
|    mean_reward_advan... | 3217.8118      |
|    n_updates            | 1860           |
|    nu                   | 2.88           |
|    nu_loss              | -0.00225       |
|    policy_gradient_loss | -3.23e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 1.11e+07       |
|    total_cost           | 8.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 822           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1307          |
|    iterations           | 188           |
|    time_elapsed         | 1472          |
|    total_timesteps      | 1925120       |
| train/                  |               |
|    approx_kl            | 0.00018333185 |
|    average_cost         | 0.0013671875  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.45         |
|    cost_value_loss      | 0.0138        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0692       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.51e+06      |
|    mean_cost_advantages | 0.0022322477  |
|    mean_reward_advan... | 3182.5723     |
|    n_updates            | 1870          |
|    nu                   | 2.89          |
|    nu_loss              | -0.00394      |
|    policy_gradient_loss | -4.47e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.09e+07      |
|    total_cost           | 14.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00107        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1307           |
|    iterations           | 189            |
|    time_elapsed         | 1480           |
|    total_timesteps      | 1935360        |
| train/                  |                |
|    approx_kl            | -5.9827325e-05 |
|    average_cost         | 0.0014648438   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.75          |
|    cost_value_loss      | 0.0144         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.073         |
|    learning_rate        | 0.0003         |
|    loss                 | 5.04e+06       |
|    mean_cost_advantages | 0.0045297225   |
|    mean_reward_advan... | 3132.043       |
|    n_updates            | 1880           |
|    nu                   | 2.89           |
|    nu_loss              | -0.00423       |
|    policy_gradient_loss | -4.5e-05       |
|    reward_explained_... | -3.93e+16      |
|    reward_value_loss    | 1.06e+07       |
|    total_cost           | 15.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 821           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1306          |
|    iterations           | 190           |
|    time_elapsed         | 1488          |
|    total_timesteps      | 1945600       |
| train/                  |               |
|    approx_kl            | 0.00011275012 |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.72         |
|    cost_value_loss      | 0.0117        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0685       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.08e+06      |
|    mean_cost_advantages | -0.0022892742 |
|    mean_reward_advan... | 3136.5503     |
|    n_updates            | 1890          |
|    nu                   | 2.89          |
|    nu_loss              | -0.0031       |
|    policy_gradient_loss | -3.92e-05     |
|    reward_explained_... | -3.58e+12     |
|    reward_value_loss    | 1.06e+07      |
|    total_cost           | 11.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1306          |
|    iterations           | 191           |
|    time_elapsed         | 1496          |
|    total_timesteps      | 1955840       |
| train/                  |               |
|    approx_kl            | 0.00027518123 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.44         |
|    cost_value_loss      | 0.00376       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0693       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.8e+06       |
|    mean_cost_advantages | -0.0023146528 |
|    mean_reward_advan... | 3084.679      |
|    n_updates            | 1900          |
|    nu                   | 2.89          |
|    nu_loss              | -0.00141      |
|    policy_gradient_loss | -4.65e-05     |
|    reward_explained_... | -3.73e+12     |
|    reward_value_loss    | 1.03e+07      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00293       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1306          |
|    iterations           | 192           |
|    time_elapsed         | 1505          |
|    total_timesteps      | 1966080       |
| train/                  |               |
|    approx_kl            | 0.00019665933 |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.36         |
|    cost_value_loss      | 0.00892       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0671       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.36e+06      |
|    mean_cost_advantages | 0.0013989785  |
|    mean_reward_advan... | 3085.3027     |
|    n_updates            | 1910          |
|    nu                   | 2.9           |
|    nu_loss              | -0.00311      |
|    policy_gradient_loss | -6.23e-05     |
|    reward_explained_... | -8.96e+11     |
|    reward_value_loss    | 1.03e+07      |
|    total_cost           | 11.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 849          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1306         |
|    iterations           | 193          |
|    time_elapsed         | 1513         |
|    total_timesteps      | 1976320      |
| train/                  |              |
|    approx_kl            | 0.0004349369 |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.00207      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -8.89        |
|    cost_value_loss      | 0.13         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0649      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.71e+06     |
|    mean_cost_advantages | 0.0137155475 |
|    mean_reward_advan... | 3058.8691    |
|    n_updates            | 1920         |
|    nu                   | 2.9          |
|    nu_loss              | -0.00849     |
|    policy_gradient_loss | -0.000358    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 1.01e+07     |
|    total_cost           | 30.0         |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000488       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1305           |
|    iterations           | 194            |
|    time_elapsed         | 1521           |
|    total_timesteps      | 1986560        |
| train/                  |                |
|    approx_kl            | -3.6751597e-05 |
|    average_cost         | 0.0001953125   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.916         |
|    cost_value_loss      | 0.00123        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0645        |
|    learning_rate        | 0.0003         |
|    loss                 | 5.52e+06       |
|    mean_cost_advantages | -0.010153609   |
|    mean_reward_advan... | 3068.3257      |
|    n_updates            | 1930           |
|    nu                   | 2.9            |
|    nu_loss              | -0.000566      |
|    policy_gradient_loss | -1.28e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 1.01e+07       |
|    total_cost           | 2.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000781       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 841            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1305           |
|    iterations           | 195            |
|    time_elapsed         | 1529           |
|    total_timesteps      | 1996800        |
| train/                  |                |
|    approx_kl            | 9.3254494e-07  |
|    average_cost         | 0.00048828125  |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -4.76          |
|    cost_value_loss      | 0.00499        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0652        |
|    learning_rate        | 0.0003         |
|    loss                 | 5.09e+06       |
|    mean_cost_advantages | -0.00018673744 |
|    mean_reward_advan... | 3018.03        |
|    n_updates            | 1940           |
|    nu                   | 2.91           |
|    nu_loss              | -0.00142       |
|    policy_gradient_loss | -2.24e-05      |
|    reward_explained_... | -8.51e+11      |
|    reward_value_loss    | 9.83e+06       |
|    total_cost           | 5.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1305          |
|    iterations           | 196           |
|    time_elapsed         | 1537          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.00013942548 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.913        |
|    cost_value_loss      | 0.00813       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0626       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.82e+06      |
|    mean_cost_advantages | 0.0001604491  |
|    mean_reward_advan... | 3005.1018     |
|    n_updates            | 1950          |
|    nu                   | 2.91          |
|    nu_loss              | -0.00227      |
|    policy_gradient_loss | -4.92e-05     |
|    reward_explained_... | -8.42e+11     |
|    reward_value_loss    | 9.74e+06      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 824           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1304          |
|    iterations           | 197           |
|    time_elapsed         | 1545          |
|    total_timesteps      | 2017280       |
| train/                  |               |
|    approx_kl            | 4.6820867e-05 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.34         |
|    cost_value_loss      | 0.00928       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0617       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.87e+06      |
|    mean_cost_advantages | -0.0008240005 |
|    mean_reward_advan... | 2971.0188     |
|    n_updates            | 1960          |
|    nu                   | 2.91          |
|    nu_loss              | -0.00227      |
|    policy_gradient_loss | -4.65e-05     |
|    reward_explained_... | -8.34e+11     |
|    reward_value_loss    | 9.53e+06      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1304          |
|    iterations           | 198           |
|    time_elapsed         | 1554          |
|    total_timesteps      | 2027520       |
| train/                  |               |
|    approx_kl            | 0.00057950086 |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0.0004        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.86         |
|    cost_value_loss      | 0.0273        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0628       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.55e+06      |
|    mean_cost_advantages | 0.0067894245  |
|    mean_reward_advan... | 2923.952      |
|    n_updates            | 1970          |
|    nu                   | 2.91          |
|    nu_loss              | -0.00313      |
|    policy_gradient_loss | -0.000106     |
|    reward_explained_... | -3.44e+12     |
|    reward_value_loss    | 9.28e+06      |
|    total_cost           | 11.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1304          |
|    iterations           | 199           |
|    time_elapsed         | 1562          |
|    total_timesteps      | 2037760       |
| train/                  |               |
|    approx_kl            | 6.141182e-05  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.32         |
|    cost_value_loss      | 0.00335       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0624       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.57e+06      |
|    mean_cost_advantages | -0.0053664623 |
|    mean_reward_advan... | 2918.2915     |
|    n_updates            | 1980          |
|    nu                   | 2.92          |
|    nu_loss              | -0.00142      |
|    policy_gradient_loss | -1.72e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 9.22e+06      |
|    total_cost           | 5.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000586       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 844            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1304           |
|    iterations           | 200            |
|    time_elapsed         | 1570           |
|    total_timesteps      | 2048000        |
| train/                  |                |
|    approx_kl            | -2.9237406e-05 |
|    average_cost         | 0.00068359374  |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.804         |
|    cost_value_loss      | 0.00568        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0614        |
|    learning_rate        | 0.0003         |
|    loss                 | 4.63e+06       |
|    mean_cost_advantages | -0.00051552494 |
|    mean_reward_advan... | 2899.8413      |
|    n_updates            | 1990           |
|    nu                   | 2.92           |
|    nu_loss              | -0.00199       |
|    policy_gradient_loss | -1.44e-05      |
|    reward_explained_... | -8.1e+11       |
|    reward_value_loss    | 9.09e+06       |
|    total_cost           | 7.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 837          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1303         |
|    iterations           | 201          |
|    time_elapsed         | 1578         |
|    total_timesteps      | 2058240      |
| train/                  |              |
|    approx_kl            | 2.374146e-05 |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.4         |
|    cost_value_loss      | 0.00642      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0609      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.58e+06     |
|    mean_cost_advantages | 0.0011868093 |
|    mean_reward_advan... | 2894.443     |
|    n_updates            | 2000         |
|    nu                   | 2.92         |
|    nu_loss              | -0.00171     |
|    policy_gradient_loss | -1.38e-05    |
|    reward_explained_... | -7.78e+11    |
|    reward_value_loss    | 9.03e+06     |
|    total_cost           | 6.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 840          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1303         |
|    iterations           | 202          |
|    time_elapsed         | 1586         |
|    total_timesteps      | 2068480      |
| train/                  |              |
|    approx_kl            | 6.501229e-05 |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.96        |
|    cost_value_loss      | 0.00662      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0592      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.83e+06     |
|    mean_cost_advantages | 0.0012895705 |
|    mean_reward_advan... | 2854.7358    |
|    n_updates            | 2010         |
|    nu                   | 2.92         |
|    nu_loss              | -0.00171     |
|    policy_gradient_loss | -3.11e-05    |
|    reward_explained_... | -3.19e+12    |
|    reward_value_loss    | 8.82e+06     |
|    total_cost           | 6.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00176      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 839          |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1303         |
|    iterations           | 203          |
|    time_elapsed         | 1594         |
|    total_timesteps      | 2078720      |
| train/                  |              |
|    approx_kl            | 3.661255e-05 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.64        |
|    cost_value_loss      | 0.00432      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0578      |
|    learning_rate        | 0.0003       |
|    loss                 | 3.76e+06     |
|    mean_cost_advantages | -0.001009446 |
|    mean_reward_advan... | 2840.411     |
|    n_updates            | 2020         |
|    nu                   | 2.93         |
|    nu_loss              | -0.00114     |
|    policy_gradient_loss | -1.36e-05    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 8.71e+06     |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1303          |
|    iterations           | 204           |
|    time_elapsed         | 1603          |
|    total_timesteps      | 2088960       |
| train/                  |               |
|    approx_kl            | 0.00022830421 |
|    average_cost         | 0.0017578125  |
|    clip_fraction        | 0.00202       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.54         |
|    cost_value_loss      | 0.0239        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.059        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.05e+06      |
|    mean_cost_advantages | 0.012424255   |
|    mean_reward_advan... | 2811.439      |
|    n_updates            | 2030          |
|    nu                   | 2.93          |
|    nu_loss              | -0.00514      |
|    policy_gradient_loss | -0.000125     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 8.56e+06      |
|    total_cost           | 18.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 843           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1302          |
|    iterations           | 205           |
|    time_elapsed         | 1611          |
|    total_timesteps      | 2099200       |
| train/                  |               |
|    approx_kl            | 0.00019823977 |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0.000283      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.43         |
|    cost_value_loss      | 0.0127        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.064        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.96e+06      |
|    mean_cost_advantages | 0.00210424    |
|    mean_reward_advan... | 2765.6724     |
|    n_updates            | 2040          |
|    nu                   | 2.93          |
|    nu_loss              | -0.00457      |
|    policy_gradient_loss | -7.29e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 8.35e+06      |
|    total_cost           | 16.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1302          |
|    iterations           | 206           |
|    time_elapsed         | 1619          |
|    total_timesteps      | 2109440       |
| train/                  |               |
|    approx_kl            | 0.00019359746 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.000703      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.47         |
|    cost_value_loss      | 0.00465       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.059        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.65e+06      |
|    mean_cost_advantages | -0.008621721  |
|    mean_reward_advan... | 2772.9194     |
|    n_updates            | 2050          |
|    nu                   | 2.93          |
|    nu_loss              | -0.00172      |
|    policy_gradient_loss | -8.32e-05     |
|    reward_explained_... | -2.99e+12     |
|    reward_value_loss    | 8.32e+06      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 840           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1302          |
|    iterations           | 207           |
|    time_elapsed         | 1627          |
|    total_timesteps      | 2119680       |
| train/                  |               |
|    approx_kl            | 1.3158785e-05 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.72         |
|    cost_value_loss      | 0.0106        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0579       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.22e+06      |
|    mean_cost_advantages | -0.002798664  |
|    mean_reward_advan... | 2734.6716     |
|    n_updates            | 2060          |
|    nu                   | 2.93          |
|    nu_loss              | -0.002        |
|    policy_gradient_loss | -1.32e-05     |
|    reward_explained_... | -7.54e+11     |
|    reward_value_loss    | 8.11e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 845          |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1302         |
|    iterations           | 208          |
|    time_elapsed         | 1635         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 5.831574e-05 |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.25        |
|    cost_value_loss      | 0.013        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0579      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.22e+06     |
|    mean_cost_advantages | 0.0045463606 |
|    mean_reward_advan... | 2713.8372    |
|    n_updates            | 2070         |
|    nu                   | 2.94         |
|    nu_loss              | -0.00287     |
|    policy_gradient_loss | -2.8e-05     |
|    reward_explained_... | -7.53e+11    |
|    reward_value_loss    | 8e+06        |
|    total_cost           | 10.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 847           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1301          |
|    iterations           | 209           |
|    time_elapsed         | 1644          |
|    total_timesteps      | 2140160       |
| train/                  |               |
|    approx_kl            | 8.6856635e-05 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.689        |
|    cost_value_loss      | 0.00443       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0561       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.96e+06      |
|    mean_cost_advantages | -0.004930069  |
|    mean_reward_advan... | 2707.22       |
|    n_updates            | 2080          |
|    nu                   | 2.94          |
|    nu_loss              | -0.00172      |
|    policy_gradient_loss | -3.15e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 7.92e+06      |
|    total_cost           | 6.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 848          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1301         |
|    iterations           | 210          |
|    time_elapsed         | 1652         |
|    total_timesteps      | 2150400      |
| train/                  |              |
|    approx_kl            | 0.0003365307 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.000166     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.68        |
|    cost_value_loss      | 0.0046       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0549      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.47e+06     |
|    mean_cost_advantages | 0.0011091793 |
|    mean_reward_advan... | 2687.8281    |
|    n_updates            | 2090         |
|    nu                   | 2.94         |
|    nu_loss              | -0.00115     |
|    policy_gradient_loss | -5.45e-05    |
|    reward_explained_... | -2.78e+12    |
|    reward_value_loss    | 7.8e+06      |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1301          |
|    iterations           | 211           |
|    time_elapsed         | 1660          |
|    total_timesteps      | 2160640       |
| train/                  |               |
|    approx_kl            | 3.0996744e-06 |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.49         |
|    cost_value_loss      | 0.0119        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0525       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.75e+06      |
|    mean_cost_advantages | 0.0012864984  |
|    mean_reward_advan... | 2664.6284     |
|    n_updates            | 2100          |
|    nu                   | 2.94          |
|    nu_loss              | -0.00287      |
|    policy_gradient_loss | -2.57e-05     |
|    reward_explained_... | -6.94e+11     |
|    reward_value_loss    | 7.68e+06      |
|    total_cost           | 10.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 846          |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1301         |
|    iterations           | 212          |
|    time_elapsed         | 1668         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0008090919 |
|    average_cost         | 0.0013671875 |
|    clip_fraction        | 0.00192      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.24        |
|    cost_value_loss      | 0.0123       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0546      |
|    learning_rate        | 0.0003       |
|    loss                 | 3.58e+06     |
|    mean_cost_advantages | 0.0017232597 |
|    mean_reward_advan... | 2609.445     |
|    n_updates            | 2110         |
|    nu                   | 2.95         |
|    nu_loss              | -0.00403     |
|    policy_gradient_loss | -0.000152    |
|    reward_explained_... | -7.45e+11    |
|    reward_value_loss    | 7.44e+06     |
|    total_cost           | 14.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 840           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1300          |
|    iterations           | 213           |
|    time_elapsed         | 1676          |
|    total_timesteps      | 2181120       |
| train/                  |               |
|    approx_kl            | 5.6285025e-05 |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.84         |
|    cost_value_loss      | 0.0122        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0554       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.17e+06      |
|    mean_cost_advantages | -0.0049850233 |
|    mean_reward_advan... | 2604.626      |
|    n_updates            | 2120          |
|    nu                   | 2.95          |
|    nu_loss              | -0.00288      |
|    policy_gradient_loss | -3.29e-05     |
|    reward_explained_... | -7.09e+11     |
|    reward_value_loss    | 7.38e+06      |
|    total_cost           | 10.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 827           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1300          |
|    iterations           | 214           |
|    time_elapsed         | 1685          |
|    total_timesteps      | 2191360       |
| train/                  |               |
|    approx_kl            | 0.00038426748 |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.000703      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.07         |
|    cost_value_loss      | 0.0107        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0553       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.64e+06      |
|    mean_cost_advantages | 0.001804733   |
|    mean_reward_advan... | 2572.1685     |
|    n_updates            | 2130          |
|    nu                   | 2.95          |
|    nu_loss              | -0.00288      |
|    policy_gradient_loss | -5.55e-05     |
|    reward_explained_... | -2.84e+12     |
|    reward_value_loss    | 7.21e+06      |
|    total_cost           | 10.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 844           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1300          |
|    iterations           | 215           |
|    time_elapsed         | 1693          |
|    total_timesteps      | 2201600       |
| train/                  |               |
|    approx_kl            | 0.00023532545 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.44         |
|    cost_value_loss      | 0.00508       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0578       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.67e+06      |
|    mean_cost_advantages | -0.0032046451 |
|    mean_reward_advan... | 2524.392      |
|    n_updates            | 2140          |
|    nu                   | 2.95          |
|    nu_loss              | -0.00173      |
|    policy_gradient_loss | -4.81e-05     |
|    reward_explained_... | -2.91e+12     |
|    reward_value_loss    | 6.99e+06      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 846           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1299          |
|    iterations           | 216           |
|    time_elapsed         | 1701          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 0.00046630326 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.00149       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.41         |
|    cost_value_loss      | 0.00377       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0566       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.62e+06      |
|    mean_cost_advantages | -0.0009882597 |
|    mean_reward_advan... | 2531.0852     |
|    n_updates            | 2150          |
|    nu                   | 2.96          |
|    nu_loss              | -0.00115      |
|    policy_gradient_loss | -0.000103     |
|    reward_explained_... | -6.86e+11     |
|    reward_value_loss    | 6.98e+06      |
|    total_cost           | 4.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000781       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 851            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1299           |
|    iterations           | 217            |
|    time_elapsed         | 1709           |
|    total_timesteps      | 2222080        |
| train/                  |                |
|    approx_kl            | 0.000121284    |
|    average_cost         | 0.00048828125  |
|    clip_fraction        | 0.00292        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.814         |
|    cost_value_loss      | 0.00292        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0511        |
|    learning_rate        | 0.0003         |
|    loss                 | 3.55e+06       |
|    mean_cost_advantages | -0.00014836495 |
|    mean_reward_advan... | 2516.3389      |
|    n_updates            | 2160           |
|    nu                   | 2.96           |
|    nu_loss              | -0.00144       |
|    policy_gradient_loss | -0.000106      |
|    reward_explained_... | -6.55e+11      |
|    reward_value_loss    | 6.88e+06       |
|    total_cost           | 5.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 856          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1299         |
|    iterations           | 218          |
|    time_elapsed         | 1717         |
|    total_timesteps      | 2232320      |
| train/                  |              |
|    approx_kl            | 4.212345e-05 |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.96        |
|    cost_value_loss      | 0.0135       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0509      |
|    learning_rate        | 0.0003       |
|    loss                 | 3.36e+06     |
|    mean_cost_advantages | 0.0036225386 |
|    mean_reward_advan... | 2498.292     |
|    n_updates            | 2170         |
|    nu                   | 2.96         |
|    nu_loss              | -0.00231     |
|    policy_gradient_loss | -2.49e-05    |
|    reward_explained_... | -2.58e+12    |
|    reward_value_loss    | 6.78e+06     |
|    total_cost           | 8.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000293       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 845            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1299           |
|    iterations           | 219            |
|    time_elapsed         | 1725           |
|    total_timesteps      | 2242560        |
| train/                  |                |
|    approx_kl            | -2.7890259e-05 |
|    average_cost         | 0.00029296876  |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.961         |
|    cost_value_loss      | 0.00263        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0491        |
|    learning_rate        | 0.0003         |
|    loss                 | 3.5e+06        |
|    mean_cost_advantages | -0.002476911   |
|    mean_reward_advan... | 2482.4556      |
|    n_updates            | 2180           |
|    nu                   | 2.96           |
|    nu_loss              | -0.000867      |
|    policy_gradient_loss | -9.8e-06       |
|    reward_explained_... | nan            |
|    reward_value_loss    | 6.69e+06       |
|    total_cost           | 3.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 846           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1299          |
|    iterations           | 220           |
|    time_elapsed         | 1734          |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 0.0006416544  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000811      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.34         |
|    cost_value_loss      | 0.00273       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0503       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.27e+06      |
|    mean_cost_advantages | -4.039598e-05 |
|    mean_reward_advan... | 2441.61       |
|    n_updates            | 2190          |
|    nu                   | 2.96          |
|    nu_loss              | -0.000868     |
|    policy_gradient_loss | -6.05e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 6.5e+06       |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 849           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1298          |
|    iterations           | 221           |
|    time_elapsed         | 1742          |
|    total_timesteps      | 2263040       |
| train/                  |               |
|    approx_kl            | -3.38187e-05  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.87         |
|    cost_value_loss      | 0.00912       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0484       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.3e+06       |
|    mean_cost_advantages | 0.0022833825  |
|    mean_reward_advan... | 2420.4734     |
|    n_updates            | 2200          |
|    nu                   | 2.97          |
|    nu_loss              | -0.00203      |
|    policy_gradient_loss | -3.66e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 6.38e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 854           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1298          |
|    iterations           | 222           |
|    time_elapsed         | 1750          |
|    total_timesteps      | 2273280       |
| train/                  |               |
|    approx_kl            | 0.00018629362 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.51         |
|    cost_value_loss      | 0.000977      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0492       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.12e+06      |
|    mean_cost_advantages | -0.0041947365 |
|    mean_reward_advan... | 2400.2039     |
|    n_updates            | 2210          |
|    nu                   | 2.97          |
|    nu_loss              | -0.00029      |
|    policy_gradient_loss | -1.63e-05     |
|    reward_explained_... | -2.48e+12     |
|    reward_value_loss    | 6.28e+06      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000879      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 839           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1298          |
|    iterations           | 223           |
|    time_elapsed         | 1758          |
|    total_timesteps      | 2283520       |
| train/                  |               |
|    approx_kl            | 5.1816238e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.17         |
|    cost_value_loss      | 0.000766      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0473       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.92e+06      |
|    mean_cost_advantages | -0.0009448112 |
|    mean_reward_advan... | 2386.8257     |
|    n_updates            | 2220          |
|    nu                   | 2.97          |
|    nu_loss              | -0.00029      |
|    policy_gradient_loss | -2.78e-05     |
|    reward_explained_... | -5.95e+11     |
|    reward_value_loss    | 6.19e+06      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 837           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1298          |
|    iterations           | 224           |
|    time_elapsed         | 1766          |
|    total_timesteps      | 2293760       |
| train/                  |               |
|    approx_kl            | 1.4701882e-05 |
|    average_cost         | 0.0008789062  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.36         |
|    cost_value_loss      | 0.00873       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0497       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.02e+06      |
|    mean_cost_advantages | 0.0016871756  |
|    mean_reward_advan... | 2336.406      |
|    n_updates            | 2230          |
|    nu                   | 2.97          |
|    nu_loss              | -0.00261      |
|    policy_gradient_loss | -2.22e-05     |
|    reward_explained_... | -6.23e+11     |
|    reward_value_loss    | 5.98e+06      |
|    total_cost           | 9.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 844           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1297          |
|    iterations           | 225           |
|    time_elapsed         | 1775          |
|    total_timesteps      | 2304000       |
| train/                  |               |
|    approx_kl            | 0.00020767414 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.87         |
|    cost_value_loss      | 0.00638       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.049        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.46e+06      |
|    mean_cost_advantages | 0.0019833837  |
|    mean_reward_advan... | 2304.3723     |
|    n_updates            | 2240          |
|    nu                   | 2.97          |
|    nu_loss              | -0.00232      |
|    policy_gradient_loss | -3.8e-05      |
|    reward_explained_... | -2.54e+12     |
|    reward_value_loss    | 5.84e+06      |
|    total_cost           | 8.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 852          |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1297         |
|    iterations           | 226          |
|    time_elapsed         | 1783         |
|    total_timesteps      | 2314240      |
| train/                  |              |
|    approx_kl            | 3.790881e-05 |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.35        |
|    cost_value_loss      | 0.00972      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0475      |
|    learning_rate        | 0.0003       |
|    loss                 | 3.06e+06     |
|    mean_cost_advantages | -0.000803704 |
|    mean_reward_advan... | 2297.5671    |
|    n_updates            | 2250         |
|    nu                   | 2.97         |
|    nu_loss              | -0.00232     |
|    policy_gradient_loss | -3.22e-05    |
|    reward_explained_... | -2.37e+12    |
|    reward_value_loss    | 5.77e+06     |
|    total_cost           | 8.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00117       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 843           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1297          |
|    iterations           | 227           |
|    time_elapsed         | 1791          |
|    total_timesteps      | 2324480       |
| train/                  |               |
|    approx_kl            | 1.1062331e-05 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.09         |
|    cost_value_loss      | 0.00655       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0465       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.2e+06       |
|    mean_cost_advantages | -0.0020374164 |
|    mean_reward_advan... | 2280.6687     |
|    n_updates            | 2260          |
|    nu                   | 2.98          |
|    nu_loss              | -0.00116      |
|    policy_gradient_loss | -2.15e-05     |
|    reward_explained_... | -5.95e+11     |
|    reward_value_loss    | 5.7e+06       |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00176       |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | 858           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1297          |
|    iterations           | 228           |
|    time_elapsed         | 1799          |
|    total_timesteps      | 2334720       |
| train/                  |               |
|    approx_kl            | 0.00033411258 |
|    average_cost         | 0.001171875   |
|    clip_fraction        | 0.00211       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.57         |
|    cost_value_loss      | 0.0202        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0467       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.57e+06      |
|    mean_cost_advantages | 0.0033439163  |
|    mean_reward_advan... | 2244.616      |
|    n_updates            | 2270          |
|    nu                   | 2.98          |
|    nu_loss              | -0.00349      |
|    policy_gradient_loss | -0.000117     |
|    reward_explained_... | -6e+11        |
|    reward_value_loss    | 5.54e+06      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 846           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1297          |
|    iterations           | 229           |
|    time_elapsed         | 1807          |
|    total_timesteps      | 2344960       |
| train/                  |               |
|    approx_kl            | 0.00031492804 |
|    average_cost         | 0.0017578125  |
|    clip_fraction        | 0.000186      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.67         |
|    cost_value_loss      | 0.0254        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0477       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.57e+06      |
|    mean_cost_advantages | 0.0038255032  |
|    mean_reward_advan... | 2245.5364     |
|    n_updates            | 2280          |
|    nu                   | 2.98          |
|    nu_loss              | -0.00523      |
|    policy_gradient_loss | -7.64e-05     |
|    reward_explained_... | -5.63e+11     |
|    reward_value_loss    | 5.51e+06      |
|    total_cost           | 18.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 847           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1296          |
|    iterations           | 230           |
|    time_elapsed         | 1816          |
|    total_timesteps      | 2355200       |
| train/                  |               |
|    approx_kl            | 0.00014338309 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.95         |
|    cost_value_loss      | 0.00911       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0466       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.66e+06      |
|    mean_cost_advantages | -0.00319964   |
|    mean_reward_advan... | 2204.6023     |
|    n_updates            | 2290          |
|    nu                   | 2.98          |
|    nu_loss              | -0.00204      |
|    policy_gradient_loss | -3.51e-05     |
|    reward_explained_... | -2.54e+11     |
|    reward_value_loss    | 5.34e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 850           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1296          |
|    iterations           | 231           |
|    time_elapsed         | 1824          |
|    total_timesteps      | 2365440       |
| train/                  |               |
|    approx_kl            | 8.936401e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0357        |
|    cost_value_loss      | 0.000197      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0458       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.71e+06      |
|    mean_cost_advantages | -0.0029097865 |
|    mean_reward_advan... | 2183.0605     |
|    n_updates            | 2300          |
|    nu                   | 2.98          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.57e-05     |
|    reward_explained_... | -5.57e+11     |
|    reward_value_loss    | 5.23e+06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000781       |
| infos/                  |                |
|    cost                 | 0.03           |
| rollout/                |                |
|    adjusted_reward      | 840            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1296           |
|    iterations           | 232            |
|    time_elapsed         | 1832           |
|    total_timesteps      | 2375680        |
| train/                  |                |
|    approx_kl            | 0.0002915032   |
|    average_cost         | 0.0            |
|    clip_fraction        | 9.77e-06       |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0786         |
|    cost_value_loss      | 0.00026        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0464        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.44e+06       |
|    mean_cost_advantages | -0.00044236387 |
|    mean_reward_advan... | 2159.3965      |
|    n_updates            | 2310           |
|    nu                   | 2.99           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.87e-05      |
|    reward_explained_... | -5.59e+11      |
|    reward_value_loss    | 5.13e+06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 851           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1296          |
|    iterations           | 233           |
|    time_elapsed         | 1840          |
|    total_timesteps      | 2385920       |
| train/                  |               |
|    approx_kl            | 0.00015476234 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.58         |
|    cost_value_loss      | 0.00707       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0482       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.68e+06      |
|    mean_cost_advantages | 0.0050392607  |
|    mean_reward_advan... | 2118.73       |
|    n_updates            | 2320          |
|    nu                   | 2.99          |
|    nu_loss              | -0.00233      |
|    policy_gradient_loss | -3.19e-05     |
|    reward_explained_... | -2.29e+12     |
|    reward_value_loss    | 4.97e+06      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 854           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1295          |
|    iterations           | 234           |
|    time_elapsed         | 1848          |
|    total_timesteps      | 2396160       |
| train/                  |               |
|    approx_kl            | 0.00011567534 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.18         |
|    cost_value_loss      | 0.0127        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0457       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.39e+06      |
|    mean_cost_advantages | 0.0029294933  |
|    mean_reward_advan... | 2112.3901     |
|    n_updates            | 2330          |
|    nu                   | 2.99          |
|    nu_loss              | -0.00233      |
|    policy_gradient_loss | -5.26e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 4.92e+06      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 847           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1295          |
|    iterations           | 235           |
|    time_elapsed         | 1857          |
|    total_timesteps      | 2406400       |
| train/                  |               |
|    approx_kl            | 1.0061124e-05 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.444        |
|    cost_value_loss      | 0.00252       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0438       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.29e+06      |
|    mean_cost_advantages | -0.003914968  |
|    mean_reward_advan... | 2095.657      |
|    n_updates            | 2340          |
|    nu                   | 2.99          |
|    nu_loss              | -0.000584     |
|    policy_gradient_loss | -1.07e-05     |
|    reward_explained_... | -2.11e+12     |
|    reward_value_loss    | 4.83e+06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 850           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1295          |
|    iterations           | 236           |
|    time_elapsed         | 1865          |
|    total_timesteps      | 2416640       |
| train/                  |               |
|    approx_kl            | 0.00041587497 |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0.00159       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.6          |
|    cost_value_loss      | 0.0162        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0453       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.44e+06      |
|    mean_cost_advantages | 0.0068423324  |
|    mean_reward_advan... | 2061.2395     |
|    n_updates            | 2350          |
|    nu                   | 2.99          |
|    nu_loss              | -0.00321      |
|    policy_gradient_loss | -0.000105     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 4.69e+06      |
|    total_cost           | 11.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 847           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1295          |
|    iterations           | 237           |
|    time_elapsed         | 1873          |
|    total_timesteps      | 2426880       |
| train/                  |               |
|    approx_kl            | 0.00011256498 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0.0016        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.41         |
|    cost_value_loss      | 0.00888       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0444       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.53e+06      |
|    mean_cost_advantages | -0.003581231  |
|    mean_reward_advan... | 2043.9043     |
|    n_updates            | 2360          |
|    nu                   | 2.99          |
|    nu_loss              | -0.00234      |
|    policy_gradient_loss | -7.98e-05     |
|    reward_explained_... | -2.07e+12     |
|    reward_value_loss    | 4.61e+06      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 856           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1295          |
|    iterations           | 238           |
|    time_elapsed         | 1881          |
|    total_timesteps      | 2437120       |
| train/                  |               |
|    approx_kl            | 4.195811e-05  |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.92         |
|    cost_value_loss      | 0.00136       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0435       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.4e+06       |
|    mean_cost_advantages | -0.0031210962 |
|    mean_reward_advan... | 2012.5039     |
|    n_updates            | 2370          |
|    nu                   | 3             |
|    nu_loss              | -0.000585     |
|    policy_gradient_loss | -6.88e-06     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 4.49e+06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 851           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1294          |
|    iterations           | 239           |
|    time_elapsed         | 1889          |
|    total_timesteps      | 2447360       |
| train/                  |               |
|    approx_kl            | 2.9069652e-05 |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.01         |
|    cost_value_loss      | 0.0123        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0445       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.29e+06      |
|    mean_cost_advantages | 0.0073388917  |
|    mean_reward_advan... | 2000.1254     |
|    n_updates            | 2380          |
|    nu                   | 3             |
|    nu_loss              | -0.00322      |
|    policy_gradient_loss | -1.26e-05     |
|    reward_explained_... | -2.06e+12     |
|    reward_value_loss    | 4.43e+06      |
|    total_cost           | 11.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 15            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 849           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1294          |
|    iterations           | 240           |
|    time_elapsed         | 1898          |
|    total_timesteps      | 2457600       |
| train/                  |               |
|    approx_kl            | 3.1703967e-06 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.05         |
|    cost_value_loss      | 0.00474       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0441       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.16e+06      |
|    mean_cost_advantages | -0.007915625  |
|    mean_reward_advan... | 1972.0657     |
|    n_updates            | 2390          |
|    nu                   | 3             |
|    nu_loss              | -0.000878     |
|    policy_gradient_loss | -1.15e-05     |
|    reward_explained_... | -5.01e+11     |
|    reward_value_loss    | 4.31e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 854           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1294          |
|    iterations           | 241           |
|    time_elapsed         | 1906          |
|    total_timesteps      | 2467840       |
| train/                  |               |
|    approx_kl            | 5.8066937e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.7          |
|    cost_value_loss      | 0.00199       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0434       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.85e+06      |
|    mean_cost_advantages | -0.00272454   |
|    mean_reward_advan... | 1943.9945     |
|    n_updates            | 2400          |
|    nu                   | 3             |
|    nu_loss              | -0.000879     |
|    policy_gradient_loss | -1.76e-05     |
|    reward_explained_... | -5e+11        |
|    reward_value_loss    | 4.2e+06       |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000684       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 851            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1294           |
|    iterations           | 242            |
|    time_elapsed         | 1914           |
|    total_timesteps      | 2478080        |
| train/                  |                |
|    approx_kl            | -2.0325324e-05 |
|    average_cost         | 0.000390625    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.399         |
|    cost_value_loss      | 0.003          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0417        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.08e+06       |
|    mean_cost_advantages | 0.0005855948   |
|    mean_reward_advan... | 1929.8528      |
|    n_updates            | 2410           |
|    nu                   | 3              |
|    nu_loss              | -0.00117       |
|    policy_gradient_loss | -6.02e-06      |
|    reward_explained_... | -2.14e+11      |
|    reward_value_loss    | 4.12e+06       |
|    total_cost           | 4.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 846           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1294          |
|    iterations           | 243           |
|    time_elapsed         | 1922          |
|    total_timesteps      | 2488320       |
| train/                  |               |
|    approx_kl            | 2.165977e-05  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.65         |
|    cost_value_loss      | 0.00859       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0412       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.17e+06      |
|    mean_cost_advantages | 0.0009063184  |
|    mean_reward_advan... | 1901.3035     |
|    n_updates            | 2420          |
|    nu                   | 3.01          |
|    nu_loss              | -0.00205      |
|    policy_gradient_loss | -1.97e-05     |
|    reward_explained_... | -1.2e+11      |
|    reward_value_loss    | 4.01e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 853           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1293          |
|    iterations           | 244           |
|    time_elapsed         | 1931          |
|    total_timesteps      | 2498560       |
| train/                  |               |
|    approx_kl            | 0.00020714833 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.91         |
|    cost_value_loss      | 0.00973       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0427       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.05e+06      |
|    mean_cost_advantages | 0.00017934106 |
|    mean_reward_advan... | 1866.252      |
|    n_updates            | 2430          |
|    nu                   | 3.01          |
|    nu_loss              | -0.00176      |
|    policy_gradient_loss | -2.49e-05     |
|    reward_explained_... | -4.94e+11     |
|    reward_value_loss    | 3.9e+06       |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 854           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1293          |
|    iterations           | 245           |
|    time_elapsed         | 1939          |
|    total_timesteps      | 2508800       |
| train/                  |               |
|    approx_kl            | 0.00037137826 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.59         |
|    cost_value_loss      | 0.00283       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0416       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.96e+06      |
|    mean_cost_advantages | -0.0038446567 |
|    mean_reward_advan... | 1853.7595     |
|    n_updates            | 2440          |
|    nu                   | 3.01          |
|    nu_loss              | -0.00117      |
|    policy_gradient_loss | -6.72e-05     |
|    reward_explained_... | -1.17e+11     |
|    reward_value_loss    | 3.83e+06      |
|    total_cost           | 4.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 851          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1293         |
|    iterations           | 246          |
|    time_elapsed         | 1947         |
|    total_timesteps      | 2519040      |
| train/                  |              |
|    approx_kl            | 6.807968e-08 |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.43        |
|    cost_value_loss      | 0.0112       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0426      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.69e+06     |
|    mean_cost_advantages | 0.0014562153 |
|    mean_reward_advan... | 1831.68      |
|    n_updates            | 2450         |
|    nu                   | 3.01         |
|    nu_loss              | -0.00235     |
|    policy_gradient_loss | -2.03e-05    |
|    reward_explained_... | -2.06e+11    |
|    reward_value_loss    | 3.74e+06     |
|    total_cost           | 8.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00117        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 852            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1293           |
|    iterations           | 247            |
|    time_elapsed         | 1955           |
|    total_timesteps      | 2529280        |
| train/                  |                |
|    approx_kl            | -2.2798962e-05 |
|    average_cost         | 0.00068359374  |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -4.13          |
|    cost_value_loss      | 0.00752        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0419        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.69e+06       |
|    mean_cost_advantages | -0.001684276   |
|    mean_reward_advan... | 1803.3268      |
|    n_updates            | 2460           |
|    nu                   | 3.01           |
|    nu_loss              | -0.00206       |
|    policy_gradient_loss | -2.32e-05      |
|    reward_explained_... | -4.61e+11      |
|    reward_value_loss    | 3.64e+06       |
|    total_cost           | 7.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000195       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 854            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1293           |
|    iterations           | 248            |
|    time_elapsed         | 1963           |
|    total_timesteps      | 2539520        |
| train/                  |                |
|    approx_kl            | 0.000109847424 |
|    average_cost         | 0.001171875    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.49          |
|    cost_value_loss      | 0.0168         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0424        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.66e+06       |
|    mean_cost_advantages | 0.0014882747   |
|    mean_reward_advan... | 1781.5973      |
|    n_updates            | 2470           |
|    nu                   | 3.01           |
|    nu_loss              | -0.00353       |
|    policy_gradient_loss | -5.11e-05      |
|    reward_explained_... | -4.5e+11       |
|    reward_value_loss    | 3.55e+06       |
|    total_cost           | 12.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 846           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1292          |
|    iterations           | 249           |
|    time_elapsed         | 1972          |
|    total_timesteps      | 2549760       |
| train/                  |               |
|    approx_kl            | 0.00054165727 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.00338       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.85         |
|    cost_value_loss      | 0.00123       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0415       |
|    learning_rate        | 0.0003        |
|    loss                 | 2e+06         |
|    mean_cost_advantages | -0.002611166  |
|    mean_reward_advan... | 1759.916      |
|    n_updates            | 2480          |
|    nu                   | 3.02          |
|    nu_loss              | -0.000589     |
|    policy_gradient_loss | -0.00011      |
|    reward_explained_... | -1.78e+12     |
|    reward_value_loss    | 3.47e+06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 850           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1292          |
|    iterations           | 250           |
|    time_elapsed         | 1980          |
|    total_timesteps      | 2560000       |
| train/                  |               |
|    approx_kl            | 0.0004024231  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.00337       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.41         |
|    cost_value_loss      | 0.00703       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0417       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.58e+06      |
|    mean_cost_advantages | 0.0018747218  |
|    mean_reward_advan... | 1724.0527     |
|    n_updates            | 2490          |
|    nu                   | 3.02          |
|    nu_loss              | -0.00206      |
|    policy_gradient_loss | -0.000151     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 3.35e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 851          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1292         |
|    iterations           | 251          |
|    time_elapsed         | 1988         |
|    total_timesteps      | 2570240      |
| train/                  |              |
|    approx_kl            | 0.0001545375 |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.4         |
|    cost_value_loss      | 0.0012       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0404      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.38e+06     |
|    mean_cost_advantages | -0.003059579 |
|    mean_reward_advan... | 1708.1481    |
|    n_updates            | 2500         |
|    nu                   | 3.02         |
|    nu_loss              | -0.000589    |
|    policy_gradient_loss | -3.03e-05    |
|    reward_explained_... | -1.73e+12    |
|    reward_value_loss    | 3.28e+06     |
|    total_cost           | 2.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000879      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 851           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1292          |
|    iterations           | 252           |
|    time_elapsed         | 1996          |
|    total_timesteps      | 2580480       |
| train/                  |               |
|    approx_kl            | 0.00023654872 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000996      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.517        |
|    cost_value_loss      | 0.000762      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0386       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.6e+06       |
|    mean_cost_advantages | -0.0016029011 |
|    mean_reward_advan... | 1680.0934     |
|    n_updates            | 2510          |
|    nu                   | 3.02          |
|    nu_loss              | -0.000295     |
|    policy_gradient_loss | -5.08e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 3.19e+06      |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.00127      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 857          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1292         |
|    iterations           | 253          |
|    time_elapsed         | 2004         |
|    total_timesteps      | 2590720      |
| train/                  |              |
|    approx_kl            | 0.0001338212 |
|    average_cost         | 0.0008789062 |
|    clip_fraction        | 0.00203      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.89        |
|    cost_value_loss      | 0.0132       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0394      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.55e+06     |
|    mean_cost_advantages | 0.007668174  |
|    mean_reward_advan... | 1655.2966    |
|    n_updates            | 2520         |
|    nu                   | 3.02         |
|    nu_loss              | -0.00265     |
|    policy_gradient_loss | -8.18e-05    |
|    reward_explained_... | -1.75e+12    |
|    reward_value_loss    | 3.11e+06     |
|    total_cost           | 9.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000781       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 852            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1291           |
|    iterations           | 254            |
|    time_elapsed         | 2013           |
|    total_timesteps      | 2600960        |
| train/                  |                |
|    approx_kl            | 6.091199e-06   |
|    average_cost         | 0.0012695312   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.15          |
|    cost_value_loss      | 0.0194         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0392        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.45e+06       |
|    mean_cost_advantages | -4.6808273e-05 |
|    mean_reward_advan... | 1643.6116      |
|    n_updates            | 2530           |
|    nu                   | 3.02           |
|    nu_loss              | -0.00384       |
|    policy_gradient_loss | -3.69e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 3.05e+06       |
|    total_cost           | 13.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000879      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 854           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1291          |
|    iterations           | 255           |
|    time_elapsed         | 2021          |
|    total_timesteps      | 2611200       |
| train/                  |               |
|    approx_kl            | 1.7028278e-05 |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.51         |
|    cost_value_loss      | 0.00861       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0387       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.43e+06      |
|    mean_cost_advantages | -0.0015366696 |
|    mean_reward_advan... | 1612.3024     |
|    n_updates            | 2540          |
|    nu                   | 3.03          |
|    nu_loss              | -0.00236      |
|    policy_gradient_loss | -1.76e-05     |
|    reward_explained_... | -1.68e+12     |
|    reward_value_loss    | 2.95e+06      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 855           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1291          |
|    iterations           | 256           |
|    time_elapsed         | 2029          |
|    total_timesteps      | 2621440       |
| train/                  |               |
|    approx_kl            | 4.3114414e-06 |
|    average_cost         | 0.0008789062  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.72         |
|    cost_value_loss      | 0.0095        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0376       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.41e+06      |
|    mean_cost_advantages | 0.0033610384  |
|    mean_reward_advan... | 1591.022      |
|    n_updates            | 2550          |
|    nu                   | 3.03          |
|    nu_loss              | -0.00266      |
|    policy_gradient_loss | -1.51e-05     |
|    reward_explained_... | -4.09e+11     |
|    reward_value_loss    | 2.87e+06      |
|    total_cost           | 9.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00107        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 848            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1291           |
|    iterations           | 257            |
|    time_elapsed         | 2037           |
|    total_timesteps      | 2631680        |
| train/                  |                |
|    approx_kl            | 1.34968195e-05 |
|    average_cost         | 0.000390625    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.487         |
|    cost_value_loss      | 0.00319        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0359        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.25e+06       |
|    mean_cost_advantages | 0.00035006762  |
|    mean_reward_advan... | 1570.5408      |
|    n_updates            | 2560           |
|    nu                   | 3.03           |
|    nu_loss              | -0.00118       |
|    policy_gradient_loss | -1.39e-05      |
|    reward_explained_... | -3.96e+11      |
|    reward_value_loss    | 2.8e+06        |
|    total_cost           | 4.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 859            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1291           |
|    iterations           | 258            |
|    time_elapsed         | 2046           |
|    total_timesteps      | 2641920        |
| train/                  |                |
|    approx_kl            | -2.8241124e-05 |
|    average_cost         | 0.0010742188   |
|    clip_fraction        | 0.000146       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -3.54          |
|    cost_value_loss      | 0.0157         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0373        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.3e+06        |
|    mean_cost_advantages | 0.005754395    |
|    mean_reward_advan... | 1536.0482      |
|    n_updates            | 2570           |
|    nu                   | 3.03           |
|    nu_loss              | -0.00325       |
|    policy_gradient_loss | -3.98e-05      |
|    reward_explained_... | -1.8e+11       |
|    reward_value_loss    | 2.7e+06        |
|    total_cost           | 11.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 856           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1290          |
|    iterations           | 259           |
|    time_elapsed         | 2054          |
|    total_timesteps      | 2652160       |
| train/                  |               |
|    approx_kl            | 0.00016719659 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.00173       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.47         |
|    cost_value_loss      | 0.00681       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0354       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.31e+06      |
|    mean_cost_advantages | -0.004529197  |
|    mean_reward_advan... | 1528.1917     |
|    n_updates            | 2580          |
|    nu                   | 3.03          |
|    nu_loss              | -0.00118      |
|    policy_gradient_loss | -7.03e-05     |
|    reward_explained_... | -9.46e+10     |
|    reward_value_loss    | 2.65e+06      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 853           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1290          |
|    iterations           | 260           |
|    time_elapsed         | 2062          |
|    total_timesteps      | 2662400       |
| train/                  |               |
|    approx_kl            | 0.00013707664 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000322      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.56         |
|    cost_value_loss      | 0.00358       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0358       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.59e+06      |
|    mean_cost_advantages | -0.0009434946 |
|    mean_reward_advan... | 1500.8806     |
|    n_updates            | 2590          |
|    nu                   | 3.04          |
|    nu_loss              | -0.000889     |
|    policy_gradient_loss | -4.54e-05     |
|    reward_explained_... | -6e+10        |
|    reward_value_loss    | 2.56e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1290          |
|    iterations           | 261           |
|    time_elapsed         | 2070          |
|    total_timesteps      | 2672640       |
| train/                  |               |
|    approx_kl            | 0.00015035523 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.7          |
|    cost_value_loss      | 0.00589       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0352       |
|    learning_rate        | 0.0003        |
|    loss                 | 9.59e+05      |
|    mean_cost_advantages | -0.001109153  |
|    mean_reward_advan... | 1471.8259     |
|    n_updates            | 2600          |
|    nu                   | 3.04          |
|    nu_loss              | -0.000889     |
|    policy_gradient_loss | -2.77e-05     |
|    reward_explained_... | -9.47e+10     |
|    reward_value_loss    | 2.48e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 850           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1290          |
|    iterations           | 262           |
|    time_elapsed         | 2079          |
|    total_timesteps      | 2682880       |
| train/                  |               |
|    approx_kl            | 0.0001336355  |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.000234      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.16         |
|    cost_value_loss      | 0.00138       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0337       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.21e+06      |
|    mean_cost_advantages | 0.00027778672 |
|    mean_reward_advan... | 1464.8162     |
|    n_updates            | 2610          |
|    nu                   | 3.04          |
|    nu_loss              | -0.000593     |
|    policy_gradient_loss | -1.95e-05     |
|    reward_explained_... | -1.61e+11     |
|    reward_value_loss    | 2.45e+06      |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 850          |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1290         |
|    iterations           | 263          |
|    time_elapsed         | 2087         |
|    total_timesteps      | 2693120      |
| train/                  |              |
|    approx_kl            | 7.30057e-05  |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0.000117     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.22        |
|    cost_value_loss      | 0.00809      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0339      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.04e+06     |
|    mean_cost_advantages | 0.0019056026 |
|    mean_reward_advan... | 1420.3215    |
|    n_updates            | 2620         |
|    nu                   | 3.04         |
|    nu_loss              | -0.00178     |
|    policy_gradient_loss | -3.22e-05    |
|    reward_explained_... | -3.69e+11    |
|    reward_value_loss    | 2.33e+06     |
|    total_cost           | 6.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 854           |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1289          |
|    iterations           | 264           |
|    time_elapsed         | 2095          |
|    total_timesteps      | 2703360       |
| train/                  |               |
|    approx_kl            | 0.0002118231  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000361      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0257        |
|    cost_value_loss      | 7.63e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0335       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.12e+06      |
|    mean_cost_advantages | -0.0042510526 |
|    mean_reward_advan... | 1398.6385     |
|    n_updates            | 2630          |
|    nu                   | 3.04          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.85e-05     |
|    reward_explained_... | -3.6e+11      |
|    reward_value_loss    | 2.26e+06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1289          |
|    iterations           | 265           |
|    time_elapsed         | 2103          |
|    total_timesteps      | 2713600       |
| train/                  |               |
|    approx_kl            | 0.00013007547 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0012        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.57         |
|    cost_value_loss      | 0.00124       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0331       |
|    learning_rate        | 0.0003        |
|    loss                 | 9.72e+05      |
|    mean_cost_advantages | -6.668464e-05 |
|    mean_reward_advan... | 1375.5609     |
|    n_updates            | 2640          |
|    nu                   | 3.04          |
|    nu_loss              | -0.000594     |
|    policy_gradient_loss | -5.39e-05     |
|    reward_explained_... | -1.44e+12     |
|    reward_value_loss    | 2.19e+06      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1289          |
|    iterations           | 266           |
|    time_elapsed         | 2112          |
|    total_timesteps      | 2723840       |
| train/                  |               |
|    approx_kl            | 0.00026160403 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000791      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00921      |
|    cost_value_loss      | 7.18e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0322       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.09e+06      |
|    mean_cost_advantages | 1.4558062e-05 |
|    mean_reward_advan... | 1364.7877     |
|    n_updates            | 2650          |
|    nu                   | 3.04          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.65e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 2.14e+06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 857           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1289          |
|    iterations           | 267           |
|    time_elapsed         | 2120          |
|    total_timesteps      | 2734080       |
| train/                  |               |
|    approx_kl            | 2.327715e-05  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.752        |
|    cost_value_loss      | 0.00185       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0328       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.19e+06      |
|    mean_cost_advantages | 0.00033199418 |
|    mean_reward_advan... | 1340.9664     |
|    n_updates            | 2660          |
|    nu                   | 3.05          |
|    nu_loss              | -0.000892     |
|    policy_gradient_loss | -1.37e-05     |
|    reward_explained_... | -1.33e+12     |
|    reward_value_loss    | 2.07e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 844           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1289          |
|    iterations           | 268           |
|    time_elapsed         | 2128          |
|    total_timesteps      | 2744320       |
| train/                  |               |
|    approx_kl            | 0.0003886079  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.00138       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.9          |
|    cost_value_loss      | 0.00573       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0326       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.09e+06      |
|    mean_cost_advantages | 0.0012453446  |
|    mean_reward_advan... | 1308.7345     |
|    n_updates            | 2670          |
|    nu                   | 3.05          |
|    nu_loss              | -0.00149      |
|    policy_gradient_loss | -6.8e-05      |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.99e+06      |
|    total_cost           | 5.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 851          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1289         |
|    iterations           | 269          |
|    time_elapsed         | 2136         |
|    total_timesteps      | 2754560      |
| train/                  |              |
|    approx_kl            | 7.420604e-05 |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.25        |
|    cost_value_loss      | 0.000792     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0324      |
|    learning_rate        | 0.0003       |
|    loss                 | 9.69e+05     |
|    mean_cost_advantages | -0.003971578 |
|    mean_reward_advan... | 1268.654     |
|    n_updates            | 2680         |
|    nu                   | 3.05         |
|    nu_loss              | -0.000298    |
|    policy_gradient_loss | -3.27e-05    |
|    reward_explained_... | -1.39e+12    |
|    reward_value_loss    | 1.9e+06      |
|    total_cost           | 1.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 848           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1288          |
|    iterations           | 270           |
|    time_elapsed         | 2145          |
|    total_timesteps      | 2764800       |
| train/                  |               |
|    approx_kl            | 0.00011042396 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.98         |
|    cost_value_loss      | 0.00758       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0316       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.78e+05      |
|    mean_cost_advantages | 0.0006559686  |
|    mean_reward_advan... | 1254.9647     |
|    n_updates            | 2690          |
|    nu                   | 3.05          |
|    nu_loss              | -0.00179      |
|    policy_gradient_loss | -2.11e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.85e+06      |
|    total_cost           | 6.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000586       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 855            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1288           |
|    iterations           | 271            |
|    time_elapsed         | 2153           |
|    total_timesteps      | 2775040        |
| train/                  |                |
|    approx_kl            | -3.9793667e-05 |
|    average_cost         | 0.0001953125   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -5.39          |
|    cost_value_loss      | 0.00169        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.031         |
|    learning_rate        | 0.0003         |
|    loss                 | 8.24e+05       |
|    mean_cost_advantages | -0.002044152   |
|    mean_reward_advan... | 1227.009       |
|    n_updates            | 2700           |
|    nu                   | 3.05           |
|    nu_loss              | -0.000596      |
|    policy_gradient_loss | -1.07e-05      |
|    reward_explained_... | -1.32e+12      |
|    reward_value_loss    | 1.78e+06       |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 852           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1288          |
|    iterations           | 272           |
|    time_elapsed         | 2161          |
|    total_timesteps      | 2785280       |
| train/                  |               |
|    approx_kl            | -5.509658e-06 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.89         |
|    cost_value_loss      | 0.00672       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0313       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.65e+05      |
|    mean_cost_advantages | 0.000967398   |
|    mean_reward_advan... | 1211.6536     |
|    n_updates            | 2710          |
|    nu                   | 3.05          |
|    nu_loss              | -0.00179      |
|    policy_gradient_loss | -1.23e-05     |
|    reward_explained_... | -3.16e+11     |
|    reward_value_loss    | 1.73e+06      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 856           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1288          |
|    iterations           | 273           |
|    time_elapsed         | 2169          |
|    total_timesteps      | 2795520       |
| train/                  |               |
|    approx_kl            | 3.5221317e-05 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.9          |
|    cost_value_loss      | 0.00288       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0311       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.13e+05      |
|    mean_cost_advantages | 6.7568384e-05 |
|    mean_reward_advan... | 1185.3887     |
|    n_updates            | 2720          |
|    nu                   | 3.05          |
|    nu_loss              | -0.00119      |
|    policy_gradient_loss | -1.12e-05     |
|    reward_explained_... | -3.13e+11     |
|    reward_value_loss    | 1.67e+06      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 851           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1288          |
|    iterations           | 274           |
|    time_elapsed         | 2177          |
|    total_timesteps      | 2805760       |
| train/                  |               |
|    approx_kl            | 7.835929e-05  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.21         |
|    cost_value_loss      | 0.00736       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0296       |
|    learning_rate        | 0.0003        |
|    loss                 | 9.19e+05      |
|    mean_cost_advantages | 0.00019168788 |
|    mean_reward_advan... | 1164.8159     |
|    n_updates            | 2730          |
|    nu                   | 3.05          |
|    nu_loss              | -0.00149      |
|    policy_gradient_loss | -1.74e-05     |
|    reward_explained_... | -1.37e+11     |
|    reward_value_loss    | 1.61e+06      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1288          |
|    iterations           | 275           |
|    time_elapsed         | 2186          |
|    total_timesteps      | 2816000       |
| train/                  |               |
|    approx_kl            | 0.00015098986 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000723      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.21         |
|    cost_value_loss      | 0.000426      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0295       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.25e+05      |
|    mean_cost_advantages | 0.0005205014  |
|    mean_reward_advan... | 1134.7844     |
|    n_updates            | 2740          |
|    nu                   | 3.06          |
|    nu_loss              | -0.000298     |
|    policy_gradient_loss | -5.33e-05     |
|    reward_explained_... | -7.66e+10     |
|    reward_value_loss    | 1.55e+06      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1287          |
|    iterations           | 276           |
|    time_elapsed         | 2194          |
|    total_timesteps      | 2826240       |
| train/                  |               |
|    approx_kl            | 2.1539163e-06 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.06         |
|    cost_value_loss      | 0.00192       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0306       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.55e+05      |
|    mean_cost_advantages | 0.00094452326 |
|    mean_reward_advan... | 1118.4749     |
|    n_updates            | 2750          |
|    nu                   | 3.06          |
|    nu_loss              | -0.000895     |
|    policy_gradient_loss | -1.3e-05      |
|    reward_explained_... | -4.92e+10     |
|    reward_value_loss    | 1.51e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 859           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1287          |
|    iterations           | 277           |
|    time_elapsed         | 2202          |
|    total_timesteps      | 2836480       |
| train/                  |               |
|    approx_kl            | 0.00013921985 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 9.77e-06      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.47         |
|    cost_value_loss      | 0.00126       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0293       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.58e+05      |
|    mean_cost_advantages | -0.0013173721 |
|    mean_reward_advan... | 1097.1312     |
|    n_updates            | 2760          |
|    nu                   | 3.06          |
|    nu_loss              | -0.000597     |
|    policy_gradient_loss | -1.6e-05      |
|    reward_explained_... | -7.32e+10     |
|    reward_value_loss    | 1.45e+06      |
|    total_cost           | 2.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.00156        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 850            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1287           |
|    iterations           | 278            |
|    time_elapsed         | 2210           |
|    total_timesteps      | 2846720        |
| train/                  |                |
|    approx_kl            | 0.0003641492   |
|    average_cost         | 0.00029296876  |
|    clip_fraction        | 0.000879       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.812         |
|    cost_value_loss      | 0.00182        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0299        |
|    learning_rate        | 0.0003         |
|    loss                 | 7.29e+05       |
|    mean_cost_advantages | -0.00017809986 |
|    mean_reward_advan... | 1071.7683      |
|    n_updates            | 2770           |
|    nu                   | 3.06           |
|    nu_loss              | -0.000896      |
|    policy_gradient_loss | -6.39e-05      |
|    reward_explained_... | -1.27e+11      |
|    reward_value_loss    | 1.39e+06       |
|    total_cost           | 3.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11           |
|    mean_reward          | 1e+04        |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 864          |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1287         |
|    iterations           | 279          |
|    time_elapsed         | 2219         |
|    total_timesteps      | 2856960      |
| train/                  |              |
|    approx_kl            | 0.0004015242 |
|    average_cost         | 0.0015625    |
|    clip_fraction        | 0.00227      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.2         |
|    cost_value_loss      | 0.0191       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0318      |
|    learning_rate        | 0.0003       |
|    loss                 | 6.9e+05      |
|    mean_cost_advantages | 0.0074741677 |
|    mean_reward_advan... | 1036.4948    |
|    n_updates            | 2780         |
|    nu                   | 3.06         |
|    nu_loss              | -0.00478     |
|    policy_gradient_loss | -0.000236    |
|    reward_explained_... | -2.92e+11    |
|    reward_value_loss    | 1.32e+06     |
|    total_cost           | 16.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 859           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1287          |
|    iterations           | 280           |
|    time_elapsed         | 2227          |
|    total_timesteps      | 2867200       |
| train/                  |               |
|    approx_kl            | 8.368243e-05  |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.00289       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.67         |
|    cost_value_loss      | 0.000555      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0302       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.18e+05      |
|    mean_cost_advantages | -0.0023106574 |
|    mean_reward_advan... | 1028.2864     |
|    n_updates            | 2790          |
|    nu                   | 3.06          |
|    nu_loss              | -0.000299     |
|    policy_gradient_loss | -8.87e-05     |
|    reward_explained_... | -2.73e+11     |
|    reward_value_loss    | 1.29e+06      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 855           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1287          |
|    iterations           | 281           |
|    time_elapsed         | 2235          |
|    total_timesteps      | 2877440       |
| train/                  |               |
|    approx_kl            | 0.00025598268 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.344        |
|    cost_value_loss      | 0.00248       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0294       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.05e+05      |
|    mean_cost_advantages | 0.00090923754 |
|    mean_reward_advan... | 999.63965     |
|    n_updates            | 2800          |
|    nu                   | 3.06          |
|    nu_loss              | -0.000897     |
|    policy_gradient_loss | -8.59e-05     |
|    reward_explained_... | -1.09e+12     |
|    reward_value_loss    | 1.23e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000781       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 854            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1286           |
|    iterations           | 282            |
|    time_elapsed         | 2243           |
|    total_timesteps      | 2887680        |
| train/                  |                |
|    approx_kl            | -2.6064086e-05 |
|    average_cost         | 0.00029296876  |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.44          |
|    cost_value_loss      | 0.00281        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.028         |
|    learning_rate        | 0.0003         |
|    loss                 | 5.78e+05       |
|    mean_cost_advantages | 0.0016636078   |
|    mean_reward_advan... | 972.026        |
|    n_updates            | 2810           |
|    nu                   | 3.06           |
|    nu_loss              | -0.000897      |
|    policy_gradient_loss | -5.13e-06      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 1.17e+06       |
|    total_cost           | 3.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 856            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1286           |
|    iterations           | 283            |
|    time_elapsed         | 2252           |
|    total_timesteps      | 2897920        |
| train/                  |                |
|    approx_kl            | -3.1523243e-05 |
|    average_cost         | 0.00078125     |
|    clip_fraction        | 0.00164        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.24          |
|    cost_value_loss      | 0.00594        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0307        |
|    learning_rate        | 0.0003         |
|    loss                 | 4.86e+05       |
|    mean_cost_advantages | 0.002364036    |
|    mean_reward_advan... | 940.95996      |
|    n_updates            | 2820           |
|    nu                   | 3.07           |
|    nu_loss              | -0.00239       |
|    policy_gradient_loss | -4.88e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 1.13e+06       |
|    total_cost           | 8.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000195       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 861            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1286           |
|    iterations           | 284            |
|    time_elapsed         | 2260           |
|    total_timesteps      | 2908160        |
| train/                  |                |
|    approx_kl            | 0.00020138     |
|    average_cost         | 0.000390625    |
|    clip_fraction        | 0.00122        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -11.7          |
|    cost_value_loss      | 0.00425        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0283        |
|    learning_rate        | 0.0003         |
|    loss                 | 4.69e+05       |
|    mean_cost_advantages | 0.000117005824 |
|    mean_reward_advan... | 924.2697       |
|    n_updates            | 2830           |
|    nu                   | 3.07           |
|    nu_loss              | -0.0012        |
|    policy_gradient_loss | -6.21e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 1.08e+06       |
|    total_cost           | 4.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 861            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1286           |
|    iterations           | 285            |
|    time_elapsed         | 2268           |
|    total_timesteps      | 2918400        |
| train/                  |                |
|    approx_kl            | 0.00019985813  |
|    average_cost         | 0.0001953125   |
|    clip_fraction        | 0.000137       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.93          |
|    cost_value_loss      | 0.000974       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0283        |
|    learning_rate        | 0.0003         |
|    loss                 | 5.52e+05       |
|    mean_cost_advantages | -0.00094523496 |
|    mean_reward_advan... | 906.304        |
|    n_updates            | 2840           |
|    nu                   | 3.07           |
|    nu_loss              | -0.000599      |
|    policy_gradient_loss | -2.61e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 1.03e+06       |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 858           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1286          |
|    iterations           | 286           |
|    time_elapsed         | 2276          |
|    total_timesteps      | 2928640       |
| train/                  |               |
|    approx_kl            | 4.3183914e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.739        |
|    cost_value_loss      | 0.000766      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0283       |
|    learning_rate        | 0.0003        |
|    loss                 | 5.04e+05      |
|    mean_cost_advantages | -0.001338349  |
|    mean_reward_advan... | 879.87415     |
|    n_updates            | 2850          |
|    nu                   | 3.07          |
|    nu_loss              | -0.0003       |
|    policy_gradient_loss | -6.67e-06     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 9.89e+05      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 852           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1286          |
|    iterations           | 287           |
|    time_elapsed         | 2285          |
|    total_timesteps      | 2938880       |
| train/                  |               |
|    approx_kl            | 2.7662934e-05 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -10.1         |
|    cost_value_loss      | 0.00472       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0287       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.05e+05      |
|    mean_cost_advantages | 0.0042413496  |
|    mean_reward_advan... | 851.7131      |
|    n_updates            | 2860          |
|    nu                   | 3.07          |
|    nu_loss              | -0.0015       |
|    policy_gradient_loss | -3.21e-05     |
|    reward_explained_... | -2.53e+11     |
|    reward_value_loss    | 9.4e+05       |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 873           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1285          |
|    iterations           | 288           |
|    time_elapsed         | 2293          |
|    total_timesteps      | 2949120       |
| train/                  |               |
|    approx_kl            | 0.00078967883 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0023        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.07         |
|    cost_value_loss      | 0.00201       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0262       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.27e+05      |
|    mean_cost_advantages | -0.0013693144 |
|    mean_reward_advan... | 824.568       |
|    n_updates            | 2870          |
|    nu                   | 3.07          |
|    nu_loss              | -0.0006       |
|    policy_gradient_loss | -0.000106     |
|    reward_explained_... | -2.46e+11     |
|    reward_value_loss    | 8.89e+05      |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 858          |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1285         |
|    iterations           | 289          |
|    time_elapsed         | 2301         |
|    total_timesteps      | 2959360      |
| train/                  |              |
|    approx_kl            | 6.633904e-06 |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.52        |
|    cost_value_loss      | 0.00912      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0251      |
|    learning_rate        | 0.0003       |
|    loss                 | 3.89e+05     |
|    mean_cost_advantages | 0.0024949892 |
|    mean_reward_advan... | 817.2426     |
|    n_updates            | 2880         |
|    nu                   | 3.07         |
|    nu_loss              | -0.0018      |
|    policy_gradient_loss | -2.32e-05    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 8.64e+05     |
|    total_cost           | 6.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 858           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1285          |
|    iterations           | 290           |
|    time_elapsed         | 2309          |
|    total_timesteps      | 2969600       |
| train/                  |               |
|    approx_kl            | 0.00026115563 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.00187       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.63         |
|    cost_value_loss      | 0.00945       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0253       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.06e+05      |
|    mean_cost_advantages | -0.0001646149 |
|    mean_reward_advan... | 781.1217      |
|    n_updates            | 2890          |
|    nu                   | 3.07          |
|    nu_loss              | -0.0021       |
|    policy_gradient_loss | -0.00016      |
|    reward_explained_... | -2.34e+11     |
|    reward_value_loss    | 8.09e+05      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 859           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1285          |
|    iterations           | 291           |
|    time_elapsed         | 2317          |
|    total_timesteps      | 2979840       |
| train/                  |               |
|    approx_kl            | 1.4672475e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.65         |
|    cost_value_loss      | 0.00147       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0258       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.96e+05      |
|    mean_cost_advantages | -0.008605578  |
|    mean_reward_advan... | 755.9323      |
|    n_updates            | 2900          |
|    nu                   | 3.08          |
|    nu_loss              | -0.000901     |
|    policy_gradient_loss | -7.05e-06     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 7.68e+05      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1285          |
|    iterations           | 292           |
|    time_elapsed         | 2326          |
|    total_timesteps      | 2990080       |
| train/                  |               |
|    approx_kl            | 0.00017612865 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000566      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.843        |
|    cost_value_loss      | 0.00182       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.025        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.65e+05      |
|    mean_cost_advantages | -0.0005879003 |
|    mean_reward_advan... | 731.9967      |
|    n_updates            | 2910          |
|    nu                   | 3.08          |
|    nu_loss              | -0.000901     |
|    policy_gradient_loss | -5.41e-05     |
|    reward_explained_... | -2.26e+11     |
|    reward_value_loss    | 7.28e+05      |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 856            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1285           |
|    iterations           | 293            |
|    time_elapsed         | 2334           |
|    total_timesteps      | 3000320        |
| train/                  |                |
|    approx_kl            | -4.436127e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 3.91e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0226         |
|    cost_value_loss      | 0.000141       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0241        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.69e+05       |
|    mean_cost_advantages | -0.00017513867 |
|    mean_reward_advan... | 710.0025       |
|    n_updates            | 2920           |
|    nu                   | 3.08           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.16e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 6.92e+05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1285          |
|    iterations           | 294           |
|    time_elapsed         | 2342          |
|    total_timesteps      | 3010560       |
| train/                  |               |
|    approx_kl            | 8.1092214e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.029        |
|    cost_value_loss      | 5.75e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0235       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.08e+05      |
|    mean_cost_advantages | -6.240919e-05 |
|    mean_reward_advan... | 680.30457     |
|    n_updates            | 2930          |
|    nu                   | 3.08          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -7.83e-06     |
|    reward_explained_... | -2.2e+11      |
|    reward_value_loss    | 6.51e+05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 858           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 295           |
|    time_elapsed         | 2351          |
|    total_timesteps      | 3020800       |
| train/                  |               |
|    approx_kl            | 7.5573524e-05 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 3.91e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.33         |
|    cost_value_loss      | 0.00415       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0238       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.39e+05      |
|    mean_cost_advantages | 0.0016090041  |
|    mean_reward_advan... | 662.8675      |
|    n_updates            | 2940          |
|    nu                   | 3.08          |
|    nu_loss              | -0.0018       |
|    policy_gradient_loss | -6.03e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 6.2e+05       |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 853           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 296           |
|    time_elapsed         | 2359          |
|    total_timesteps      | 3031040       |
| train/                  |               |
|    approx_kl            | 0.00018151337 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 8.79e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.63         |
|    cost_value_loss      | 0.00617       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0253       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.96e+05      |
|    mean_cost_advantages | 0.0037763708  |
|    mean_reward_advan... | 632.4417      |
|    n_updates            | 2950          |
|    nu                   | 3.08          |
|    nu_loss              | -0.0018       |
|    policy_gradient_loss | -3.08e-05     |
|    reward_explained_... | -2.11e+11     |
|    reward_value_loss    | 5.82e+05      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 14.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 858           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 297           |
|    time_elapsed         | 2367          |
|    total_timesteps      | 3041280       |
| train/                  |               |
|    approx_kl            | 1.7328397e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.392        |
|    cost_value_loss      | 0.0024        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0249       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.53e+05      |
|    mean_cost_advantages | -0.0027136113 |
|    mean_reward_advan... | 603.73474     |
|    n_updates            | 2960          |
|    nu                   | 3.08          |
|    nu_loss              | -0.000903     |
|    policy_gradient_loss | -1.87e-05     |
|    reward_explained_... | -1.07e+15     |
|    reward_value_loss    | 5.44e+05      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 860           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 298           |
|    time_elapsed         | 2375          |
|    total_timesteps      | 3051520       |
| train/                  |               |
|    approx_kl            | 0.00010980885 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000117      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.4          |
|    cost_value_loss      | 0.0007        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.025        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.33e+05      |
|    mean_cost_advantages | -8.460288e-05 |
|    mean_reward_advan... | 581.6727      |
|    n_updates            | 2970          |
|    nu                   | 3.08          |
|    nu_loss              | -0.000301     |
|    policy_gradient_loss | -7.35e-06     |
|    reward_explained_... | -2.05e+11     |
|    reward_value_loss    | 5.16e+05      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 858           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 299           |
|    time_elapsed         | 2384          |
|    total_timesteps      | 3061760       |
| train/                  |               |
|    approx_kl            | 0.00012471946 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000254      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.14         |
|    cost_value_loss      | 0.00144       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0244       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.16e+05      |
|    mean_cost_advantages | 0.0012598861  |
|    mean_reward_advan... | 558.42444     |
|    n_updates            | 2980          |
|    nu                   | 3.08          |
|    nu_loss              | -0.000903     |
|    policy_gradient_loss | -3.36e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 4.86e+05      |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 863            |
|    ep_len_mean          | 11.3           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 300            |
|    time_elapsed         | 2392           |
|    total_timesteps      | 3072000        |
| train/                  |                |
|    approx_kl            | 2.0634476e-05  |
|    average_cost         | 0.0001953125   |
|    clip_fraction        | 3.91e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.6           |
|    cost_value_loss      | 0.00111        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0235        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.81e+05       |
|    mean_cost_advantages | -0.00057803525 |
|    mean_reward_advan... | 532.89856      |
|    n_updates            | 2990           |
|    nu                   | 3.09           |
|    nu_loss              | -0.000602      |
|    policy_gradient_loss | -1.4e-05       |
|    reward_explained_... | -1.96e+11      |
|    reward_value_loss    | 4.55e+05       |
|    total_cost           | 2.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 856            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 301            |
|    time_elapsed         | 2400           |
|    total_timesteps      | 3082240        |
| train/                  |                |
|    approx_kl            | 0.00027265647  |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0.000576       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.09          |
|    cost_value_loss      | 0.000666       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0241        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.84e+05       |
|    mean_cost_advantages | -0.00025097054 |
|    mean_reward_advan... | 512.69604      |
|    n_updates            | 3000           |
|    nu                   | 3.09           |
|    nu_loss              | -0.000301      |
|    policy_gradient_loss | -5.76e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 4.28e+05       |
|    total_cost           | 1.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 860            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 302            |
|    time_elapsed         | 2408           |
|    total_timesteps      | 3092480        |
| train/                  |                |
|    approx_kl            | 0.00020189246  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00221        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0311        |
|    cost_value_loss      | 3.53e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0221        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.55e+05       |
|    mean_cost_advantages | -0.00018744703 |
|    mean_reward_advan... | 484.4166       |
|    n_updates            | 3010           |
|    nu                   | 3.09           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -6.21e-05      |
|    reward_explained_... | -1.85e+11      |
|    reward_value_loss    | 3.96e+05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000293       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 857            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 303            |
|    time_elapsed         | 2417           |
|    total_timesteps      | 3102720        |
| train/                  |                |
|    approx_kl            | 3.1658776e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0391        |
|    cost_value_loss      | 3.19e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0207        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.69e+05       |
|    mean_cost_advantages | -0.00012670716 |
|    mean_reward_advan... | 463.1778       |
|    n_updates            | 3020           |
|    nu                   | 3.09           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.3e-05       |
|    reward_explained_... | nan            |
|    reward_value_loss    | 3.72e+05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 304           |
|    time_elapsed         | 2425          |
|    total_timesteps      | 3112960       |
| train/                  |               |
|    approx_kl            | -7.964927e-06 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000127      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.26         |
|    cost_value_loss      | 0.00226       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0221       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.48e+05      |
|    mean_cost_advantages | 0.001377421   |
|    mean_reward_advan... | 432.9651      |
|    n_updates            | 3030          |
|    nu                   | 3.09          |
|    nu_loss              | -0.000905     |
|    policy_gradient_loss | -5.69e-06     |
|    reward_explained_... | -1.84e+11     |
|    reward_value_loss    | 3.49e+05      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 305           |
|    time_elapsed         | 2433          |
|    total_timesteps      | 3123200       |
| train/                  |               |
|    approx_kl            | 0.00020087119 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.00106       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.49         |
|    cost_value_loss      | 0.000528      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0202       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.85e+05      |
|    mean_cost_advantages | 0.0019353557  |
|    mean_reward_advan... | 414.13696     |
|    n_updates            | 3040          |
|    nu                   | 3.09          |
|    nu_loss              | -0.000302     |
|    policy_gradient_loss | -2.04e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 3.25e+05      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 306           |
|    time_elapsed         | 2441          |
|    total_timesteps      | 3133440       |
| train/                  |               |
|    approx_kl            | 2.1041022e-05 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 5.86e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.96         |
|    cost_value_loss      | 0.00644       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0192       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.37e+05      |
|    mean_cost_advantages | 0.0011780101  |
|    mean_reward_advan... | 389.4359      |
|    n_updates            | 3050          |
|    nu                   | 3.09          |
|    nu_loss              | -0.00151      |
|    policy_gradient_loss | -4.13e-05     |
|    reward_explained_... | -1.71e+11     |
|    reward_value_loss    | 3.03e+05      |
|    total_cost           | 5.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 13           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000879     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 856          |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1283         |
|    iterations           | 307          |
|    time_elapsed         | 2450         |
|    total_timesteps      | 3143680      |
| train/                  |              |
|    approx_kl            | 8.344124e-05 |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.00041      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.51        |
|    cost_value_loss      | 0.00154      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0202      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02e+05     |
|    mean_cost_advantages | -0.009586071 |
|    mean_reward_advan... | 365.1163     |
|    n_updates            | 3060         |
|    nu                   | 3.09         |
|    nu_loss              | -0.000604    |
|    policy_gradient_loss | -1.41e-05    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 2.86e+05     |
|    total_cost           | 2.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 864          |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1282         |
|    iterations           | 308          |
|    time_elapsed         | 2458         |
|    total_timesteps      | 3153920      |
| train/                  |              |
|    approx_kl            | 6.997401e-05 |
|    average_cost         | 0.0008789062 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.11        |
|    cost_value_loss      | 0.00978      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0205      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.18e+05     |
|    mean_cost_advantages | 0.0005852833 |
|    mean_reward_advan... | 334.3626     |
|    n_updates            | 3070         |
|    nu                   | 3.09         |
|    nu_loss              | -0.00272     |
|    policy_gradient_loss | -6.89e-05    |
|    reward_explained_... | -1.68e+11    |
|    reward_value_loss    | 2.62e+05     |
|    total_cost           | 9.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 858            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1282           |
|    iterations           | 309            |
|    time_elapsed         | 2466           |
|    total_timesteps      | 3164160        |
| train/                  |                |
|    approx_kl            | 0.00026061758  |
|    average_cost         | 0.0001953125   |
|    clip_fraction        | 0.0017         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.8           |
|    cost_value_loss      | 0.00106        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0194        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.46e+05       |
|    mean_cost_advantages | -4.8175452e-06 |
|    mean_reward_advan... | 315.44037      |
|    n_updates            | 3080           |
|    nu                   | 3.09           |
|    nu_loss              | -0.000604      |
|    policy_gradient_loss | -6.82e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 2.44e+05       |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 310           |
|    time_elapsed         | 2474          |
|    total_timesteps      | 3174400       |
| train/                  |               |
|    approx_kl            | 9.6605116e-05 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.09         |
|    cost_value_loss      | 0.00806       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0189       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.02e+05      |
|    mean_cost_advantages | 0.0009964227  |
|    mean_reward_advan... | 286.6291      |
|    n_updates            | 3090          |
|    nu                   | 3.09          |
|    nu_loss              | -0.00121      |
|    policy_gradient_loss | -5.8e-05      |
|    reward_explained_... | -1.59e+11     |
|    reward_value_loss    | 2.25e+05      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 861           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 311           |
|    time_elapsed         | 2483          |
|    total_timesteps      | 3184640       |
| train/                  |               |
|    approx_kl            | 0.00023640133 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.002         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0344        |
|    cost_value_loss      | 6.89e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0176       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.21e+05      |
|    mean_cost_advantages | -0.007901878  |
|    mean_reward_advan... | 272.9303      |
|    n_updates            | 3100          |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -7.8e-05      |
|    reward_explained_... | nan           |
|    reward_value_loss    | 2.09e+05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 312           |
|    time_elapsed         | 2491          |
|    total_timesteps      | 3194880       |
| train/                  |               |
|    approx_kl            | 0.00022266821 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.000664      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.23         |
|    cost_value_loss      | 0.0103        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0169       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.04e+05      |
|    mean_cost_advantages | 0.0013784645  |
|    mean_reward_advan... | 240.04366     |
|    n_updates            | 3110          |
|    nu                   | 3.1           |
|    nu_loss              | -0.00121      |
|    policy_gradient_loss | -6.96e-05     |
|    reward_explained_... | -1.52e+11     |
|    reward_value_loss    | 1.95e+05      |
|    total_cost           | 4.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 857            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1282           |
|    iterations           | 313            |
|    time_elapsed         | 2499           |
|    total_timesteps      | 3205120        |
| train/                  |                |
|    approx_kl            | 0.00025942965  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000771       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00329       |
|    cost_value_loss      | 4.6e-05        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0162        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.01e+05       |
|    mean_cost_advantages | -0.00014177605 |
|    mean_reward_advan... | 219.57358      |
|    n_updates            | 3120           |
|    nu                   | 3.1            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.77e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 1.8e+05        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 314           |
|    time_elapsed         | 2507          |
|    total_timesteps      | 3215360       |
| train/                  |               |
|    approx_kl            | 0.00032018722 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00141       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0673        |
|    cost_value_loss      | 8.53e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0151       |
|    learning_rate        | 0.0003        |
|    loss                 | 9.4e+04       |
|    mean_cost_advantages | 0.0009212613  |
|    mean_reward_advan... | 188.98622     |
|    n_updates            | 3130          |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.96e-05     |
|    reward_explained_... | -1.45e+11     |
|    reward_value_loss    | 1.68e+05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 315           |
|    time_elapsed         | 2516          |
|    total_timesteps      | 3225600       |
| train/                  |               |
|    approx_kl            | 0.00024096225 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.00126       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -36.1         |
|    cost_value_loss      | 0.00402       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0154       |
|    learning_rate        | 0.0003        |
|    loss                 | 9.16e+04      |
|    mean_cost_advantages | 0.005502534   |
|    mean_reward_advan... | 166.24597     |
|    n_updates            | 3140          |
|    nu                   | 3.1           |
|    nu_loss              | -0.00121      |
|    policy_gradient_loss | -0.000113     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.6e+05       |
|    total_cost           | 4.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 864          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1281         |
|    iterations           | 316          |
|    time_elapsed         | 2524         |
|    total_timesteps      | 3235840      |
| train/                  |              |
|    approx_kl            | 0.0003571772 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0793      |
|    cost_value_loss      | 0.000199     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0255      |
|    learning_rate        | 0.0003       |
|    loss                 | 6.82e+04     |
|    mean_cost_advantages | 0.0046694237 |
|    mean_reward_advan... | 145.84602    |
|    n_updates            | 3150         |
|    nu                   | 3.1          |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00122     |
|    reward_explained_... | nan          |
|    reward_value_loss    | 1.46e+05     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 317           |
|    time_elapsed         | 2532          |
|    total_timesteps      | 3246080       |
| train/                  |               |
|    approx_kl            | 0.00016085373 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.00136       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.98         |
|    cost_value_loss      | 0.00267       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0147       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.48e+04      |
|    mean_cost_advantages | 0.0037095633  |
|    mean_reward_advan... | 116.94928     |
|    n_updates            | 3160          |
|    nu                   | 3.1           |
|    nu_loss              | -0.000605     |
|    policy_gradient_loss | 8.96e-05      |
|    reward_explained_... | -3.42e+10     |
|    reward_value_loss    | 1.41e+05      |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 684           |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 318           |
|    time_elapsed         | 2537          |
|    total_timesteps      | 3256320       |
| train/                  |               |
|    approx_kl            | 0.017573891   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00833       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0675       |
|    cost_value_loss      | 1.99e-05      |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.0184       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.88e+04      |
|    mean_cost_advantages | -0.0038250491 |
|    mean_reward_advan... | 100.62054     |
|    n_updates            | 3170          |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | 7.13e-05      |
|    reward_explained_... | -1.29e+11     |
|    reward_value_loss    | 1.32e+05      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.13
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 845          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1284         |
|    iterations           | 319          |
|    time_elapsed         | 2542         |
|    total_timesteps      | 3266560      |
| train/                  |              |
|    approx_kl            | 0.13331404   |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.7         |
|    cost_value_loss      | 0.0309       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0917      |
|    learning_rate        | 0.0003       |
|    loss                 | 8.11e+04     |
|    mean_cost_advantages | 0.0015945245 |
|    mean_reward_advan... | -58.676838   |
|    n_updates            | 3180         |
|    nu                   | 3.1          |
|    nu_loss              | -0.000606    |
|    policy_gradient_loss | -0.00193     |
|    reward_explained_... | -4.86e+10    |
|    reward_value_loss    | 1.89e+05     |
|    total_cost           | 2.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 854          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1284         |
|    iterations           | 320          |
|    time_elapsed         | 2550         |
|    total_timesteps      | 3276800      |
| train/                  |              |
|    approx_kl            | 0.0006579956 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.00813      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.46        |
|    cost_value_loss      | 0.00219      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0335      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.81e+04     |
|    mean_cost_advantages | 0.026266634  |
|    mean_reward_advan... | 79.53907     |
|    n_updates            | 3190         |
|    nu                   | 3.1          |
|    nu_loss              | -0.00121     |
|    policy_gradient_loss | -0.000277    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 1.03e+05     |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 852           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 321           |
|    time_elapsed         | 2558          |
|    total_timesteps      | 3287040       |
| train/                  |               |
|    approx_kl            | 7.7867575e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00453       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00297       |
|    cost_value_loss      | 5.61e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0288       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.38e+04      |
|    mean_cost_advantages | 0.004370122   |
|    mean_reward_advan... | 164.54692     |
|    n_updates            | 3200          |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000181     |
|    reward_explained_... | -3.66         |
|    reward_value_loss    | 9.65e+04      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 861           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 322           |
|    time_elapsed         | 2566          |
|    total_timesteps      | 3297280       |
| train/                  |               |
|    approx_kl            | 0.00015125555 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.00405       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.86         |
|    cost_value_loss      | 0.00607       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0249       |
|    learning_rate        | 0.0003        |
|    loss                 | 4.49e+04      |
|    mean_cost_advantages | 0.0039889016  |
|    mean_reward_advan... | 160.57378     |
|    n_updates            | 3210          |
|    nu                   | 3.1           |
|    nu_loss              | -0.00121      |
|    policy_gradient_loss | -0.000171     |
|    reward_explained_... | -1.92         |
|    reward_value_loss    | 8.8e+04       |
|    total_cost           | 4.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 850            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 323            |
|    time_elapsed         | 2574           |
|    total_timesteps      | 3307520        |
| train/                  |                |
|    approx_kl            | -0.00019318241 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00141        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0224        |
|    cost_value_loss      | 2.52e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0215        |
|    learning_rate        | 0.0003         |
|    loss                 | 3.45e+04       |
|    mean_cost_advantages | -0.0020063533  |
|    mean_reward_advan... | 177.1851       |
|    n_updates            | 3220           |
|    nu                   | 3.1            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -7.22e-05      |
|    reward_explained_... | -0.706         |
|    reward_value_loss    | 8e+04          |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 862            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 324            |
|    time_elapsed         | 2582           |
|    total_timesteps      | 3317760        |
| train/                  |                |
|    approx_kl            | -2.3955106e-05 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0019         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00353       |
|    cost_value_loss      | 3.48e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0192        |
|    learning_rate        | 0.0003         |
|    loss                 | 3.34e+04       |
|    mean_cost_advantages | -0.0004528092  |
|    mean_reward_advan... | 153.4825       |
|    n_updates            | 3230           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000106      |
|    reward_explained_... | -0.658         |
|    reward_value_loss    | 7.27e+04       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 863            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 325            |
|    time_elapsed         | 2590           |
|    total_timesteps      | 3328000        |
| train/                  |                |
|    approx_kl            | 4.716681e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00175        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0211        |
|    cost_value_loss      | 4.53e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0171        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.73e+04       |
|    mean_cost_advantages | -0.00017522268 |
|    mean_reward_advan... | 152.81412      |
|    n_updates            | 3240           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00012       |
|    reward_explained_... | -0.263         |
|    reward_value_loss    | 6.49e+04       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 869            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 326            |
|    time_elapsed         | 2598           |
|    total_timesteps      | 3338240        |
| train/                  |                |
|    approx_kl            | 3.7386337e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00104        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0634         |
|    cost_value_loss      | 0.000147       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0158        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.37e+04       |
|    mean_cost_advantages | -0.00017145151 |
|    mean_reward_advan... | 136.43289      |
|    n_updates            | 3250           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -6.97e-05      |
|    reward_explained_... | -0.154         |
|    reward_value_loss    | 5.96e+04       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 860           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 327           |
|    time_elapsed         | 2606          |
|    total_timesteps      | 3348480       |
| train/                  |               |
|    approx_kl            | 0.00014089374 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0365       |
|    cost_value_loss      | 1.85e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0136       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.36e+04      |
|    mean_cost_advantages | 0.00045112436 |
|    mean_reward_advan... | 133.54512     |
|    n_updates            | 3260          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -8.73e-05     |
|    reward_explained_... | 0.0957        |
|    reward_value_loss    | 5.19e+04      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 864            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 328            |
|    time_elapsed         | 2614           |
|    total_timesteps      | 3358720        |
| train/                  |                |
|    approx_kl            | -2.1682145e-05 |
|    average_cost         | 0.00048828125  |
|    clip_fraction        | 0.000146       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.38          |
|    cost_value_loss      | 0.00737        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0129        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.15e+04       |
|    mean_cost_advantages | 0.0033644873   |
|    mean_reward_advan... | 112.31627      |
|    n_updates            | 3270           |
|    nu                   | 3.11           |
|    nu_loss              | -0.00152       |
|    policy_gradient_loss | -2.65e-05      |
|    reward_explained_... | 0.0672         |
|    reward_value_loss    | 4.86e+04       |
|    total_cost           | 5.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000879      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 329           |
|    time_elapsed         | 2621          |
|    total_timesteps      | 3368960       |
| train/                  |               |
|    approx_kl            | 4.419009e-06  |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.68         |
|    cost_value_loss      | 0.00785       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0124       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.54e+04      |
|    mean_cost_advantages | 0.00036504416 |
|    mean_reward_advan... | 112.39968     |
|    n_updates            | 3280          |
|    nu                   | 3.11          |
|    nu_loss              | -0.00121      |
|    policy_gradient_loss | -1.61e-05     |
|    reward_explained_... | 0.288         |
|    reward_value_loss    | 4.35e+04      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 330           |
|    time_elapsed         | 2629          |
|    total_timesteps      | 3379200       |
| train/                  |               |
|    approx_kl            | 0.00023886301 |
|    average_cost         | 0.0008789062  |
|    clip_fraction        | 0.000811      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.83         |
|    cost_value_loss      | 0.00537       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0121       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.21e+04      |
|    mean_cost_advantages | -0.0004850145 |
|    mean_reward_advan... | 102.23214     |
|    n_updates            | 3290          |
|    nu                   | 3.11          |
|    nu_loss              | -0.00273      |
|    policy_gradient_loss | -0.000135     |
|    reward_explained_... | 0.399         |
|    reward_value_loss    | 3.83e+04      |
|    total_cost           | 9.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 331           |
|    time_elapsed         | 2637          |
|    total_timesteps      | 3389440       |
| train/                  |               |
|    approx_kl            | 9.8792676e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000322      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0526        |
|    cost_value_loss      | 1.43e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.011        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.44e+04      |
|    mean_cost_advantages | -0.004902163  |
|    mean_reward_advan... | 93.37634      |
|    n_updates            | 3300          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.8e-05      |
|    reward_explained_... | 0.482         |
|    reward_value_loss    | 3.43e+04      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 868            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 332            |
|    time_elapsed         | 2645           |
|    total_timesteps      | 3399680        |
| train/                  |                |
|    approx_kl            | 0.000101530764 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000332       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00423       |
|    cost_value_loss      | 1.69e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0106        |
|    learning_rate        | 0.0003         |
|    loss                 | 1.81e+04       |
|    mean_cost_advantages | -0.00090572424 |
|    mean_reward_advan... | 82.94273       |
|    n_updates            | 3310           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.39e-05      |
|    reward_explained_... | 0.539          |
|    reward_value_loss    | 3.02e+04       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000195       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 867            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 333            |
|    time_elapsed         | 2653           |
|    total_timesteps      | 3409920        |
| train/                  |                |
|    approx_kl            | 4.548796e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0443        |
|    cost_value_loss      | 1.25e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0103        |
|    learning_rate        | 0.0003         |
|    loss                 | 7.93e+03       |
|    mean_cost_advantages | -1.3284407e-05 |
|    mean_reward_advan... | 75.08819       |
|    n_updates            | 3320           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -6.05e-06      |
|    reward_explained_... | 0.607          |
|    reward_value_loss    | 2.72e+04       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 334           |
|    time_elapsed         | 2661          |
|    total_timesteps      | 3420160       |
| train/                  |               |
|    approx_kl            | 9.9056306e-05 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.81         |
|    cost_value_loss      | 0.00127       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0104       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.19e+03      |
|    mean_cost_advantages | 0.0006479891  |
|    mean_reward_advan... | 68.293594     |
|    n_updates            | 3330          |
|    nu                   | 3.11          |
|    nu_loss              | -0.000608     |
|    policy_gradient_loss | -2.79e-05     |
|    reward_explained_... | 0.667         |
|    reward_value_loss    | 2.4e+04       |
|    total_cost           | 2.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 869            |
|    ep_len_mean          | 11.3           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 335            |
|    time_elapsed         | 2669           |
|    total_timesteps      | 3430400        |
| train/                  |                |
|    approx_kl            | 9.374707e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000127       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.023         |
|    cost_value_loss      | 3.18e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0103        |
|    learning_rate        | 0.0003         |
|    loss                 | 5e+03          |
|    mean_cost_advantages | -1.3495637e-05 |
|    mean_reward_advan... | 60.694805      |
|    n_updates            | 3340           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.28e-05      |
|    reward_explained_... | 0.717          |
|    reward_value_loss    | 2.06e+04       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 336           |
|    time_elapsed         | 2677          |
|    total_timesteps      | 3440640       |
| train/                  |               |
|    approx_kl            | 0.00017151199 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.000283      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.93         |
|    cost_value_loss      | 0.0072        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0101       |
|    learning_rate        | 0.0003        |
|    loss                 | 8.33e+03      |
|    mean_cost_advantages | 0.0023724935  |
|    mean_reward_advan... | 56.37193      |
|    n_updates            | 3350          |
|    nu                   | 3.11          |
|    nu_loss              | -0.00122      |
|    policy_gradient_loss | -6.07e-05     |
|    reward_explained_... | 0.766         |
|    reward_value_loss    | 1.8e+04       |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 337           |
|    time_elapsed         | 2685          |
|    total_timesteps      | 3450880       |
| train/                  |               |
|    approx_kl            | 1.1979649e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0475        |
|    cost_value_loss      | 1.23e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0102       |
|    learning_rate        | 0.0003        |
|    loss                 | 6.86e+03      |
|    mean_cost_advantages | -0.004325488  |
|    mean_reward_advan... | 49.597065     |
|    n_updates            | 3360          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.35e-05     |
|    reward_explained_... | 0.803         |
|    reward_value_loss    | 1.6e+04       |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 859           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 338           |
|    time_elapsed         | 2693          |
|    total_timesteps      | 3461120       |
| train/                  |               |
|    approx_kl            | 0.00012062902 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0004        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.09         |
|    cost_value_loss      | 0.00241       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0101       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.49e+03      |
|    mean_cost_advantages | 5.287769e-05  |
|    mean_reward_advan... | 35.242542     |
|    n_updates            | 3370          |
|    nu                   | 3.11          |
|    nu_loss              | -0.000912     |
|    policy_gradient_loss | -6.42e-05     |
|    reward_explained_... | 0.807         |
|    reward_value_loss    | 1.41e+04      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 871           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 339           |
|    time_elapsed         | 2701          |
|    total_timesteps      | 3471360       |
| train/                  |               |
|    approx_kl            | 7.536893e-05  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 9.77e-06      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.15         |
|    cost_value_loss      | 0.00684       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0102       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.03e+03      |
|    mean_cost_advantages | 0.0022561853  |
|    mean_reward_advan... | 35.532314     |
|    n_updates            | 3380          |
|    nu                   | 3.11          |
|    nu_loss              | -0.00152      |
|    policy_gradient_loss | -2.57e-05     |
|    reward_explained_... | 0.839         |
|    reward_value_loss    | 1.32e+04      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 340           |
|    time_elapsed         | 2709          |
|    total_timesteps      | 3481600       |
| train/                  |               |
|    approx_kl            | 7.495127e-05  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000156      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.14         |
|    cost_value_loss      | 0.00416       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00961      |
|    learning_rate        | 0.0003        |
|    loss                 | 5.78e+03      |
|    mean_cost_advantages | 0.0009285415  |
|    mean_reward_advan... | 33.88489      |
|    n_updates            | 3390          |
|    nu                   | 3.12          |
|    nu_loss              | -0.000913     |
|    policy_gradient_loss | -4.07e-05     |
|    reward_explained_... | 0.862         |
|    reward_value_loss    | 1.14e+04      |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 859            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 341            |
|    time_elapsed         | 2717           |
|    total_timesteps      | 3491840        |
| train/                  |                |
|    approx_kl            | 8.5395295e-06  |
|    average_cost         | 0.0001953125   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -32.8          |
|    cost_value_loss      | 0.00178        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00935       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.55e+03       |
|    mean_cost_advantages | -0.00019030008 |
|    mean_reward_advan... | 24.664608      |
|    n_updates            | 3400           |
|    nu                   | 3.12           |
|    nu_loss              | -0.000609      |
|    policy_gradient_loss | -5.81e-06      |
|    reward_explained_... | 0.867          |
|    reward_value_loss    | 1.08e+04       |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 342           |
|    time_elapsed         | 2725          |
|    total_timesteps      | 3502080       |
| train/                  |               |
|    approx_kl            | 4.5844306e-05 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.000127      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.17         |
|    cost_value_loss      | 0.00225       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00958      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.46e+03      |
|    mean_cost_advantages | 0.00040091452 |
|    mean_reward_advan... | 21.678549     |
|    n_updates            | 3410          |
|    nu                   | 3.12          |
|    nu_loss              | -0.00122      |
|    policy_gradient_loss | -2.31e-05     |
|    reward_explained_... | 0.901         |
|    reward_value_loss    | 9.06e+03      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 343           |
|    time_elapsed         | 2733          |
|    total_timesteps      | 3512320       |
| train/                  |               |
|    approx_kl            | 6.745823e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00872      |
|    cost_value_loss      | 1.54e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0092       |
|    learning_rate        | 0.0003        |
|    loss                 | 2.7e+03       |
|    mean_cost_advantages | -0.0011129279 |
|    mean_reward_advan... | 15.177612     |
|    n_updates            | 3420          |
|    nu                   | 3.12          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -8.28e-06     |
|    reward_explained_... | 0.897         |
|    reward_value_loss    | 9e+03         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 863            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 344            |
|    time_elapsed         | 2741           |
|    total_timesteps      | 3522560        |
| train/                  |                |
|    approx_kl            | 5.1290866e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 6.84e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0363        |
|    cost_value_loss      | 1.35e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00896       |
|    learning_rate        | 0.0003         |
|    loss                 | 1.98e+03       |
|    mean_cost_advantages | -0.00036057818 |
|    mean_reward_advan... | 16.030315      |
|    n_updates            | 3430           |
|    nu                   | 3.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.81e-05      |
|    reward_explained_... | 0.925          |
|    reward_value_loss    | 6.73e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 863            |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 345            |
|    time_elapsed         | 2749           |
|    total_timesteps      | 3532800        |
| train/                  |                |
|    approx_kl            | 1.30510425e-05 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0478        |
|    cost_value_loss      | 1.05e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00836       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.61e+03       |
|    mean_cost_advantages | -2.0550571e-05 |
|    mean_reward_advan... | 10.604833      |
|    n_updates            | 3440           |
|    nu                   | 3.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.41e-06      |
|    reward_explained_... | 0.92           |
|    reward_value_loss    | 7.5e+03        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 346           |
|    time_elapsed         | 2758          |
|    total_timesteps      | 3543040       |
| train/                  |               |
|    approx_kl            | 1.3461429e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0474       |
|    cost_value_loss      | 1.04e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00824      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.26e+03      |
|    mean_cost_advantages | -0.0001275482 |
|    mean_reward_advan... | 14.973241     |
|    n_updates            | 3450          |
|    nu                   | 3.12          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 1.72e-08      |
|    reward_explained_... | 0.938         |
|    reward_value_loss    | 6.28e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 347           |
|    time_elapsed         | 2765          |
|    total_timesteps      | 3553280       |
| train/                  |               |
|    approx_kl            | 1.5856558e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000254      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.106         |
|    cost_value_loss      | 8.14e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00858      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.11e+03      |
|    mean_cost_advantages | -0.0002652483 |
|    mean_reward_advan... | 5.885456      |
|    n_updates            | 3460          |
|    nu                   | 3.12          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.1e-05      |
|    reward_explained_... | 0.928         |
|    reward_value_loss    | 7.21e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 868            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 348            |
|    time_elapsed         | 2774           |
|    total_timesteps      | 3563520        |
| train/                  |                |
|    approx_kl            | 6.118188e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 4.88e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0507        |
|    cost_value_loss      | 1e-05          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00812       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.17e+03       |
|    mean_cost_advantages | -0.00011285439 |
|    mean_reward_advan... | 5.2226243      |
|    n_updates            | 3470           |
|    nu                   | 3.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.51e-05      |
|    reward_explained_... | 0.935          |
|    reward_value_loss    | 6.61e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 868          |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1284         |
|    iterations           | 349          |
|    time_elapsed         | 2782         |
|    total_timesteps      | 3573760      |
| train/                  |              |
|    approx_kl            | 5.971426e-05 |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.73        |
|    cost_value_loss      | 0.000573     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.00872     |
|    learning_rate        | 0.0003       |
|    loss                 | 3.43e+03     |
|    mean_cost_advantages | 0.0008072743 |
|    mean_reward_advan... | 7.5598783    |
|    n_updates            | 3480         |
|    nu                   | 3.12         |
|    nu_loss              | -0.000305    |
|    policy_gradient_loss | -1.47e-05    |
|    reward_explained_... | 0.941        |
|    reward_value_loss    | 5.83e+03     |
|    total_cost           | 1.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 350           |
|    time_elapsed         | 2789          |
|    total_timesteps      | 3584000       |
| train/                  |               |
|    approx_kl            | 8.462344e-05  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.86         |
|    cost_value_loss      | 0.00374       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00913      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.45e+03      |
|    mean_cost_advantages | 0.0021276805  |
|    mean_reward_advan... | 2.8704445     |
|    n_updates            | 3490          |
|    nu                   | 3.12          |
|    nu_loss              | -0.00152      |
|    policy_gradient_loss | -3.89e-05     |
|    reward_explained_... | 0.939         |
|    reward_value_loss    | 6.17e+03      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 351           |
|    time_elapsed         | 2798          |
|    total_timesteps      | 3594240       |
| train/                  |               |
|    approx_kl            | 3.4122263e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00129       |
|    cost_value_loss      | 1.11e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00872      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.66e+03      |
|    mean_cost_advantages | -0.0023275665 |
|    mean_reward_advan... | 3.001926      |
|    n_updates            | 3500          |
|    nu                   | 3.12          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -8.48e-06     |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 5.74e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 868            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 352            |
|    time_elapsed         | 2806           |
|    total_timesteps      | 3604480        |
| train/                  |                |
|    approx_kl            | -2.1450687e-07 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0441         |
|    cost_value_loss      | 7.19e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00865       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.29e+03       |
|    mean_cost_advantages | -0.00065270707 |
|    mean_reward_advan... | -0.793482      |
|    n_updates            | 3510           |
|    nu                   | 3.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -9.76e-06      |
|    reward_explained_... | 0.938          |
|    reward_value_loss    | 6.18e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 870            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 353            |
|    time_elapsed         | 2814           |
|    total_timesteps      | 3614720        |
| train/                  |                |
|    approx_kl            | -1.8252525e-06 |
|    average_cost         | 0.0            |
|    clip_fraction        | 9.77e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0881         |
|    cost_value_loss      | 6.57e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00816       |
|    learning_rate        | 0.0003         |
|    loss                 | 6.04e+03       |
|    mean_cost_advantages | 4.871862e-05   |
|    mean_reward_advan... | 1.4628382      |
|    n_updates            | 3520           |
|    nu                   | 3.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.7e-05       |
|    reward_explained_... | 0.943          |
|    reward_value_loss    | 5.86e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 871           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 354           |
|    time_elapsed         | 2822          |
|    total_timesteps      | 3624960       |
| train/                  |               |
|    approx_kl            | 5.4508077e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 8.79e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0488       |
|    cost_value_loss      | 1.14e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0079       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.77e+03      |
|    mean_cost_advantages | 0.0004917983  |
|    mean_reward_advan... | -2.9812565    |
|    n_updates            | 3530          |
|    nu                   | 3.12          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.58e-05     |
|    reward_explained_... | 0.934         |
|    reward_value_loss    | 6.45e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 871           |
|    ep_len_mean          | 11.1          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 355           |
|    time_elapsed         | 2830          |
|    total_timesteps      | 3635200       |
| train/                  |               |
|    approx_kl            | 7.8192215e-06 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.17         |
|    cost_value_loss      | 0.00549       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00798      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.28e+03      |
|    mean_cost_advantages | 0.0018481783  |
|    mean_reward_advan... | 5.056262      |
|    n_updates            | 3540          |
|    nu                   | 3.12          |
|    nu_loss              | -0.00122      |
|    policy_gradient_loss | -1.4e-05      |
|    reward_explained_... | 0.949         |
|    reward_value_loss    | 5.36e+03      |
|    total_cost           | 4.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 876            |
|    ep_len_mean          | 11.2           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 356            |
|    time_elapsed         | 2838           |
|    total_timesteps      | 3645440        |
| train/                  |                |
|    approx_kl            | 8.530282e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000352       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0284        |
|    cost_value_loss      | 1.21e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00747       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.74e+03       |
|    mean_cost_advantages | -0.00084158423 |
|    mean_reward_advan... | 1.9426111      |
|    n_updates            | 3550           |
|    nu                   | 3.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.52e-05      |
|    reward_explained_... | 0.946          |
|    reward_value_loss    | 5.66e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 867            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 357            |
|    time_elapsed         | 2846           |
|    total_timesteps      | 3655680        |
| train/                  |                |
|    approx_kl            | 7.928081e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000176       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00173       |
|    cost_value_loss      | 1.68e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00748       |
|    learning_rate        | 0.0003         |
|    loss                 | 1.8e+03        |
|    mean_cost_advantages | -0.00014340685 |
|    mean_reward_advan... | 3.1066604      |
|    n_updates            | 3560           |
|    nu                   | 3.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.82e-05      |
|    reward_explained_... | 0.954          |
|    reward_value_loss    | 4.75e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 876           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 358           |
|    time_elapsed         | 2854          |
|    total_timesteps      | 3665920       |
| train/                  |               |
|    approx_kl            | -5.438102e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000332      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -11           |
|    cost_value_loss      | 0.000352      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00745      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.48e+03      |
|    mean_cost_advantages | 0.0005685432  |
|    mean_reward_advan... | -7.6576347    |
|    n_updates            | 3570          |
|    nu                   | 3.13          |
|    nu_loss              | -0.000305     |
|    policy_gradient_loss | -1.74e-05     |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 6.48e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 13             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 863            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 359            |
|    time_elapsed         | 2862           |
|    total_timesteps      | 3676160        |
| train/                  |                |
|    approx_kl            | -3.655511e-06  |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -8.48          |
|    cost_value_loss      | 0.000518       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00742       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.25e+03       |
|    mean_cost_advantages | -0.00056892197 |
|    mean_reward_advan... | 3.3001041      |
|    n_updates            | 3580           |
|    nu                   | 3.13           |
|    nu_loss              | -0.000305      |
|    policy_gradient_loss | -4.57e-07      |
|    reward_explained_... | 0.948          |
|    reward_value_loss    | 4.94e+03       |
|    total_cost           | 1.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000684       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 865            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 360            |
|    time_elapsed         | 2870           |
|    total_timesteps      | 3686400        |
| train/                  |                |
|    approx_kl            | 6.427788e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000391       |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0926         |
|    cost_value_loss      | 4.97e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00764       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.48e+03       |
|    mean_cost_advantages | -0.00046822344 |
|    mean_reward_advan... | 3.5378926      |
|    n_updates            | 3590           |
|    nu                   | 3.13           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.13e-05      |
|    reward_explained_... | 0.942          |
|    reward_value_loss    | 6.21e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 361           |
|    time_elapsed         | 2878          |
|    total_timesteps      | 3696640       |
| train/                  |               |
|    approx_kl            | 9.701851e-05  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.000313      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.42         |
|    cost_value_loss      | 0.00601       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00857      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.32e+03      |
|    mean_cost_advantages | 0.0044664126  |
|    mean_reward_advan... | 4.6175604     |
|    n_updates            | 3600          |
|    nu                   | 3.13          |
|    nu_loss              | -0.00214      |
|    policy_gradient_loss | -7.22e-05     |
|    reward_explained_... | 0.938         |
|    reward_value_loss    | 6.83e+03      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 362           |
|    time_elapsed         | 2886          |
|    total_timesteps      | 3706880       |
| train/                  |               |
|    approx_kl            | 2.8440752e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000166      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -16.3         |
|    cost_value_loss      | 0.000403      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00805      |
|    learning_rate        | 0.0003        |
|    loss                 | 872           |
|    mean_cost_advantages | -0.0031939198 |
|    mean_reward_advan... | 6.733199      |
|    n_updates            | 3610          |
|    nu                   | 3.13          |
|    nu_loss              | -0.000305     |
|    policy_gradient_loss | -8.57e-06     |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 5.02e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 863            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 363            |
|    time_elapsed         | 2894           |
|    total_timesteps      | 3717120        |
| train/                  |                |
|    approx_kl            | 5.25492e-05    |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0.000166       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -27.5          |
|    cost_value_loss      | 0.000401       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00769       |
|    learning_rate        | 0.0003         |
|    loss                 | 1.5e+03        |
|    mean_cost_advantages | -0.00030047836 |
|    mean_reward_advan... | 2.4512467      |
|    n_updates            | 3620           |
|    nu                   | 3.13           |
|    nu_loss              | -0.000305      |
|    policy_gradient_loss | -4e-05         |
|    reward_explained_... | 0.953          |
|    reward_value_loss    | 5.01e+03       |
|    total_cost           | 1.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 364           |
|    time_elapsed         | 2902          |
|    total_timesteps      | 3727360       |
| train/                  |               |
|    approx_kl            | 2.2425596e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0318       |
|    cost_value_loss      | 9.94e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00714      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.99e+03      |
|    mean_cost_advantages | -0.0005073479 |
|    mean_reward_advan... | -6.141827     |
|    n_updates            | 3630          |
|    nu                   | 3.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -5.7e-06      |
|    reward_explained_... | 0.943         |
|    reward_value_loss    | 5.69e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 862          |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1284         |
|    iterations           | 365          |
|    time_elapsed         | 2910         |
|    total_timesteps      | 3737600      |
| train/                  |              |
|    approx_kl            | 6.568502e-05 |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.88        |
|    cost_value_loss      | 0.000569     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.00827     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.89e+03     |
|    mean_cost_advantages | 0.0002588711 |
|    mean_reward_advan... | -5.5658336   |
|    n_updates            | 3640         |
|    nu                   | 3.13         |
|    nu_loss              | -0.000305    |
|    policy_gradient_loss | -2.18e-05    |
|    reward_explained_... | 0.925        |
|    reward_value_loss    | 7.25e+03     |
|    total_cost           | 1.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 873            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 366            |
|    time_elapsed         | 2918           |
|    total_timesteps      | 3747840        |
| train/                  |                |
|    approx_kl            | 4.7551606e-05  |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -11.8          |
|    cost_value_loss      | 0.000511       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00788       |
|    learning_rate        | 0.0003         |
|    loss                 | 6.62e+03       |
|    mean_cost_advantages | -0.00015439038 |
|    mean_reward_advan... | 0.2412776      |
|    n_updates            | 3650           |
|    nu                   | 3.13           |
|    nu_loss              | -0.000306      |
|    policy_gradient_loss | -1.26e-05      |
|    reward_explained_... | 0.934          |
|    reward_value_loss    | 6.83e+03       |
|    total_cost           | 1.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 871            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1284           |
|    iterations           | 367            |
|    time_elapsed         | 2926           |
|    total_timesteps      | 3758080        |
| train/                  |                |
|    approx_kl            | 7.088131e-05   |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0.000488       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -30.8          |
|    cost_value_loss      | 0.00034        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00721       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.91e+03       |
|    mean_cost_advantages | -0.00011567277 |
|    mean_reward_advan... | 1.570673       |
|    n_updates            | 3660           |
|    nu                   | 3.13           |
|    nu_loss              | -0.000306      |
|    policy_gradient_loss | -8.21e-05      |
|    reward_explained_... | 0.951          |
|    reward_value_loss    | 5.05e+03       |
|    total_cost           | 1.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 860           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1284          |
|    iterations           | 368           |
|    time_elapsed         | 2934          |
|    total_timesteps      | 3768320       |
| train/                  |               |
|    approx_kl            | 3.1750347e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0113       |
|    cost_value_loss      | 3.05e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00732      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.29e+03      |
|    mean_cost_advantages | -4.527578e-05 |
|    mean_reward_advan... | 0.24376488    |
|    n_updates            | 3670          |
|    nu                   | 3.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -9.31e-07     |
|    reward_explained_... | 0.948         |
|    reward_value_loss    | 5.37e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 369           |
|    time_elapsed         | 2942          |
|    total_timesteps      | 3778560       |
| train/                  |               |
|    approx_kl            | 3.7370388e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000215      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.7          |
|    cost_value_loss      | 0.00471       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00684      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.52e+03      |
|    mean_cost_advantages | 0.001896815   |
|    mean_reward_advan... | -6.744081     |
|    n_updates            | 3680          |
|    nu                   | 3.13          |
|    nu_loss              | -0.000917     |
|    policy_gradient_loss | -4.07e-05     |
|    reward_explained_... | 0.937         |
|    reward_value_loss    | 6.47e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 874           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 370           |
|    time_elapsed         | 2950          |
|    total_timesteps      | 3788800       |
| train/                  |               |
|    approx_kl            | 0.00010313243 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000205      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -39           |
|    cost_value_loss      | 0.000397      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00704      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.77e+03      |
|    mean_cost_advantages | -0.0014242176 |
|    mean_reward_advan... | 1.5622917     |
|    n_updates            | 3690          |
|    nu                   | 3.13          |
|    nu_loss              | -0.000306     |
|    policy_gradient_loss | -2.84e-05     |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 6.38e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 875           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 371           |
|    time_elapsed         | 2959          |
|    total_timesteps      | 3799040       |
| train/                  |               |
|    approx_kl            | 0.00015601926 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000557      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -8.34         |
|    cost_value_loss      | 0.00331       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00704      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.32e+03      |
|    mean_cost_advantages | 0.0012608335  |
|    mean_reward_advan... | 5.2181377     |
|    n_updates            | 3700          |
|    nu                   | 3.13          |
|    nu_loss              | -0.000917     |
|    policy_gradient_loss | -0.000101     |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 5.02e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 372           |
|    time_elapsed         | 2967          |
|    total_timesteps      | 3809280       |
| train/                  |               |
|    approx_kl            | 5.5230317e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000107      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.029        |
|    cost_value_loss      | 9.77e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00633      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.9e+03       |
|    mean_cost_advantages | -0.0005060411 |
|    mean_reward_advan... | 3.7684798     |
|    n_updates            | 3710          |
|    nu                   | 3.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.82e-05     |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 4.94e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 870            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 373            |
|    time_elapsed         | 2975           |
|    total_timesteps      | 3819520        |
| train/                  |                |
|    approx_kl            | 4.0358864e-06  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0479        |
|    cost_value_loss      | 9.41e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00615       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.7e+03        |
|    mean_cost_advantages | -1.1783786e-05 |
|    mean_reward_advan... | -2.8304315     |
|    n_updates            | 3720           |
|    nu                   | 3.13           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.41e-06      |
|    reward_explained_... | 0.935          |
|    reward_value_loss    | 6.64e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 374           |
|    time_elapsed         | 2983          |
|    total_timesteps      | 3829760       |
| train/                  |               |
|    approx_kl            | 4.882426e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000205      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0492       |
|    cost_value_loss      | 9.47e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00628      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.97e+03      |
|    mean_cost_advantages | 2.6423688e-05 |
|    mean_reward_advan... | 2.8575413     |
|    n_updates            | 3730          |
|    nu                   | 3.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.75e-05     |
|    reward_explained_... | 0.947         |
|    reward_value_loss    | 5.59e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 375           |
|    time_elapsed         | 2991          |
|    total_timesteps      | 3840000       |
| train/                  |               |
|    approx_kl            | 1.8179184e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -54.3         |
|    cost_value_loss      | 0.000496      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00624      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.68e+03      |
|    mean_cost_advantages | 0.0005660434  |
|    mean_reward_advan... | -1.5104703    |
|    n_updates            | 3740          |
|    nu                   | 3.13          |
|    nu_loss              | -0.000306     |
|    policy_gradient_loss | -6.82e-06     |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 5.78e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 862           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 376           |
|    time_elapsed         | 2999          |
|    total_timesteps      | 3850240       |
| train/                  |               |
|    approx_kl            | 2.3659202e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.6          |
|    cost_value_loss      | 0.00178       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00648      |
|    learning_rate        | 0.0003        |
|    loss                 | 7.3e+03       |
|    mean_cost_advantages | 0.0008357374  |
|    mean_reward_advan... | -4.0099745    |
|    n_updates            | 3750          |
|    nu                   | 3.13          |
|    nu_loss              | -0.000918     |
|    policy_gradient_loss | -1.95e-05     |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 6.19e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 866            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 377            |
|    time_elapsed         | 3007           |
|    total_timesteps      | 3860480        |
| train/                  |                |
|    approx_kl            | 1.9498355e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 3.91e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.039          |
|    cost_value_loss      | 1.23e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00608       |
|    learning_rate        | 0.0003         |
|    loss                 | 5.04e+03       |
|    mean_cost_advantages | -0.00012054436 |
|    mean_reward_advan... | -6.9935427     |
|    n_updates            | 3760           |
|    nu                   | 3.13           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.07e-05      |
|    reward_explained_... | 0.936          |
|    reward_value_loss    | 6.63e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 875            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 378            |
|    time_elapsed         | 3015           |
|    total_timesteps      | 3870720        |
| train/                  |                |
|    approx_kl            | 2.2801152e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0296        |
|    cost_value_loss      | 1.23e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0063        |
|    learning_rate        | 0.0003         |
|    loss                 | 2.74e+03       |
|    mean_cost_advantages | -4.9810158e-05 |
|    mean_reward_advan... | 0.4071943      |
|    n_updates            | 3770           |
|    nu                   | 3.13           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 6.36e-06       |
|    reward_explained_... | 0.95           |
|    reward_value_loss    | 5.22e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 379           |
|    time_elapsed         | 3023          |
|    total_timesteps      | 3880960       |
| train/                  |               |
|    approx_kl            | 2.7740141e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 1.95e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0502       |
|    cost_value_loss      | 9.24e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00593      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.02e+03      |
|    mean_cost_advantages | -6.194714e-05 |
|    mean_reward_advan... | 1.8173561     |
|    n_updates            | 3780          |
|    nu                   | 3.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.45e-05     |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 4.97e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 866            |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 380            |
|    time_elapsed         | 3032           |
|    total_timesteps      | 3891200        |
| train/                  |                |
|    approx_kl            | 2.9784185e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0492        |
|    cost_value_loss      | 9.32e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00587       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.99e+03       |
|    mean_cost_advantages | -0.00012175364 |
|    mean_reward_advan... | -3.0004406     |
|    n_updates            | 3790           |
|    nu                   | 3.13           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.29e-06      |
|    reward_explained_... | 0.95           |
|    reward_value_loss    | 5.03e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 381           |
|    time_elapsed         | 3040          |
|    total_timesteps      | 3901440       |
| train/                  |               |
|    approx_kl            | 3.1953212e-05 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 6.84e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.17         |
|    cost_value_loss      | 0.00216       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00583      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.46e+03      |
|    mean_cost_advantages | 0.0011796029  |
|    mean_reward_advan... | -3.6257057    |
|    n_updates            | 3800          |
|    nu                   | 3.13          |
|    nu_loss              | -0.00122      |
|    policy_gradient_loss | -3.45e-05     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.11e+03      |
|    total_cost           | 4.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 864            |
|    ep_len_mean          | 11.7           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 382            |
|    time_elapsed         | 3048           |
|    total_timesteps      | 3911680        |
| train/                  |                |
|    approx_kl            | 0.0001994113   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000615       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0371        |
|    cost_value_loss      | 1.27e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00599       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.72e+03       |
|    mean_cost_advantages | -2.7209357e-05 |
|    mean_reward_advan... | 6.021707       |
|    n_updates            | 3810           |
|    nu                   | 3.14           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -8.14e-05      |
|    reward_explained_... | 0.949          |
|    reward_value_loss    | 5.54e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 868            |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1283           |
|    iterations           | 383            |
|    time_elapsed         | 3056           |
|    total_timesteps      | 3921920        |
| train/                  |                |
|    approx_kl            | -1.9939826e-05 |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0.000215       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -6.94          |
|    cost_value_loss      | 0.000723       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00635       |
|    learning_rate        | 0.0003         |
|    loss                 | 4.35e+03       |
|    mean_cost_advantages | 0.00073438336  |
|    mean_reward_advan... | -1.9499905     |
|    n_updates            | 3820           |
|    nu                   | 3.14           |
|    nu_loss              | -0.000306      |
|    policy_gradient_loss | -4.74e-05      |
|    reward_explained_... | 0.932          |
|    reward_value_loss    | 7.26e+03       |
|    total_cost           | 1.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 860           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1283          |
|    iterations           | 384           |
|    time_elapsed         | 3064          |
|    total_timesteps      | 3932160       |
| train/                  |               |
|    approx_kl            | 1.0200683e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0513       |
|    cost_value_loss      | 9.45e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00585      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.88e+03      |
|    mean_cost_advantages | 4.192869e-06  |
|    mean_reward_advan... | -0.33947343   |
|    n_updates            | 3830          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.49e-06     |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 5.24e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000391       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 867            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1282           |
|    iterations           | 385            |
|    time_elapsed         | 3072           |
|    total_timesteps      | 3942400        |
| train/                  |                |
|    approx_kl            | 5.98873e-06    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0713         |
|    cost_value_loss      | 0.00012        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00664       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.43e+03       |
|    mean_cost_advantages | -0.00023184638 |
|    mean_reward_advan... | -5.2407713     |
|    n_updates            | 3840           |
|    nu                   | 3.14           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -2.3e-06       |
|    reward_explained_... | 0.936          |
|    reward_value_loss    | 6.54e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 386           |
|    time_elapsed         | 3081          |
|    total_timesteps      | 3952640       |
| train/                  |               |
|    approx_kl            | 4.7314425e-05 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.11         |
|    cost_value_loss      | 0.0055        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00587      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.33e+03      |
|    mean_cost_advantages | 0.0017795332  |
|    mean_reward_advan... | 5.4573317     |
|    n_updates            | 3850          |
|    nu                   | 3.14          |
|    nu_loss              | -0.00123      |
|    policy_gradient_loss | -4.07e-05     |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 5.1e+03       |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 387           |
|    time_elapsed         | 3089          |
|    total_timesteps      | 3962880       |
| train/                  |               |
|    approx_kl            | 0.00010698589 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000127      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.053         |
|    cost_value_loss      | 8.56e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00597      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.47e+03      |
|    mean_cost_advantages | 0.00031449605 |
|    mean_reward_advan... | -2.9463193    |
|    n_updates            | 3860          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.32e-05     |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 5.79e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 388           |
|    time_elapsed         | 3097          |
|    total_timesteps      | 3973120       |
| train/                  |               |
|    approx_kl            | 5.2733812e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0501       |
|    cost_value_loss      | 9.27e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00545      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.03e+03      |
|    mean_cost_advantages | 0.00033489248 |
|    mean_reward_advan... | -1.8833662    |
|    n_updates            | 3870          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.19e-05     |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 5.78e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 389           |
|    time_elapsed         | 3105          |
|    total_timesteps      | 3983360       |
| train/                  |               |
|    approx_kl            | 7.9720285e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0548       |
|    cost_value_loss      | 8.93e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00542      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.71e+03      |
|    mean_cost_advantages | 2.3077711e-05 |
|    mean_reward_advan... | -2.5898998    |
|    n_updates            | 3880          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.66e-06     |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 6.25e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 390           |
|    time_elapsed         | 3113          |
|    total_timesteps      | 3993600       |
| train/                  |               |
|    approx_kl            | 2.2931607e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0346        |
|    cost_value_loss      | 3.13e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00564      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.23e+03      |
|    mean_cost_advantages | 0.00011300484 |
|    mean_reward_advan... | -2.2098289    |
|    n_updates            | 3890          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -7.68e-06     |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 5.66e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 873           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 391           |
|    time_elapsed         | 3121          |
|    total_timesteps      | 4003840       |
| train/                  |               |
|    approx_kl            | 0.00010917806 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000186      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0995        |
|    cost_value_loss      | 9.12e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00609      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.68e+03      |
|    mean_cost_advantages | -5.223468e-05 |
|    mean_reward_advan... | -1.7442415    |
|    n_updates            | 3900          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.23e-05     |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 6.46e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 392           |
|    time_elapsed         | 3129          |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 2.5501777e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 1.95e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.046        |
|    cost_value_loss      | 9.05e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0057       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.04e+03      |
|    mean_cost_advantages | 0.00033338118 |
|    mean_reward_advan... | -5.495763     |
|    n_updates            | 3910          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 4.34e-06      |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 5.83e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 858           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 393           |
|    time_elapsed         | 3138          |
|    total_timesteps      | 4024320       |
| train/                  |               |
|    approx_kl            | 3.0352501e-05 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.89         |
|    cost_value_loss      | 0.00768       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00553      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.66e+03      |
|    mean_cost_advantages | 0.002698161   |
|    mean_reward_advan... | -4.9074907    |
|    n_updates            | 3920          |
|    nu                   | 3.14          |
|    nu_loss              | -0.00153      |
|    policy_gradient_loss | -2.29e-05     |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 6.07e+03      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 875           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 394           |
|    time_elapsed         | 3146          |
|    total_timesteps      | 4034560       |
| train/                  |               |
|    approx_kl            | 4.4621876e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0956        |
|    cost_value_loss      | 9.89e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00577      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.4e+03       |
|    mean_cost_advantages | 0.0004204535  |
|    mean_reward_advan... | -4.3042135    |
|    n_updates            | 3930          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 5.26e-07      |
|    reward_explained_... | 0.929         |
|    reward_value_loss    | 7.15e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000293       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 868            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1282           |
|    iterations           | 395            |
|    time_elapsed         | 3154           |
|    total_timesteps      | 4044800        |
| train/                  |                |
|    approx_kl            | -1.6431557e-06 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0492        |
|    cost_value_loss      | 8.8e-06        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00513       |
|    learning_rate        | 0.0003         |
|    loss                 | 1.83e+03       |
|    mean_cost_advantages | 8.857105e-05   |
|    mean_reward_advan... | 6.9286184      |
|    n_updates            | 3940           |
|    nu                   | 3.14           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.83e-06      |
|    reward_explained_... | 0.95           |
|    reward_value_loss    | 5.2e+03        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 396           |
|    time_elapsed         | 3162          |
|    total_timesteps      | 4055040       |
| train/                  |               |
|    approx_kl            | 5.632173e-07  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.52         |
|    cost_value_loss      | 0.00564       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00507      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.61e+03      |
|    mean_cost_advantages | 0.0023809741  |
|    mean_reward_advan... | 0.19444099    |
|    n_updates            | 3950          |
|    nu                   | 3.14          |
|    nu_loss              | -0.00092      |
|    policy_gradient_loss | -7.65e-06     |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 5.24e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000293       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 873            |
|    ep_len_mean          | 11.2           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1282           |
|    iterations           | 397            |
|    time_elapsed         | 3170           |
|    total_timesteps      | 4065280        |
| train/                  |                |
|    approx_kl            | 9.830729e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000127       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0453        |
|    cost_value_loss      | 8.96e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00515       |
|    learning_rate        | 0.0003         |
|    loss                 | 4.9e+03        |
|    mean_cost_advantages | -0.00011874436 |
|    mean_reward_advan... | -2.9131212     |
|    n_updates            | 3960           |
|    nu                   | 3.14           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.35e-05      |
|    reward_explained_... | 0.939          |
|    reward_value_loss    | 6.31e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 398           |
|    time_elapsed         | 3178          |
|    total_timesteps      | 4075520       |
| train/                  |               |
|    approx_kl            | 3.27596e-05   |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 9.77e-06      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.62         |
|    cost_value_loss      | 0.00177       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00514      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.89e+03      |
|    mean_cost_advantages | 0.0006306319  |
|    mean_reward_advan... | 2.3755455     |
|    n_updates            | 3970          |
|    nu                   | 3.14          |
|    nu_loss              | -0.00092      |
|    policy_gradient_loss | -2.06e-05     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.19e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1282          |
|    iterations           | 399           |
|    time_elapsed         | 3186          |
|    total_timesteps      | 4085760       |
| train/                  |               |
|    approx_kl            | 1.6405247e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 9.77e-06      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0402       |
|    cost_value_loss      | 1.04e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00529      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.54e+03      |
|    mean_cost_advantages | -9.423371e-05 |
|    mean_reward_advan... | 2.240786      |
|    n_updates            | 3980          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.58e-06      |
|    reward_explained_... | 0.948         |
|    reward_value_loss    | 5.33e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 400           |
|    time_elapsed         | 3195          |
|    total_timesteps      | 4096000       |
| train/                  |               |
|    approx_kl            | 4.095435e-05  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 9.77e-06      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.25         |
|    cost_value_loss      | 0.00632       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00604      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.35e+03      |
|    mean_cost_advantages | 0.0026703365  |
|    mean_reward_advan... | -2.1257374    |
|    n_updates            | 3990          |
|    nu                   | 3.14          |
|    nu_loss              | -0.00153      |
|    policy_gradient_loss | 4.58e-06      |
|    reward_explained_... | 0.932         |
|    reward_value_loss    | 6.98e+03      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 401           |
|    time_elapsed         | 3203          |
|    total_timesteps      | 4106240       |
| train/                  |               |
|    approx_kl            | 8.4999556e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0428       |
|    cost_value_loss      | 0.000109      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00567      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.63e+03      |
|    mean_cost_advantages | -0.0032767765 |
|    mean_reward_advan... | 2.1588304     |
|    n_updates            | 4000          |
|    nu                   | 3.14          |
|    nu_loss              | -0.000307     |
|    policy_gradient_loss | -6.78e-05     |
|    reward_explained_... | 0.937         |
|    reward_value_loss    | 6.92e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 13            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 402           |
|    time_elapsed         | 3211          |
|    total_timesteps      | 4116480       |
| train/                  |               |
|    approx_kl            | 9.731781e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 6.84e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0358       |
|    cost_value_loss      | 8.77e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00524      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.61e+03      |
|    mean_cost_advantages | -0.0006498351 |
|    mean_reward_advan... | -0.032927893  |
|    n_updates            | 4010          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2e-05        |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 5.72e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 869            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1281           |
|    iterations           | 403            |
|    time_elapsed         | 3219           |
|    total_timesteps      | 4126720        |
| train/                  |                |
|    approx_kl            | -1.1111517e-05 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0454        |
|    cost_value_loss      | 8.86e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00535       |
|    learning_rate        | 0.0003         |
|    loss                 | 1.35e+03       |
|    mean_cost_advantages | -8.637906e-05  |
|    mean_reward_advan... | 5.944449       |
|    n_updates            | 4020           |
|    nu                   | 3.14           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 4.57e-07       |
|    reward_explained_... | 0.955          |
|    reward_value_loss    | 4.86e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 861           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 404           |
|    time_elapsed         | 3227          |
|    total_timesteps      | 4136960       |
| train/                  |               |
|    approx_kl            | -7.695378e-06 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -16.3         |
|    cost_value_loss      | 0.000462      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00533      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.46e+03      |
|    mean_cost_advantages | 0.0005015334  |
|    mean_reward_advan... | -1.1651185    |
|    n_updates            | 4030          |
|    nu                   | 3.14          |
|    nu_loss              | -0.000307     |
|    policy_gradient_loss | 2.7e-06       |
|    reward_explained_... | 0.947         |
|    reward_value_loss    | 5.57e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 405           |
|    time_elapsed         | 3235          |
|    total_timesteps      | 4147200       |
| train/                  |               |
|    approx_kl            | 8.490544e-05  |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 7.81e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -45           |
|    cost_value_loss      | 0.00045       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00549      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.62e+03      |
|    mean_cost_advantages | 0.00035911036 |
|    mean_reward_advan... | 1.5885332     |
|    n_updates            | 4040          |
|    nu                   | 3.14          |
|    nu_loss              | -0.000307     |
|    policy_gradient_loss | -1.56e-05     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.19e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 406           |
|    time_elapsed         | 3244          |
|    total_timesteps      | 4157440       |
| train/                  |               |
|    approx_kl            | 8.4126367e-07 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.125        |
|    cost_value_loss      | 0.00208       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00561      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.54e+03      |
|    mean_cost_advantages | 0.00068618025 |
|    mean_reward_advan... | -1.0696607    |
|    n_updates            | 4050          |
|    nu                   | 3.14          |
|    nu_loss              | -0.000614     |
|    policy_gradient_loss | -1.11e-05     |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 4.65e+03      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 874           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 407           |
|    time_elapsed         | 3252          |
|    total_timesteps      | 4167680       |
| train/                  |               |
|    approx_kl            | 2.4598558e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 8.79e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.000186      |
|    cost_value_loss      | 3e-05         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00549      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.11e+03      |
|    mean_cost_advantages | -0.0017488949 |
|    mean_reward_advan... | 0.2029089     |
|    n_updates            | 4060          |
|    nu                   | 3.14          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.84e-05     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.14e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 873            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1281           |
|    iterations           | 408            |
|    time_elapsed         | 3260           |
|    total_timesteps      | 4177920        |
| train/                  |                |
|    approx_kl            | 0.00012520133  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000508       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0364        |
|    cost_value_loss      | 1.09e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00509       |
|    learning_rate        | 0.0003         |
|    loss                 | 1.38e+03       |
|    mean_cost_advantages | -0.00043845727 |
|    mean_reward_advan... | 0.52096057     |
|    n_updates            | 4070           |
|    nu                   | 3.15           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -8.08e-05      |
|    reward_explained_... | 0.95           |
|    reward_value_loss    | 5.16e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 860           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 409           |
|    time_elapsed         | 3268          |
|    total_timesteps      | 4188160       |
| train/                  |               |
|    approx_kl            | -3.509922e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0497       |
|    cost_value_loss      | 8.63e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00482      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.24e+03      |
|    mean_cost_advantages | 2.751236e-05  |
|    mean_reward_advan... | -1.1411022    |
|    n_updates            | 4080          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.28e-06     |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 5.13e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 875           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 410           |
|    time_elapsed         | 3276          |
|    total_timesteps      | 4198400       |
| train/                  |               |
|    approx_kl            | 8.276291e-05  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.91         |
|    cost_value_loss      | 0.00502       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00489      |
|    learning_rate        | 0.0003        |
|    loss                 | 816           |
|    mean_cost_advantages | 0.0021764252  |
|    mean_reward_advan... | -6.5965347    |
|    n_updates            | 4090          |
|    nu                   | 3.15          |
|    nu_loss              | -0.000922     |
|    policy_gradient_loss | -3.66e-05     |
|    reward_explained_... | 0.93          |
|    reward_value_loss    | 7.21e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 411           |
|    time_elapsed         | 3284          |
|    total_timesteps      | 4208640       |
| train/                  |               |
|    approx_kl            | -8.217525e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.044        |
|    cost_value_loss      | 8.89e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00502      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.24e+03      |
|    mean_cost_advantages | 8.902584e-05  |
|    mean_reward_advan... | 3.0563982     |
|    n_updates            | 4100          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.84e-06     |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 5.19e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 412           |
|    time_elapsed         | 3292          |
|    total_timesteps      | 4218880       |
| train/                  |               |
|    approx_kl            | 3.910195e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000205      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0538       |
|    cost_value_loss      | 8.55e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00456      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.86e+03      |
|    mean_cost_advantages | -9.079951e-05 |
|    mean_reward_advan... | -7.2467437    |
|    n_updates            | 4110          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.59e-05     |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 4.84e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 413           |
|    time_elapsed         | 3301          |
|    total_timesteps      | 4229120       |
| train/                  |               |
|    approx_kl            | 6.1039274e-05 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 3.91e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.54         |
|    cost_value_loss      | 0.00362       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00482      |
|    learning_rate        | 0.0003        |
|    loss                 | 6.67e+03      |
|    mean_cost_advantages | 0.0017876292  |
|    mean_reward_advan... | -4.331734     |
|    n_updates            | 4120          |
|    nu                   | 3.15          |
|    nu_loss              | -0.00154      |
|    policy_gradient_loss | -3.16e-05     |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 5.08e+03      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1281          |
|    iterations           | 414           |
|    time_elapsed         | 3309          |
|    total_timesteps      | 4239360       |
| train/                  |               |
|    approx_kl            | 5.0328952e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0224       |
|    cost_value_loss      | 9.23e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00481      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.75e+03      |
|    mean_cost_advantages | -0.0012052724 |
|    mean_reward_advan... | 4.292142      |
|    n_updates            | 4130          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.11e-05     |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 5.87e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 415           |
|    time_elapsed         | 3317          |
|    total_timesteps      | 4249600       |
| train/                  |               |
|    approx_kl            | 2.4163606e-05 |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 6.84e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -13.7         |
|    cost_value_loss      | 0.001         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00524      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.56e+03      |
|    mean_cost_advantages | 0.0011326396  |
|    mean_reward_advan... | -1.8917106    |
|    n_updates            | 4140          |
|    nu                   | 3.15          |
|    nu_loss              | -0.000615     |
|    policy_gradient_loss | 2.6e-06       |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 6.11e+03      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 416           |
|    time_elapsed         | 3325          |
|    total_timesteps      | 4259840       |
| train/                  |               |
|    approx_kl            | 1.6790302e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00773      |
|    cost_value_loss      | 9.03e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00471      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.13e+03      |
|    mean_cost_advantages | -0.001458914  |
|    mean_reward_advan... | 4.7064366     |
|    n_updates            | 4150          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.44e-06     |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 5.25e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 417           |
|    time_elapsed         | 3333          |
|    total_timesteps      | 4270080       |
| train/                  |               |
|    approx_kl            | 3.2547698e-05 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 3.91e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.58         |
|    cost_value_loss      | 0.00352       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0048       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.68e+03      |
|    mean_cost_advantages | 0.00114829    |
|    mean_reward_advan... | 2.6914752     |
|    n_updates            | 4160          |
|    nu                   | 3.15          |
|    nu_loss              | -0.00184      |
|    policy_gradient_loss | -6.51e-05     |
|    reward_explained_... | 0.948         |
|    reward_value_loss    | 5.49e+03      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 418           |
|    time_elapsed         | 3341          |
|    total_timesteps      | 4280320       |
| train/                  |               |
|    approx_kl            | 1.4128117e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.93         |
|    cost_value_loss      | 0.00543       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00467      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.29e+03      |
|    mean_cost_advantages | 0.0021432624  |
|    mean_reward_advan... | -2.3299716    |
|    n_updates            | 4170          |
|    nu                   | 3.15          |
|    nu_loss              | -0.000923     |
|    policy_gradient_loss | -1.37e-05     |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 5.22e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 419           |
|    time_elapsed         | 3350          |
|    total_timesteps      | 4290560       |
| train/                  |               |
|    approx_kl            | -4.550442e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0513       |
|    cost_value_loss      | 1.08e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00463      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.72e+03      |
|    mean_cost_advantages | 0.0004425954  |
|    mean_reward_advan... | -2.3295617    |
|    n_updates            | 4180          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.76e-06     |
|    reward_explained_... | 0.943         |
|    reward_value_loss    | 6.01e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 420           |
|    time_elapsed         | 3358          |
|    total_timesteps      | 4300800       |
| train/                  |               |
|    approx_kl            | 0.00013187625 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000352      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0522       |
|    cost_value_loss      | 9.11e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00459      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.46e+03      |
|    mean_cost_advantages | 0.00010832512 |
|    mean_reward_advan... | -0.73682      |
|    n_updates            | 4190          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -7.34e-05     |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 5.85e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 421           |
|    time_elapsed         | 3366          |
|    total_timesteps      | 4311040       |
| train/                  |               |
|    approx_kl            | 8.850484e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0226        |
|    cost_value_loss      | 4.45e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00472      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.91e+03      |
|    mean_cost_advantages | -8.536682e-05 |
|    mean_reward_advan... | -3.8063717    |
|    n_updates            | 4200          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.94e-05     |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 5.55e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 868            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1280           |
|    iterations           | 422            |
|    time_elapsed         | 3374           |
|    total_timesteps      | 4321280        |
| train/                  |                |
|    approx_kl            | 1.1661393e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0182        |
|    cost_value_loss      | 1.22e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00478       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.58e+03       |
|    mean_cost_advantages | 0.000117450254 |
|    mean_reward_advan... | -1.9420369     |
|    n_updates            | 4210           |
|    nu                   | 3.15           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -6.48e-06      |
|    reward_explained_... | 0.944          |
|    reward_value_loss    | 5.85e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11           |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 866          |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1280         |
|    iterations           | 423          |
|    time_elapsed         | 3382         |
|    total_timesteps      | 4331520      |
| train/                  |              |
|    approx_kl            | 4.979805e-06 |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -34.3        |
|    cost_value_loss      | 0.000338     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0045      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.62e+03     |
|    mean_cost_advantages | 0.0005828079 |
|    mean_reward_advan... | -1.0078506   |
|    n_updates            | 4220         |
|    nu                   | 3.15         |
|    nu_loss              | -0.000308    |
|    policy_gradient_loss | -4.19e-07    |
|    reward_explained_... | 0.948        |
|    reward_value_loss    | 5.52e+03     |
|    total_cost           | 1.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 870          |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1280         |
|    iterations           | 424          |
|    time_elapsed         | 3390         |
|    total_timesteps      | 4341760      |
| train/                  |              |
|    approx_kl            | 4.258356e-05 |
|    average_cost         | 0.0          |
|    clip_fraction        | 6.84e-05     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0475      |
|    cost_value_loss      | 8.74e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0046      |
|    learning_rate        | 0.0003       |
|    loss                 | 4.16e+03     |
|    mean_cost_advantages | -0.00106787  |
|    mean_reward_advan... | -5.0268707   |
|    n_updates            | 4230         |
|    nu                   | 3.15         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -7.86e-06    |
|    reward_explained_... | 0.95         |
|    reward_value_loss    | 5.06e+03     |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 867            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1280           |
|    iterations           | 425            |
|    time_elapsed         | 3398           |
|    total_timesteps      | 4352000        |
| train/                  |                |
|    approx_kl            | 3.9898907e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000254       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0456        |
|    cost_value_loss      | 9.8e-06        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00447       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.22e+03       |
|    mean_cost_advantages | -0.00020201303 |
|    mean_reward_advan... | -1.2986443     |
|    n_updates            | 4240           |
|    nu                   | 3.15           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.75e-05      |
|    reward_explained_... | 0.948          |
|    reward_value_loss    | 5.41e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 871           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 426           |
|    time_elapsed         | 3407          |
|    total_timesteps      | 4362240       |
| train/                  |               |
|    approx_kl            | -9.108032e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00289      |
|    cost_value_loss      | 2.8e-05       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00448      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.31e+03      |
|    mean_cost_advantages | -5.366208e-07 |
|    mean_reward_advan... | -4.3032312    |
|    n_updates            | 4250          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 4.57e-06      |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 5.64e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 427           |
|    time_elapsed         | 3415          |
|    total_timesteps      | 4372480       |
| train/                  |               |
|    approx_kl            | 2.2312627e-06 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.56         |
|    cost_value_loss      | 0.00176       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00411      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.09e+03      |
|    mean_cost_advantages | 0.0007694542  |
|    mean_reward_advan... | 2.1874466     |
|    n_updates            | 4260          |
|    nu                   | 3.15          |
|    nu_loss              | -0.000924     |
|    policy_gradient_loss | -1.81e-05     |
|    reward_explained_... | 0.948         |
|    reward_value_loss    | 5.46e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 428           |
|    time_elapsed         | 3423          |
|    total_timesteps      | 4382720       |
| train/                  |               |
|    approx_kl            | 2.334034e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 1.95e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0452       |
|    cost_value_loss      | 8.75e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00416      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.38e+03      |
|    mean_cost_advantages | -0.0003293025 |
|    mean_reward_advan... | 0.8064526     |
|    n_updates            | 4270          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.41e-06     |
|    reward_explained_... | 0.949         |
|    reward_value_loss    | 5.35e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 429           |
|    time_elapsed         | 3431          |
|    total_timesteps      | 4392960       |
| train/                  |               |
|    approx_kl            | 4.2649823e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0516       |
|    cost_value_loss      | 9.71e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00401      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.6e+03       |
|    mean_cost_advantages | 8.039535e-06  |
|    mean_reward_advan... | 0.5569271     |
|    n_updates            | 4280          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.45e-05     |
|    reward_explained_... | 0.947         |
|    reward_value_loss    | 5.46e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 430           |
|    time_elapsed         | 3439          |
|    total_timesteps      | 4403200       |
| train/                  |               |
|    approx_kl            | 4.0981454e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0483       |
|    cost_value_loss      | 8.54e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00389      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.63e+03      |
|    mean_cost_advantages | 4.2658405e-05 |
|    mean_reward_advan... | -2.15121      |
|    n_updates            | 4290          |
|    nu                   | 3.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 8.54e-08      |
|    reward_explained_... | 0.942         |
|    reward_value_loss    | 6.11e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1280          |
|    iterations           | 431           |
|    time_elapsed         | 3447          |
|    total_timesteps      | 4413440       |
| train/                  |               |
|    approx_kl            | 1.6553608e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.54         |
|    cost_value_loss      | 0.00176       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00395      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.17e+03      |
|    mean_cost_advantages | 0.00081677455 |
|    mean_reward_advan... | -2.5598192    |
|    n_updates            | 4300          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000924     |
|    policy_gradient_loss | -1.68e-05     |
|    reward_explained_... | 0.943         |
|    reward_value_loss    | 5.86e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 870            |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1280           |
|    iterations           | 432            |
|    time_elapsed         | 3455           |
|    total_timesteps      | 4423680        |
| train/                  |                |
|    approx_kl            | 0.000116214665 |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0.000137       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -16.5          |
|    cost_value_loss      | 0.000516       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00439       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.73e+03       |
|    mean_cost_advantages | -0.0033437647  |
|    mean_reward_advan... | -4.3037105     |
|    n_updates            | 4310           |
|    nu                   | 3.16           |
|    nu_loss              | -0.000308      |
|    policy_gradient_loss | -4.77e-05      |
|    reward_explained_... | 0.927          |
|    reward_value_loss    | 6.62e+03       |
|    total_cost           | 1.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 433           |
|    time_elapsed         | 3464          |
|    total_timesteps      | 4433920       |
| train/                  |               |
|    approx_kl            | 3.522262e-06  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0129       |
|    cost_value_loss      | 8.92e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00414      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.69e+03      |
|    mean_cost_advantages | -0.0013925771 |
|    mean_reward_advan... | -0.48301974   |
|    n_updates            | 4320          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.61e-06     |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 5.19e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 861            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 434            |
|    time_elapsed         | 3472           |
|    total_timesteps      | 4444160        |
| train/                  |                |
|    approx_kl            | 2.5855983e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 2.93e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0416        |
|    cost_value_loss      | 8.99e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00403       |
|    learning_rate        | 0.0003         |
|    loss                 | 4.41e+03       |
|    mean_cost_advantages | -0.00024012578 |
|    mean_reward_advan... | -3.2358906     |
|    n_updates            | 4330           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.21e-06      |
|    reward_explained_... | 0.945          |
|    reward_value_loss    | 5.62e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 867            |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 435            |
|    time_elapsed         | 3480           |
|    total_timesteps      | 4454400        |
| train/                  |                |
|    approx_kl            | -8.6962245e-08 |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -9.33          |
|    cost_value_loss      | 0.000454       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00402       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.01e+03       |
|    mean_cost_advantages | 0.0005863101   |
|    mean_reward_advan... | -3.6160474     |
|    n_updates            | 4340           |
|    nu                   | 3.16           |
|    nu_loss              | -0.000308      |
|    policy_gradient_loss | -2.11e-06      |
|    reward_explained_... | 0.922          |
|    reward_value_loss    | 8.05e+03       |
|    total_cost           | 1.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 863            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 436            |
|    time_elapsed         | 3488           |
|    total_timesteps      | 4464640        |
| train/                  |                |
|    approx_kl            | 2.8995564e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0384        |
|    cost_value_loss      | 8.89e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00376       |
|    learning_rate        | 0.0003         |
|    loss                 | 4.51e+03       |
|    mean_cost_advantages | -0.00053788425 |
|    mean_reward_advan... | 5.6629705      |
|    n_updates            | 4350           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -8.91e-06      |
|    reward_explained_... | 0.953          |
|    reward_value_loss    | 5e+03          |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 866            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 437            |
|    time_elapsed         | 3496           |
|    total_timesteps      | 4474880        |
| train/                  |                |
|    approx_kl            | 2.204054e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 9.77e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0446        |
|    cost_value_loss      | 1.01e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00379       |
|    learning_rate        | 0.0003         |
|    loss                 | 4.41e+03       |
|    mean_cost_advantages | -0.00019155737 |
|    mean_reward_advan... | -0.9892271     |
|    n_updates            | 4360           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -2.07e-05      |
|    reward_explained_... | 0.945          |
|    reward_value_loss    | 5.77e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 864            |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 438            |
|    time_elapsed         | 3504           |
|    total_timesteps      | 4485120        |
| train/                  |                |
|    approx_kl            | 5.3897686e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000127       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0475        |
|    cost_value_loss      | 8.65e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00379       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.21e+03       |
|    mean_cost_advantages | -0.00015552624 |
|    mean_reward_advan... | 5.4815536      |
|    n_updates            | 4370           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -2.16e-05      |
|    reward_explained_... | 0.943          |
|    reward_value_loss    | 6.03e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 864           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 439           |
|    time_elapsed         | 3512          |
|    total_timesteps      | 4495360       |
| train/                  |               |
|    approx_kl            | 3.2403135e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000107      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0479       |
|    cost_value_loss      | 8.62e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00376      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.12e+03      |
|    mean_cost_advantages | -0.0001143859 |
|    mean_reward_advan... | 0.48016423    |
|    n_updates            | 4380          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.06e-05     |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 5.27e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0.000293       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 860            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 440            |
|    time_elapsed         | 3520           |
|    total_timesteps      | 4505600        |
| train/                  |                |
|    approx_kl            | 6.103036e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 9.77e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0502        |
|    cost_value_loss      | 8.77e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00369       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.42e+03       |
|    mean_cost_advantages | -5.5032102e-05 |
|    mean_reward_advan... | -1.3622663     |
|    n_updates            | 4390           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.31e-05      |
|    reward_explained_... | 0.949          |
|    reward_value_loss    | 5.31e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 441           |
|    time_elapsed         | 3529          |
|    total_timesteps      | 4515840       |
| train/                  |               |
|    approx_kl            | 7.802043e-05  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.5          |
|    cost_value_loss      | 0.00473       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00373      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.77e+03      |
|    mean_cost_advantages | 0.0018716983  |
|    mean_reward_advan... | 0.123487666   |
|    n_updates            | 4400          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000925     |
|    policy_gradient_loss | -7.93e-05     |
|    reward_explained_... | 0.948         |
|    reward_value_loss    | 5.58e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 442           |
|    time_elapsed         | 3537          |
|    total_timesteps      | 4526080       |
| train/                  |               |
|    approx_kl            | 3.0717998e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 1.95e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -42.6         |
|    cost_value_loss      | 0.000397      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00356      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.21e+03      |
|    mean_cost_advantages | -0.0031808722 |
|    mean_reward_advan... | 1.2932947     |
|    n_updates            | 4410          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000308     |
|    policy_gradient_loss | -8.86e-06     |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 5.04e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 865            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 443            |
|    time_elapsed         | 3545           |
|    total_timesteps      | 4536320        |
| train/                  |                |
|    approx_kl            | 0.000103356455 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000195       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0311        |
|    cost_value_loss      | 8.65e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00356       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.7e+03        |
|    mean_cost_advantages | -0.00055291195 |
|    mean_reward_advan... | -7.0358634     |
|    n_updates            | 4420           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.69e-05      |
|    reward_explained_... | 0.943          |
|    reward_value_loss    | 5.82e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 871           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 444           |
|    time_elapsed         | 3553          |
|    total_timesteps      | 4546560       |
| train/                  |               |
|    approx_kl            | 5.6616727e-06 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -12.7         |
|    cost_value_loss      | 0.000494      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00399      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.05e+03      |
|    mean_cost_advantages | 0.00042219897 |
|    mean_reward_advan... | -0.43191868   |
|    n_updates            | 4430          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000308     |
|    policy_gradient_loss | 4.54e-06      |
|    reward_explained_... | 0.938         |
|    reward_value_loss    | 5.52e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 445           |
|    time_elapsed         | 3561          |
|    total_timesteps      | 4556800       |
| train/                  |               |
|    approx_kl            | 6.477218e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0485       |
|    cost_value_loss      | 8.55e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00356      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.02e+03      |
|    mean_cost_advantages | -9.561459e-05 |
|    mean_reward_advan... | -0.44040173   |
|    n_updates            | 4440          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.6e-05      |
|    reward_explained_... | 0.949         |
|    reward_value_loss    | 5.29e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 874           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 446           |
|    time_elapsed         | 3569          |
|    total_timesteps      | 4567040       |
| train/                  |               |
|    approx_kl            | 2.375748e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000166      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0503       |
|    cost_value_loss      | 8.59e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00352      |
|    learning_rate        | 0.0003        |
|    loss                 | 8.74e+03      |
|    mean_cost_advantages | 3.1317893e-05 |
|    mean_reward_advan... | -6.0665383    |
|    n_updates            | 4450          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.39e-05     |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 5.11e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 447           |
|    time_elapsed         | 3577          |
|    total_timesteps      | 4577280       |
| train/                  |               |
|    approx_kl            | 2.2654654e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0499       |
|    cost_value_loss      | 8.46e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0034       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.31e+03      |
|    mean_cost_advantages | -6.780933e-05 |
|    mean_reward_advan... | 0.8104397     |
|    n_updates            | 4460          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.21e-05     |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 5.02e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1279          |
|    iterations           | 448           |
|    time_elapsed         | 3586          |
|    total_timesteps      | 4587520       |
| train/                  |               |
|    approx_kl            | 6.576441e-06  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0515       |
|    cost_value_loss      | 8.39e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00337      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.93e+03      |
|    mean_cost_advantages | 0.00011090882 |
|    mean_reward_advan... | -6.698152     |
|    n_updates            | 4470          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.37e-06     |
|    reward_explained_... | 0.939         |
|    reward_value_loss    | 6.28e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 874            |
|    ep_len_mean          | 11.3           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 449            |
|    time_elapsed         | 3594           |
|    total_timesteps      | 4597760        |
| train/                  |                |
|    approx_kl            | 4.005199e-06   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0251         |
|    cost_value_loss      | 4.5e-05        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00379       |
|    learning_rate        | 0.0003         |
|    loss                 | 5.9e+03        |
|    mean_cost_advantages | -0.00013174159 |
|    mean_reward_advan... | -4.3048835     |
|    n_updates            | 4480           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -6.6e-06       |
|    reward_explained_... | 0.934          |
|    reward_value_loss    | 6.38e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 859            |
|    ep_len_mean          | 11.9           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 450            |
|    time_elapsed         | 3602           |
|    total_timesteps      | 4608000        |
| train/                  |                |
|    approx_kl            | 7.391565e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00042        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0398        |
|    cost_value_loss      | 1.08e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00372       |
|    learning_rate        | 0.0003         |
|    loss                 | 1.44e+03       |
|    mean_cost_advantages | -0.00020384455 |
|    mean_reward_advan... | 1.6752446      |
|    n_updates            | 4490           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -6.85e-05      |
|    reward_explained_... | 0.946          |
|    reward_value_loss    | 5.33e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 870            |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 451            |
|    time_elapsed         | 3610           |
|    total_timesteps      | 4618240        |
| train/                  |                |
|    approx_kl            | 0.000108631    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000371       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0169        |
|    cost_value_loss      | 1.15e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00405       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.58e+03       |
|    mean_cost_advantages | -9.9179844e-05 |
|    mean_reward_advan... | -6.1817217     |
|    n_updates            | 4500           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.99e-05      |
|    reward_explained_... | 0.936          |
|    reward_value_loss    | 6.39e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 12             |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 873            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1279           |
|    iterations           | 452            |
|    time_elapsed         | 3618           |
|    total_timesteps      | 4628480        |
| train/                  |                |
|    approx_kl            | 5.5267545e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000264       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0477        |
|    cost_value_loss      | 9.55e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00368       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.36e+03       |
|    mean_cost_advantages | 0.000100992875 |
|    mean_reward_advan... | 4.199905       |
|    n_updates            | 4510           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.58e-05      |
|    reward_explained_... | 0.953          |
|    reward_value_loss    | 4.99e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 453           |
|    time_elapsed         | 3626          |
|    total_timesteps      | 4638720       |
| train/                  |               |
|    approx_kl            | 0.00013540075 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000381      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0374       |
|    cost_value_loss      | 1.07e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00358      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.09e+03      |
|    mean_cost_advantages | 3.8594655e-05 |
|    mean_reward_advan... | -0.5726043    |
|    n_updates            | 4520          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -9e-05        |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 4.59e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 873          |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1278         |
|    iterations           | 454          |
|    time_elapsed         | 3635         |
|    total_timesteps      | 4648960      |
| train/                  |              |
|    approx_kl            | 3.009825e-05 |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -20.9        |
|    cost_value_loss      | 0.00177      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.00382     |
|    learning_rate        | 0.0003       |
|    loss                 | 6.9e+03      |
|    mean_cost_advantages | 0.0015882648 |
|    mean_reward_advan... | 1.6372573    |
|    n_updates            | 4530         |
|    nu                   | 3.16         |
|    nu_loss              | -0.000617    |
|    policy_gradient_loss | -1.25e-05    |
|    reward_explained_... | 0.936        |
|    reward_value_loss    | 6.15e+03     |
|    total_cost           | 2.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 455           |
|    time_elapsed         | 3643          |
|    total_timesteps      | 4659200       |
| train/                  |               |
|    approx_kl            | 5.1542183e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0295       |
|    cost_value_loss      | 9.81e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00338      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.79e+03      |
|    mean_cost_advantages | -0.0010272439 |
|    mean_reward_advan... | 2.180445      |
|    n_updates            | 4540          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -7.69e-07     |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 5e+03         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 456           |
|    time_elapsed         | 3651          |
|    total_timesteps      | 4669440       |
| train/                  |               |
|    approx_kl            | 4.7551956e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 2.93e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.04         |
|    cost_value_loss      | 0.00423       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00353      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.62e+03      |
|    mean_cost_advantages | 0.0015305022  |
|    mean_reward_advan... | 0.82509583    |
|    n_updates            | 4550          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000926     |
|    policy_gradient_loss | -3.3e-05      |
|    reward_explained_... | 0.955         |
|    reward_value_loss    | 4.57e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 457           |
|    time_elapsed         | 3659          |
|    total_timesteps      | 4679680       |
| train/                  |               |
|    approx_kl            | 1.8922636e-05 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.05         |
|    cost_value_loss      | 0.00423       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00341      |
|    learning_rate        | 0.0003        |
|    loss                 | 866           |
|    mean_cost_advantages | 5.562189e-05  |
|    mean_reward_advan... | -2.4117317    |
|    n_updates            | 4560          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000926     |
|    policy_gradient_loss | -2e-05        |
|    reward_explained_... | 0.954         |
|    reward_value_loss    | 4.84e+03      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 458           |
|    time_elapsed         | 3667          |
|    total_timesteps      | 4689920       |
| train/                  |               |
|    approx_kl            | 1.2691645e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0138       |
|    cost_value_loss      | 8.28e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00322      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.52e+03      |
|    mean_cost_advantages | -0.0026973211 |
|    mean_reward_advan... | -0.18786898   |
|    n_updates            | 4570          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -5.48e-06     |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 5.82e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 861           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 459           |
|    time_elapsed         | 3675          |
|    total_timesteps      | 4700160       |
| train/                  |               |
|    approx_kl            | 0.00012315318 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000264      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0199       |
|    cost_value_loss      | 1.29e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00349      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.49e+03      |
|    mean_cost_advantages | -0.000456633  |
|    mean_reward_advan... | 0.22164908    |
|    n_updates            | 4580          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.36e-05     |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 5.68e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 863           |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 460           |
|    time_elapsed         | 3683          |
|    total_timesteps      | 4710400       |
| train/                  |               |
|    approx_kl            | 5.3784763e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000176      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0472       |
|    cost_value_loss      | 8.34e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00322      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.07e+03      |
|    mean_cost_advantages | 2.3503435e-05 |
|    mean_reward_advan... | -2.9323394    |
|    n_updates            | 4590          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.92e-05     |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 5.84e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 871           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 461           |
|    time_elapsed         | 3691          |
|    total_timesteps      | 4720640       |
| train/                  |               |
|    approx_kl            | 1.8763403e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 7.81e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0508       |
|    cost_value_loss      | 9.44e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00329      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.62e+03      |
|    mean_cost_advantages | -7.786454e-05 |
|    mean_reward_advan... | 4.1776037     |
|    n_updates            | 4600          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.87e-05     |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 5.85e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 874            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1278           |
|    iterations           | 462            |
|    time_elapsed         | 3700           |
|    total_timesteps      | 4730880        |
| train/                  |                |
|    approx_kl            | -6.4712947e-07 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0526        |
|    cost_value_loss      | 8.04e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0031        |
|    learning_rate        | 0.0003         |
|    loss                 | 3.99e+03       |
|    mean_cost_advantages | 0.00017965431  |
|    mean_reward_advan... | -1.160744      |
|    n_updates            | 4610           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 2.47e-08       |
|    reward_explained_... | 0.949          |
|    reward_value_loss    | 5.28e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 877            |
|    ep_len_mean          | 11.2           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1278           |
|    iterations           | 463            |
|    time_elapsed         | 3708           |
|    total_timesteps      | 4741120        |
| train/                  |                |
|    approx_kl            | 3.5285902e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 6.84e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0489        |
|    cost_value_loss      | 8.16e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00317       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.16e+03       |
|    mean_cost_advantages | 1.47034125e-05 |
|    mean_reward_advan... | -1.3074863     |
|    n_updates            | 4620           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.91e-05      |
|    reward_explained_... | 0.95           |
|    reward_value_loss    | 5.2e+03        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 871           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 464           |
|    time_elapsed         | 3716          |
|    total_timesteps      | 4751360       |
| train/                  |               |
|    approx_kl            | 5.0622923e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 2.93e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0511       |
|    cost_value_loss      | 8.1e-06       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00328      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.24e+03      |
|    mean_cost_advantages | -8.131268e-05 |
|    mean_reward_advan... | 5.473572      |
|    n_updates            | 4630          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.52e-05     |
|    reward_explained_... | 0.96          |
|    reward_value_loss    | 4.22e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 864            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1278           |
|    iterations           | 465            |
|    time_elapsed         | 3724           |
|    total_timesteps      | 4761600        |
| train/                  |                |
|    approx_kl            | 1.1134706e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 9.77e-06       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0497        |
|    cost_value_loss      | 8.02e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00321       |
|    learning_rate        | 0.0003         |
|    loss                 | 2.96e+03       |
|    mean_cost_advantages | -3.9654125e-05 |
|    mean_reward_advan... | -7.968052      |
|    n_updates            | 4640           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.01e-05      |
|    reward_explained_... | 0.94           |
|    reward_value_loss    | 5.94e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 873           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 466           |
|    time_elapsed         | 3732          |
|    total_timesteps      | 4771840       |
| train/                  |               |
|    approx_kl            | 5.9453887e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000264      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.45         |
|    cost_value_loss      | 0.000784      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00412      |
|    learning_rate        | 0.0003        |
|    loss                 | 5.33e+03      |
|    mean_cost_advantages | 0.0007826701  |
|    mean_reward_advan... | -4.6958985    |
|    n_updates            | 4650          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000309     |
|    policy_gradient_loss | -4.43e-05     |
|    reward_explained_... | 0.925         |
|    reward_value_loss    | 6.78e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 10.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 860            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1278           |
|    iterations           | 467            |
|    time_elapsed         | 3740           |
|    total_timesteps      | 4782080        |
| train/                  |                |
|    approx_kl            | 1.5058508e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0459        |
|    cost_value_loss      | 8.2e-06        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00339       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.74e+03       |
|    mean_cost_advantages | -3.4521603e-05 |
|    mean_reward_advan... | 0.7694721      |
|    n_updates            | 4660           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.84e-06      |
|    reward_explained_... | 0.95           |
|    reward_value_loss    | 5.33e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 468           |
|    time_elapsed         | 3749          |
|    total_timesteps      | 4792320       |
| train/                  |               |
|    approx_kl            | 2.1767988e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 8.79e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -8.92         |
|    cost_value_loss      | 0.000506      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00382      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.22e+03      |
|    mean_cost_advantages | 0.00041891233 |
|    mean_reward_advan... | -5.727463     |
|    n_updates            | 4670          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000309     |
|    policy_gradient_loss | 1.02e-05      |
|    reward_explained_... | 0.926         |
|    reward_value_loss    | 7.64e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 870          |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1278         |
|    iterations           | 469          |
|    time_elapsed         | 3757         |
|    total_timesteps      | 4802560      |
| train/                  |              |
|    approx_kl            | 6.796792e-06 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0538      |
|    cost_value_loss      | 8.32e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.00328     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.95e+03     |
|    mean_cost_advantages | 4.928622e-05 |
|    mean_reward_advan... | 2.0761602    |
|    n_updates            | 4680         |
|    nu                   | 3.16         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -3.06e-06    |
|    reward_explained_... | 0.949        |
|    reward_value_loss    | 5.4e+03      |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 860           |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 470           |
|    time_elapsed         | 3765          |
|    total_timesteps      | 4812800       |
| train/                  |               |
|    approx_kl            | -9.664334e-07 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0546       |
|    cost_value_loss      | 7.96e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00325      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.97e+03      |
|    mean_cost_advantages | 4.5717046e-05 |
|    mean_reward_advan... | -0.10616579   |
|    n_updates            | 4690          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.4e-08       |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 5.73e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 471           |
|    time_elapsed         | 3773          |
|    total_timesteps      | 4823040       |
| train/                  |               |
|    approx_kl            | 1.5913229e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0502       |
|    cost_value_loss      | 7.91e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00334      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.9e+03       |
|    mean_cost_advantages | 1.4322269e-05 |
|    mean_reward_advan... | -3.9276986    |
|    n_updates            | 4700          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.9e-06      |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.03e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 472           |
|    time_elapsed         | 3781          |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 3.3531433e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000107      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -50.7         |
|    cost_value_loss      | 0.000396      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00324      |
|    learning_rate        | 0.0003        |
|    loss                 | 6.13e+03      |
|    mean_cost_advantages | 0.00055572396 |
|    mean_reward_advan... | 1.151118      |
|    n_updates            | 4710          |
|    nu                   | 3.16          |
|    nu_loss              | -0.000309     |
|    policy_gradient_loss | -3.3e-05      |
|    reward_explained_... | 0.947         |
|    reward_value_loss    | 5.56e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | 1e+04          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 865            |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1278           |
|    iterations           | 473            |
|    time_elapsed         | 3789           |
|    total_timesteps      | 4843520        |
| train/                  |                |
|    approx_kl            | 4.179473e-06   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0715         |
|    cost_value_loss      | 1.52e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00327       |
|    learning_rate        | 0.0003         |
|    loss                 | 5.36e+03       |
|    mean_cost_advantages | -9.2145594e-05 |
|    mean_reward_advan... | 0.73125637     |
|    n_updates            | 4720           |
|    nu                   | 3.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.31e-06      |
|    reward_explained_... | 0.946          |
|    reward_value_loss    | 5.55e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 867           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 474           |
|    time_elapsed         | 3797          |
|    total_timesteps      | 4853760       |
| train/                  |               |
|    approx_kl            | 5.1215757e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0504       |
|    cost_value_loss      | 7.87e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00301      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.66e+03      |
|    mean_cost_advantages | 6.6837296e-05 |
|    mean_reward_advan... | -2.1086888    |
|    n_updates            | 4730          |
|    nu                   | 3.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.78e-06     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.3e+03       |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1278          |
|    iterations           | 475           |
|    time_elapsed         | 3805          |
|    total_timesteps      | 4864000       |
| train/                  |               |
|    approx_kl            | 2.4834018e-05 |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.51         |
|    cost_value_loss      | 0.00611       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00312      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.98e+03      |
|    mean_cost_advantages | 0.0020510424  |
|    mean_reward_advan... | -3.4098008    |
|    n_updates            | 4740          |
|    nu                   | 3.17          |
|    nu_loss              | -0.00155      |
|    policy_gradient_loss | -1.77e-05     |
|    reward_explained_... | 0.943         |
|    reward_value_loss    | 5.96e+03      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 865           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 476           |
|    time_elapsed         | 3814          |
|    total_timesteps      | 4874240       |
| train/                  |               |
|    approx_kl            | 4.7526555e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000127      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -38           |
|    cost_value_loss      | 0.000396      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00298      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.08e+03      |
|    mean_cost_advantages | -0.0044354955 |
|    mean_reward_advan... | 1.0116452     |
|    n_updates            | 4750          |
|    nu                   | 3.17          |
|    nu_loss              | -0.000309     |
|    policy_gradient_loss | -3.25e-05     |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 5.88e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 859           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 477           |
|    time_elapsed         | 3822          |
|    total_timesteps      | 4884480       |
| train/                  |               |
|    approx_kl            | 9.752344e-06  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0166        |
|    cost_value_loss      | 8.21e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00296      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.48e+03      |
|    mean_cost_advantages | -0.0018815389 |
|    mean_reward_advan... | 0.6650802     |
|    n_updates            | 4760          |
|    nu                   | 3.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.12e-06     |
|    reward_explained_... | 0.943         |
|    reward_value_loss    | 6.1e+03       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 873            |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1277           |
|    iterations           | 478            |
|    time_elapsed         | 3830           |
|    total_timesteps      | 4894720        |
| train/                  |                |
|    approx_kl            | 1.3515819e-05  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0413        |
|    cost_value_loss      | 7.92e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00288       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.1e+03        |
|    mean_cost_advantages | -0.00029776082 |
|    mean_reward_advan... | -8.036667      |
|    n_updates            | 4770           |
|    nu                   | 3.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.72e-06      |
|    reward_explained_... | 0.937          |
|    reward_value_loss    | 6.43e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11            |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 479           |
|    time_elapsed         | 3838          |
|    total_timesteps      | 4904960       |
| train/                  |               |
|    approx_kl            | 3.7343336e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 2.93e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -74.4         |
|    cost_value_loss      | 0.000572      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00291      |
|    learning_rate        | 0.0003        |
|    loss                 | 4.91e+03      |
|    mean_cost_advantages | 0.00063769997 |
|    mean_reward_advan... | 8.679143      |
|    n_updates            | 4780          |
|    nu                   | 3.17          |
|    nu_loss              | -0.000309     |
|    policy_gradient_loss | -1.69e-05     |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 5.05e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 868           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 480           |
|    time_elapsed         | 3846          |
|    total_timesteps      | 4915200       |
| train/                  |               |
|    approx_kl            | 8.064229e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 7.81e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0323       |
|    cost_value_loss      | 9.28e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00301      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.4e+03       |
|    mean_cost_advantages | -6.642281e-05 |
|    mean_reward_advan... | -5.94601      |
|    n_updates            | 4790          |
|    nu                   | 3.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.45e-05     |
|    reward_explained_... | 0.939         |
|    reward_value_loss    | 6.23e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 861           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 481           |
|    time_elapsed         | 3854          |
|    total_timesteps      | 4925440       |
| train/                  |               |
|    approx_kl            | 1.9617659e-05 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 7.81e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.62         |
|    cost_value_loss      | 0.00489       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00303      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.46e+03      |
|    mean_cost_advantages | 0.0022948482  |
|    mean_reward_advan... | 2.1389432     |
|    n_updates            | 4800          |
|    nu                   | 3.17          |
|    nu_loss              | -0.00124      |
|    policy_gradient_loss | -3.44e-05     |
|    reward_explained_... | 0.949         |
|    reward_value_loss    | 5.5e+03       |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 10.2          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 872           |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 482           |
|    time_elapsed         | 3862          |
|    total_timesteps      | 4935680       |
| train/                  |               |
|    approx_kl            | 5.3421827e-06 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -21           |
|    cost_value_loss      | 0.000595      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00317      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.62e+03      |
|    mean_cost_advantages | -0.0012206648 |
|    mean_reward_advan... | -10.58214     |
|    n_updates            | 4810          |
|    nu                   | 3.17          |
|    nu_loss              | -0.000309     |
|    policy_gradient_loss | 2.98e-06      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 6.41e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 863          |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1277         |
|    iterations           | 483          |
|    time_elapsed         | 3870         |
|    total_timesteps      | 4945920      |
| train/                  |              |
|    approx_kl            | 8.956483e-06 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0205      |
|    cost_value_loss      | 8.14e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.00291     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.47e+03     |
|    mean_cost_advantages | -0.001049595 |
|    mean_reward_advan... | 2.815287     |
|    n_updates            | 4820         |
|    nu                   | 3.17         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -6.7e-06     |
|    reward_explained_... | 0.955        |
|    reward_value_loss    | 4.72e+03     |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1e+04          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | 1e+04          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 865            |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | 1e+04          |
| time/                   |                |
|    fps                  | 1277           |
|    iterations           | 484            |
|    time_elapsed         | 3879           |
|    total_timesteps      | 4956160        |
| train/                  |                |
|    approx_kl            | -1.0762363e-06 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0481        |
|    cost_value_loss      | 7.9e-06        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00287       |
|    learning_rate        | 0.0003         |
|    loss                 | 3.27e+03       |
|    mean_cost_advantages | -0.00020398002 |
|    mean_reward_advan... | -5.506025      |
|    n_updates            | 4830           |
|    nu                   | 3.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -2.85e-07      |
|    reward_explained_... | 0.944          |
|    reward_value_loss    | 5.79e+03       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 485           |
|    time_elapsed         | 3887          |
|    total_timesteps      | 4966400       |
| train/                  |               |
|    approx_kl            | 1.0279333e-05 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -15.8         |
|    cost_value_loss      | 0.00046       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00307      |
|    learning_rate        | 0.0003        |
|    loss                 | 2.4e+03       |
|    mean_cost_advantages | 0.00045654373 |
|    mean_reward_advan... | -0.09548249   |
|    n_updates            | 4840          |
|    nu                   | 3.17          |
|    nu_loss              | -0.000309     |
|    policy_gradient_loss | 2.24e-06      |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.33e+03      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 870           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 486           |
|    time_elapsed         | 3895          |
|    total_timesteps      | 4976640       |
| train/                  |               |
|    approx_kl            | 1.8882565e-07 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0481       |
|    cost_value_loss      | 7.82e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0028       |
|    learning_rate        | 0.0003        |
|    loss                 | 3.63e+03      |
|    mean_cost_advantages | -7.959705e-05 |
|    mean_reward_advan... | 0.7577172     |
|    n_updates            | 4850          |
|    nu                   | 3.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.82e-08     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 6.33e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 869           |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 487           |
|    time_elapsed         | 3903          |
|    total_timesteps      | 4986880       |
| train/                  |               |
|    approx_kl            | 2.1926128e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0494       |
|    cost_value_loss      | 7.84e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00284      |
|    learning_rate        | 0.0003        |
|    loss                 | 3.42e+03      |
|    mean_cost_advantages | 6.888372e-05  |
|    mean_reward_advan... | 1.9707134     |
|    n_updates            | 4860          |
|    nu                   | 3.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.61e-07     |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 5.99e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1e+04         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | 1e+04         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 866           |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1e+04         |
| time/                   |               |
|    fps                  | 1277          |
|    iterations           | 488           |
|    time_elapsed         | 3911          |
|    total_timesteps      | 4997120       |
| train/                  |               |
|    approx_kl            | 2.9606744e-07 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0547       |
|    cost_value_loss      | 7.75e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00281      |
|    learning_rate        | 0.0003        |
|    loss                 | 6.09e+03      |
|    mean_cost_advantages | 7.480898e-05  |
|    mean_reward_advan... | -0.3068356    |
|    n_updates            | 4870          |
|    nu                   | 3.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.11e-08     |
|    reward_explained_... | 0.943         |
|    reward_value_loss    | 5.97e+03      |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1e+04        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | 1e+04        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 881          |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 1e+04        |
| time/                   |              |
|    fps                  | 1277         |
|    iterations           | 489          |
|    time_elapsed         | 3919         |
|    total_timesteps      | 5007360      |
| train/                  |              |
|    approx_kl            | 4.211981e-05 |
|    average_cost         | 0.0          |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0515      |
|    cost_value_loss      | 7.8e-06      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.00288     |
|    learning_rate        | 0.0003       |
|    loss                 | 2.79e+03     |
|    mean_cost_advantages | 0.0002043997 |
|    mean_reward_advan... | -2.055786    |
|    n_updates            | 4880         |
|    nu                   | 3.17         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -1.17e-05    |
|    reward_explained_... | 0.943        |
|    reward_value_loss    | 5.96e+03     |
|    total_cost           | 0.0          |
------------------------------------------
/home/jovyan/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
[32mTime taken: 65.48 minutes