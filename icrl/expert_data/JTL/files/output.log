/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
[32mConfigured folder /tmp/wandb/run-20220630_045243-24nr5sjr/files for saving
[32mName: JTL-v0_CJTL-v0_dnc_True_dno_True_dnr_True_goal_0_ws_True_s_20_sid_-1
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
Wrapping eval env in a VecNormalize.
Using cpu device
----------------------------------
| eval/               |          |
|    best_mean_reward | -77.2    |
|    mean_ep_length   | 114      |
|    mean_reward      | -77.2    |
|    true_cost        | 0.5      |
| infos/              |          |
|    cost             | 0.55     |
| rollout/            |          |
|    adjusted_reward  | -1.21    |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | -101     |
| time/               |          |
|    fps              | 1127     |
|    iterations       | 1        |
|    time_elapsed     | 9        |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -77.2       |
|    mean_ep_length       | 138         |
|    mean_reward          | -93.4       |
|    true_cost            | 0.399       |
| infos/                  |             |
|    cost                 | 0.51        |
| rollout/                |             |
|    adjusted_reward      | -1.1        |
|    ep_len_mean          | 149         |
|    ep_rew_mean          | -102        |
| time/                   |             |
|    fps                  | 1021        |
|    iterations           | 2           |
|    time_elapsed         | 20          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.017965512 |
|    average_cost         | 0.50039065  |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -268        |
|    cost_value_loss      | 30.7        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    mean_cost_advantages | 7.6933722   |
|    mean_reward_advan... | -10.876623  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.5        |
|    policy_gradient_loss | -0.0173     |
|    reward_explained_... | -404        |
|    reward_value_loss    | 56.2        |
|    total_cost           | 5124.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.04
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -51.3      |
|    mean_ep_length       | 90.2       |
|    mean_reward          | -51.3      |
|    true_cost            | 0.195      |
| infos/                  |            |
|    cost                 | 0.16       |
| rollout/                |            |
|    adjusted_reward      | -0.884     |
|    ep_len_mean          | 133        |
|    ep_rew_mean          | -89.6      |
| time/                   |            |
|    fps                  | 986        |
|    iterations           | 3          |
|    time_elapsed         | 31         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.04044429 |
|    average_cost         | 0.39941406 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -7.54      |
|    cost_value_loss      | 27.8       |
|    early_stop_epoch     | 2          |
|    entropy_loss         | -1.32      |
|    learning_rate        | 0.0003     |
|    loss                 | 27.6       |
|    mean_cost_advantages | 4.4417934  |
|    mean_reward_advan... | -7.5115724 |
|    n_updates            | 20         |
|    nu                   | 1.13       |
|    nu_loss              | -0.425     |
|    policy_gradient_loss | -0.0287    |
|    reward_explained_... | -11.4      |
|    reward_value_loss    | 42.9       |
|    total_cost           | 4090.0     |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -38.6       |
|    mean_ep_length       | 71.6        |
|    mean_reward          | -38.6       |
|    true_cost            | 0.137       |
| infos/                  |             |
|    cost                 | 0.33        |
| rollout/                |             |
|    adjusted_reward      | -0.643      |
|    ep_len_mean          | 89.9        |
|    ep_rew_mean          | -42.7       |
| time/                   |             |
|    fps                  | 977         |
|    iterations           | 4           |
|    time_elapsed         | 41          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.025352394 |
|    average_cost         | 0.1953125   |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.6        |
|    cost_value_loss      | 19.3        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 28.3        |
|    mean_cost_advantages | 0.23266777  |
|    mean_reward_advan... | -5.1723185  |
|    n_updates            | 30          |
|    nu                   | 1.19        |
|    nu_loss              | -0.221      |
|    policy_gradient_loss | -0.025      |
|    reward_explained_... | -6.74       |
|    reward_value_loss    | 53.1        |
|    total_cost           | 2000.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -15.8       |
|    mean_ep_length       | 36.6        |
|    mean_reward          | -15.8       |
|    true_cost            | 0.121       |
| infos/                  |             |
|    cost                 | 0.17        |
| rollout/                |             |
|    adjusted_reward      | -0.61       |
|    ep_len_mean          | 58.2        |
|    ep_rew_mean          | -25.7       |
| time/                   |             |
|    fps                  | 977         |
|    iterations           | 5           |
|    time_elapsed         | 52          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.020883128 |
|    average_cost         | 0.13681641  |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.28       |
|    cost_value_loss      | 15          |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.09       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    mean_cost_advantages | -1.0299315  |
|    mean_reward_advan... | -0.1804277  |
|    n_updates            | 40          |
|    nu                   | 1.25        |
|    nu_loss              | -0.163      |
|    policy_gradient_loss | -0.0149     |
|    reward_explained_... | -0.634      |
|    reward_value_loss    | 34.9        |
|    total_cost           | 1401.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -13.2       |
|    mean_ep_length       | 39.2        |
|    mean_reward          | -13.2       |
|    true_cost            | 0.101       |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.538      |
|    ep_len_mean          | 41.4        |
|    ep_rew_mean          | -16.8       |
| time/                   |             |
|    fps                  | 977         |
|    iterations           | 6           |
|    time_elapsed         | 62          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.022203824 |
|    average_cost         | 0.12109375  |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.924      |
|    cost_value_loss      | 10.1        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.995      |
|    learning_rate        | 0.0003      |
|    loss                 | 26.6        |
|    mean_cost_advantages | -1.7274132  |
|    mean_reward_advan... | 2.150828    |
|    n_updates            | 50          |
|    nu                   | 1.31        |
|    nu_loss              | -0.152      |
|    policy_gradient_loss | -0.022      |
|    reward_explained_... | 0.133       |
|    reward_value_loss    | 40.9        |
|    total_cost           | 1240.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -12.5       |
|    mean_ep_length       | 33.4        |
|    mean_reward          | -12.5       |
|    true_cost            | 0.0823      |
| infos/                  |             |
|    cost                 | 0.07        |
| rollout/                |             |
|    adjusted_reward      | -0.509      |
|    ep_len_mean          | 31.8        |
|    ep_rew_mean          | -13.2       |
| time/                   |             |
|    fps                  | 979         |
|    iterations           | 7           |
|    time_elapsed         | 73          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.02509963  |
|    average_cost         | 0.101464845 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.386      |
|    cost_value_loss      | 6.23        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.906      |
|    learning_rate        | 0.0003      |
|    loss                 | 17.5        |
|    mean_cost_advantages | -1.8378223  |
|    mean_reward_advan... | 3.9339352   |
|    n_updates            | 60          |
|    nu                   | 1.37        |
|    nu_loss              | -0.133      |
|    policy_gradient_loss | -0.0249     |
|    reward_explained_... | 0.408       |
|    reward_value_loss    | 34          |
|    total_cost           | 1039.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -4.82      |
|    mean_ep_length       | 14.8       |
|    mean_reward          | -4.82      |
|    true_cost            | 0.0666     |
| infos/                  |            |
|    cost                 | 0.09       |
| rollout/                |            |
|    adjusted_reward      | -0.455     |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | -7.87      |
| time/                   |            |
|    fps                  | 978        |
|    iterations           | 8          |
|    time_elapsed         | 83         |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.03576922 |
|    average_cost         | 0.08232422 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.173      |
|    cost_value_loss      | 2.74       |
|    early_stop_epoch     | 1          |
|    entropy_loss         | -0.748     |
|    learning_rate        | 0.0003     |
|    loss                 | 14.7       |
|    mean_cost_advantages | -1.5809561 |
|    mean_reward_advan... | 3.0293133  |
|    n_updates            | 70         |
|    nu                   | 1.43       |
|    nu_loss              | -0.113     |
|    policy_gradient_loss | -0.0227    |
|    reward_explained_... | 0.294      |
|    reward_value_loss    | 26.3       |
|    total_cost           | 843.0      |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.92       |
|    mean_ep_length       | 11.4        |
|    mean_reward          | -3.92       |
|    true_cost            | 0.0536      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.415      |
|    ep_len_mean          | 13.4        |
|    ep_rew_mean          | -4.56       |
| time/                   |             |
|    fps                  | 977         |
|    iterations           | 9           |
|    time_elapsed         | 94          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.033050247 |
|    average_cost         | 0.06660156  |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.16       |
|    cost_value_loss      | 1.65        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.554      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.52        |
|    mean_cost_advantages | -1.0610201  |
|    mean_reward_advan... | 2.6133714   |
|    n_updates            | 80          |
|    nu                   | 1.48        |
|    nu_loss              | -0.0952     |
|    policy_gradient_loss | -0.0166     |
|    reward_explained_... | 0.467       |
|    reward_value_loss    | 12.3        |
|    total_cost           | 682.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.92       |
|    mean_ep_length       | 13          |
|    mean_reward          | -4.72       |
|    true_cost            | 0.05        |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.411      |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | -3.86       |
| time/                   |             |
|    fps                  | 977         |
|    iterations           | 10          |
|    time_elapsed         | 104         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.026495898 |
|    average_cost         | 0.053613283 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.167       |
|    cost_value_loss      | 0.412       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.324      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.745       |
|    mean_cost_advantages | -0.47673273 |
|    mean_reward_advan... | 1.4162343   |
|    n_updates            | 90          |
|    nu                   | 1.54        |
|    nu_loss              | -0.0795     |
|    policy_gradient_loss | -0.0108     |
|    reward_explained_... | 0.522       |
|    reward_value_loss    | 1.4         |
|    total_cost           | 549.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 10          |
|    mean_reward          | -3.08       |
|    true_cost            | 0.0457      |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | -0.401      |
|    ep_len_mean          | 10.9        |
|    ep_rew_mean          | -3.58       |
| time/                   |             |
|    fps                  | 972         |
|    iterations           | 11          |
|    time_elapsed         | 115         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.029175546 |
|    average_cost         | 0.05        |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.148       |
|    cost_value_loss      | 0.174       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.151      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    mean_cost_advantages | -0.18458284 |
|    mean_reward_advan... | 0.6995613   |
|    n_updates            | 100         |
|    nu                   | 1.59        |
|    nu_loss              | -0.0768     |
|    policy_gradient_loss | -0.00937    |
|    reward_explained_... | 0.853       |
|    reward_value_loss    | 0.294       |
|    total_cost           | 512.0       |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 10.2        |
|    mean_reward          | -3.22       |
|    true_cost            | 0.0473      |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | -0.403      |
|    ep_len_mean          | 10.5        |
|    ep_rew_mean          | -3.38       |
| time/                   |             |
|    fps                  | 962         |
|    iterations           | 12          |
|    time_elapsed         | 127         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.017896075 |
|    average_cost         | 0.045703124 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.208       |
|    cost_value_loss      | 0.0806      |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -0.0617     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0598      |
|    mean_cost_advantages | -0.02403427 |
|    mean_reward_advan... | 0.20226093  |
|    n_updates            | 110         |
|    nu                   | 1.63        |
|    nu_loss              | -0.0725     |
|    policy_gradient_loss | -0.00491    |
|    reward_explained_... | 0.941       |
|    reward_value_loss    | 0.0768      |
|    total_cost           | 468.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 10.8        |
|    mean_reward          | -3.62       |
|    true_cost            | 0.0483      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.407      |
|    ep_len_mean          | 10.6        |
|    ep_rew_mean          | -3.44       |
| time/                   |             |
|    fps                  | 913         |
|    iterations           | 13          |
|    time_elapsed         | 145         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.010449771 |
|    average_cost         | 0.047265626 |
|    clip_fraction        | 0.00929     |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.00716    |
|    cost_value_loss      | 0.0795      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.0261     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0392      |
|    mean_cost_advantages | 0.016992096 |
|    mean_reward_advan... | 0.06625642  |
|    n_updates            | 120         |
|    nu                   | 1.68        |
|    nu_loss              | -0.0772     |
|    policy_gradient_loss | -0.00249    |
|    reward_explained_... | 0.983       |
|    reward_value_loss    | 0.019       |
|    total_cost           | 484.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 10.2         |
|    mean_reward          | -3.22        |
|    true_cost            | 0.047        |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | -0.406       |
|    ep_len_mean          | 10.5         |
|    ep_rew_mean          | -3.43        |
| time/                   |              |
|    fps                  | 875          |
|    iterations           | 14           |
|    time_elapsed         | 163          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0013417198 |
|    average_cost         | 0.048339844  |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0155      |
|    cost_value_loss      | 0.0806       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.014       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0351       |
|    mean_cost_advantages | 0.0016339037 |
|    mean_reward_advan... | 0.0064055314 |
|    n_updates            | 130          |
|    nu                   | 1.73         |
|    nu_loss              | -0.0813      |
|    policy_gradient_loss | -0.000952    |
|    reward_explained_... | 0.994        |
|    reward_value_loss    | 0.00678      |
|    total_cost           | 495.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 10.2          |
|    mean_reward          | -3.22         |
|    true_cost            | 0.0463        |
| infos/                  |               |
|    cost                 | 0.07          |
| rollout/                |               |
|    adjusted_reward      | -0.408        |
|    ep_len_mean          | 10.5          |
|    ep_rew_mean          | -3.41         |
| time/                   |               |
|    fps                  | 845           |
|    iterations           | 15            |
|    time_elapsed         | 181           |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 0.0008217705  |
|    average_cost         | 0.046972655   |
|    clip_fraction        | 0.00185       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0699        |
|    cost_value_loss      | 0.0732        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00767      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0432        |
|    mean_cost_advantages | -0.0050987867 |
|    mean_reward_advan... | 0.0042787916  |
|    n_updates            | 140           |
|    nu                   | 1.77          |
|    nu_loss              | -0.0811       |
|    policy_gradient_loss | -0.000742     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 0.00136       |
|    total_cost           | 481.0         |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 11          |
|    mean_reward          | -3.75       |
|    true_cost            | 0.0486      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.415      |
|    ep_len_mean          | 10.5        |
|    ep_rew_mean          | -3.42       |
| time/                   |             |
|    fps                  | 819         |
|    iterations           | 16          |
|    time_elapsed         | 199         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.00027027  |
|    average_cost         | 0.046289064 |
|    clip_fraction        | 0.00082     |
|    clip_range           | 0.2         |
|    cost_explained_va... | -9.83e-05   |
|    cost_value_loss      | 0.0734      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.00387    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0364      |
|    mean_cost_advantages | 0.03548972  |
|    mean_reward_advan... | 0.004519469 |
|    n_updates            | 150         |
|    nu                   | 1.82        |
|    nu_loss              | -0.0821     |
|    policy_gradient_loss | -0.000278   |
|    reward_explained_... | 0.999       |
|    reward_value_loss    | 0.000844    |
|    total_cost           | 474.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 17.6         |
|    mean_reward          | -8.15        |
|    true_cost            | 0.0185       |
| infos/                  |              |
|    cost                 | 0.11         |
| rollout/                |              |
|    adjusted_reward      | -0.585       |
|    ep_len_mean          | 30.7         |
|    ep_rew_mean          | -17          |
| time/                   |              |
|    fps                  | 828          |
|    iterations           | 17           |
|    time_elapsed         | 210          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.086139664  |
|    average_cost         | 0.04863281   |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0141       |
|    cost_value_loss      | 0.0758       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0258      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0458       |
|    mean_cost_advantages | -0.004887571 |
|    mean_reward_advan... | 0.0006353062 |
|    n_updates            | 160          |
|    nu                   | 1.86         |
|    nu_loss              | -0.0884      |
|    policy_gradient_loss | 0.342        |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.003        |
|    total_cost           | 498.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 21           |
|    mean_reward          | -10.4        |
|    true_cost            | 0.0341       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.486       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -6.82        |
| time/                   |              |
|    fps                  | 841          |
|    iterations           | 18           |
|    time_elapsed         | 219          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.02626684   |
|    average_cost         | 0.01845703   |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.3         |
|    cost_value_loss      | 0.284        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.108       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.34         |
|    mean_cost_advantages | -0.024538932 |
|    mean_reward_advan... | -5.086212    |
|    n_updates            | 170          |
|    nu                   | 1.9          |
|    nu_loss              | -0.0344      |
|    policy_gradient_loss | -0.00792     |
|    reward_explained_... | -9.74        |
|    reward_value_loss    | 12.6         |
|    total_cost           | 189.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 20.4         |
|    mean_reward          | -10          |
|    true_cost            | 0.0391       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | -0.453       |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | -4.36        |
| time/                   |              |
|    fps                  | 818          |
|    iterations           | 19           |
|    time_elapsed         | 237          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.005478284  |
|    average_cost         | 0.034082033  |
|    clip_fraction        | 0.0784       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0616       |
|    cost_value_loss      | 0.0739       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.075       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.84         |
|    mean_cost_advantages | -0.011956892 |
|    mean_reward_advan... | 0.77630794   |
|    n_updates            | 180          |
|    nu                   | 1.94         |
|    nu_loss              | -0.0649      |
|    policy_gradient_loss | -0.00654     |
|    reward_explained_... | 0.633        |
|    reward_value_loss    | 6.9          |
|    total_cost           | 349.0        |
------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.88        |
|    true_cost            | 0.04         |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.436       |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | -4.66        |
| time/                   |              |
|    fps                  | 802          |
|    iterations           | 20           |
|    time_elapsed         | 255          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.028521368  |
|    average_cost         | 0.0390625    |
|    clip_fraction        | 0.0733       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.00452      |
|    cost_value_loss      | 0.076        |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.0587      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.18         |
|    mean_cost_advantages | 0.0011947972 |
|    mean_reward_advan... | 0.45064038   |
|    n_updates            | 190          |
|    nu                   | 1.98         |
|    nu_loss              | -0.0759      |
|    policy_gradient_loss | -0.00746     |
|    reward_explained_... | 0.7          |
|    reward_value_loss    | 2.88         |
|    total_cost           | 400.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 12          |
|    mean_reward          | -3.88       |
|    true_cost            | 0.0438      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.424      |
|    ep_len_mean          | 11.3        |
|    ep_rew_mean          | -3.78       |
| time/                   |             |
|    fps                  | 813         |
|    iterations           | 21          |
|    time_elapsed         | 264         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.037846707 |
|    average_cost         | 0.040039062 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0868      |
|    cost_value_loss      | 0.0673      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0514     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    mean_cost_advantages | 0.02098834  |
|    mean_reward_advan... | 0.05003797  |
|    n_updates            | 200         |
|    nu                   | 2.02        |
|    nu_loss              | -0.0794     |
|    policy_gradient_loss | -0.00719    |
|    reward_explained_... | 0.553       |
|    reward_value_loss    | 1.66        |
|    total_cost           | 410.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 10.4        |
|    mean_reward          | -3.35       |
|    true_cost            | 0.0475      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.424      |
|    ep_len_mean          | 10.5        |
|    ep_rew_mean          | -3.4        |
| time/                   |             |
|    fps                  | 823         |
|    iterations           | 22          |
|    time_elapsed         | 273         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.044664875 |
|    average_cost         | 0.043847658 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0738     |
|    cost_value_loss      | 0.0874      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.012      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.226       |
|    mean_cost_advantages | 0.008325021 |
|    mean_reward_advan... | 0.27726382  |
|    n_updates            | 210         |
|    nu                   | 2.06        |
|    nu_loss              | -0.0887     |
|    policy_gradient_loss | -0.00406    |
|    reward_explained_... | 0.822       |
|    reward_value_loss    | 0.358       |
|    total_cost           | 449.0       |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -4.78        |
|    true_cost            | 0.0311       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.362       |
|    ep_len_mean          | 14.3         |
|    ep_rew_mean          | -4.23        |
| time/                   |              |
|    fps                  | 825          |
|    iterations           | 23           |
|    time_elapsed         | 285          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.05404023   |
|    average_cost         | 0.047460936  |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0813       |
|    cost_value_loss      | 0.073        |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.0232      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0338       |
|    mean_cost_advantages | 0.0056753657 |
|    mean_reward_advan... | 0.20805927   |
|    n_updates            | 220          |
|    nu                   | 2.1          |
|    nu_loss              | -0.0979      |
|    policy_gradient_loss | -0.00122     |
|    reward_explained_... | 0.968        |
|    reward_value_loss    | 0.00994      |
|    total_cost           | 486.0        |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | -3.95        |
|    true_cost            | 0.0344       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.373       |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | -3.87        |
| time/                   |              |
|    fps                  | 826          |
|    iterations           | 24           |
|    time_elapsed         | 297          |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.016047705  |
|    average_cost         | 0.031054687  |
|    clip_fraction        | 0.288        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.127        |
|    cost_value_loss      | 0.0613       |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.416       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.063        |
|    mean_cost_advantages | -0.024468746 |
|    mean_reward_advan... | -0.34217903  |
|    n_updates            | 230          |
|    nu                   | 2.14         |
|    nu_loss              | -0.0653      |
|    policy_gradient_loss | -0.0139      |
|    reward_explained_... | 0.903        |
|    reward_value_loss    | 0.104        |
|    total_cost           | 318.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11            |
|    mean_reward          | -3.42         |
|    true_cost            | 0.0378        |
| infos/                  |               |
|    cost                 | 0.05          |
| rollout/                |               |
|    adjusted_reward      | -0.39         |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | -3.82         |
| time/                   |               |
|    fps                  | 834           |
|    iterations           | 25            |
|    time_elapsed         | 306           |
|    total_timesteps      | 256000        |
| train/                  |               |
|    approx_kl            | 0.032841664   |
|    average_cost         | 0.034375      |
|    clip_fraction        | 0.362         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0165       |
|    cost_value_loss      | 0.0647        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.3          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0663        |
|    mean_cost_advantages | -0.0035729215 |
|    mean_reward_advan... | 0.043790285   |
|    n_updates            | 240           |
|    nu                   | 2.18          |
|    nu_loss              | -0.0737       |
|    policy_gradient_loss | -0.0129       |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 0.0709        |
|    total_cost           | 352.0         |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -3.52        |
|    true_cost            | 0.0394       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.403       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -3.63        |
| time/                   |              |
|    fps                  | 843          |
|    iterations           | 26           |
|    time_elapsed         | 315          |
|    total_timesteps      | 266240       |
| train/                  |              |
|    approx_kl            | 0.028573196  |
|    average_cost         | 0.03779297   |
|    clip_fraction        | 0.318        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.211       |
|    cost_value_loss      | 0.078        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.226       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0378       |
|    mean_cost_advantages | 0.0030310508 |
|    mean_reward_advan... | 0.064342245  |
|    n_updates            | 250          |
|    nu                   | 2.22         |
|    nu_loss              | -0.0825      |
|    policy_gradient_loss | -0.0113      |
|    reward_explained_... | 0.965        |
|    reward_value_loss    | 0.0498       |
|    total_cost           | 387.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11           |
|    mean_reward          | -3.52        |
|    true_cost            | 0.0461       |
| infos/                  |              |
|    cost                 | 0.1          |
| rollout/                |              |
|    adjusted_reward      | -0.425       |
|    ep_len_mean          | 10.9         |
|    ep_rew_mean          | -3.53        |
| time/                   |              |
|    fps                  | 851          |
|    iterations           | 27           |
|    time_elapsed         | 324          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.030307513  |
|    average_cost         | 0.039355468  |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.312       |
|    cost_value_loss      | 0.081        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.124       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0528       |
|    mean_cost_advantages | -0.004349863 |
|    mean_reward_advan... | 0.09334674   |
|    n_updates            | 260          |
|    nu                   | 2.26         |
|    nu_loss              | -0.0874      |
|    policy_gradient_loss | -0.0067      |
|    reward_explained_... | 0.98         |
|    reward_value_loss    | 0.0275       |
|    total_cost           | 403.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -3.7         |
|    true_cost            | 0.045        |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | -0.427       |
|    ep_len_mean          | 10.8         |
|    ep_rew_mean          | -3.46        |
| time/                   |              |
|    fps                  | 837          |
|    iterations           | 28           |
|    time_elapsed         | 342          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.0068885325 |
|    average_cost         | 0.04609375   |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.138       |
|    cost_value_loss      | 0.0776       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0914      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0461       |
|    mean_cost_advantages | 0.008169894  |
|    mean_reward_advan... | 0.056765758  |
|    n_updates            | 270          |
|    nu                   | 2.3          |
|    nu_loss              | -0.104       |
|    policy_gradient_loss | -0.00442     |
|    reward_explained_... | 0.988        |
|    reward_value_loss    | 0.0153       |
|    total_cost           | 472.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | -3.5          |
|    true_cost            | 0.0436        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | -0.425        |
|    ep_len_mean          | 10.6          |
|    ep_rew_mean          | -3.46         |
| time/                   |               |
|    fps                  | 824           |
|    iterations           | 29            |
|    time_elapsed         | 360           |
|    total_timesteps      | 296960        |
| train/                  |               |
|    approx_kl            | 0.0020074262  |
|    average_cost         | 0.04501953    |
|    clip_fraction        | 0.0178        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00704      |
|    cost_value_loss      | 0.0715        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0556       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0238        |
|    mean_cost_advantages | -0.0036022938 |
|    mean_reward_advan... | 0.04136847    |
|    n_updates            | 280           |
|    nu                   | 2.34          |
|    nu_loss              | -0.104        |
|    policy_gradient_loss | -0.00263      |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 0.00564       |
|    total_cost           | 461.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 10.4         |
|    mean_reward          | -3.35        |
|    true_cost            | 0.0457       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | -0.433       |
|    ep_len_mean          | 10.6         |
|    ep_rew_mean          | -3.41        |
| time/                   |              |
|    fps                  | 811          |
|    iterations           | 30           |
|    time_elapsed         | 378          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0013042394 |
|    average_cost         | 0.043554686  |
|    clip_fraction        | 0.00993      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0632       |
|    cost_value_loss      | 0.0698       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.035       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0254       |
|    mean_cost_advantages | -0.010661681 |
|    mean_reward_advan... | 0.021562561  |
|    n_updates            | 290          |
|    nu                   | 2.38         |
|    nu_loss              | -0.102       |
|    policy_gradient_loss | -0.00175     |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.0023       |
|    total_cost           | 446.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | -3.62        |
|    true_cost            | 0.0487       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.443       |
|    ep_len_mean          | 10.6         |
|    ep_rew_mean          | -3.45        |
| time/                   |              |
|    fps                  | 799          |
|    iterations           | 31           |
|    time_elapsed         | 397          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0055847084 |
|    average_cost         | 0.045703124  |
|    clip_fraction        | 0.00851      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0573       |
|    cost_value_loss      | 0.0708       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0172      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.034        |
|    mean_cost_advantages | 0.0006240894 |
|    mean_reward_advan... | 0.0056253662 |
|    n_updates            | 300          |
|    nu                   | 2.42         |
|    nu_loss              | -0.109       |
|    policy_gradient_loss | -0.00148     |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.00199      |
|    total_cost           | 468.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.29
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 23.4         |
|    mean_reward          | -7.4         |
|    true_cost            | 0.035        |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.388       |
|    ep_len_mean          | 21.3         |
|    ep_rew_mean          | -6.37        |
| time/                   |              |
|    fps                  | 803          |
|    iterations           | 32           |
|    time_elapsed         | 407          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.29106292   |
|    average_cost         | 0.04873047   |
|    clip_fraction        | 0.0795       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.173       |
|    cost_value_loss      | 0.0894       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0406      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0422       |
|    mean_cost_advantages | -0.024051998 |
|    mean_reward_advan... | 0.020815857  |
|    n_updates            | 310          |
|    nu                   | 2.47         |
|    nu_loss              | -0.118       |
|    policy_gradient_loss | 0.00265      |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 0.00393      |
|    total_cost           | 499.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.18
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 13.4        |
|    mean_reward          | -3.98       |
|    true_cost            | 0.0324      |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | -0.379      |
|    ep_len_mean          | 14.4        |
|    ep_rew_mean          | -4.38       |
| time/                   |             |
|    fps                  | 811         |
|    iterations           | 33          |
|    time_elapsed         | 416         |
|    total_timesteps      | 337920      |
| train/                  |             |
|    approx_kl            | 0.18145819  |
|    average_cost         | 0.034960937 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.37       |
|    cost_value_loss      | 0.077       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.131      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    mean_cost_advantages | 0.039699335 |
|    mean_reward_advan... | -0.94894904 |
|    n_updates            | 320         |
|    nu                   | 2.51        |
|    nu_loss              | -0.0862     |
|    policy_gradient_loss | -0.0114     |
|    reward_explained_... | 0.677       |
|    reward_value_loss    | 0.433       |
|    total_cost           | 358.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 13.2         |
|    mean_reward          | -4.2         |
|    true_cost            | 0.0426       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | -0.43        |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | -3.92        |
| time/                   |              |
|    fps                  | 817          |
|    iterations           | 34           |
|    time_elapsed         | 425          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.108794354  |
|    average_cost         | 0.032421876  |
|    clip_fraction        | 0.161        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.392        |
|    cost_value_loss      | 0.0553       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0854      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.176        |
|    mean_cost_advantages | -0.021721218 |
|    mean_reward_advan... | 0.09481182   |
|    n_updates            | 330          |
|    nu                   | 2.55         |
|    nu_loss              | -0.0814      |
|    policy_gradient_loss | -0.00632     |
|    reward_explained_... | 0.834        |
|    reward_value_loss    | 0.25         |
|    total_cost           | 332.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 10.2          |
|    mean_reward          | -3.22         |
|    true_cost            | 0.0463        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | -0.446        |
|    ep_len_mean          | 10.7          |
|    ep_rew_mean          | -3.48         |
| time/                   |               |
|    fps                  | 823           |
|    iterations           | 35            |
|    time_elapsed         | 435           |
|    total_timesteps      | 358400        |
| train/                  |               |
|    approx_kl            | 0.055525713   |
|    average_cost         | 0.042578124   |
|    clip_fraction        | 0.13          |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.241         |
|    cost_value_loss      | 0.0629        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0388       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.108         |
|    mean_cost_advantages | -0.0041873353 |
|    mean_reward_advan... | 0.3349935     |
|    n_updates            | 340           |
|    nu                   | 2.59          |
|    nu_loss              | -0.109        |
|    policy_gradient_loss | -0.00744      |
|    reward_explained_... | 0.911         |
|    reward_value_loss    | 0.148         |
|    total_cost           | 436.0         |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 10.6          |
|    mean_reward          | -3.48         |
|    true_cost            | 0.0493        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | -0.455        |
|    ep_len_mean          | 10.4          |
|    ep_rew_mean          | -3.34         |
| time/                   |               |
|    fps                  | 828           |
|    iterations           | 36            |
|    time_elapsed         | 444           |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.015651839   |
|    average_cost         | 0.046289064   |
|    clip_fraction        | 0.00762       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.164         |
|    cost_value_loss      | 0.0698        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.00689      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0604        |
|    mean_cost_advantages | -0.0006329037 |
|    mean_reward_advan... | 0.3074849     |
|    n_updates            | 350           |
|    nu                   | 2.64          |
|    nu_loss              | -0.12         |
|    policy_gradient_loss | -0.00289      |
|    reward_explained_... | 0.965         |
|    reward_value_loss    | 0.03          |
|    total_cost           | 474.0         |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 10.6        |
|    mean_reward          | -3.48       |
|    true_cost            | 0.0469      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.451      |
|    ep_len_mean          | 10.4        |
|    ep_rew_mean          | -3.37       |
| time/                   |             |
|    fps                  | 818         |
|    iterations           | 37          |
|    time_elapsed         | 462         |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.000599441 |
|    average_cost         | 0.049316406 |
|    clip_fraction        | 0.000527    |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.176       |
|    cost_value_loss      | 0.0764      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.00296    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.034       |
|    mean_cost_advantages | 0.048674397 |
|    mean_reward_advan... | 0.08489083  |
|    n_updates            | 360         |
|    nu                   | 2.68        |
|    nu_loss              | -0.13       |
|    policy_gradient_loss | -0.000228   |
|    reward_explained_... | 0.995       |
|    reward_value_loss    | 0.00324     |
|    total_cost           | 505.0       |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.09
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | -3.77        |
|    true_cost            | 0.0354       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.385       |
|    ep_len_mean          | 13.7         |
|    ep_rew_mean          | -3.89        |
| time/                   |              |
|    fps                  | 815          |
|    iterations           | 38           |
|    time_elapsed         | 476          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.090070836  |
|    average_cost         | 0.046875     |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.1          |
|    cost_value_loss      | 0.0705       |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.0161      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0196       |
|    mean_cost_advantages | -0.017085437 |
|    mean_reward_advan... | 0.023949286  |
|    n_updates            | 370          |
|    nu                   | 2.73         |
|    nu_loss              | -0.126       |
|    policy_gradient_loss | -0.000599    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.000455     |
|    total_cost           | 480.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11           |
|    mean_reward          | -3.43        |
|    true_cost            | 0.0417       |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | -0.413       |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | -3.6         |
| time/                   |              |
|    fps                  | 806          |
|    iterations           | 39           |
|    time_elapsed         | 495          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.01358791   |
|    average_cost         | 0.035449218  |
|    clip_fraction        | 0.236        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.241        |
|    cost_value_loss      | 0.0572       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.293       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.024        |
|    mean_cost_advantages | -0.015231734 |
|    mean_reward_advan... | -0.23475178  |
|    n_updates            | 380          |
|    nu                   | 2.78         |
|    nu_loss              | -0.0967      |
|    policy_gradient_loss | -0.0104      |
|    reward_explained_... | 0.958        |
|    reward_value_loss    | 0.0465       |
|    total_cost           | 363.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | -3.98         |
|    true_cost            | 0.0409        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | -0.444        |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | -3.68         |
| time/                   |               |
|    fps                  | 811           |
|    iterations           | 40            |
|    time_elapsed         | 505           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 0.11313219    |
|    average_cost         | 0.04169922    |
|    clip_fraction        | 0.323         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.147         |
|    cost_value_loss      | 0.062         |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.131        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0393        |
|    mean_cost_advantages | 0.00032061123 |
|    mean_reward_advan... | 0.043808945   |
|    n_updates            | 390           |
|    nu                   | 2.82          |
|    nu_loss              | -0.116        |
|    policy_gradient_loss | -0.0117       |
|    reward_explained_... | 0.976         |
|    reward_value_loss    | 0.0291        |
|    total_cost           | 427.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 10.6         |
|    mean_reward          | -3.25        |
|    true_cost            | 0.0488       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.463       |
|    ep_len_mean          | 10.8         |
|    ep_rew_mean          | -3.52        |
| time/                   |              |
|    fps                  | 803          |
|    iterations           | 41           |
|    time_elapsed         | 522          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.013799044  |
|    average_cost         | 0.04091797   |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.106        |
|    cost_value_loss      | 0.0691       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0693      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0298       |
|    mean_cost_advantages | -0.052805953 |
|    mean_reward_advan... | 0.101837896  |
|    n_updates            | 400          |
|    nu                   | 2.87         |
|    nu_loss              | -0.115       |
|    policy_gradient_loss | -0.00301     |
|    reward_explained_... | 0.906        |
|    reward_value_loss    | 0.11         |
|    total_cost           | 419.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 10.2        |
|    mean_reward          | -3.12       |
|    true_cost            | 0.046       |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.458      |
|    ep_len_mean          | 10.6        |
|    ep_rew_mean          | -3.42       |
| time/                   |             |
|    fps                  | 795         |
|    iterations           | 42          |
|    time_elapsed         | 540         |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.004025181 |
|    average_cost         | 0.048828125 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0715      |
|    cost_value_loss      | 0.0705      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.0504     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.029       |
|    mean_cost_advantages | 0.020133058 |
|    mean_reward_advan... | 0.06699221  |
|    n_updates            | 410         |
|    nu                   | 2.92        |
|    nu_loss              | -0.14       |
|    policy_gradient_loss | -0.000943   |
|    reward_explained_... | 0.982       |
|    reward_value_loss    | 0.0123      |
|    total_cost           | 500.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11            |
|    mean_reward          | -3.52         |
|    true_cost            | 0.048         |
| infos/                  |               |
|    cost                 | 0.07          |
| rollout/                |               |
|    adjusted_reward      | -0.466        |
|    ep_len_mean          | 10.6          |
|    ep_rew_mean          | -3.42         |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 43            |
|    time_elapsed         | 558           |
|    total_timesteps      | 440320        |
| train/                  |               |
|    approx_kl            | 0.0005363902  |
|    average_cost         | 0.045996092   |
|    clip_fraction        | 0.00915       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.169         |
|    cost_value_loss      | 0.0704        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0339       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0252        |
|    mean_cost_advantages | -0.0098545905 |
|    mean_reward_advan... | 0.028192684   |
|    n_updates            | 420           |
|    nu                   | 2.97          |
|    nu_loss              | -0.134        |
|    policy_gradient_loss | -0.00127      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 0.00171       |
|    total_cost           | 471.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 10.2         |
|    mean_reward          | -3.22        |
|    true_cost            | 0.0479       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.469       |
|    ep_len_mean          | 10.6         |
|    ep_rew_mean          | -3.48        |
| time/                   |              |
|    fps                  | 781          |
|    iterations           | 44           |
|    time_elapsed         | 576          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.0027716598 |
|    average_cost         | 0.048046876  |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.116        |
|    cost_value_loss      | 0.0709       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0203      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0406       |
|    mean_cost_advantages | 0.0038700458 |
|    mean_reward_advan... | 0.00947831   |
|    n_updates            | 430          |
|    nu                   | 3.02         |
|    nu_loss              | -0.142       |
|    policy_gradient_loss | -0.00132     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00146      |
|    total_cost           | 492.0        |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.32
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 86           |
|    mean_reward          | -55.2        |
|    true_cost            | 0.00205      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.659       |
|    ep_len_mean          | 98           |
|    ep_rew_mean          | -63.4        |
| time/                   |              |
|    fps                  | 777          |
|    iterations           | 45           |
|    time_elapsed         | 592          |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 0.31877813   |
|    average_cost         | 0.047851562  |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.141        |
|    cost_value_loss      | 0.0703       |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.0087      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0378       |
|    mean_cost_advantages | -0.017182346 |
|    mean_reward_advan... | 0.014031658  |
|    n_updates            | 440          |
|    nu                   | 3.07         |
|    nu_loss              | -0.144       |
|    policy_gradient_loss | -0.0013      |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.000818     |
|    total_cost           | 490.0        |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 10.2         |
|    mean_reward          | -3.22        |
|    true_cost            | 0.0319       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | -0.548       |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | -7.44        |
| time/                   |              |
|    fps                  | 780          |
|    iterations           | 46           |
|    time_elapsed         | 603          |
|    total_timesteps      | 471040       |
| train/                  |              |
|    approx_kl            | 0.050162695  |
|    average_cost         | 0.0020507812 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.318       |
|    cost_value_loss      | 0.0113       |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.0379      |
|    learning_rate        | 0.0003       |
|    loss                 | 2.16         |
|    mean_cost_advantages | -0.12105179  |
|    mean_reward_advan... | -8.975505    |
|    n_updates            | 450          |
|    nu                   | 3.11         |
|    nu_loss              | -0.00629     |
|    policy_gradient_loss | 0.00658      |
|    reward_explained_... | -9.08        |
|    reward_value_loss    | 9.64         |
|    total_cost           | 21.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 10.4        |
|    mean_reward          | -3.35       |
|    true_cost            | 0.0386      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | -0.538      |
|    ep_len_mean          | 14.4        |
|    ep_rew_mean          | -6.01       |
| time/                   |             |
|    fps                  | 785         |
|    iterations           | 47          |
|    time_elapsed         | 613         |
|    total_timesteps      | 481280      |
| train/                  |             |
|    approx_kl            | 0.019654945 |
|    average_cost         | 0.031933594 |
|    clip_fraction        | 0.0712      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0349      |
|    cost_value_loss      | 0.0662      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.0788     |
|    learning_rate        | 0.0003      |
|    loss                 | 5.47        |
|    mean_cost_advantages | 0.027733589 |
|    mean_reward_advan... | 0.75424874  |
|    n_updates            | 460         |
|    nu                   | 3.16        |
|    nu_loss              | -0.0994     |
|    policy_gradient_loss | -0.00153    |
|    reward_explained_... | 0.623       |
|    reward_value_loss    | 12.8        |
|    total_cost           | 327.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 12.8        |
|    mean_reward          | -4.95       |
|    true_cost            | 0.0437      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | -0.498      |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | -4.07       |
| time/                   |             |
|    fps                  | 779         |
|    iterations           | 48          |
|    time_elapsed         | 630         |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.011812548 |
|    average_cost         | 0.03857422  |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.11e-05   |
|    cost_value_loss      | 0.0701      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.0734     |
|    learning_rate        | 0.0003      |
|    loss                 | 4.23        |
|    mean_cost_advantages | 0.03824113  |
|    mean_reward_advan... | 0.7772962   |
|    n_updates            | 470         |
|    nu                   | 3.21        |
|    nu_loss              | -0.122      |
|    policy_gradient_loss | -0.00347    |
|    reward_explained_... | 0.674       |
|    reward_value_loss    | 7.02        |
|    total_cost           | 395.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 10.8         |
|    mean_reward          | -3.52        |
|    true_cost            | 0.017        |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.4         |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | -3.82        |
| time/                   |              |
|    fps                  | 783          |
|    iterations           | 49           |
|    time_elapsed         | 640          |
|    total_timesteps      | 501760       |
| train/                  |              |
|    approx_kl            | 0.024653276  |
|    average_cost         | 0.043652344  |
|    clip_fraction        | 0.0805       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.184        |
|    cost_value_loss      | 0.0752       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0704      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.475        |
|    mean_cost_advantages | -0.010711277 |
|    mean_reward_advan... | 0.5652643    |
|    n_updates            | 480          |
|    nu                   | 3.26         |
|    nu_loss              | -0.14        |
|    policy_gradient_loss | -0.00551     |
|    reward_explained_... | 0.716        |
|    reward_value_loss    | 1.25         |
|    total_cost           | 447.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 11.6        |
|    mean_reward          | -3.95       |
|    true_cost            | 0.00117     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.334      |
|    ep_len_mean          | 11.6        |
|    ep_rew_mean          | -3.84       |
| time/                   |             |
|    fps                  | 788         |
|    iterations           | 50          |
|    time_elapsed         | 649         |
|    total_timesteps      | 512000      |
| train/                  |             |
|    approx_kl            | 0.04727028  |
|    average_cost         | 0.016992187 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0704      |
|    cost_value_loss      | 0.0592      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0709     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.233       |
|    mean_cost_advantages | -0.13179058 |
|    mean_reward_advan... | 0.056820154 |
|    n_updates            | 490         |
|    nu                   | 3.3         |
|    nu_loss              | -0.0553     |
|    policy_gradient_loss | -0.0106     |
|    reward_explained_... | 0.856       |
|    reward_value_loss    | 0.283       |
|    total_cost           | 174.0       |
-----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.27
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 200          |
|    mean_reward          | -43.5        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.217       |
|    ep_len_mean          | 106          |
|    ep_rew_mean          | -23.5        |
| time/                   |              |
|    fps                  | 781          |
|    iterations           | 51           |
|    time_elapsed         | 667          |
|    total_timesteps      | 522240       |
| train/                  |              |
|    approx_kl            | 0.27336532   |
|    average_cost         | 0.001171875  |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.00333     |
|    cost_value_loss      | 0.00567      |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -0.0393      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0224       |
|    mean_cost_advantages | -0.09054883  |
|    mean_reward_advan... | -0.041897155 |
|    n_updates            | 500          |
|    nu                   | 3.34         |
|    nu_loss              | -0.00387     |
|    policy_gradient_loss | -0.00101     |
|    reward_explained_... | 0.925        |
|    reward_value_loss    | 0.0818       |
|    total_cost           | 12.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 111         |
|    mean_reward          | -22.2       |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.198      |
|    ep_len_mean          | 167         |
|    ep_rew_mean          | -34.2       |
| time/                   |             |
|    fps                  | 785         |
|    iterations           | 52          |
|    time_elapsed         | 678         |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.03176347  |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.0415      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.174      |
|    cost_value_loss      | 0.00171     |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.093      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.329       |
|    mean_cost_advantages | -0.03555127 |
|    mean_reward_advan... | -3.0992324  |
|    n_updates            | 510         |
|    nu                   | 3.38        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000227   |
|    reward_explained_... | -0.958      |
|    reward_value_loss    | 1.53        |
|    total_cost           | 0.0         |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -4.2         |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.217       |
|    ep_len_mean          | 55.8         |
|    ep_rew_mean          | -12          |
| time/                   |              |
|    fps                  | 780          |
|    iterations           | 53           |
|    time_elapsed         | 695          |
|    total_timesteps      | 542720       |
| train/                  |              |
|    approx_kl            | 0.01201664   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0969       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.9         |
|    cost_value_loss      | 0.00051      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.176       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.458        |
|    mean_cost_advantages | -0.030268008 |
|    mean_reward_advan... | -1.9389985   |
|    n_updates            | 520          |
|    nu                   | 3.42         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00125     |
|    reward_explained_... | -8.65        |
|    reward_value_loss    | 1.51         |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 13.8         |
|    mean_reward          | -4.17        |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.299       |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | -4.77        |
| time/                   |              |
|    fps                  | 779          |
|    iterations           | 54           |
|    time_elapsed         | 709          |
|    total_timesteps      | 552960       |
| train/                  |              |
|    approx_kl            | 0.01649611   |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.45        |
|    cost_value_loss      | 0.000865     |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.188       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.21         |
|    mean_cost_advantages | -0.029884899 |
|    mean_reward_advan... | -0.5899128   |
|    n_updates            | 530          |
|    nu                   | 3.45         |
|    nu_loss              | -0.000334    |
|    policy_gradient_loss | -0.00419     |
|    reward_explained_... | -3.55        |
|    reward_value_loss    | 3.7          |
|    total_cost           | 1.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -3.8         |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.332       |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | -4.01        |
| time/                   |              |
|    fps                  | 773          |
|    iterations           | 55           |
|    time_elapsed         | 727          |
|    total_timesteps      | 563200       |
| train/                  |              |
|    approx_kl            | 0.012203613  |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.99        |
|    cost_value_loss      | 0.00477      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.098       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.24         |
|    mean_cost_advantages | -0.02351805  |
|    mean_reward_advan... | 1.2786402    |
|    n_updates            | 540          |
|    nu                   | 3.48         |
|    nu_loss              | -0.00202     |
|    policy_gradient_loss | -0.00516     |
|    reward_explained_... | 0.198        |
|    reward_value_loss    | 3.38         |
|    total_cost           | 6.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 13.6         |
|    mean_reward          | -4.12        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.308       |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | -4.03        |
| time/                   |              |
|    fps                  | 777          |
|    iterations           | 56           |
|    time_elapsed         | 737          |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.10433551   |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0.0807       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.76        |
|    cost_value_loss      | 0.00624      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0839      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.373        |
|    mean_cost_advantages | -0.011831251 |
|    mean_reward_advan... | 0.60262024   |
|    n_updates            | 550          |
|    nu                   | 3.5          |
|    nu_loss              | -0.00204     |
|    policy_gradient_loss | 0.00401      |
|    reward_explained_... | 0.753        |
|    reward_value_loss    | 0.631        |
|    total_cost           | 6.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -3.85         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.318        |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | -3.96         |
| time/                   |               |
|    fps                  | 772           |
|    iterations           | 57            |
|    time_elapsed         | 756           |
|    total_timesteps      | 583680        |
| train/                  |               |
|    approx_kl            | 0.009796178   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.165         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0164        |
|    cost_value_loss      | 1.68e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.184        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0504        |
|    mean_cost_advantages | -0.0016508999 |
|    mean_reward_advan... | -0.025136363  |
|    n_updates            | 560           |
|    nu                   | 3.53          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00271      |
|    reward_explained_... | 0.92          |
|    reward_value_loss    | 0.122         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 10.4          |
|    mean_reward          | -3.3          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.332        |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | -3.78         |
| time/                   |               |
|    fps                  | 775           |
|    iterations           | 58            |
|    time_elapsed         | 765           |
|    total_timesteps      | 593920        |
| train/                  |               |
|    approx_kl            | 0.06751531    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.145         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0188       |
|    cost_value_loss      | 8.04e-06      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0649       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0928        |
|    mean_cost_advantages | -0.0015926622 |
|    mean_reward_advan... | 0.021588674   |
|    n_updates            | 570           |
|    nu                   | 3.55          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00209      |
|    reward_explained_... | 0.924         |
|    reward_value_loss    | 0.116         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 11.2           |
|    mean_reward          | -3.68          |
|    true_cost            | 0.00195        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.347         |
|    ep_len_mean          | 11.8           |
|    ep_rew_mean          | -4             |
| time/                   |                |
|    fps                  | 770            |
|    iterations           | 59             |
|    time_elapsed         | 783            |
|    total_timesteps      | 604160         |
| train/                  |                |
|    approx_kl            | 0.006918161    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0114         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0428        |
|    cost_value_loss      | 5.45e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0367        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0356         |
|    mean_cost_advantages | -0.00022444536 |
|    mean_reward_advan... | 0.09857048     |
|    n_updates            | 580            |
|    nu                   | 3.57           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000268      |
|    reward_explained_... | 0.926          |
|    reward_value_loss    | 0.115          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 12          |
|    mean_reward          | -4.12       |
|    true_cost            | 9.77e-05    |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.336      |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | -3.92       |
| time/                   |             |
|    fps                  | 772         |
|    iterations           | 60          |
|    time_elapsed         | 794         |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.017172363 |
|    average_cost         | 0.001953125 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.73       |
|    cost_value_loss      | 0.0316      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.0249     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0417      |
|    mean_cost_advantages | 0.008732167 |
|    mean_reward_advan... | 0.03768234  |
|    n_updates            | 590         |
|    nu                   | 3.59        |
|    nu_loss              | -0.00697    |
|    policy_gradient_loss | -0.00183    |
|    reward_explained_... | 0.881       |
|    reward_value_loss    | 0.197       |
|    total_cost           | 20.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -3.73        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.333       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -3.87        |
| time/                   |              |
|    fps                  | 768          |
|    iterations           | 61           |
|    time_elapsed         | 812          |
|    total_timesteps      | 624640       |
| train/                  |              |
|    approx_kl            | 0.0024613186 |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.00286      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.59        |
|    cost_value_loss      | 0.000405     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0133      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0339       |
|    mean_cost_advantages | -0.00465205  |
|    mean_reward_advan... | 0.03022736   |
|    n_updates            | 600          |
|    nu                   | 3.6          |
|    nu_loss              | -0.00035     |
|    policy_gradient_loss | -0.000273    |
|    reward_explained_... | 0.92         |
|    reward_value_loss    | 0.133        |
|    total_cost           | 1.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 10.8          |
|    mean_reward          | -3.57         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.335        |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | -3.8          |
| time/                   |               |
|    fps                  | 764           |
|    iterations           | 62            |
|    time_elapsed         | 830           |
|    total_timesteps      | 634880        |
| train/                  |               |
|    approx_kl            | 0.0007859176  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00218       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0813        |
|    cost_value_loss      | 3.54e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00798      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.077         |
|    mean_cost_advantages | -0.0021347925 |
|    mean_reward_advan... | 0.015192663   |
|    n_updates            | 610           |
|    nu                   | 3.62          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 5.14e-06      |
|    reward_explained_... | 0.937         |
|    reward_value_loss    | 0.103         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -4.13         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.324        |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | -4            |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 63            |
|    time_elapsed         | 849           |
|    total_timesteps      | 645120        |
| train/                  |               |
|    approx_kl            | 0.026299898   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0052        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0275        |
|    cost_value_loss      | 3.68e-06      |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.00886      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0657        |
|    mean_cost_advantages | -0.0006757125 |
|    mean_reward_advan... | 0.018152952   |
|    n_updates            | 620           |
|    nu                   | 3.63          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000225     |
|    reward_explained_... | 0.926         |
|    reward_value_loss    | 0.114         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 13.2          |
|    mean_reward          | -4.57         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.333        |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | -3.9          |
| time/                   |               |
|    fps                  | 763           |
|    iterations           | 64            |
|    time_elapsed         | 858           |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.10870411    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.035         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.288         |
|    cost_value_loss      | 7.27e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.00308      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0994        |
|    mean_cost_advantages | -0.0034586596 |
|    mean_reward_advan... | -0.09741065   |
|    n_updates            | 630           |
|    nu                   | 3.64          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00305      |
|    reward_explained_... | 0.814         |
|    reward_value_loss    | 0.259         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 12             |
|    mean_reward          | -4.17          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.332         |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | -3.78          |
| time/                   |                |
|    fps                  | 759            |
|    iterations           | 65             |
|    time_elapsed         | 876            |
|    total_timesteps      | 665600         |
| train/                  |                |
|    approx_kl            | -1.9942596e-05 |
|    average_cost         | 0.0            |
|    clip_fraction        | 8.79e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0809        |
|    cost_value_loss      | 2.26e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.000918      |
|    learning_rate        | 0.0003         |
|    loss                 | 0.043          |
|    mean_cost_advantages | 0.00055998005  |
|    mean_reward_advan... | 0.06652216     |
|    n_updates            | 640            |
|    nu                   | 3.66           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -8.23e-07      |
|    reward_explained_... | 0.935          |
|    reward_value_loss    | 0.0988         |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11            |
|    mean_reward          | -3.65         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.334        |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | -3.84         |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 66            |
|    time_elapsed         | 894           |
|    total_timesteps      | 675840        |
| train/                  |               |
|    approx_kl            | 0.00019112094 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000283      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0567       |
|    cost_value_loss      | 1.78e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00158      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0374        |
|    mean_cost_advantages | 0.0003242348  |
|    mean_reward_advan... | 0.044879794   |
|    n_updates            | 650           |
|    nu                   | 3.67          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -7.08e-06     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 0.0885        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 23.6         |
|    mean_reward          | -5.7         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.275       |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | -4.84        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 67           |
|    time_elapsed         | 907          |
|    total_timesteps      | 686080       |
| train/                  |              |
|    approx_kl            | 0.019296365  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.00693      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -649         |
|    cost_value_loss      | 0.000424     |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.00665     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0361       |
|    mean_cost_advantages | 0.0006197398 |
|    mean_reward_advan... | -0.013162106 |
|    n_updates            | 660          |
|    nu                   | 3.68         |
|    nu_loss              | -0.000358    |
|    policy_gradient_loss | -6.02e-05    |
|    reward_explained_... | 0.94         |
|    reward_value_loss    | 0.0931       |
|    total_cost           | 1.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.48
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -4.15        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.333       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -3.85        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 68           |
|    time_elapsed         | 916          |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.47904676   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.154        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.446        |
|    cost_value_loss      | 0.000107     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.00776     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.398        |
|    mean_cost_advantages | -0.004287566 |
|    mean_reward_advan... | -0.54720867  |
|    n_updates            | 670          |
|    nu                   | 3.68         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00719     |
|    reward_explained_... | 0.152        |
|    reward_value_loss    | 0.802        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.17
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 104            |
|    mean_reward          | -14.9          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.139         |
|    ep_len_mean          | 81.9           |
|    ep_rew_mean          | -11.3          |
| time/                   |                |
|    fps                  | 763            |
|    iterations           | 69             |
|    time_elapsed         | 925            |
|    total_timesteps      | 706560         |
| train/                  |                |
|    approx_kl            | 0.17134067     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0475         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00712       |
|    cost_value_loss      | 1.22e-06       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.0197        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0143         |
|    mean_cost_advantages | -0.00064677873 |
|    mean_reward_advan... | 0.22652611     |
|    n_updates            | 680            |
|    nu                   | 3.69           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00144       |
|    reward_explained_... | 0.916          |
|    reward_value_loss    | 0.112          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
---------------------------------------------
| eval/                   |                 |
|    best_mean_reward     | -3.08           |
|    mean_ep_length       | 41.4            |
|    mean_reward          | -7.42           |
|    true_cost            | 0               |
| infos/                  |                 |
|    cost                 | 0               |
| rollout/                |                 |
|    adjusted_reward      | -0.157          |
|    ep_len_mean          | 48.6            |
|    ep_rew_mean          | -7.79           |
| time/                   |                 |
|    fps                  | 767             |
|    iterations           | 70              |
|    time_elapsed         | 934             |
|    total_timesteps      | 716800          |
| train/                  |                 |
|    approx_kl            | 0.0275059       |
|    average_cost         | 0.0             |
|    clip_fraction        | 0.0994          |
|    clip_range           | 0.2             |
|    cost_explained_va... | 0.21            |
|    cost_value_loss      | 2.55e-07        |
|    early_stop_epoch     | 0               |
|    entropy_loss         | -0.237          |
|    learning_rate        | 0.0003          |
|    loss                 | 0.0751          |
|    mean_cost_advantages | -0.000113368136 |
|    mean_reward_advan... | -1.3038267      |
|    n_updates            | 690             |
|    nu                   | 3.7             |
|    nu_loss              | -0              |
|    policy_gradient_loss | -0.00307        |
|    reward_explained_... | 0.372           |
|    reward_value_loss    | 0.403           |
|    total_cost           | 0.0             |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 21            |
|    mean_reward          | -4.98         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.211        |
|    ep_len_mean          | 25.1          |
|    ep_rew_mean          | -5.42         |
| time/                   |               |
|    fps                  | 770           |
|    iterations           | 71            |
|    time_elapsed         | 943           |
|    total_timesteps      | 727040        |
| train/                  |               |
|    approx_kl            | 0.031654924   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.128         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0575        |
|    cost_value_loss      | 2.24e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.311        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.239         |
|    mean_cost_advantages | -0.0001116253 |
|    mean_reward_advan... | -0.5080217    |
|    n_updates            | 700           |
|    nu                   | 3.7           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00428      |
|    reward_explained_... | 0.453         |
|    reward_value_loss    | 0.536         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 17             |
|    mean_reward          | -4.42          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.272         |
|    ep_len_mean          | 16.2           |
|    ep_rew_mean          | -4.39          |
| time/                   |                |
|    fps                  | 774            |
|    iterations           | 72             |
|    time_elapsed         | 952            |
|    total_timesteps      | 737280         |
| train/                  |                |
|    approx_kl            | 0.036486797    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.2            |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0866        |
|    cost_value_loss      | 4.14e-07       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.378         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.202          |
|    mean_cost_advantages | -0.00023322697 |
|    mean_reward_advan... | 0.35110435     |
|    n_updates            | 710            |
|    nu                   | 3.71           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0056        |
|    reward_explained_... | 0.701          |
|    reward_value_loss    | 0.497          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | -3.85         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.305        |
|    ep_len_mean          | 13.9          |
|    ep_rew_mean          | -4.21         |
| time/                   |               |
|    fps                  | 777           |
|    iterations           | 73            |
|    time_elapsed         | 962           |
|    total_timesteps      | 747520        |
| train/                  |               |
|    approx_kl            | 0.043146286   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.378         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0333       |
|    cost_value_loss      | 4.81e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.311        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.118         |
|    mean_cost_advantages | -8.686022e-06 |
|    mean_reward_advan... | 0.7070591     |
|    n_updates            | 720           |
|    nu                   | 3.72          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00735      |
|    reward_explained_... | 0.889         |
|    reward_value_loss    | 0.257         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 12.8           |
|    mean_reward          | -4.23          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.314         |
|    ep_len_mean          | 12.8           |
|    ep_rew_mean          | -4.01          |
| time/                   |                |
|    fps                  | 779            |
|    iterations           | 74             |
|    time_elapsed         | 972            |
|    total_timesteps      | 757760         |
| train/                  |                |
|    approx_kl            | 0.021632383    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.236          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0186        |
|    cost_value_loss      | 7.49e-07       |
|    early_stop_epoch     | 1              |
|    entropy_loss         | -0.201         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0776         |
|    mean_cost_advantages | -0.00011920477 |
|    mean_reward_advan... | 0.4748016      |
|    n_updates            | 730            |
|    nu                   | 3.72           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0045        |
|    reward_explained_... | 0.923          |
|    reward_value_loss    | 0.156          |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | -3.63         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.32         |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | -3.8          |
| time/                   |               |
|    fps                  | 775           |
|    iterations           | 75            |
|    time_elapsed         | 989           |
|    total_timesteps      | 768000        |
| train/                  |               |
|    approx_kl            | 0.0059078997  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0641        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0256       |
|    cost_value_loss      | 9.36e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.149        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0501        |
|    mean_cost_advantages | 0.00029397028 |
|    mean_reward_advan... | 0.20357458    |
|    n_updates            | 740           |
|    nu                   | 3.72          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00193      |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 0.104         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 12.6          |
|    mean_reward          | -4.3          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.324        |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | -3.89         |
| time/                   |               |
|    fps                  | 772           |
|    iterations           | 76            |
|    time_elapsed         | 1007          |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 0.0044530793  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0579        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0122       |
|    cost_value_loss      | 1.19e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.116        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0332        |
|    mean_cost_advantages | 4.9079856e-05 |
|    mean_reward_advan... | 0.09651472    |
|    n_updates            | 750           |
|    nu                   | 3.73          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00123      |
|    reward_explained_... | 0.942         |
|    reward_value_loss    | 0.0964        |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -4.28        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.329       |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | -3.84        |
| time/                   |              |
|    fps                  | 769          |
|    iterations           | 77           |
|    time_elapsed         | 1024         |
|    total_timesteps      | 788480       |
| train/                  |              |
|    approx_kl            | 0.009879901  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0451      |
|    cost_value_loss      | 1.68e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0841      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.115        |
|    mean_cost_advantages | 3.846935e-06 |
|    mean_reward_advan... | 0.01883744   |
|    n_updates            | 760          |
|    nu                   | 3.73         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000753    |
|    reward_explained_... | 0.929        |
|    reward_value_loss    | 0.113        |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 12.8           |
|    mean_reward          | -4.7           |
|    true_cost            | 0.00117        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.343         |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | -4.02          |
| time/                   |                |
|    fps                  | 766            |
|    iterations           | 78             |
|    time_elapsed         | 1042           |
|    total_timesteps      | 798720         |
| train/                  |                |
|    approx_kl            | 0.006229808    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0169         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0013        |
|    cost_value_loss      | 7.47e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0469        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0596         |
|    mean_cost_advantages | -0.00065233925 |
|    mean_reward_advan... | 0.018107671    |
|    n_updates            | 770            |
|    nu                   | 3.73           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000352      |
|    reward_explained_... | 0.933          |
|    reward_value_loss    | 0.104          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -3.9         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.334       |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | -3.92        |
| time/                   |              |
|    fps                  | 769          |
|    iterations           | 79           |
|    time_elapsed         | 1051         |
|    total_timesteps      | 808960       |
| train/                  |              |
|    approx_kl            | 0.030748516  |
|    average_cost         | 0.001171875  |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.653       |
|    cost_value_loss      | 0.0121       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.022       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.116        |
|    mean_cost_advantages | 0.0026069793 |
|    mean_reward_advan... | -0.05971632  |
|    n_updates            | 780          |
|    nu                   | 3.74         |
|    nu_loss              | -0.00438     |
|    policy_gradient_loss | -0.00266     |
|    reward_explained_... | 0.876        |
|    reward_value_loss    | 0.196        |
|    total_cost           | 12.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11           |
|    mean_reward          | -3.55        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.333       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -3.8         |
| time/                   |              |
|    fps                  | 766          |
|    iterations           | 80           |
|    time_elapsed         | 1068         |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 5.617533e-05 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0257      |
|    cost_value_loss      | 2.71e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0108      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.036        |
|    mean_cost_advantages | 0.0022253343 |
|    mean_reward_advan... | 0.06732036   |
|    n_updates            | 790          |
|    nu                   | 3.74         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -2.02e-05    |
|    reward_explained_... | 0.931        |
|    reward_value_loss    | 0.111        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11            |
|    mean_reward          | -3.7          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.331        |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | -3.84         |
| time/                   |               |
|    fps                  | 766           |
|    iterations           | 81            |
|    time_elapsed         | 1081          |
|    total_timesteps      | 829440        |
| train/                  |               |
|    approx_kl            | 0.01926523    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00908       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0405        |
|    cost_value_loss      | 6.48e-07      |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.0162       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0332        |
|    mean_cost_advantages | 0.00038467586 |
|    mean_reward_advan... | 0.019497419   |
|    n_updates            | 800           |
|    nu                   | 3.74          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.88e-05     |
|    reward_explained_... | 0.939         |
|    reward_value_loss    | 0.0947        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 11.8           |
|    mean_reward          | -4.03          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.334         |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | -3.83          |
| time/                   |                |
|    fps                  | 770            |
|    iterations           | 82             |
|    time_elapsed         | 1090           |
|    total_timesteps      | 839680         |
| train/                  |                |
|    approx_kl            | 0.01847877     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.016          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.221          |
|    cost_value_loss      | 1.1e-05        |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.0083        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0612         |
|    mean_cost_advantages | -0.00077196985 |
|    mean_reward_advan... | -0.06740077    |
|    n_updates            | 810            |
|    nu                   | 3.75           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00142       |
|    reward_explained_... | 0.889          |
|    reward_value_loss    | 0.168          |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11            |
|    mean_reward          | -3.6          |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.34         |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | -3.83         |
| time/                   |               |
|    fps                  | 767           |
|    iterations           | 83            |
|    time_elapsed         | 1108          |
|    total_timesteps      | 849920        |
| train/                  |               |
|    approx_kl            | 0.0015470083  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000596      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.265         |
|    cost_value_loss      | 6e-07         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00557      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0806        |
|    mean_cost_advantages | -0.0016008677 |
|    mean_reward_advan... | 0.05787729    |
|    n_updates            | 820           |
|    nu                   | 3.75          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.94e-06     |
|    reward_explained_... | 0.93          |
|    reward_value_loss    | 0.111         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -3.95        |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.334       |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | -3.78        |
| time/                   |              |
|    fps                  | 769          |
|    iterations           | 84           |
|    time_elapsed         | 1117         |
|    total_timesteps      | 860160       |
| train/                  |              |
|    approx_kl            | 0.02072238   |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.191        |
|    cost_value_loss      | 0.00713      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.00613     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.105        |
|    mean_cost_advantages | 0.000303831  |
|    mean_reward_advan... | -0.013390094 |
|    n_updates            | 830          |
|    nu                   | 3.75         |
|    nu_loss              | -0.0022      |
|    policy_gradient_loss | -0.00153     |
|    reward_explained_... | 0.906        |
|    reward_value_loss    | 0.151        |
|    total_cost           | 6.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -4.15         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.334        |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | -3.86         |
| time/                   |               |
|    fps                  | 766           |
|    iterations           | 85            |
|    time_elapsed         | 1135          |
|    total_timesteps      | 870400        |
| train/                  |               |
|    approx_kl            | 0.00048128547 |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.000625      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.04         |
|    cost_value_loss      | 0.00907       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00314      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.125         |
|    mean_cost_advantages | 0.00079003396 |
|    mean_reward_advan... | 0.0023591123  |
|    n_updates            | 840           |
|    nu                   | 3.75          |
|    nu_loss              | -0.00146      |
|    policy_gradient_loss | -0.000235     |
|    reward_explained_... | 0.932         |
|    reward_value_loss    | 0.104         |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -4.08         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.333        |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | -3.8          |
| time/                   |               |
|    fps                  | 763           |
|    iterations           | 86            |
|    time_elapsed         | 1154          |
|    total_timesteps      | 880640        |
| train/                  |               |
|    approx_kl            | 0.00066553836 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.000615      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -10.4         |
|    cost_value_loss      | 0.000336      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00241      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00974       |
|    mean_cost_advantages | -0.012927187  |
|    mean_reward_advan... | -0.0050188988 |
|    n_updates            | 850           |
|    nu                   | 3.75          |
|    nu_loss              | -0.000366     |
|    policy_gradient_loss | -3.45e-05     |
|    reward_explained_... | 0.938         |
|    reward_value_loss    | 0.0952        |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | -3.77         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.334        |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | -3.91         |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 87            |
|    time_elapsed         | 1172          |
|    total_timesteps      | 890880        |
| train/                  |               |
|    approx_kl            | 0.00095359527 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.000186      |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.282         |
|    cost_value_loss      | 1.79e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.00145      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0417        |
|    mean_cost_advantages | -0.0033028978 |
|    mean_reward_advan... | -0.045934122  |
|    n_updates            | 860           |
|    nu                   | 3.75          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2e-05        |
|    reward_explained_... | 0.92          |
|    reward_value_loss    | 0.115         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 14.2           |
|    mean_reward          | -4.82          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.284         |
|    ep_len_mean          | 16.1           |
|    ep_rew_mean          | -4.64          |
| time/                   |                |
|    fps                  | 762            |
|    iterations           | 88             |
|    time_elapsed         | 1182           |
|    total_timesteps      | 901120         |
| train/                  |                |
|    approx_kl            | 0.029515231    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0456         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.059          |
|    cost_value_loss      | 3.95e-07       |
|    early_stop_epoch     | 1              |
|    entropy_loss         | -0.0369        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0583         |
|    mean_cost_advantages | -0.00033089286 |
|    mean_reward_advan... | 0.052281536    |
|    n_updates            | 870            |
|    nu                   | 3.76           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000194      |
|    reward_explained_... | 0.933          |
|    reward_value_loss    | 0.103          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.08       |
|    mean_ep_length       | 12.4        |
|    mean_reward          | -4.22       |
|    true_cost            | 0           |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.325      |
|    ep_len_mean          | 12.3        |
|    ep_rew_mean          | -4.1        |
| time/                   |             |
|    fps                  | 765         |
|    iterations           | 89          |
|    time_elapsed         | 1191        |
|    total_timesteps      | 911360      |
| train/                  |             |
|    approx_kl            | 0.02318403  |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.3         |
|    cost_value_loss      | 0.000117    |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0947     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.231       |
|    mean_cost_advantages | 0.005338027 |
|    mean_reward_advan... | -0.47386122 |
|    n_updates            | 880         |
|    nu                   | 3.76        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.005      |
|    reward_explained_... | 0.396       |
|    reward_value_loss    | 0.629       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 14.2         |
|    mean_reward          | -5.02        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.333       |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | -4.07        |
| time/                   |              |
|    fps                  | 767          |
|    iterations           | 90           |
|    time_elapsed         | 1200         |
|    total_timesteps      | 921600       |
| train/                  |              |
|    approx_kl            | 0.043955326  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.148        |
|    cost_value_loss      | 1.28e-05     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0168      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.219        |
|    mean_cost_advantages | 0.0034915663 |
|    mean_reward_advan... | 0.21408907   |
|    n_updates            | 890          |
|    nu                   | 3.76         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00395     |
|    reward_explained_... | 0.807        |
|    reward_value_loss    | 0.284        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.08        |
|    mean_ep_length       | 11           |
|    mean_reward          | -3.7         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.334       |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | -3.79        |
| time/                   |              |
|    fps                  | 769          |
|    iterations           | 91           |
|    time_elapsed         | 1210         |
|    total_timesteps      | 931840       |
| train/                  |              |
|    approx_kl            | 0.016879035  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0915       |
|    cost_value_loss      | 1.64e-06     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.00332     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0823       |
|    mean_cost_advantages | 0.0015736312 |
|    mean_reward_advan... | 0.15769067   |
|    n_updates            | 900          |
|    nu                   | 3.76         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0011      |
|    reward_explained_... | 0.924        |
|    reward_value_loss    | 0.126        |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 11.4           |
|    mean_reward          | -3.77          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.333         |
|    ep_len_mean          | 11.4           |
|    ep_rew_mean          | -3.82          |
| time/                   |                |
|    fps                  | 767            |
|    iterations           | 92             |
|    time_elapsed         | 1228           |
|    total_timesteps      | 942080         |
| train/                  |                |
|    approx_kl            | 0.00057404285  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.000186       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0796        |
|    cost_value_loss      | 2.75e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.000336      |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0465         |
|    mean_cost_advantages | -0.00016448236 |
|    mean_reward_advan... | 0.030724648    |
|    n_updates            | 910            |
|    nu                   | 3.76           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -1.99e-05      |
|    reward_explained_... | 0.924          |
|    reward_value_loss    | 0.121          |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | -3.38          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.333         |
|    ep_len_mean          | 11.6           |
|    ep_rew_mean          | -3.89          |
| time/                   |                |
|    fps                  | 764            |
|    iterations           | 93             |
|    time_elapsed         | 1246           |
|    total_timesteps      | 952320         |
| train/                  |                |
|    approx_kl            | 9.587621e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 8.79e-05       |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0377        |
|    cost_value_loss      | 4.39e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.00023       |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0736         |
|    mean_cost_advantages | -4.2327603e-05 |
|    mean_reward_advan... | 0.028262204    |
|    n_updates            | 920            |
|    nu                   | 3.76           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -5.81e-07      |
|    reward_explained_... | 0.936          |
|    reward_value_loss    | 0.0983         |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | -3.87         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.333        |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | -3.79         |
| time/                   |               |
|    fps                  | 761           |
|    iterations           | 94            |
|    time_elapsed         | 1264          |
|    total_timesteps      | 962560        |
| train/                  |               |
|    approx_kl            | 1.9131228e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.353        |
|    cost_value_loss      | 5.83e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.000237     |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0682        |
|    mean_cost_advantages | 0.0011315314  |
|    mean_reward_advan... | 0.022743825   |
|    n_updates            | 930           |
|    nu                   | 3.76          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.29e-08     |
|    reward_explained_... | 0.937         |
|    reward_value_loss    | 0.0975        |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 12.8          |
|    mean_reward          | -4.35         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.332        |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | -3.84         |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 95            |
|    time_elapsed         | 1282          |
|    total_timesteps      | 972800        |
| train/                  |               |
|    approx_kl            | 3.2640994e-06 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0864        |
|    cost_value_loss      | 8.98e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.000271     |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0335        |
|    mean_cost_advantages | 0.00053417333 |
|    mean_reward_advan... | -0.016997343  |
|    n_updates            | 940           |
|    nu                   | 3.76          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.51e-08     |
|    reward_explained_... | 0.93          |
|    reward_value_loss    | 0.106         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.08         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -4.03         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.333        |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | -3.73         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 96            |
|    time_elapsed         | 1300          |
|    total_timesteps      | 983040        |
| train/                  |               |
|    approx_kl            | 2.844259e-07  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.128        |
|    cost_value_loss      | 5.29e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.000279     |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0374        |
|    mean_cost_advantages | 0.00033405176 |
|    mean_reward_advan... | -0.03917309   |
|    n_updates            | 950           |
|    nu                   | 3.76          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -8.11e-08     |
|    reward_explained_... | 0.926         |
|    reward_value_loss    | 0.108         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 11             |
|    mean_reward          | -3.65          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.333         |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | -3.83          |
| time/                   |                |
|    fps                  | 753            |
|    iterations           | 97             |
|    time_elapsed         | 1318           |
|    total_timesteps      | 993280         |
| train/                  |                |
|    approx_kl            | 8.246442e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0902        |
|    cost_value_loss      | 5.29e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.000466      |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0385         |
|    mean_cost_advantages | -0.00010658216 |
|    mean_reward_advan... | 0.039797682    |
|    n_updates            | 960            |
|    nu                   | 3.76           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -9.49e-08      |
|    reward_explained_... | 0.941          |
|    reward_value_loss    | 0.0902         |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.08          |
|    mean_ep_length       | 10.6           |
|    mean_reward          | -3.43          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.333         |
|    ep_len_mean          | 11.5           |
|    ep_rew_mean          | -3.82          |
| time/                   |                |
|    fps                  | 750            |
|    iterations           | 98             |
|    time_elapsed         | 1336           |
|    total_timesteps      | 1003520        |
| train/                  |                |
|    approx_kl            | -4.0585546e-05 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.009         |
|    cost_value_loss      | 5.73e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.000566      |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0273         |
|    mean_cost_advantages | -0.00017178683 |
|    mean_reward_advan... | 0.020231787    |
|    n_updates            | 970            |
|    nu                   | 3.76           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 4.82e-07       |
|    reward_explained_... | 0.94           |
|    reward_value_loss    | 0.0907         |
|    total_cost           | 0.0            |
--------------------------------------------
[32mTime taken: 22.57 minutes
/home/jovyan/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)