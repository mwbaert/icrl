[32;1mConfigured folder ./cpg/wandb/run-20220609_114902-3pgm9js9/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
----------------------------------
| eval/               |          |
|    best_mean_reward | -3.5e+05 |
|    mean_ep_length   | 140      |
|    mean_reward      | -3.5e+05 |
|    true_cost        | 0.0615   |
| infos/              |          |
|    cost             | 0.0399   |
| rollout/            |          |
|    adjusted_reward  | -3.83    |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 1.84e+03 |
| time/               |          |
|    fps              | 883      |
|    iterations       | 1        |
|    time_elapsed     | 2        |
|    total_timesteps  | 2048     |
----------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.5e+05     |
|    mean_ep_length       | 130          |
|    mean_reward          | -4.04e+05    |
|    true_cost            | 0.0532       |
| infos/                  |              |
|    cost                 | 0.0106       |
| rollout/                |              |
|    adjusted_reward      | 15.2         |
|    ep_len_mean          | 164          |
|    ep_rew_mean          | 3.62e+03     |
| time/                   |              |
|    fps                  | 804          |
|    iterations           | 2            |
|    time_elapsed         | 5            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0151735945 |
|    average_cost         | 0.061523438  |
|    clip_fraction        | 0.0713       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.19        |
|    cost_value_loss      | 0.294        |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -1.38        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.498        |
|    mean_cost_advantages | 0.51165473   |
|    mean_reward_advan... | -0.12428294  |
|    n_updates            | 10           |
|    nu                   | 1.06         |
|    nu_loss              | -0.0615      |
|    policy_gradient_loss | -0.00521     |
|    reward_explained_... | -13.4        |
|    reward_value_loss    | 1.07         |
|    total_cost           | 126.0        |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.5e+05    |
|    mean_ep_length       | 159         |
|    mean_reward          | -4.64e+05   |
|    true_cost            | 0.04        |
| infos/                  |             |
|    cost                 | 0.0319      |
| rollout/                |             |
|    adjusted_reward      | 37.1        |
|    ep_len_mean          | 141         |
|    ep_rew_mean          | 5e+03       |
| time/                   |             |
|    fps                  | 782         |
|    iterations           | 3           |
|    time_elapsed         | 7           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.021321561 |
|    average_cost         | 0.053222656 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.85       |
|    cost_value_loss      | 0.168       |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.408       |
|    mean_cost_advantages | 0.28906557  |
|    mean_reward_advan... | 0.5064568   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.0566     |
|    policy_gradient_loss | -0.00628    |
|    reward_explained_... | -67.9       |
|    reward_value_loss    | 2.29        |
|    total_cost           | 109.0       |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.36e+05   |
|    mean_ep_length       | 60.6        |
|    mean_reward          | -1.36e+05   |
|    true_cost            | 0.0278      |
| infos/                  |             |
|    cost                 | 0.0215      |
| rollout/                |             |
|    adjusted_reward      | 158         |
|    ep_len_mean          | 98.4        |
|    ep_rew_mean          | 7.15e+03    |
| time/                   |             |
|    fps                  | 795         |
|    iterations           | 4           |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015916072 |
|    average_cost         | 0.040039062 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.07       |
|    cost_value_loss      | 0.136       |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.54        |
|    mean_cost_advantages | 0.058368824 |
|    mean_reward_advan... | 0.92251116  |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.0453     |
|    policy_gradient_loss | -0.00913    |
|    reward_explained_... | -6.02       |
|    reward_value_loss    | 3.11        |
|    total_cost           | 82.0        |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.02e+05   |
|    mean_ep_length       | 64.2        |
|    mean_reward          | -1.02e+05   |
|    true_cost            | 0.0254      |
| infos/                  |             |
|    cost                 | 0.0273      |
| rollout/                |             |
|    adjusted_reward      | 340         |
|    ep_len_mean          | 38.9        |
|    ep_rew_mean          | 9.78e+03    |
| time/                   |             |
|    fps                  | 749         |
|    iterations           | 5           |
|    time_elapsed         | 13          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.014367132 |
|    average_cost         | 0.027832031 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.09       |
|    cost_value_loss      | 0.0844      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.25       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.83        |
|    mean_cost_advantages | -0.18536782 |
|    mean_reward_advan... | 1.945643    |
|    n_updates            | 40          |
|    nu                   | 1.26        |
|    nu_loss              | -0.0333     |
|    policy_gradient_loss | -0.0139     |
|    reward_explained_... | -1.15       |
|    reward_value_loss    | 3.68        |
|    total_cost           | 57.0        |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3e+04      |
|    mean_ep_length       | 28.2        |
|    mean_reward          | -3e+04      |
|    true_cost            | 0.0225      |
| infos/                  |             |
|    cost                 | 0.00558     |
| rollout/                |             |
|    adjusted_reward      | 467         |
|    ep_len_mean          | 23          |
|    ep_rew_mean          | 9.89e+03    |
| time/                   |             |
|    fps                  | 771         |
|    iterations           | 6           |
|    time_elapsed         | 15          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.016318556 |
|    average_cost         | 0.025390625 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.227      |
|    cost_value_loss      | 0.0691      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.17       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.747       |
|    mean_cost_advantages | -0.234549   |
|    mean_reward_advan... | 1.9830518   |
|    n_updates            | 50          |
|    nu                   | 1.33        |
|    nu_loss              | -0.0321     |
|    policy_gradient_loss | -0.0094     |
|    reward_explained_... | 0.622       |
|    reward_value_loss    | 2.3         |
|    total_cost           | 52.0        |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 19.2        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0137      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 535         |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 7           |
|    time_elapsed         | 19          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011342593 |
|    average_cost         | 0.022460938 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.171       |
|    cost_value_loss      | 0.0439      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.994      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    mean_cost_advantages | -0.1857908  |
|    mean_reward_advan... | 0.569404    |
|    n_updates            | 60          |
|    nu                   | 1.4         |
|    nu_loss              | -0.0299     |
|    policy_gradient_loss | -0.0145     |
|    reward_explained_... | 0.825       |
|    reward_value_loss    | 0.492       |
|    total_cost           | 46.0        |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 567          |
|    ep_len_mean          | 18.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 8            |
|    time_elapsed         | 21           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.015770903  |
|    average_cost         | 0.013671875  |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.688       |
|    cost_value_loss      | 0.0363       |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.888       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0514       |
|    mean_cost_advantages | -0.111037955 |
|    mean_reward_advan... | -0.4074378   |
|    n_updates            | 70           |
|    nu                   | 1.46         |
|    nu_loss              | -0.0191      |
|    policy_gradient_loss | -0.0059      |
|    reward_explained_... | 0.472        |
|    reward_value_loss    | 0.111        |
|    total_cost           | 28.0         |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.6         |
|    mean_reward          | -4.01e+03    |
|    true_cost            | 0.00293      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 634          |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 765          |
|    iterations           | 9            |
|    time_elapsed         | 24           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.015224601  |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.0816       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0933      |
|    cost_value_loss      | 0.00751      |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.812       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0149       |
|    mean_cost_advantages | -0.07663276  |
|    mean_reward_advan... | -0.5256201   |
|    n_updates            | 80           |
|    nu                   | 1.52         |
|    nu_loss              | -0.005       |
|    policy_gradient_loss | -0.00511     |
|    reward_explained_... | 0.497        |
|    reward_value_loss    | 0.0532       |
|    total_cost           | 7.0          |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00195      |
| infos/                  |              |
|    cost                 | 0.00623      |
| rollout/                |              |
|    adjusted_reward      | 671          |
|    ep_len_mean          | 15.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 772          |
|    iterations           | 10           |
|    time_elapsed         | 26           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.01664157   |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.4         |
|    cost_value_loss      | 0.00484      |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.751       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0123       |
|    mean_cost_advantages | -0.026032774 |
|    mean_reward_advan... | -0.42743564  |
|    n_updates            | 90           |
|    nu                   | 1.58         |
|    nu_loss              | -0.00446     |
|    policy_gradient_loss | -0.00696     |
|    reward_explained_... | 0.4          |
|    reward_value_loss    | 0.0331       |
|    total_cost           | 6.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0.00642       |
| rollout/                |               |
|    adjusted_reward      | 702           |
|    ep_len_mean          | 14.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 763           |
|    iterations           | 11            |
|    time_elapsed         | 29            |
|    total_timesteps      | 22528         |
| train/                  |               |
|    approx_kl            | 0.014558245   |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.129         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -16.6         |
|    cost_value_loss      | 0.00405       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.659        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0123       |
|    mean_cost_advantages | -0.0032132734 |
|    mean_reward_advan... | -0.30318916   |
|    n_updates            | 100           |
|    nu                   | 1.63          |
|    nu_loss              | -0.00308      |
|    policy_gradient_loss | -0.00691      |
|    reward_explained_... | 0.48          |
|    reward_value_loss    | 0.0147        |
|    total_cost           | 4.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 755           |
|    ep_len_mean          | 13.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 777           |
|    iterations           | 12            |
|    time_elapsed         | 31            |
|    total_timesteps      | 24576         |
| train/                  |               |
|    approx_kl            | 0.01648036    |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.101         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -17.1         |
|    cost_value_loss      | 0.0023        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.622        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00966      |
|    mean_cost_advantages | -0.0060214475 |
|    mean_reward_advan... | -0.23361619   |
|    n_updates            | 110           |
|    nu                   | 1.67          |
|    nu_loss              | -0.00159      |
|    policy_gradient_loss | -0.00637      |
|    reward_explained_... | 0.715         |
|    reward_value_loss    | 0.0148        |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 783           |
|    ep_len_mean          | 13            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 770           |
|    iterations           | 13            |
|    time_elapsed         | 34            |
|    total_timesteps      | 26624         |
| train/                  |               |
|    approx_kl            | 0.015584303   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.11          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -16.3         |
|    cost_value_loss      | 0.0012        |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.52         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00156       |
|    mean_cost_advantages | -0.0021397183 |
|    mean_reward_advan... | -0.17806986   |
|    n_updates            | 120           |
|    nu                   | 1.72          |
|    nu_loss              | -0.000818     |
|    policy_gradient_loss | -0.00696      |
|    reward_explained_... | 0.72          |
|    reward_value_loss    | 0.00694       |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 811           |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 779           |
|    iterations           | 14            |
|    time_elapsed         | 36            |
|    total_timesteps      | 28672         |
| train/                  |               |
|    approx_kl            | 0.015610724   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.115         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -30.5         |
|    cost_value_loss      | 0.00245       |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.467        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00502       |
|    mean_cost_advantages | 1.3858662e-06 |
|    mean_reward_advan... | -0.15070696   |
|    n_updates            | 130           |
|    nu                   | 1.76          |
|    nu_loss              | -0.00168      |
|    policy_gradient_loss | -0.00463      |
|    reward_explained_... | 0.833         |
|    reward_value_loss    | 0.00523       |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 814           |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 773           |
|    iterations           | 15            |
|    time_elapsed         | 39            |
|    total_timesteps      | 30720         |
| train/                  |               |
|    approx_kl            | 0.0073773963  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0491        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -30.4         |
|    cost_value_loss      | 0.00141       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.418        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00168      |
|    mean_cost_advantages | -0.0033239645 |
|    mean_reward_advan... | -0.1264104    |
|    n_updates            | 140           |
|    nu                   | 1.79          |
|    nu_loss              | -0.000857     |
|    policy_gradient_loss | -0.00295      |
|    reward_explained_... | 0.9           |
|    reward_value_loss    | 0.00229       |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0.00734       |
| rollout/                |               |
|    adjusted_reward      | 823           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 767           |
|    iterations           | 16            |
|    time_elapsed         | 42            |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 0.0071993335  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0731        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0153        |
|    cost_value_loss      | 4.97e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.383        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0128       |
|    mean_cost_advantages | -0.0031506275 |
|    mean_reward_advan... | -0.11962279   |
|    n_updates            | 150           |
|    nu                   | 1.82          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00415      |
|    reward_explained_... | 0.865         |
|    reward_value_loss    | 0.00309       |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 827           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 763           |
|    iterations           | 17            |
|    time_elapsed         | 45            |
|    total_timesteps      | 34816         |
| train/                  |               |
|    approx_kl            | 0.0072127115  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0832        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -68.3         |
|    cost_value_loss      | 0.00149       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.343        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00114      |
|    mean_cost_advantages | 0.00093707244 |
|    mean_reward_advan... | -0.095697835  |
|    n_updates            | 160           |
|    nu                   | 1.85          |
|    nu_loss              | -0.000891     |
|    policy_gradient_loss | -0.00258      |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 0.00112       |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 826          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 18           |
|    time_elapsed         | 48           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.004573161  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0743       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.028        |
|    cost_value_loss      | 1.6e-05      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.327       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000613    |
|    mean_cost_advantages | 0.0018553829 |
|    mean_reward_advan... | -0.08054295  |
|    n_updates            | 170          |
|    nu                   | 1.88         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00173     |
|    reward_explained_... | 0.958        |
|    reward_value_loss    | 0.000898     |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 747            |
|    iterations           | 19             |
|    time_elapsed         | 52             |
|    total_timesteps      | 38912          |
| train/                  |                |
|    approx_kl            | 0.0068311756   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0533         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0274        |
|    cost_value_loss      | 1.56e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.29          |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00391       |
|    mean_cost_advantages | -0.00016852445 |
|    mean_reward_advan... | -0.0705594     |
|    n_updates            | 180            |
|    nu                   | 1.91           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00244       |
|    reward_explained_... | 0.949          |
|    reward_value_loss    | 0.00101        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 829            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 735            |
|    iterations           | 20             |
|    time_elapsed         | 55             |
|    total_timesteps      | 40960          |
| train/                  |                |
|    approx_kl            | 0.0032228194   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0183         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0382        |
|    cost_value_loss      | 1.04e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.257         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000587       |
|    mean_cost_advantages | -4.0240717e-05 |
|    mean_reward_advan... | -0.05442418    |
|    n_updates            | 190            |
|    nu                   | 1.93           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000473      |
|    reward_explained_... | 0.974          |
|    reward_value_loss    | 0.000499       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 729           |
|    iterations           | 21            |
|    time_elapsed         | 58            |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | -8.771429e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0289        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0454       |
|    cost_value_loss      | 7.29e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.232        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0015        |
|    mean_cost_advantages | -8.816067e-05 |
|    mean_reward_advan... | -0.050164     |
|    n_updates            | 200           |
|    nu                   | 1.95          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00102      |
|    reward_explained_... | 0.98          |
|    reward_value_loss    | 0.000383      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 724            |
|    iterations           | 22             |
|    time_elapsed         | 62             |
|    total_timesteps      | 45056          |
| train/                  |                |
|    approx_kl            | 0.0010656512   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0288         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0319        |
|    cost_value_loss      | 5.82e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.22          |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00636        |
|    mean_cost_advantages | -0.00033448965 |
|    mean_reward_advan... | -0.039618097   |
|    n_updates            | 210            |
|    nu                   | 1.97           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000227      |
|    reward_explained_... | 0.988          |
|    reward_value_loss    | 0.000238       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 713           |
|    iterations           | 23            |
|    time_elapsed         | 65            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.004726214   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0418        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0531       |
|    cost_value_loss      | 5.36e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.206        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000851      |
|    mean_cost_advantages | -0.0005508704 |
|    mean_reward_advan... | -0.03909213   |
|    n_updates            | 220           |
|    nu                   | 1.99          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00138      |
|    reward_explained_... | 0.986         |
|    reward_value_loss    | 0.000278      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 711            |
|    iterations           | 24             |
|    time_elapsed         | 69             |
|    total_timesteps      | 49152          |
| train/                  |                |
|    approx_kl            | 0.012596768    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0571         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0421        |
|    cost_value_loss      | 4.65e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.204         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00212       |
|    mean_cost_advantages | -0.00020894843 |
|    mean_reward_advan... | -0.03261656    |
|    n_updates            | 230            |
|    nu                   | 2              |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00132       |
|    reward_explained_... | 0.988          |
|    reward_value_loss    | 0.000233       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 695            |
|    iterations           | 25             |
|    time_elapsed         | 73             |
|    total_timesteps      | 51200          |
| train/                  |                |
|    approx_kl            | 0.0036088356   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0556         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0454        |
|    cost_value_loss      | 3.64e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.193         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00315       |
|    mean_cost_advantages | -8.2747974e-05 |
|    mean_reward_advan... | -0.028162632   |
|    n_updates            | 240            |
|    nu                   | 2.02           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00133       |
|    reward_explained_... | 0.991          |
|    reward_value_loss    | 0.000164       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 837            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 683            |
|    iterations           | 26             |
|    time_elapsed         | 77             |
|    total_timesteps      | 53248          |
| train/                  |                |
|    approx_kl            | 0.0049600345   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0415         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0722        |
|    cost_value_loss      | 3.07e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.191         |
|    learning_rate        | 0.0003         |
|    loss                 | -4.92e-05      |
|    mean_cost_advantages | -0.00035018433 |
|    mean_reward_advan... | -0.025170851   |
|    n_updates            | 250            |
|    nu                   | 2.03           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -9.86e-05      |
|    reward_explained_... | 0.994          |
|    reward_value_loss    | 0.00012        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 676            |
|    iterations           | 27             |
|    time_elapsed         | 81             |
|    total_timesteps      | 55296          |
| train/                  |                |
|    approx_kl            | 0.0025108806   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0372         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0442        |
|    cost_value_loss      | 2.37e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.185         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00134       |
|    mean_cost_advantages | -0.00029351772 |
|    mean_reward_advan... | -0.022713285   |
|    n_updates            | 260            |
|    nu                   | 2.04           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000207       |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 6.39e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000488       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 670            |
|    iterations           | 28             |
|    time_elapsed         | 85             |
|    total_timesteps      | 57344          |
| train/                  |                |
|    approx_kl            | 0.0099257855   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0335         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0405        |
|    cost_value_loss      | 2.51e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.192         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000449       |
|    mean_cost_advantages | -0.00028650925 |
|    mean_reward_advan... | -0.021944288   |
|    n_updates            | 270            |
|    nu                   | 2.05           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000594      |
|    reward_explained_... | 0.995          |
|    reward_value_loss    | 0.000102       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 668           |
|    iterations           | 29            |
|    time_elapsed         | 88            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.0022893948  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0259        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -125          |
|    cost_value_loss      | 0.00256       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.189        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00393       |
|    mean_cost_advantages | 0.0028750966  |
|    mean_reward_advan... | -0.019451804  |
|    n_updates            | 280           |
|    nu                   | 2.06          |
|    nu_loss              | -0.001        |
|    policy_gradient_loss | -0.000625     |
|    reward_explained_... | 0.97          |
|    reward_value_loss    | 0.000466      |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 668          |
|    iterations           | 30           |
|    time_elapsed         | 91           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0011734797 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.00791     |
|    cost_value_loss      | 6.86e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.173       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0014       |
|    mean_cost_advantages | 0.003593301  |
|    mean_reward_advan... | -0.012721386 |
|    n_updates            | 290          |
|    nu                   | 2.07         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -8.54e-05    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 3.62e-05     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 666           |
|    iterations           | 31            |
|    time_elapsed         | 95            |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 0.004904139   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.05          |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0422        |
|    cost_value_loss      | 1.66e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.167        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00019      |
|    mean_cost_advantages | 0.00061541866 |
|    mean_reward_advan... | -0.014443285  |
|    n_updates            | 300           |
|    nu                   | 2.08          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000786     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.84e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 665           |
|    iterations           | 32            |
|    time_elapsed         | 98            |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.0013831558  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0265        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0489       |
|    cost_value_loss      | 1.24e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.165        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00273       |
|    mean_cost_advantages | 1.8249051e-05 |
|    mean_reward_advan... | -0.014999818  |
|    n_updates            | 310           |
|    nu                   | 2.09          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000135     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 7.23e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 665           |
|    iterations           | 33            |
|    time_elapsed         | 101           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.009535206   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0603        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0476       |
|    cost_value_loss      | 1.03e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.172        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00479      |
|    mean_cost_advantages | 8.113379e-05  |
|    mean_reward_advan... | -0.0146778235 |
|    n_updates            | 320           |
|    nu                   | 2.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00138      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.4e-05       |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 664           |
|    iterations           | 34            |
|    time_elapsed         | 104           |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.006643202   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0403        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0481       |
|    cost_value_loss      | 9.09e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.168        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00285       |
|    mean_cost_advantages | 8.7930035e-05 |
|    mean_reward_advan... | -0.013077151  |
|    n_updates            | 330           |
|    nu                   | 2.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000634     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 6.3e-05       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 836            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 659            |
|    iterations           | 35             |
|    time_elapsed         | 108            |
|    total_timesteps      | 71680          |
| train/                  |                |
|    approx_kl            | 0.001834041    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0409         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0541        |
|    cost_value_loss      | 7.34e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.164         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000921      |
|    mean_cost_advantages | -0.00010120326 |
|    mean_reward_advan... | -0.010582922   |
|    n_updates            | 340            |
|    nu                   | 2.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000196      |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 6.61e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 657           |
|    iterations           | 36            |
|    time_elapsed         | 112           |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.000288649   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0418        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0442       |
|    cost_value_loss      | 7.08e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.163        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00153      |
|    mean_cost_advantages | -7.558153e-05 |
|    mean_reward_advan... | -0.013973828  |
|    n_updates            | 350           |
|    nu                   | 2.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000141      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 2.13e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 826            |
|    ep_len_mean          | 12.2           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 657            |
|    iterations           | 37             |
|    time_elapsed         | 115            |
|    total_timesteps      | 75776          |
| train/                  |                |
|    approx_kl            | -0.0004220293  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0366         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0621        |
|    cost_value_loss      | 6.31e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.158         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000814       |
|    mean_cost_advantages | -0.00010412556 |
|    mean_reward_advan... | -0.011407185   |
|    n_updates            | 360            |
|    nu                   | 2.12           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000319      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 4.39e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 658          |
|    iterations           | 38           |
|    time_elapsed         | 118          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0075651323 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.307        |
|    cost_value_loss      | 5.16e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.135       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00992     |
|    mean_cost_advantages | -0.000397419 |
|    mean_reward_advan... | -0.013916409 |
|    n_updates            | 370          |
|    nu                   | 2.12         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00202     |
|    reward_explained_... | 0.947        |
|    reward_value_loss    | 0.00115      |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 658           |
|    iterations           | 39            |
|    time_elapsed         | 121           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.0013939366  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0199        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0323       |
|    cost_value_loss      | 6.77e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.119        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00013       |
|    mean_cost_advantages | 0.00043849958 |
|    mean_reward_advan... | -0.0055785007 |
|    n_updates            | 380           |
|    nu                   | 2.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000151     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.04e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 662           |
|    iterations           | 40            |
|    time_elapsed         | 123           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.015374694   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0361        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0523       |
|    cost_value_loss      | 3.07e-07      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.132        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00114       |
|    mean_cost_advantages | -4.386055e-05 |
|    mean_reward_advan... | -0.006363351  |
|    n_updates            | 390           |
|    nu                   | 2.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00137      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.33e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 661           |
|    iterations           | 41            |
|    time_elapsed         | 126           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.0011375066  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.25e+04     |
|    cost_value_loss      | 0.00357       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.129        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000102      |
|    mean_cost_advantages | 0.0036731104  |
|    mean_reward_advan... | -0.0069348635 |
|    n_updates            | 400           |
|    nu                   | 2.14          |
|    nu_loss              | -0.00104      |
|    policy_gradient_loss | -0.000861     |
|    reward_explained_... | 0.989         |
|    reward_value_loss    | 9.78e-05      |
|    total_cost           | 1.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 661          |
|    iterations           | 42           |
|    time_elapsed         | 130          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0021384084 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.065        |
|    cost_value_loss      | 1.29e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.115       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000375     |
|    mean_cost_advantages | 0.007527641  |
|    mean_reward_advan... | -0.006056552 |
|    n_updates            | 410          |
|    nu                   | 2.14         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000217    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 6.41e-05     |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 660          |
|    iterations           | 43           |
|    time_elapsed         | 133          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0011515289 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.129       |
|    cost_value_loss      | 5.03e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0977      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00195     |
|    mean_cost_advantages | -0.00111231  |
|    mean_reward_advan... | -0.008697663 |
|    n_updates            | 420          |
|    nu                   | 2.14         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000653    |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 5.1e-05      |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 657            |
|    iterations           | 44             |
|    time_elapsed         | 137            |
|    total_timesteps      | 90112          |
| train/                  |                |
|    approx_kl            | 0.0016123363   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0105         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.00403        |
|    cost_value_loss      | 1.6e-07        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0767        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000137      |
|    mean_cost_advantages | -0.00043347492 |
|    mean_reward_advan... | -0.006776823   |
|    n_updates            | 430            |
|    nu                   | 2.14           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000402      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.02e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 657            |
|    iterations           | 45             |
|    time_elapsed         | 140            |
|    total_timesteps      | 92160          |
| train/                  |                |
|    approx_kl            | 0.00050337834  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00928        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0536        |
|    cost_value_loss      | 1.04e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0641        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00143       |
|    mean_cost_advantages | -0.00012247844 |
|    mean_reward_advan... | -0.008708252   |
|    n_updates            | 440            |
|    nu                   | 2.15           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -8.45e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 7.68e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 657            |
|    iterations           | 46             |
|    time_elapsed         | 143            |
|    total_timesteps      | 94208          |
| train/                  |                |
|    approx_kl            | 0.0027458146   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0101         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0459        |
|    cost_value_loss      | 8.38e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.053         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00273       |
|    mean_cost_advantages | -3.6508667e-05 |
|    mean_reward_advan... | -0.006988965   |
|    n_updates            | 450            |
|    nu                   | 2.15           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00029       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.01e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 659            |
|    iterations           | 47             |
|    time_elapsed         | 145            |
|    total_timesteps      | 96256          |
| train/                  |                |
|    approx_kl            | 0.020914156    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0171         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.053         |
|    cost_value_loss      | 6.5e-08        |
|    early_stop_epoch     | 4              |
|    entropy_loss         | -0.0516        |
|    learning_rate        | 0.0003         |
|    loss                 | -1.64e-05      |
|    mean_cost_advantages | -2.2640526e-05 |
|    mean_reward_advan... | -0.006139587   |
|    n_updates            | 460            |
|    nu                   | 2.15           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00103       |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 3.57e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 658           |
|    iterations           | 48            |
|    time_elapsed         | 149           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.0055147717  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0201        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0411       |
|    cost_value_loss      | 9.22e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0567       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000822     |
|    mean_cost_advantages | -3.993575e-05 |
|    mean_reward_advan... | -0.006493967  |
|    n_updates            | 470           |
|    nu                   | 2.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000425     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2e-05         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 658           |
|    iterations           | 49            |
|    time_elapsed         | 152           |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.0012785175  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0128        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0648       |
|    cost_value_loss      | 4.23e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.063        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00135      |
|    mean_cost_advantages | -3.945384e-05 |
|    mean_reward_advan... | -0.0055612964 |
|    n_updates            | 480           |
|    nu                   | 2.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000213      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 9.44e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12.2           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 656            |
|    iterations           | 50             |
|    time_elapsed         | 155            |
|    total_timesteps      | 102400         |
| train/                  |                |
|    approx_kl            | 0.0021225668   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00708        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0638        |
|    cost_value_loss      | 3.48e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.045         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00223        |
|    mean_cost_advantages | -4.7354264e-05 |
|    mean_reward_advan... | -0.00524869    |
|    n_updates            | 490            |
|    nu                   | 2.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00062       |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 3.8e-05        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 656            |
|    iterations           | 51             |
|    time_elapsed         | 159            |
|    total_timesteps      | 104448         |
| train/                  |                |
|    approx_kl            | 0.0105812475   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00518        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0519        |
|    cost_value_loss      | 2.81e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0377        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00419       |
|    mean_cost_advantages | -4.5020497e-05 |
|    mean_reward_advan... | -0.0053428584  |
|    n_updates            | 500            |
|    nu                   | 2.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000686      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 3.79e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 655           |
|    iterations           | 52            |
|    time_elapsed         | 162           |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 0.01564663    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0207        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0331       |
|    cost_value_loss      | 5.47e-08      |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.0958       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000133     |
|    mean_cost_advantages | 7.85357e-06   |
|    mean_reward_advan... | -0.0049864743 |
|    n_updates            | 510           |
|    nu                   | 2.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000164     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2e-05         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 655           |
|    iterations           | 53            |
|    time_elapsed         | 165           |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.0015042995  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0358        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0491       |
|    cost_value_loss      | 4.02e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.122        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.17e-05      |
|    mean_cost_advantages | -1.557622e-05 |
|    mean_reward_advan... | -0.0027022243 |
|    n_updates            | 520           |
|    nu                   | 2.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 6.86e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 5.27e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 657            |
|    iterations           | 54             |
|    time_elapsed         | 168            |
|    total_timesteps      | 110592         |
| train/                  |                |
|    approx_kl            | 0.01986061     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0347         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0411        |
|    cost_value_loss      | 3.99e-08       |
|    early_stop_epoch     | 5              |
|    entropy_loss         | -0.135         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00731       |
|    mean_cost_advantages | -1.5168773e-06 |
|    mean_reward_advan... | -0.0043762242  |
|    n_updates            | 530            |
|    nu                   | 2.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00135       |
|    reward_explained_... | 0.996          |
|    reward_value_loss    | 8.89e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 655            |
|    iterations           | 55             |
|    time_elapsed         | 171            |
|    total_timesteps      | 112640         |
| train/                  |                |
|    approx_kl            | 0.0016361349   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.049          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0469        |
|    cost_value_loss      | 3.83e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.138         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00065       |
|    mean_cost_advantages | -3.3910055e-05 |
|    mean_reward_advan... | -0.004155106   |
|    n_updates            | 540            |
|    nu                   | 2.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000218      |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 5.59e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 651            |
|    iterations           | 56             |
|    time_elapsed         | 175            |
|    total_timesteps      | 114688         |
| train/                  |                |
|    approx_kl            | 0.003436082    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0224         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0737        |
|    cost_value_loss      | 5.95e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.121         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000861      |
|    mean_cost_advantages | -4.6100045e-05 |
|    mean_reward_advan... | -0.0026201408  |
|    n_updates            | 550            |
|    nu                   | 2.16           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000356      |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 6.69e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 649           |
|    iterations           | 57            |
|    time_elapsed         | 179           |
|    total_timesteps      | 116736        |
| train/                  |               |
|    approx_kl            | 0.00463733    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.046         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0465       |
|    cost_value_loss      | 5.12e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.111        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000949      |
|    mean_cost_advantages | 1.8750274e-05 |
|    mean_reward_advan... | -0.0026832663 |
|    n_updates            | 560           |
|    nu                   | 2.16          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000239     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.14e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 647           |
|    iterations           | 58            |
|    time_elapsed         | 183           |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 0.013246543   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0297        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.047        |
|    cost_value_loss      | 9.5e-08       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.123        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0048       |
|    mean_cost_advantages | 6.6764114e-05 |
|    mean_reward_advan... | -0.0032866993 |
|    n_updates            | 570           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000395     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.44e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 646            |
|    iterations           | 59             |
|    time_elapsed         | 186            |
|    total_timesteps      | 120832         |
| train/                  |                |
|    approx_kl            | 0.0023010455   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.024          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.312         |
|    cost_value_loss      | 2.97e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.147         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.0014        |
|    mean_cost_advantages | -0.00020982604 |
|    mean_reward_advan... | -0.0036853086  |
|    n_updates            | 580            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000259       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 3.43e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 645           |
|    iterations           | 60            |
|    time_elapsed         | 190           |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 0.0046536126  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.038         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00247       |
|    cost_value_loss      | 1.44e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.137        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000868      |
|    mean_cost_advantages | 6.421162e-05  |
|    mean_reward_advan... | -0.0039915433 |
|    n_updates            | 590           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000133     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 1.03e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 645           |
|    iterations           | 61            |
|    time_elapsed         | 193           |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.009828396   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0519        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0113        |
|    cost_value_loss      | 1.77e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.133        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000664      |
|    mean_cost_advantages | 5.9458354e-05 |
|    mean_reward_advan... | -0.0025413735 |
|    n_updates            | 600           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000954     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 5.2e-05       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 648            |
|    iterations           | 62             |
|    time_elapsed         | 195            |
|    total_timesteps      | 126976         |
| train/                  |                |
|    approx_kl            | 0.0154351955   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0633         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0344        |
|    cost_value_loss      | 2.32e-07       |
|    early_stop_epoch     | 2              |
|    entropy_loss         | -0.145         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00645       |
|    mean_cost_advantages | -3.5818324e-05 |
|    mean_reward_advan... | 0.00019893699  |
|    n_updates            | 610            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00216       |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 3.41e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 647            |
|    iterations           | 63             |
|    time_elapsed         | 199            |
|    total_timesteps      | 129024         |
| train/                  |                |
|    approx_kl            | 0.005128377    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0535         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0313        |
|    cost_value_loss      | 5.03e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.132         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000587      |
|    mean_cost_advantages | -4.3208465e-06 |
|    mean_reward_advan... | -0.0019791604  |
|    n_updates            | 620            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000507      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.32e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 647           |
|    iterations           | 64            |
|    time_elapsed         | 202           |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 0.0062877145  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.047         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00291       |
|    cost_value_loss      | 6.82e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.131        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00183      |
|    mean_cost_advantages | -0.0002073123 |
|    mean_reward_advan... | -0.002326929  |
|    n_updates            | 630           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00109      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 2.25e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 647           |
|    iterations           | 65            |
|    time_elapsed         | 205           |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.004537895   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0298        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.269        |
|    cost_value_loss      | 8.3e-07       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.121        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00115      |
|    mean_cost_advantages | -0.0005032633 |
|    mean_reward_advan... | -0.003057844  |
|    n_updates            | 640           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000291     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 7.83e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 648           |
|    iterations           | 66            |
|    time_elapsed         | 208           |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 0.004261898   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0288        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00719       |
|    cost_value_loss      | 8.73e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.126        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00259       |
|    mean_cost_advantages | 0.00074523763 |
|    mean_reward_advan... | -0.003356995  |
|    n_updates            | 650           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000136      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 4.39e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 646           |
|    iterations           | 67            |
|    time_elapsed         | 212           |
|    total_timesteps      | 137216        |
| train/                  |               |
|    approx_kl            | 0.0021905818  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.027         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0414        |
|    cost_value_loss      | 0.000104      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.141        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00089       |
|    mean_cost_advantages | 0.0001881659  |
|    mean_reward_advan... | -0.0028699469 |
|    n_updates            | 660           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000137     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 6.2e-05       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 645            |
|    iterations           | 68             |
|    time_elapsed         | 215            |
|    total_timesteps      | 139264         |
| train/                  |                |
|    approx_kl            | 0.0044230716   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0247         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.052         |
|    cost_value_loss      | 3.25e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.148         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000304      |
|    mean_cost_advantages | -8.5548745e-06 |
|    mean_reward_advan... | -0.0018237003  |
|    n_updates            | 670            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000182      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.25e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 644           |
|    iterations           | 69            |
|    time_elapsed         | 219           |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 0.0015210505  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0391        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.035        |
|    cost_value_loss      | 1.69e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.155        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.9e-05       |
|    mean_cost_advantages | -3.466442e-06 |
|    mean_reward_advan... | -0.0031334923 |
|    n_updates            | 680           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000219     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.22e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 642           |
|    iterations           | 70            |
|    time_elapsed         | 223           |
|    total_timesteps      | 143360        |
| train/                  |               |
|    approx_kl            | 0.0040073385  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0296        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0479       |
|    cost_value_loss      | 1.75e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.144        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00174      |
|    mean_cost_advantages | -1.588065e-05 |
|    mean_reward_advan... | -0.0021983236 |
|    n_updates            | 690           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 6.62e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 7.95e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 641          |
|    iterations           | 71           |
|    time_elapsed         | 226          |
|    total_timesteps      | 145408       |
| train/                  |              |
|    approx_kl            | 0.0010953627 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.00462      |
|    cost_value_loss      | 1.92e-08     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.144       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000246    |
|    mean_cost_advantages | 3.203127e-05 |
|    mean_reward_advan... | -0.00233356  |
|    n_updates            | 700          |
|    nu                   | 2.17         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00019     |
|    reward_explained_... | 1            |
|    reward_value_loss    | 6.47e-06     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 640           |
|    iterations           | 72            |
|    time_elapsed         | 230           |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 0.00063010264 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0256        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0258       |
|    cost_value_loss      | 3.95e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.143        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00157       |
|    mean_cost_advantages | 1.8842225e-05 |
|    mean_reward_advan... | -0.0020723084 |
|    n_updates            | 710           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000551     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3e-06         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 640            |
|    iterations           | 73             |
|    time_elapsed         | 233            |
|    total_timesteps      | 149504         |
| train/                  |                |
|    approx_kl            | 0.0004908048   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0176         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0398        |
|    cost_value_loss      | 6.08e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.135         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0025         |
|    mean_cost_advantages | -2.5094323e-07 |
|    mean_reward_advan... | -0.0015963981  |
|    n_updates            | 720            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000298      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.43e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 640           |
|    iterations           | 74            |
|    time_elapsed         | 236           |
|    total_timesteps      | 151552        |
| train/                  |               |
|    approx_kl            | 0.0037760932  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0406        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0696       |
|    cost_value_loss      | 1.31e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.135        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00302      |
|    mean_cost_advantages | -3.618545e-05 |
|    mean_reward_advan... | 0.0012298148  |
|    n_updates            | 730           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000189     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 6.85e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 75             |
|    time_elapsed         | 240            |
|    total_timesteps      | 153600         |
| train/                  |                |
|    approx_kl            | 0.0029555173   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0206         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0682        |
|    cost_value_loss      | 3.54e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.132         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00208       |
|    mean_cost_advantages | -0.00011741526 |
|    mean_reward_advan... | -0.0016467867  |
|    n_updates            | 740            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -8.64e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 2.67e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 76             |
|    time_elapsed         | 243            |
|    total_timesteps      | 155648         |
| train/                  |                |
|    approx_kl            | 0.0021653553   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0387         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0654        |
|    cost_value_loss      | 3.87e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.129         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000427      |
|    mean_cost_advantages | -0.00020247753 |
|    mean_reward_advan... | -0.0019034274  |
|    n_updates            | 750            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000907      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.33e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 77             |
|    time_elapsed         | 246            |
|    total_timesteps      | 157696         |
| train/                  |                |
|    approx_kl            | 0.0033063022   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.039          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0699        |
|    cost_value_loss      | 3.78e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.116         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000177      |
|    mean_cost_advantages | -5.2040014e-05 |
|    mean_reward_advan... | -0.004316464   |
|    n_updates            | 760            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000711      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 4.33e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 638           |
|    iterations           | 78            |
|    time_elapsed         | 250           |
|    total_timesteps      | 159744        |
| train/                  |               |
|    approx_kl            | 0.0028857645  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.041         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.148        |
|    cost_value_loss      | 1.67e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.107        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00234      |
|    mean_cost_advantages | -0.0001934859 |
|    mean_reward_advan... | -0.0023000694 |
|    n_updates            | 770           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000435     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 1.04e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 79             |
|    time_elapsed         | 253            |
|    total_timesteps      | 161792         |
| train/                  |                |
|    approx_kl            | 0.002848475    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0178         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0459        |
|    cost_value_loss      | 9.11e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.107         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000152      |
|    mean_cost_advantages | -1.6898262e-05 |
|    mean_reward_advan... | -0.0017580907  |
|    n_updates            | 780            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000124      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 7.98e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 638           |
|    iterations           | 80            |
|    time_elapsed         | 256           |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.008052748   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0542        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0266       |
|    cost_value_loss      | 1.28e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.121        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000587     |
|    mean_cost_advantages | 0.00012338326 |
|    mean_reward_advan... | -0.0018291408 |
|    n_updates            | 790           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000419     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.52e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 638            |
|    iterations           | 81             |
|    time_elapsed         | 259            |
|    total_timesteps      | 165888         |
| train/                  |                |
|    approx_kl            | 0.003155569    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0156         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0349        |
|    cost_value_loss      | 4.06e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.116         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000689      |
|    mean_cost_advantages | -1.6376523e-05 |
|    mean_reward_advan... | -0.004314758   |
|    n_updates            | 800            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000141       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1e-05          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 82             |
|    time_elapsed         | 262            |
|    total_timesteps      | 167936         |
| train/                  |                |
|    approx_kl            | 0.01732272     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.044          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0335         |
|    cost_value_loss      | 2.08e-06       |
|    early_stop_epoch     | 4              |
|    entropy_loss         | -0.126         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00493       |
|    mean_cost_advantages | -0.00025125328 |
|    mean_reward_advan... | -0.002608242   |
|    n_updates            | 810            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00153       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.62e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 638           |
|    iterations           | 83            |
|    time_elapsed         | 266           |
|    total_timesteps      | 169984        |
| train/                  |               |
|    approx_kl            | 0.0036290032  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0491        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0318        |
|    cost_value_loss      | 3.57e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.142        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000967     |
|    mean_cost_advantages | 0.00033119024 |
|    mean_reward_advan... | -0.0011769072 |
|    n_updates            | 820           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000446     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.63e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 638            |
|    iterations           | 84             |
|    time_elapsed         | 269            |
|    total_timesteps      | 172032         |
| train/                  |                |
|    approx_kl            | 0.0011619931   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0208         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.000313      |
|    cost_value_loss      | 9.07e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.132         |
|    learning_rate        | 0.0003         |
|    loss                 | 3.75e-05       |
|    mean_cost_advantages | -8.065249e-05  |
|    mean_reward_advan... | -0.00053236715 |
|    n_updates            | 830            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000393      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.71e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 638            |
|    iterations           | 85             |
|    time_elapsed         | 272            |
|    total_timesteps      | 174080         |
| train/                  |                |
|    approx_kl            | 0.0035574785   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0295         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0345         |
|    cost_value_loss      | 5.32e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.14          |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00145        |
|    mean_cost_advantages | -0.00013019462 |
|    mean_reward_advan... | -0.0013838913  |
|    n_updates            | 840            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000177      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 5.73e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 638            |
|    iterations           | 86             |
|    time_elapsed         | 276            |
|    total_timesteps      | 176128         |
| train/                  |                |
|    approx_kl            | 0.0026255385   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.039          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.123          |
|    cost_value_loss      | 6.29e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.135         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000724      |
|    mean_cost_advantages | -0.00017538125 |
|    mean_reward_advan... | -0.0016093436  |
|    n_updates            | 850            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00073       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.14e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 638           |
|    iterations           | 87            |
|    time_elapsed         | 279           |
|    total_timesteps      | 178176        |
| train/                  |               |
|    approx_kl            | 0.003724357   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0274        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0321        |
|    cost_value_loss      | 2.46e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.133        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00113      |
|    mean_cost_advantages | -0.0002667254 |
|    mean_reward_advan... | -0.0024052672 |
|    n_updates            | 860           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 9.36e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 5.46e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 638            |
|    iterations           | 88             |
|    time_elapsed         | 282            |
|    total_timesteps      | 180224         |
| train/                  |                |
|    approx_kl            | 0.0045949565   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0263         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0622        |
|    cost_value_loss      | 1.84e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.142         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00624        |
|    mean_cost_advantages | -4.8001333e-05 |
|    mean_reward_advan... | -0.0014010984  |
|    n_updates            | 870            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000443       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4e-06          |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 638           |
|    iterations           | 89            |
|    time_elapsed         | 285           |
|    total_timesteps      | 182272        |
| train/                  |               |
|    approx_kl            | 0.005857895   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0469        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.13         |
|    cost_value_loss      | 8.98e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.136        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00023      |
|    mean_cost_advantages | -5.874811e-05 |
|    mean_reward_advan... | -0.001500257  |
|    n_updates            | 880           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000208     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.02e-07      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 638            |
|    iterations           | 90             |
|    time_elapsed         | 288            |
|    total_timesteps      | 184320         |
| train/                  |                |
|    approx_kl            | 0.0056048236   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0373         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0375        |
|    cost_value_loss      | 3.03e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.15          |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000113      |
|    mean_cost_advantages | -0.00021873298 |
|    mean_reward_advan... | -0.0013052755  |
|    n_updates            | 890            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -5.87e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 8.25e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 639           |
|    iterations           | 91            |
|    time_elapsed         | 291           |
|    total_timesteps      | 186368        |
| train/                  |               |
|    approx_kl            | 0.0021422561  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0145        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0306        |
|    cost_value_loss      | 1.36e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.136        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000171     |
|    mean_cost_advantages | -0.0001221326 |
|    mean_reward_advan... | 0.0003446922  |
|    n_updates            | 900           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000164     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.16e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 639           |
|    iterations           | 92            |
|    time_elapsed         | 294           |
|    total_timesteps      | 188416        |
| train/                  |               |
|    approx_kl            | 0.0045435126  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0556        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.236        |
|    cost_value_loss      | 4.33e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.142        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00287       |
|    mean_cost_advantages | 0.00030864542 |
|    mean_reward_advan... | -0.0011435207 |
|    n_updates            | 910           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000103     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.67e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 639           |
|    iterations           | 93            |
|    time_elapsed         | 297           |
|    total_timesteps      | 190464        |
| train/                  |               |
|    approx_kl            | 0.005937906   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0506        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0539        |
|    cost_value_loss      | 2.26e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.137        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000191     |
|    mean_cost_advantages | 0.00013181998 |
|    mean_reward_advan... | -0.0017119286 |
|    n_updates            | 920           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000916     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 0.000109      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 94             |
|    time_elapsed         | 301            |
|    total_timesteps      | 192512         |
| train/                  |                |
|    approx_kl            | 0.0032582371   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0454         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0229        |
|    cost_value_loss      | 2.56e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.124         |
|    learning_rate        | 0.0003         |
|    loss                 | -8.87e-05      |
|    mean_cost_advantages | -8.4797735e-05 |
|    mean_reward_advan... | -0.00013749312 |
|    n_updates            | 930            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 6.75e-05       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 5.74e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 639           |
|    iterations           | 95            |
|    time_elapsed         | 304           |
|    total_timesteps      | 194560        |
| train/                  |               |
|    approx_kl            | 0.006723171   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0418        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0755       |
|    cost_value_loss      | 1.62e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.124        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000189      |
|    mean_cost_advantages | 3.5757497e-05 |
|    mean_reward_advan... | -0.0011840055 |
|    n_updates            | 940           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000888     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.58e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 639           |
|    iterations           | 96            |
|    time_elapsed         | 307           |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.0010503968  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0146        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0286        |
|    cost_value_loss      | 3.47e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.127        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00453       |
|    mean_cost_advantages | 3.239887e-05  |
|    mean_reward_advan... | -0.0016155175 |
|    n_updates            | 950           |
|    nu                   | 2.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.09e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.72e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 97             |
|    time_elapsed         | 310            |
|    total_timesteps      | 198656         |
| train/                  |                |
|    approx_kl            | 0.0009001537   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0232         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.202          |
|    cost_value_loss      | 1.19e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.135         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000109      |
|    mean_cost_advantages | -0.00022908095 |
|    mean_reward_advan... | -0.0034345668  |
|    n_updates            | 960            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -7.06e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.18e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 639            |
|    iterations           | 98             |
|    time_elapsed         | 313            |
|    total_timesteps      | 200704         |
| train/                  |                |
|    approx_kl            | 0.0019615833   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0485         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0961         |
|    cost_value_loss      | 7.43e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.128         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000284       |
|    mean_cost_advantages | -0.00047395524 |
|    mean_reward_advan... | -0.002087821   |
|    n_updates            | 970            |
|    nu                   | 2.17           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000628      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 3.04e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
Mean reward: 9994.500000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 05.49 minutes[0m
