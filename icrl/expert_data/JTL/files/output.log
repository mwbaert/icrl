[32;1mConfigured folder ./cpg/wandb/run-20220525_160552-2eploi6h/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -1.23e+06 |
|    mean_ep_length   | 183       |
|    mean_reward      | -1.23e+06 |
|    true_cost        | 0.649     |
| infos/              |           |
|    cost             | 0.0315    |
| rollout/            |           |
|    adjusted_reward  | 4.09      |
|    ep_len_mean      | 169       |
|    ep_rew_mean      | 3.25e+03  |
| time/               |           |
|    fps              | 993       |
|    iterations       | 1         |
|    time_elapsed     | 10        |
|    total_timesteps  | 10240     |
-----------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -7.88e+05  |
|    mean_ep_length       | 133        |
|    mean_reward          | -7.88e+05  |
|    true_cost            | 0.612      |
| infos/                  |            |
|    cost                 | 0.0378     |
| rollout/                |            |
|    adjusted_reward      | 76         |
|    ep_len_mean          | 90.2       |
|    ep_rew_mean          | 9.15e+03   |
| time/                   |            |
|    fps                  | 990        |
|    iterations           | 2          |
|    time_elapsed         | 20         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.01638444 |
|    average_cost         | 0.6489258  |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -1.76      |
|    cost_value_loss      | 0.2        |
|    early_stop_epoch     | 4          |
|    entropy_loss         | -1.37      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.617      |
|    mean_cost_advantages | 0.5501987  |
|    mean_reward_advan... | 0.21639049 |
|    n_updates            | 10         |
|    nu                   | 1.06       |
|    nu_loss              | -0.649     |
|    policy_gradient_loss | -0.00629   |
|    reward_explained_... | -23.3      |
|    reward_value_loss    | 1.67       |
|    total_cost           | 6645.0     |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.02e+05   |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -3.02e+05   |
|    true_cost            | 0.667       |
| infos/                  |             |
|    cost                 | 0.0403      |
| rollout/                |             |
|    adjusted_reward      | 210         |
|    ep_len_mean          | 46.5        |
|    ep_rew_mean          | 9.98e+03    |
| time/                   |             |
|    fps                  | 1083        |
|    iterations           | 3           |
|    time_elapsed         | 28          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.017384067 |
|    average_cost         | 0.6124023   |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.76       |
|    cost_value_loss      | 0.0779      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.33       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.25        |
|    mean_cost_advantages | 0.31460685  |
|    mean_reward_advan... | 1.4952912   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.652      |
|    policy_gradient_loss | -0.0109     |
|    reward_explained_... | -52.8       |
|    reward_value_loss    | 4.82        |
|    total_cost           | 6271.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.48e+05   |
|    mean_ep_length       | 28.4        |
|    mean_reward          | -1.48e+05   |
|    true_cost            | 0.607       |
| infos/                  |             |
|    cost                 | 0.0393      |
| rollout/                |             |
|    adjusted_reward      | 331         |
|    ep_len_mean          | 32.7        |
|    ep_rew_mean          | 9.98e+03    |
| time/                   |             |
|    fps                  | 1109        |
|    iterations           | 4           |
|    time_elapsed         | 36          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.021089375 |
|    average_cost         | 0.6671875   |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.65       |
|    cost_value_loss      | 0.103       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.26       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.6         |
|    mean_cost_advantages | 0.11788404  |
|    mean_reward_advan... | 2.4343848   |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.755      |
|    policy_gradient_loss | -0.02       |
|    reward_explained_... | -1.02       |
|    reward_value_loss    | 4.04        |
|    total_cost           | 6832.0      |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.48e+05    |
|    mean_ep_length       | 27.4         |
|    mean_reward          | -1.54e+05    |
|    true_cost            | 0.588        |
| infos/                  |              |
|    cost                 | 0.032        |
| rollout/                |              |
|    adjusted_reward      | 492          |
|    ep_len_mean          | 22.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1136         |
|    iterations           | 5            |
|    time_elapsed         | 45           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0170534    |
|    average_cost         | 0.60664064   |
|    clip_fraction        | 0.225        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.406       |
|    cost_value_loss      | 0.0865       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -1.18        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.414        |
|    mean_cost_advantages | -0.105340526 |
|    mean_reward_advan... | 1.6939595    |
|    n_updates            | 40           |
|    nu                   | 1.27         |
|    nu_loss              | -0.728       |
|    policy_gradient_loss | -0.0133      |
|    reward_explained_... | 0.64         |
|    reward_value_loss    | 1.62         |
|    total_cost           | 6212.0       |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -7.8e+04    |
|    mean_ep_length       | 14.6        |
|    mean_reward          | -7.8e+04    |
|    true_cost            | 0.562       |
| infos/                  |             |
|    cost                 | 0.031       |
| rollout/                |             |
|    adjusted_reward      | 631         |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 1155        |
|    iterations           | 6           |
|    time_elapsed         | 53          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.021636676 |
|    average_cost         | 0.5881836   |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.292       |
|    cost_value_loss      | 0.0553      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -1.04       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0626      |
|    mean_cost_advantages | -0.17680208 |
|    mean_reward_advan... | 0.36711258  |
|    n_updates            | 50          |
|    nu                   | 1.34        |
|    nu_loss              | -0.748      |
|    policy_gradient_loss | -0.00897    |
|    reward_explained_... | 0.532       |
|    reward_value_loss    | 0.287       |
|    total_cost           | 6023.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5e+04      |
|    mean_ep_length       | 14.6        |
|    mean_reward          | -5e+04      |
|    true_cost            | 0.534       |
| infos/                  |             |
|    cost                 | 0.0322      |
| rollout/                |             |
|    adjusted_reward      | 683         |
|    ep_len_mean          | 15.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 1157        |
|    iterations           | 7           |
|    time_elapsed         | 61          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.017587941 |
|    average_cost         | 0.5619141   |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.658       |
|    cost_value_loss      | 0.0203      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.894      |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0271      |
|    mean_cost_advantages | -0.13309577 |
|    mean_reward_advan... | -0.4361433  |
|    n_updates            | 60          |
|    nu                   | 1.42        |
|    nu_loss              | -0.755      |
|    policy_gradient_loss | -0.00429    |
|    reward_explained_... | 0.445       |
|    reward_value_loss    | 0.0595      |
|    total_cost           | 5754.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5e+04       |
|    mean_ep_length       | 13.4         |
|    mean_reward          | -6.2e+04     |
|    true_cost            | 0.493        |
| infos/                  |              |
|    cost                 | 0.0354       |
| rollout/                |              |
|    adjusted_reward      | 715          |
|    ep_len_mean          | 14.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1148         |
|    iterations           | 8            |
|    time_elapsed         | 71           |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.016013488  |
|    average_cost         | 0.53408206   |
|    clip_fraction        | 0.187        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.673        |
|    cost_value_loss      | 0.0134       |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.791       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00879      |
|    mean_cost_advantages | -0.057671357 |
|    mean_reward_advan... | -0.54747075  |
|    n_updates            | 70           |
|    nu                   | 1.5          |
|    nu_loss              | -0.758       |
|    policy_gradient_loss | -0.00677     |
|    reward_explained_... | 0.437        |
|    reward_value_loss    | 0.0265       |
|    total_cost           | 5469.0       |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.4e+04     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -4.4e+04     |
|    true_cost            | 0.445        |
| infos/                  |              |
|    cost                 | 0.036        |
| rollout/                |              |
|    adjusted_reward      | 761          |
|    ep_len_mean          | 13.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1138         |
|    iterations           | 9            |
|    time_elapsed         | 80           |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.01614648   |
|    average_cost         | 0.49345702   |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.681        |
|    cost_value_loss      | 0.0107       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.712       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00236     |
|    mean_cost_advantages | -0.027988032 |
|    mean_reward_advan... | -0.40372396  |
|    n_updates            | 80           |
|    nu                   | 1.57         |
|    nu_loss              | -0.738       |
|    policy_gradient_loss | -0.00736     |
|    reward_explained_... | 0.402        |
|    reward_value_loss    | 0.0169       |
|    total_cost           | 5053.0       |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.4e+04     |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -5.2e+04     |
|    true_cost            | 0.398        |
| infos/                  |              |
|    cost                 | 0.0297       |
| rollout/                |              |
|    adjusted_reward      | 802          |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1118         |
|    iterations           | 10           |
|    time_elapsed         | 91           |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.017281203  |
|    average_cost         | 0.44521484   |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.66         |
|    cost_value_loss      | 0.00718      |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.612       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00813      |
|    mean_cost_advantages | -0.021782456 |
|    mean_reward_advan... | -0.27448112  |
|    n_updates            | 90           |
|    nu                   | 1.65         |
|    nu_loss              | -0.7         |
|    policy_gradient_loss | -0.00727     |
|    reward_explained_... | 0.666        |
|    reward_value_loss    | 0.00721      |
|    total_cost           | 4559.0       |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4e+04      |
|    mean_ep_length       | 12.8        |
|    mean_reward          | -4e+04      |
|    true_cost            | 0.348       |
| infos/                  |             |
|    cost                 | 0.023       |
| rollout/                |             |
|    adjusted_reward      | 823         |
|    ep_len_mean          | 12.5        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 1079        |
|    iterations           | 11          |
|    time_elapsed         | 104         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.012447233 |
|    average_cost         | 0.39833984  |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.728       |
|    cost_value_loss      | 0.00492     |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.53       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.00193    |
|    mean_cost_advantages | -0.02689473 |
|    mean_reward_advan... | -0.20331499 |
|    n_updates            | 100         |
|    nu                   | 1.73        |
|    nu_loss              | -0.658      |
|    policy_gradient_loss | -0.00447    |
|    reward_explained_... | 0.834       |
|    reward_value_loss    | 0.00346     |
|    total_cost           | 4079.0      |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3e+04       |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -3e+04       |
|    true_cost            | 0.319        |
| infos/                  |              |
|    cost                 | 0.0201       |
| rollout/                |              |
|    adjusted_reward      | 827          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1048         |
|    iterations           | 12           |
|    time_elapsed         | 117          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.009516986  |
|    average_cost         | 0.34814453   |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.774        |
|    cost_value_loss      | 0.00408      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.47        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00206      |
|    mean_cost_advantages | -0.021723539 |
|    mean_reward_advan... | -0.16255166  |
|    n_updates            | 110          |
|    nu                   | 1.81         |
|    nu_loss              | -0.603       |
|    policy_gradient_loss | -0.0035      |
|    reward_explained_... | 0.9          |
|    reward_value_loss    | 0.00197      |
|    total_cost           | 3565.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.2e+04     |
|    mean_ep_length       | 12.8         |
|    mean_reward          | -1.2e+04     |
|    true_cost            | 0.287        |
| infos/                  |              |
|    cost                 | 0.0169       |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 1018         |
|    iterations           | 13           |
|    time_elapsed         | 130          |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.009886649  |
|    average_cost         | 0.31904298   |
|    clip_fraction        | 0.0884       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.784        |
|    cost_value_loss      | 0.00344      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.428       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00282      |
|    mean_cost_advantages | -0.013360739 |
|    mean_reward_advan... | -0.1328675   |
|    n_updates            | 120          |
|    nu                   | 1.89         |
|    nu_loss              | -0.578       |
|    policy_gradient_loss | -0.00285     |
|    reward_explained_... | 0.936        |
|    reward_value_loss    | 0.00123      |
|    total_cost           | 3267.0       |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.2e+04    |
|    mean_ep_length       | 12          |
|    mean_reward          | -1.4e+04    |
|    true_cost            | 0.254       |
| infos/                  |             |
|    cost                 | 0.0219      |
| rollout/                |             |
|    adjusted_reward      | 838         |
|    ep_len_mean          | 12.1        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 992         |
|    iterations           | 14          |
|    time_elapsed         | 144         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.00986747  |
|    average_cost         | 0.2875      |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.768       |
|    cost_value_loss      | 0.00328     |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.378      |
|    learning_rate        | 0.0005      |
|    loss                 | -0.000553   |
|    mean_cost_advantages | -0.01277186 |
|    mean_reward_advan... | -0.10292623 |
|    n_updates            | 130         |
|    nu                   | 1.97        |
|    nu_loss              | -0.544      |
|    policy_gradient_loss | -0.00169    |
|    reward_explained_... | 0.964       |
|    reward_value_loss    | 0.0007      |
|    total_cost           | 2944.0      |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.2e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -2.8e+04     |
|    true_cost            | 0.226        |
| infos/                  |              |
|    cost                 | 0.0178       |
| rollout/                |              |
|    adjusted_reward      | 834          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 970          |
|    iterations           | 15           |
|    time_elapsed         | 158          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0029196125 |
|    average_cost         | 0.25371093   |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.707        |
|    cost_value_loss      | 0.0032       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.358       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.000752     |
|    mean_cost_advantages | -0.013300674 |
|    mean_reward_advan... | -0.08761115  |
|    n_updates            | 140          |
|    nu                   | 2.05         |
|    nu_loss              | -0.5         |
|    policy_gradient_loss | -0.00093     |
|    reward_explained_... | 0.978        |
|    reward_value_loss    | 0.000431     |
|    total_cost           | 2598.0       |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -1e+04        |
|    mean_ep_length       | 12            |
|    mean_reward          | -1e+04        |
|    true_cost            | 0.198         |
| infos/                  |               |
|    cost                 | 0.0111        |
| rollout/                |               |
|    adjusted_reward      | 838           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 953           |
|    iterations           | 16            |
|    time_elapsed         | 171           |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.0049013854  |
|    average_cost         | 0.22578125    |
|    clip_fraction        | 0.0421        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.712         |
|    cost_value_loss      | 0.00299       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.337        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.0014        |
|    mean_cost_advantages | -0.0066746413 |
|    mean_reward_advan... | -0.07352085   |
|    n_updates            | 150           |
|    nu                   | 2.12          |
|    nu_loss              | -0.462        |
|    policy_gradient_loss | -0.00101      |
|    reward_explained_... | 0.984         |
|    reward_value_loss    | 0.000323      |
|    total_cost           | 2312.0        |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -1.4e+04     |
|    true_cost            | 0.147        |
| infos/                  |              |
|    cost                 | 0.00729      |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 17           |
|    time_elapsed         | 186          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0074858554 |
|    average_cost         | 0.19785157   |
|    clip_fraction        | 0.0952       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.592        |
|    cost_value_loss      | 0.00368      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.321       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00133     |
|    mean_cost_advantages | -0.007947599 |
|    mean_reward_advan... | -0.05821837  |
|    n_updates            | 160          |
|    nu                   | 2.2          |
|    nu_loss              | -0.42        |
|    policy_gradient_loss | -0.0015      |
|    reward_explained_... | 0.989        |
|    reward_value_loss    | 0.000215     |
|    total_cost           | 2026.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -1e+04       |
|    true_cost            | 0.101        |
| infos/                  |              |
|    cost                 | 0.00496      |
| rollout/                |              |
|    adjusted_reward      | 834          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 18           |
|    time_elapsed         | 201          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.008748837  |
|    average_cost         | 0.14746094   |
|    clip_fraction        | 0.0841       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.32         |
|    cost_value_loss      | 0.00401      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.288       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.0016       |
|    mean_cost_advantages | -0.008732619 |
|    mean_reward_advan... | -0.053089242 |
|    n_updates            | 170          |
|    nu                   | 2.27         |
|    nu_loss              | -0.324       |
|    policy_gradient_loss | -0.00134     |
|    reward_explained_... | 0.99         |
|    reward_value_loss    | 0.000206     |
|    total_cost           | 1510.0       |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0469       |
| infos/                  |              |
|    cost                 | 0.000843     |
| rollout/                |              |
|    adjusted_reward      | 791          |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 904          |
|    iterations           | 19           |
|    time_elapsed         | 215          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.03001311   |
|    average_cost         | 0.10087891   |
|    clip_fraction        | 0.135        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.199        |
|    cost_value_loss      | 0.00406      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.287       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00219     |
|    mean_cost_advantages | -0.021574115 |
|    mean_reward_advan... | -0.044185936 |
|    n_updates            | 180          |
|    nu                   | 2.34         |
|    nu_loss              | -0.229       |
|    policy_gradient_loss | -0.00291     |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 0.00021      |
|    total_cost           | 1033.0       |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0338       |
| infos/                  |              |
|    cost                 | 0.0043       |
| rollout/                |              |
|    adjusted_reward      | 834          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 895          |
|    iterations           | 20           |
|    time_elapsed         | 228          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.04968017   |
|    average_cost         | 0.046875     |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.186       |
|    cost_value_loss      | 0.00255      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.245       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00731     |
|    mean_cost_advantages | -0.021749288 |
|    mean_reward_advan... | -0.053624958 |
|    n_updates            | 190          |
|    nu                   | 2.41         |
|    nu_loss              | -0.11        |
|    policy_gradient_loss | -0.0065      |
|    reward_explained_... | 0.922        |
|    reward_value_loss    | 0.00172      |
|    total_cost           | 480.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 5.99e+03     |
|    true_cost            | 0.0129       |
| infos/                  |              |
|    cost                 | 0.00175      |
| rollout/                |              |
|    adjusted_reward      | 834          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 878          |
|    iterations           | 21           |
|    time_elapsed         | 244          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.0059351632 |
|    average_cost         | 0.03378906   |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.5         |
|    cost_value_loss      | 0.00203      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.207       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.000298    |
|    mean_cost_advantages | -0.007670918 |
|    mean_reward_advan... | -0.02305666  |
|    n_updates            | 200          |
|    nu                   | 2.47         |
|    nu_loss              | -0.0813      |
|    policy_gradient_loss | -0.00126     |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 0.000123     |
|    total_cost           | 346.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00449      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 834          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 853          |
|    iterations           | 22           |
|    time_elapsed         | 263          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.0036746853 |
|    average_cost         | 0.012890625  |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.31        |
|    cost_value_loss      | 0.000848     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.175       |
|    learning_rate        | 0.0005       |
|    loss                 | -3.4e-05     |
|    mean_cost_advantages | -0.015016006 |
|    mean_reward_advan... | -0.02729423  |
|    n_updates            | 210          |
|    nu                   | 2.52         |
|    nu_loss              | -0.0318      |
|    policy_gradient_loss | -0.000192    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 8.3e-05      |
|    total_cost           | 132.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 5.99e+03      |
|    true_cost            | 0.0043        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 842           |
|    iterations           | 23            |
|    time_elapsed         | 279           |
|    total_timesteps      | 235520        |
| train/                  |               |
|    approx_kl            | 0.00033037955 |
|    average_cost         | 0.0044921874  |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.67         |
|    cost_value_loss      | 0.000323      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.16         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000989     |
|    mean_cost_advantages | -0.0038791578 |
|    mean_reward_advan... | -0.027506908  |
|    n_updates            | 220           |
|    nu                   | 2.57          |
|    nu_loss              | -0.0113       |
|    policy_gradient_loss | 5e-05         |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 5.08e-05      |
|    total_cost           | 46.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 833           |
|    iterations           | 24            |
|    time_elapsed         | 294           |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.0013343369  |
|    average_cost         | 0.004296875   |
|    clip_fraction        | 0.0174        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -13.6         |
|    cost_value_loss      | 0.000312      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.147        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000637     |
|    mean_cost_advantages | -0.0013371746 |
|    mean_reward_advan... | -0.025189226  |
|    n_updates            | 230           |
|    nu                   | 2.62          |
|    nu_loss              | -0.0111       |
|    policy_gradient_loss | 8.35e-05      |
|    reward_explained_... | 0.994         |
|    reward_value_loss    | 0.000123      |
|    total_cost           | 44.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00137      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 823          |
|    iterations           | 25           |
|    time_elapsed         | 310          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.002722479  |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.26        |
|    cost_value_loss      | 8.17e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.133       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00429     |
|    mean_cost_advantages | -0.004072013 |
|    mean_reward_advan... | -0.020314137 |
|    n_updates            | 240          |
|    nu                   | 2.66         |
|    nu_loss              | -0.00256     |
|    policy_gradient_loss | -0.000135    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 3.51e-05     |
|    total_cost           | 10.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 812           |
|    iterations           | 26            |
|    time_elapsed         | 327           |
|    total_timesteps      | 266240        |
| train/                  |               |
|    approx_kl            | 0.0014888659  |
|    average_cost         | 0.0013671875  |
|    clip_fraction        | 0.0108        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -18.4         |
|    cost_value_loss      | 0.000111      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.129        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000459     |
|    mean_cost_advantages | -0.0013381501 |
|    mean_reward_advan... | -0.01891258   |
|    n_updates            | 250           |
|    nu                   | 2.7           |
|    nu_loss              | -0.00364      |
|    policy_gradient_loss | 9.18e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.34e-05      |
|    total_cost           | 14.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 804           |
|    iterations           | 27            |
|    time_elapsed         | 343           |
|    total_timesteps      | 276480        |
| train/                  |               |
|    approx_kl            | 0.0016020827  |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0.0115        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -11.8         |
|    cost_value_loss      | 6.74e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.117        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00141       |
|    mean_cost_advantages | -0.0014760023 |
|    mean_reward_advan... | -0.017838348  |
|    n_updates            | 260           |
|    nu                   | 2.74          |
|    nu_loss              | -0.00211      |
|    policy_gradient_loss | 4.64e-05      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 4.34e-05      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 797           |
|    iterations           | 28            |
|    time_elapsed         | 359           |
|    total_timesteps      | 286720        |
| train/                  |               |
|    approx_kl            | 0.002955752   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0406       |
|    cost_value_loss      | 1.33e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.123        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000694     |
|    mean_cost_advantages | 0.00016493948 |
|    mean_reward_advan... | -0.011339547  |
|    n_updates            | 270           |
|    nu                   | 2.77          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.9e-05      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 3.2e-05       |
|    total_cost           | 0.0           |
-------------------------------------------
---------------------------------------------
| eval/                   |                 |
|    best_mean_reward     | 9.99e+03        |
|    mean_ep_length       | 12              |
|    mean_reward          | 9.99e+03        |
|    true_cost            | 0.000586        |
| infos/                  |                 |
|    cost                 | 0               |
| rollout/                |                 |
|    adjusted_reward      | 836             |
|    ep_len_mean          | 12              |
|    ep_rew_mean          | 9.99e+03        |
| time/                   |                 |
|    fps                  | 791             |
|    iterations           | 29              |
|    time_elapsed         | 375             |
|    total_timesteps      | 296960          |
| train/                  |                 |
|    approx_kl            | -0.000107563894 |
|    average_cost         | 0.00078125      |
|    clip_fraction        | 0.0119          |
|    clip_range           | 0.2             |
|    cost_explained_va... | -78.2           |
|    cost_value_loss      | 7.2e-05         |
|    early_stop_epoch     | 10              |
|    entropy_loss         | -0.123          |
|    learning_rate        | 0.0005          |
|    loss                 | -0.000455       |
|    mean_cost_advantages | 0.0006626772    |
|    mean_reward_advan... | -0.014289029    |
|    n_updates            | 280             |
|    nu                   | 2.8             |
|    nu_loss              | -0.00216        |
|    policy_gradient_loss | 2.29e-05        |
|    reward_explained_... | 0.999           |
|    reward_value_loss    | 2.03e-05        |
|    total_cost           | 8.0             |
---------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 785           |
|    iterations           | 30            |
|    time_elapsed         | 391           |
|    total_timesteps      | 307200        |
| train/                  |               |
|    approx_kl            | 0.0012262326  |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -14           |
|    cost_value_loss      | 5.63e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.115        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000542     |
|    mean_cost_advantages | 0.00012874238 |
|    mean_reward_advan... | -0.012375446  |
|    n_updates            | 290           |
|    nu                   | 2.83          |
|    nu_loss              | -0.00164      |
|    policy_gradient_loss | 1.81e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.33e-05      |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 366          |
|    ep_len_mean          | 24.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 782          |
|    iterations           | 31           |
|    time_elapsed         | 405          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.061921574  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.78        |
|    cost_value_loss      | 2.02e-05     |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.121       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00234     |
|    mean_cost_advantages | 0.0008812612 |
|    mean_reward_advan... | -0.011508969 |
|    n_updates            | 300          |
|    nu                   | 2.85         |
|    nu_loss              | -0.000552    |
|    policy_gradient_loss | -0.000486    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.4e-05      |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 603           |
|    ep_len_mean          | 16.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 783           |
|    iterations           | 32            |
|    time_elapsed         | 417           |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.018240904   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.307         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.307         |
|    cost_value_loss      | 6.83e-06      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.484        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.0121        |
|    mean_cost_advantages | 0.00081809034 |
|    mean_reward_advan... | -0.2624804    |
|    n_updates            | 310           |
|    nu                   | 2.87          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0096       |
|    reward_explained_... | -1.37         |
|    reward_value_loss    | 0.0656        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 761          |
|    ep_len_mean          | 12.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 784          |
|    iterations           | 33           |
|    time_elapsed         | 430          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.04841761   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.327        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0641       |
|    cost_value_loss      | 2.28e-06     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.276       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00287      |
|    mean_cost_advantages | 0.0007362835 |
|    mean_reward_advan... | 0.060341906  |
|    n_updates            | 320          |
|    nu                   | 2.89         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0105      |
|    reward_explained_... | -0.698       |
|    reward_value_loss    | 0.0309       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 825          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 784          |
|    iterations           | 34           |
|    time_elapsed         | 444          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.027590897  |
|    average_cost         | 0.0005859375 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -20.1        |
|    cost_value_loss      | 6.51e-05     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.171       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00626     |
|    mean_cost_advantages | 0.0005203499 |
|    mean_reward_advan... | 0.11159374   |
|    n_updates            | 330          |
|    nu                   | 2.91         |
|    nu_loss              | -0.0017      |
|    policy_gradient_loss | -0.00657     |
|    reward_explained_... | 0.706        |
|    reward_value_loss    | 0.00654      |
|    total_cost           | 6.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 823          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 777          |
|    iterations           | 35           |
|    time_elapsed         | 461          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.0015438335 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -9.2         |
|    cost_value_loss      | 4.28e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.134       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00263     |
|    mean_cost_advantages | 0.0004146859 |
|    mean_reward_advan... | 0.04281447   |
|    n_updates            | 340          |
|    nu                   | 2.93         |
|    nu_loss              | -0.00114     |
|    policy_gradient_loss | -0.000407    |
|    reward_explained_... | 0.939        |
|    reward_value_loss    | 0.000419     |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 829           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 774           |
|    iterations           | 36            |
|    time_elapsed         | 476           |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.01582339    |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0303        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.58         |
|    cost_value_loss      | 2.21e-05      |
|    early_stop_epoch     | 8             |
|    entropy_loss         | -0.152        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00376      |
|    mean_cost_advantages | -0.0016645528 |
|    mean_reward_advan... | -0.0036383744 |
|    n_updates            | 350           |
|    nu                   | 2.95          |
|    nu_loss              | -0.000572     |
|    policy_gradient_loss | -0.00122      |
|    reward_explained_... | 0.979         |
|    reward_value_loss    | 0.000262      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 771           |
|    iterations           | 37            |
|    time_elapsed         | 490           |
|    total_timesteps      | 378880        |
| train/                  |               |
|    approx_kl            | 0.0042781485  |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0271        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -9.08         |
|    cost_value_loss      | 2.4e-05       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.151        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000979     |
|    mean_cost_advantages | -0.0012081865 |
|    mean_reward_advan... | -0.006288028  |
|    n_updates            | 360           |
|    nu                   | 2.96          |
|    nu_loss              | -0.000575     |
|    policy_gradient_loss | -0.00108      |
|    reward_explained_... | 0.991         |
|    reward_value_loss    | 0.000143      |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 821          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 767          |
|    iterations           | 38           |
|    time_elapsed         | 506          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.001689905  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.00838     |
|    cost_value_loss      | 2.94e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.139       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00529      |
|    mean_cost_advantages | 8.029598e-05 |
|    mean_reward_advan... | -0.008146161 |
|    n_updates            | 370          |
|    nu                   | 2.97         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000239    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 3.11e-05     |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 768           |
|    iterations           | 39            |
|    time_elapsed         | 519           |
|    total_timesteps      | 399360        |
| train/                  |               |
|    approx_kl            | 0.025246104   |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0351        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -7.78         |
|    cost_value_loss      | 2.62e-05      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.135        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000561      |
|    mean_cost_advantages | 2.9032952e-05 |
|    mean_reward_advan... | -0.015134436  |
|    n_updates            | 380           |
|    nu                   | 2.99          |
|    nu_loss              | -0.000581     |
|    policy_gradient_loss | -0.00325      |
|    reward_explained_... | 0.762         |
|    reward_value_loss    | 0.00279       |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 766           |
|    iterations           | 40            |
|    time_elapsed         | 534           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 0.0055251024  |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0595        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -8.26         |
|    cost_value_loss      | 2.6e-05       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.128        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000908     |
|    mean_cost_advantages | 0.00095162913 |
|    mean_reward_advan... | -0.016368132  |
|    n_updates            | 390           |
|    nu                   | 3             |
|    nu_loss              | -0.000583     |
|    policy_gradient_loss | -0.000365     |
|    reward_explained_... | 0.83          |
|    reward_value_loss    | 0.000235      |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 833          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 41           |
|    time_elapsed         | 549          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.0037241585 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.0705       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -164         |
|    cost_value_loss      | 4.93e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.134       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.000359    |
|    mean_cost_advantages | 0.0009488498 |
|    mean_reward_advan... | -0.004746598 |
|    n_updates            | 400          |
|    nu                   | 3.01         |
|    nu_loss              | -0.00117     |
|    policy_gradient_loss | -0.000631    |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 1.34e-05     |
|    total_cost           | 4.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 761            |
|    iterations           | 42             |
|    time_elapsed         | 564            |
|    total_timesteps      | 430080         |
| train/                  |                |
|    approx_kl            | 0.0037271522   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0176         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0775        |
|    cost_value_loss      | 2.74e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.132         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.00032        |
|    mean_cost_advantages | -0.00073207624 |
|    mean_reward_advan... | -0.0062335497  |
|    n_updates            | 410            |
|    nu                   | 3.01           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000168      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.03e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 759            |
|    iterations           | 43             |
|    time_elapsed         | 579            |
|    total_timesteps      | 440320         |
| train/                  |                |
|    approx_kl            | 8.497082e-05   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0166         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0671        |
|    cost_value_loss      | 9.96e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.123         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000552      |
|    mean_cost_advantages | -0.00022810623 |
|    mean_reward_advan... | -0.006483377   |
|    n_updates            | 420            |
|    nu                   | 3.02           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.04e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.77e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 757            |
|    iterations           | 44             |
|    time_elapsed         | 595            |
|    total_timesteps      | 450560         |
| train/                  |                |
|    approx_kl            | 0.0013598229   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0135         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0405         |
|    cost_value_loss      | 9.63e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.128         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000943       |
|    mean_cost_advantages | -0.00012917086 |
|    mean_reward_advan... | -0.005117266   |
|    n_updates            | 430            |
|    nu                   | 3.03           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 4.33e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 7.59e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
---------------------------------------------
| eval/                   |                 |
|    best_mean_reward     | 9.99e+03        |
|    mean_ep_length       | 12              |
|    mean_reward          | 9.99e+03        |
|    true_cost            | 0               |
| infos/                  |                 |
|    cost                 | 0               |
| rollout/                |                 |
|    adjusted_reward      | 832             |
|    ep_len_mean          | 12              |
|    ep_rew_mean          | 9.99e+03        |
| time/                   |                 |
|    fps                  | 755             |
|    iterations           | 45              |
|    time_elapsed         | 609             |
|    total_timesteps      | 460800          |
| train/                  |                 |
|    approx_kl            | 0.0016851437    |
|    average_cost         | 0.0             |
|    clip_fraction        | 0.0162          |
|    clip_range           | 0.2             |
|    cost_explained_va... | 0.0501          |
|    cost_value_loss      | 2.62e-07        |
|    early_stop_epoch     | 10              |
|    entropy_loss         | -0.116          |
|    learning_rate        | 0.0005          |
|    loss                 | 0.00336         |
|    mean_cost_advantages | -0.000118559714 |
|    mean_reward_advan... | -0.005637102    |
|    n_updates            | 440             |
|    nu                   | 3.04            |
|    nu_loss              | -0              |
|    policy_gradient_loss | 7.66e-05        |
|    reward_explained_... | 1               |
|    reward_value_loss    | 1.18e-05        |
|    total_cost           | 0.0             |
---------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 46            |
|    time_elapsed         | 624           |
|    total_timesteps      | 471040        |
| train/                  |               |
|    approx_kl            | 0.027779538   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0321        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0274        |
|    cost_value_loss      | 1.74e-07      |
|    early_stop_epoch     | 8             |
|    entropy_loss         | -0.149        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00542       |
|    mean_cost_advantages | -7.763666e-05 |
|    mean_reward_advan... | -0.006029616  |
|    n_updates            | 450           |
|    nu                   | 3.04          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.05e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.65e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 47            |
|    time_elapsed         | 638           |
|    total_timesteps      | 481280        |
| train/                  |               |
|    approx_kl            | 0.0022691255  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0356        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.115         |
|    cost_value_loss      | 2.93e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.151        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00183      |
|    mean_cost_advantages | 0.0002033463  |
|    mean_reward_advan... | -0.0051706377 |
|    n_updates            | 460           |
|    nu                   | 3.05          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000994     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.17e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 48            |
|    time_elapsed         | 653           |
|    total_timesteps      | 491520        |
| train/                  |               |
|    approx_kl            | 0.0013829751  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0268        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.146         |
|    cost_value_loss      | 2.57e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.135        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000666     |
|    mean_cost_advantages | 0.00023091384 |
|    mean_reward_advan... | -0.004773207  |
|    n_updates            | 470           |
|    nu                   | 3.05          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.69e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.26e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 49            |
|    time_elapsed         | 668           |
|    total_timesteps      | 501760        |
| train/                  |               |
|    approx_kl            | 0.0037447922  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0248        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.116         |
|    cost_value_loss      | 3.48e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.135        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000937      |
|    mean_cost_advantages | 0.00038623315 |
|    mean_reward_advan... | -0.005754383  |
|    n_updates            | 480           |
|    nu                   | 3.06          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 5.14e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.64e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 748           |
|    iterations           | 50            |
|    time_elapsed         | 684           |
|    total_timesteps      | 512000        |
| train/                  |               |
|    approx_kl            | 0.0014779964  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0281        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.023         |
|    cost_value_loss      | 1.7e-07       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.143        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000183     |
|    mean_cost_advantages | 6.4325985e-05 |
|    mean_reward_advan... | -0.0050873775 |
|    n_updates            | 490           |
|    nu                   | 3.06          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.39e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.72e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 746           |
|    iterations           | 51            |
|    time_elapsed         | 699           |
|    total_timesteps      | 522240        |
| train/                  |               |
|    approx_kl            | 0.006717375   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0322        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.132        |
|    cost_value_loss      | 2.95e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.114        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00319       |
|    mean_cost_advantages | 8.0386744e-05 |
|    mean_reward_advan... | -0.004685973  |
|    n_updates            | 500           |
|    nu                   | 3.07          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.82e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.79e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.19
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00254      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 180          |
|    ep_len_mean          | 56.3         |
|    ep_rew_mean          | 9.69e+03     |
| time/                   |              |
|    fps                  | 747          |
|    iterations           | 52           |
|    time_elapsed         | 712          |
|    total_timesteps      | 532480       |
| train/                  |              |
|    approx_kl            | 0.18983606   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0304      |
|    cost_value_loss      | 3.49e-07     |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.134       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00226      |
|    mean_cost_advantages | 3.355576e-05 |
|    mean_reward_advan... | -0.004809022 |
|    n_updates            | 510          |
|    nu                   | 3.07         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00111     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.47e-05     |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 597          |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 747          |
|    iterations           | 53           |
|    time_elapsed         | 725          |
|    total_timesteps      | 542720       |
| train/                  |              |
|    approx_kl            | 0.06582584   |
|    average_cost         | 0.0025390624 |
|    clip_fraction        | 0.196        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -633         |
|    cost_value_loss      | 0.000727     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.294       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.0385       |
|    mean_cost_advantages | 0.004687771  |
|    mean_reward_advan... | -0.45076045  |
|    n_updates            | 520          |
|    nu                   | 3.07         |
|    nu_loss              | -0.0078      |
|    policy_gradient_loss | -0.00483     |
|    reward_explained_... | -9.18        |
|    reward_value_loss    | 0.0972       |
|    total_cost           | 26.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 667          |
|    ep_len_mean          | 14.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 54           |
|    time_elapsed         | 742          |
|    total_timesteps      | 552960       |
| train/                  |              |
|    approx_kl            | 0.0058576777 |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.076        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -20.5        |
|    cost_value_loss      | 5.48e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.259       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.0161       |
|    mean_cost_advantages | -0.003196989 |
|    mean_reward_advan... | 0.13244209   |
|    n_updates            | 530          |
|    nu                   | 3.08         |
|    nu_loss              | -0.0006      |
|    policy_gradient_loss | -0.0038      |
|    reward_explained_... | -15.5        |
|    reward_value_loss    | 0.0343       |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 715           |
|    ep_len_mean          | 13.8          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 745           |
|    iterations           | 55            |
|    time_elapsed         | 755           |
|    total_timesteps      | 563200        |
| train/                  |               |
|    approx_kl            | 0.02210607    |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.161         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -229          |
|    cost_value_loss      | 4.68e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.22         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00109      |
|    mean_cost_advantages | -0.0017935745 |
|    mean_reward_advan... | 0.12391782    |
|    n_updates            | 540           |
|    nu                   | 3.08          |
|    nu_loss              | -0.000601     |
|    policy_gradient_loss | -0.00572      |
|    reward_explained_... | 0.242         |
|    reward_value_loss    | 0.0154        |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 824          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 747          |
|    iterations           | 56           |
|    time_elapsed         | 767          |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.087347046  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.152        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.147        |
|    cost_value_loss      | 4.72e-07     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.163       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00192     |
|    mean_cost_advantages | -0.000820294 |
|    mean_reward_advan... | 0.067254946  |
|    n_updates            | 550          |
|    nu                   | 3.08         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00669     |
|    reward_explained_... | 0.716        |
|    reward_value_loss    | 0.00654      |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 745            |
|    iterations           | 57             |
|    time_elapsed         | 782            |
|    total_timesteps      | 583680         |
| train/                  |                |
|    approx_kl            | 0.0066591664   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0221         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00652       |
|    cost_value_loss      | 2.14e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.133         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000407       |
|    mean_cost_advantages | -2.9540952e-05 |
|    mean_reward_advan... | 0.061760157    |
|    n_updates            | 560            |
|    nu                   | 3.08           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00126       |
|    reward_explained_... | 0.982          |
|    reward_value_loss    | 0.000371       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 744           |
|    iterations           | 58            |
|    time_elapsed         | 797           |
|    total_timesteps      | 593920        |
| train/                  |               |
|    approx_kl            | 0.002984148   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0211        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0171       |
|    cost_value_loss      | 2.14e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.128        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00119       |
|    mean_cost_advantages | 2.3819972e-05 |
|    mean_reward_advan... | 0.012656825   |
|    n_updates            | 570           |
|    nu                   | 3.09          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 6.51e-05      |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 3.58e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 743            |
|    iterations           | 59             |
|    time_elapsed         | 812            |
|    total_timesteps      | 604160         |
| train/                  |                |
|    approx_kl            | 0.0025514162   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0169         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0438        |
|    cost_value_loss      | 2.19e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.116         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000164      |
|    mean_cost_advantages | -2.2534146e-05 |
|    mean_reward_advan... | -0.0016969383  |
|    n_updates            | 580            |
|    nu                   | 3.09           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000162      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 3.15e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 742           |
|    iterations           | 60            |
|    time_elapsed         | 827           |
|    total_timesteps      | 614400        |
| train/                  |               |
|    approx_kl            | 0.00327324    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0258        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00371       |
|    cost_value_loss      | 1.14e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.118        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000206      |
|    mean_cost_advantages | 2.6402715e-05 |
|    mean_reward_advan... | -0.0026832472 |
|    n_updates            | 590           |
|    nu                   | 3.09          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -1.24e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.64e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 741           |
|    iterations           | 61            |
|    time_elapsed         | 842           |
|    total_timesteps      | 624640        |
| train/                  |               |
|    approx_kl            | 0.0013360735  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0156        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.098         |
|    cost_value_loss      | 1.8e-07       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.115        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000968      |
|    mean_cost_advantages | 0.00014187835 |
|    mean_reward_advan... | -0.0039927727 |
|    n_updates            | 600           |
|    nu                   | 3.09          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 5.39e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 2.7e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 62           |
|    time_elapsed         | 856          |
|    total_timesteps      | 634880       |
| train/                  |              |
|    approx_kl            | 0.0013744652 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.00204      |
|    cost_value_loss      | 3.16e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.111       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.000104    |
|    mean_cost_advantages | 4.574475e-05 |
|    mean_reward_advan... | -0.003853929 |
|    n_updates            | 610          |
|    nu                   | 3.09         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 2.71e-05     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.18e-05     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 739           |
|    iterations           | 63            |
|    time_elapsed         | 872           |
|    total_timesteps      | 645120        |
| train/                  |               |
|    approx_kl            | 0.0010760014  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0192        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.261         |
|    cost_value_loss      | 1.51e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.118        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00109      |
|    mean_cost_advantages | 0.00033713825 |
|    mean_reward_advan... | -0.0026195494 |
|    n_updates            | 620           |
|    nu                   | 3.09          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 9.24e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.64e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 738           |
|    iterations           | 64            |
|    time_elapsed         | 887           |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.00029959955 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0113        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0244       |
|    cost_value_loss      | 1.84e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.109        |
|    learning_rate        | 0.0005        |
|    loss                 | 3.89e-05      |
|    mean_cost_advantages | -9.291004e-06 |
|    mean_reward_advan... | -0.0033283979 |
|    n_updates            | 630           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | 6.57e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.36e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 737            |
|    iterations           | 65             |
|    time_elapsed         | 902            |
|    total_timesteps      | 665600         |
| train/                  |                |
|    approx_kl            | 0.0018008234   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.021          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0484         |
|    cost_value_loss      | 4.35e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.123         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000593       |
|    mean_cost_advantages | 0.000111012385 |
|    mean_reward_advan... | -0.0032455833  |
|    n_updates            | 640            |
|    nu                   | 3.1            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -5.63e-06      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 3.36e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 736           |
|    iterations           | 66            |
|    time_elapsed         | 917           |
|    total_timesteps      | 675840        |
| train/                  |               |
|    approx_kl            | 0.0030396334  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0159        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.16          |
|    cost_value_loss      | 6.35e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.107        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000398     |
|    mean_cost_advantages | -0.0005835689 |
|    mean_reward_advan... | -0.0030637695 |
|    n_updates            | 650           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.47e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 8.8e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 735            |
|    iterations           | 67             |
|    time_elapsed         | 933            |
|    total_timesteps      | 686080         |
| train/                  |                |
|    approx_kl            | 0.008529427    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0265         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.138          |
|    cost_value_loss      | 1.78e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.085         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00157       |
|    mean_cost_advantages | -0.00018026758 |
|    mean_reward_advan... | -0.0058250516  |
|    n_updates            | 660            |
|    nu                   | 3.1            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000179      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.9e-06        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 734            |
|    iterations           | 68             |
|    time_elapsed         | 947            |
|    total_timesteps      | 696320         |
| train/                  |                |
|    approx_kl            | 0.0035943848   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0148         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0296         |
|    cost_value_loss      | 1.68e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0759        |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000611       |
|    mean_cost_advantages | -0.00012343316 |
|    mean_reward_advan... | -0.0030574142  |
|    n_updates            | 670            |
|    nu                   | 3.1            |
|    nu_loss              | -0             |
|    policy_gradient_loss | 4.72e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 3.21e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 733            |
|    iterations           | 69             |
|    time_elapsed         | 962            |
|    total_timesteps      | 706560         |
| train/                  |                |
|    approx_kl            | 0.0062343394   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0155         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.114          |
|    cost_value_loss      | 3.37e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0876        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000162      |
|    mean_cost_advantages | -0.00017506993 |
|    mean_reward_advan... | -0.003861136   |
|    n_updates            | 680            |
|    nu                   | 3.1            |
|    nu_loss              | -0             |
|    policy_gradient_loss | 5.84e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.56e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 817           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 732           |
|    iterations           | 70            |
|    time_elapsed         | 977           |
|    total_timesteps      | 716800        |
| train/                  |               |
|    approx_kl            | 0.00820488    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0229        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00811       |
|    cost_value_loss      | 2.91e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.113        |
|    learning_rate        | 0.0005        |
|    loss                 | 9.58e-05      |
|    mean_cost_advantages | 3.724194e-05  |
|    mean_reward_advan... | -0.0037767445 |
|    n_updates            | 690           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000329     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 5.64e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 731           |
|    iterations           | 71            |
|    time_elapsed         | 993           |
|    total_timesteps      | 727040        |
| train/                  |               |
|    approx_kl            | 0.0014947001  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00824       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.88          |
|    cost_value_loss      | 4.53e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.107        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000283      |
|    mean_cost_advantages | 1.4383771e-05 |
|    mean_reward_advan... | -0.02028749   |
|    n_updates            | 700           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.54e-05     |
|    reward_explained_... | -0.37         |
|    reward_value_loss    | 0.0101        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 72            |
|    time_elapsed         | 1009          |
|    total_timesteps      | 737280        |
| train/                  |               |
|    approx_kl            | 0.020306522   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0245        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.184         |
|    cost_value_loss      | 1.07e-07      |
|    early_stop_epoch     | 8             |
|    entropy_loss         | -0.113        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00178      |
|    mean_cost_advantages | 0.00048683592 |
|    mean_reward_advan... | 0.0020469406  |
|    n_updates            | 710           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00021      |
|    reward_explained_... | 0.981         |
|    reward_value_loss    | 6.13e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000781       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 729            |
|    iterations           | 73             |
|    time_elapsed         | 1025           |
|    total_timesteps      | 747520         |
| train/                  |                |
|    approx_kl            | 0.0016373806   |
|    average_cost         | 0.0024414062   |
|    clip_fraction        | 0.0304         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.92e+03      |
|    cost_value_loss      | 0.000522       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.142         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00168       |
|    mean_cost_advantages | 0.0020298734   |
|    mean_reward_advan... | -0.00090019155 |
|    n_updates            | 720            |
|    nu                   | 3.1            |
|    nu_loss              | -0.00757       |
|    policy_gradient_loss | -0.000701      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.5e-05        |
|    total_cost           | 25.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000586      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 728           |
|    iterations           | 74            |
|    time_elapsed         | 1040          |
|    total_timesteps      | 757760        |
| train/                  |               |
|    approx_kl            | 0.0018829659  |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0.0207        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.72         |
|    cost_value_loss      | 0.000161      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.128        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000389     |
|    mean_cost_advantages | -0.0017624755 |
|    mean_reward_advan... | -0.0023630864 |
|    n_updates            | 730           |
|    nu                   | 3.1           |
|    nu_loss              | -0.00242      |
|    policy_gradient_loss | -2.84e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 8.49e-06      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 818           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 727           |
|    iterations           | 75            |
|    time_elapsed         | 1055          |
|    total_timesteps      | 768000        |
| train/                  |               |
|    approx_kl            | 0.0023285884  |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.0133        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -32.9         |
|    cost_value_loss      | 0.000124      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.117        |
|    learning_rate        | 0.0005        |
|    loss                 | 5.03e-05      |
|    mean_cost_advantages | 0.00034184795 |
|    mean_reward_advan... | -0.0017266136 |
|    n_updates            | 740           |
|    nu                   | 3.1           |
|    nu_loss              | -0.00182      |
|    policy_gradient_loss | -0.000185     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.17e-06      |
|    total_cost           | 6.0           |
-------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.08
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 727           |
|    iterations           | 76            |
|    time_elapsed         | 1069          |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 0.07959354    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0198        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.272         |
|    cost_value_loss      | 7.39e-07      |
|    early_stop_epoch     | 6             |
|    entropy_loss         | -0.0941       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00541      |
|    mean_cost_advantages | -0.0005424613 |
|    mean_reward_advan... | -0.018331906  |
|    n_updates            | 750           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000961     |
|    reward_explained_... | -0.0698       |
|    reward_value_loss    | 0.00704       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 726            |
|    iterations           | 77             |
|    time_elapsed         | 1084           |
|    total_timesteps      | 788480         |
| train/                  |                |
|    approx_kl            | 0.0067029907   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0321         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0143         |
|    cost_value_loss      | 2.02e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0816        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000871      |
|    mean_cost_advantages | -0.00021341615 |
|    mean_reward_advan... | -0.0018162094  |
|    n_updates            | 760            |
|    nu                   | 3.1            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00026       |
|    reward_explained_... | 0.987          |
|    reward_value_loss    | 0.000126       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 725           |
|    iterations           | 78            |
|    time_elapsed         | 1100          |
|    total_timesteps      | 798720        |
| train/                  |               |
|    approx_kl            | 0.013444098   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0493        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.034        |
|    cost_value_loss      | 7.43e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0751       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00451       |
|    mean_cost_advantages | -4.647116e-06 |
|    mean_reward_advan... | -0.0018922088 |
|    n_updates            | 770           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000652     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 8.64e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 724           |
|    iterations           | 79            |
|    time_elapsed         | 1115          |
|    total_timesteps      | 808960        |
| train/                  |               |
|    approx_kl            | 0.00065412035 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0144        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0363       |
|    cost_value_loss      | 5.65e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0599       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000225     |
|    mean_cost_advantages | -1.806153e-06 |
|    mean_reward_advan... | -0.0020489614 |
|    n_updates            | 780           |
|    nu                   | 3.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -8.58e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.62e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 724           |
|    iterations           | 80            |
|    time_elapsed         | 1130          |
|    total_timesteps      | 819200        |
| train/                  |               |
|    approx_kl            | 0.0011756889  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0149        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0461       |
|    cost_value_loss      | 3.69e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0547       |
|    learning_rate        | 0.0005        |
|    loss                 | -8.22e-06     |
|    mean_cost_advantages | 4.379908e-06  |
|    mean_reward_advan... | -0.0028028905 |
|    n_updates            | 790           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000167     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.88e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 831            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 722            |
|    iterations           | 81             |
|    time_elapsed         | 1147           |
|    total_timesteps      | 829440         |
| train/                  |                |
|    approx_kl            | 0.0019042257   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0104         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0386        |
|    cost_value_loss      | 4.5e-08        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0544        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000758      |
|    mean_cost_advantages | -3.1799377e-06 |
|    mean_reward_advan... | -0.0029013804  |
|    n_updates            | 800            |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 2.22e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 7.08e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 721            |
|    iterations           | 82             |
|    time_elapsed         | 1163           |
|    total_timesteps      | 839680         |
| train/                  |                |
|    approx_kl            | 0.0006768318   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0135         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0629        |
|    cost_value_loss      | 1.44e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0537        |
|    learning_rate        | 0.0005         |
|    loss                 | 3.97e-05       |
|    mean_cost_advantages | -1.4877189e-05 |
|    mean_reward_advan... | -0.001914428   |
|    n_updates            | 810            |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 1.62e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.63e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 720           |
|    iterations           | 83            |
|    time_elapsed         | 1179          |
|    total_timesteps      | 849920        |
| train/                  |               |
|    approx_kl            | 5.862119e-05  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00931       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.112         |
|    cost_value_loss      | 1.17e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0537       |
|    learning_rate        | 0.0005        |
|    loss                 | -5.21e-05     |
|    mean_cost_advantages | 0.000401876   |
|    mean_reward_advan... | -0.0026109628 |
|    n_updates            | 820           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 4.78e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.77e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 719           |
|    iterations           | 84            |
|    time_elapsed         | 1195          |
|    total_timesteps      | 860160        |
| train/                  |               |
|    approx_kl            | 0.0029112839  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0212        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0735       |
|    cost_value_loss      | 1.09e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0576       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000568     |
|    mean_cost_advantages | -5.439248e-05 |
|    mean_reward_advan... | -0.0024762116 |
|    n_updates            | 830           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -5.49e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.37e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.32
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 146            |
|    mean_reward          | 5.98e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 40.8           |
|    ep_len_mean          | 102            |
|    ep_rew_mean          | 7.08e+03       |
| time/                   |                |
|    fps                  | 719            |
|    iterations           | 85             |
|    time_elapsed         | 1208           |
|    total_timesteps      | 870400         |
| train/                  |                |
|    approx_kl            | 0.3225357      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0732         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0301         |
|    cost_value_loss      | 3.59e-07       |
|    early_stop_epoch     | 1              |
|    entropy_loss         | -0.0736        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.0157        |
|    mean_cost_advantages | -0.00010455281 |
|    mean_reward_advan... | -0.0026727007  |
|    n_updates            | 840            |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00332       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.14e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 446           |
|    ep_len_mean          | 21.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 721           |
|    iterations           | 86            |
|    time_elapsed         | 1220          |
|    total_timesteps      | 880640        |
| train/                  |               |
|    approx_kl            | 0.031338517   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0649        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0154       |
|    cost_value_loss      | 6.27e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.127        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.0782        |
|    mean_cost_advantages | -7.446464e-05 |
|    mean_reward_advan... | -0.7176546    |
|    n_updates            | 850           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000751     |
|    reward_explained_... | -59.3         |
|    reward_value_loss    | 0.259         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 559           |
|    ep_len_mean          | 16.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 720           |
|    iterations           | 87            |
|    time_elapsed         | 1237          |
|    total_timesteps      | 890880        |
| train/                  |               |
|    approx_kl            | 0.0045559993  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0745        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0501       |
|    cost_value_loss      | 1.55e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.22         |
|    learning_rate        | 0.0005        |
|    loss                 | 0.0353        |
|    mean_cost_advantages | 2.7570622e-05 |
|    mean_reward_advan... | 0.14861515    |
|    n_updates            | 860           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00319      |
|    reward_explained_... | -12.6         |
|    reward_value_loss    | 0.0918        |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 656          |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 719          |
|    iterations           | 88           |
|    time_elapsed         | 1252         |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.008650729  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.178        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0542      |
|    cost_value_loss      | 1.12e-08     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.213       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.0138       |
|    mean_cost_advantages | 8.319775e-06 |
|    mean_reward_advan... | 0.20332937   |
|    n_updates            | 870          |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0068      |
|    reward_explained_... | -1.56        |
|    reward_value_loss    | 0.0362       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 801           |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 720           |
|    iterations           | 89            |
|    time_elapsed         | 1265          |
|    total_timesteps      | 911360        |
| train/                  |               |
|    approx_kl            | 0.0721491     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.165         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0455       |
|    cost_value_loss      | 1.22e-08      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.141        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00318      |
|    mean_cost_advantages | 1.3873045e-05 |
|    mean_reward_advan... | 0.14634788    |
|    n_updates            | 880           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00721      |
|    reward_explained_... | 0.514         |
|    reward_value_loss    | 0.0149        |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 814            |
|    ep_len_mean          | 12.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 719            |
|    iterations           | 90             |
|    time_elapsed         | 1280           |
|    total_timesteps      | 921600         |
| train/                  |                |
|    approx_kl            | 0.0016363893   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0273         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0435        |
|    cost_value_loss      | 1.36e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.102         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000412      |
|    mean_cost_advantages | -1.3829298e-05 |
|    mean_reward_advan... | 0.12794495     |
|    n_updates            | 890            |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00214       |
|    reward_explained_... | 0.948          |
|    reward_value_loss    | 0.00126        |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 811            |
|    ep_len_mean          | 12.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 719            |
|    iterations           | 91             |
|    time_elapsed         | 1295           |
|    total_timesteps      | 931840         |
| train/                  |                |
|    approx_kl            | 0.0010751514   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0206         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0403        |
|    cost_value_loss      | 1.27e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.096         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000504       |
|    mean_cost_advantages | -3.9973484e-06 |
|    mean_reward_advan... | 0.029976675    |
|    n_updates            | 900            |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00122       |
|    reward_explained_... | 0.96           |
|    reward_value_loss    | 0.000546       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 828           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 718           |
|    iterations           | 92            |
|    time_elapsed         | 1310          |
|    total_timesteps      | 942080        |
| train/                  |               |
|    approx_kl            | 0.01196673    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0219        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00273      |
|    cost_value_loss      | 1.47e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0744       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000976     |
|    mean_cost_advantages | 2.9085717e-05 |
|    mean_reward_advan... | 0.0050683175  |
|    n_updates            | 910           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00193      |
|    reward_explained_... | 0.97          |
|    reward_value_loss    | 0.000502      |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 833          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 718          |
|    iterations           | 93           |
|    time_elapsed         | 1325         |
|    total_timesteps      | 952320       |
| train/                  |              |
|    approx_kl            | 0.004399171  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0194       |
|    cost_value_loss      | 2.42e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0671      |
|    learning_rate        | 0.0005       |
|    loss                 | 0.000236     |
|    mean_cost_advantages | 7.952962e-05 |
|    mean_reward_advan... | 0.008350458  |
|    n_updates            | 920          |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000726    |
|    reward_explained_... | 0.99         |
|    reward_value_loss    | 0.000148     |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 574           |
|    ep_len_mean          | 17.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 718           |
|    iterations           | 94            |
|    time_elapsed         | 1339          |
|    total_timesteps      | 962560        |
| train/                  |               |
|    approx_kl            | 0.06039667    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0268        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0875        |
|    cost_value_loss      | 3.9e-08       |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.0878       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.0123       |
|    mean_cost_advantages | 0.0002027322  |
|    mean_reward_advan... | -2.627323e-05 |
|    n_updates            | 930           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000636     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.83e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 14.8           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 662            |
|    ep_len_mean          | 15.2           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 719            |
|    iterations           | 95             |
|    time_elapsed         | 1351           |
|    total_timesteps      | 972800         |
| train/                  |                |
|    approx_kl            | 0.0440005      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.207          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.013         |
|    cost_value_loss      | 2.75e-07       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.257         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000154       |
|    mean_cost_advantages | -2.9914998e-05 |
|    mean_reward_advan... | -0.11419578    |
|    n_updates            | 940            |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00692       |
|    reward_explained_... | 0.31           |
|    reward_value_loss    | 0.0127         |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 807           |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 720           |
|    iterations           | 96            |
|    time_elapsed         | 1364          |
|    total_timesteps      | 983040        |
| train/                  |               |
|    approx_kl            | 0.07776173    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.179         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00878      |
|    cost_value_loss      | 1.5e-07       |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.153        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00808      |
|    mean_cost_advantages | 2.4197383e-05 |
|    mean_reward_advan... | -0.009209903  |
|    n_updates            | 950           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00765      |
|    reward_explained_... | 0.531         |
|    reward_value_loss    | 0.0071        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 721           |
|    iterations           | 97            |
|    time_elapsed         | 1377          |
|    total_timesteps      | 993280        |
| train/                  |               |
|    approx_kl            | 0.017266428   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0215        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.089        |
|    cost_value_loss      | 3.35e-08      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0733       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00249      |
|    mean_cost_advantages | 6.2191895e-05 |
|    mean_reward_advan... | 0.06710919    |
|    n_updates            | 960           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00316      |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 0.000875      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 720           |
|    iterations           | 98            |
|    time_elapsed         | 1392          |
|    total_timesteps      | 1003520       |
| train/                  |               |
|    approx_kl            | 0.0012818399  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00828       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0251        |
|    cost_value_loss      | 9.07e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0547       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000782     |
|    mean_cost_advantages | 5.5717806e-05 |
|    mean_reward_advan... | 0.02360757    |
|    n_updates            | 970           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000442     |
|    reward_explained_... | 0.991         |
|    reward_value_loss    | 8.67e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 831            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 719            |
|    iterations           | 99             |
|    time_elapsed         | 1408           |
|    total_timesteps      | 1013760        |
| train/                  |                |
|    approx_kl            | 0.0026728017   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0232         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0953         |
|    cost_value_loss      | 3.62e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0682        |
|    learning_rate        | 0.0005         |
|    loss                 | 3.13e-05       |
|    mean_cost_advantages | -0.00013239495 |
|    mean_reward_advan... | 0.003303181    |
|    n_updates            | 980            |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000437      |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 3.91e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 719           |
|    iterations           | 100           |
|    time_elapsed         | 1423          |
|    total_timesteps      | 1024000       |
| train/                  |               |
|    approx_kl            | 0.00059884915 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0234        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.037        |
|    cost_value_loss      | 8.6e-09       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0632       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000587     |
|    mean_cost_advantages | 5.7213556e-06 |
|    mean_reward_advan... | -0.0006801266 |
|    n_updates            | 990           |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000366     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.61e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 831            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 718            |
|    iterations           | 101            |
|    time_elapsed         | 1438           |
|    total_timesteps      | 1034240        |
| train/                  |                |
|    approx_kl            | 0.0026810586   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0192         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0287        |
|    cost_value_loss      | 1.22e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0565        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000414      |
|    mean_cost_advantages | -2.4131543e-06 |
|    mean_reward_advan... | -0.0018628372  |
|    n_updates            | 1000           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000275      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.21e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 718            |
|    iterations           | 102            |
|    time_elapsed         | 1453           |
|    total_timesteps      | 1044480        |
| train/                  |                |
|    approx_kl            | 0.009151219    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0155         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.067          |
|    cost_value_loss      | 1.79e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0463        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000408      |
|    mean_cost_advantages | -0.00012573566 |
|    mean_reward_advan... | -0.0011632591  |
|    n_updates            | 1010           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000116      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.56e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 718            |
|    iterations           | 103            |
|    time_elapsed         | 1468           |
|    total_timesteps      | 1054720        |
| train/                  |                |
|    approx_kl            | 0.00019740127  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00576        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.024          |
|    cost_value_loss      | 1.08e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0387        |
|    learning_rate        | 0.0005         |
|    loss                 | 3.99e-06       |
|    mean_cost_advantages | -4.0195424e-05 |
|    mean_reward_advan... | -0.00131991    |
|    n_updates            | 1020           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -5.9e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.73e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 717            |
|    iterations           | 104            |
|    time_elapsed         | 1483           |
|    total_timesteps      | 1064960        |
| train/                  |                |
|    approx_kl            | 0.0013301292   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00683        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.211         |
|    cost_value_loss      | 9.69e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0355        |
|    learning_rate        | 0.0005         |
|    loss                 | 3.09e-05       |
|    mean_cost_advantages | -6.3509404e-05 |
|    mean_reward_advan... | 1.2161248e-05  |
|    n_updates            | 1030           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000191       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.05e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 717            |
|    iterations           | 105            |
|    time_elapsed         | 1498           |
|    total_timesteps      | 1075200        |
| train/                  |                |
|    approx_kl            | 0.0007793869   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00489        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.086          |
|    cost_value_loss      | 8.35e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0301        |
|    learning_rate        | 0.0005         |
|    loss                 | 8.22e-05       |
|    mean_cost_advantages | -5.2713993e-05 |
|    mean_reward_advan... | -0.0014114513  |
|    n_updates            | 1040           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -2.18e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 9.24e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 106           |
|    time_elapsed         | 1514          |
|    total_timesteps      | 1085440       |
| train/                  |               |
|    approx_kl            | 0.0040034153  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0074        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0216       |
|    cost_value_loss      | 1.77e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.018        |
|    learning_rate        | 0.0005        |
|    loss                 | -8.24e-05     |
|    mean_cost_advantages | -6.883392e-06 |
|    mean_reward_advan... | -0.000956117  |
|    n_updates            | 1050          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.43e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 2.5e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 107           |
|    time_elapsed         | 1529          |
|    total_timesteps      | 1095680       |
| train/                  |               |
|    approx_kl            | 0.00071954436 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00173       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0161        |
|    cost_value_loss      | 1.29e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0155       |
|    learning_rate        | 0.0005        |
|    loss                 | 2.22e-05      |
|    mean_cost_advantages | 2.1761382e-05 |
|    mean_reward_advan... | -0.001472313  |
|    n_updates            | 1060          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -9.67e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.44e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 108           |
|    time_elapsed         | 1544          |
|    total_timesteps      | 1105920       |
| train/                  |               |
|    approx_kl            | 0.00013470309 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00268       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0175       |
|    cost_value_loss      | 5.97e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0157       |
|    learning_rate        | 0.0005        |
|    loss                 | -2.19e-05     |
|    mean_cost_advantages | 2.8519245e-05 |
|    mean_reward_advan... | 0.0007073531  |
|    n_updates            | 1070          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 3.59e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.27e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 109           |
|    time_elapsed         | 1559          |
|    total_timesteps      | 1116160       |
| train/                  |               |
|    approx_kl            | 0.00053819997 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00209       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0413        |
|    cost_value_loss      | 7.79e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.013        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000299     |
|    mean_cost_advantages | 3.0748022e-05 |
|    mean_reward_advan... | -0.0005349172 |
|    n_updates            | 1080          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 4.11e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 2.16e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 110           |
|    time_elapsed         | 1574          |
|    total_timesteps      | 1126400       |
| train/                  |               |
|    approx_kl            | 0.004272186   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0134        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0269       |
|    cost_value_loss      | 1.2e-07       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0323       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00133      |
|    mean_cost_advantages | 1.3019903e-06 |
|    mean_reward_advan... | -0.0022942277 |
|    n_updates            | 1090          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.72e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.82e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 715            |
|    iterations           | 111            |
|    time_elapsed         | 1589           |
|    total_timesteps      | 1136640        |
| train/                  |                |
|    approx_kl            | 0.0008988889   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00731        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0391         |
|    cost_value_loss      | 7.73e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0372        |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000294       |
|    mean_cost_advantages | -3.5244382e-05 |
|    mean_reward_advan... | -0.0018091606  |
|    n_updates            | 1100           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 5.77e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.17e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 831            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 112            |
|    time_elapsed         | 1604           |
|    total_timesteps      | 1146880        |
| train/                  |                |
|    approx_kl            | 0.00066335057  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0121         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.186          |
|    cost_value_loss      | 9.44e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0398        |
|    learning_rate        | 0.0005         |
|    loss                 | 1.51e-05       |
|    mean_cost_advantages | -0.00028332643 |
|    mean_reward_advan... | -0.0013610732  |
|    n_updates            | 1110           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000142       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.92e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 113            |
|    time_elapsed         | 1619           |
|    total_timesteps      | 1157120        |
| train/                  |                |
|    approx_kl            | 0.00034771394  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00808        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0101         |
|    cost_value_loss      | 7.31e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0409        |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000441       |
|    mean_cost_advantages | -1.7560844e-05 |
|    mean_reward_advan... | -0.0005655313  |
|    n_updates            | 1120           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 4.96e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.24e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 114           |
|    time_elapsed         | 1634          |
|    total_timesteps      | 1167360       |
| train/                  |               |
|    approx_kl            | 0.0005577606  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00824       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00708      |
|    cost_value_loss      | 9.21e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0442       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000159     |
|    mean_cost_advantages | 1.9151976e-05 |
|    mean_reward_advan... | -0.0001594292 |
|    n_updates            | 1130          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 7.46e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.68e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.09
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 24             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 433            |
|    ep_len_mean          | 22.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 115            |
|    time_elapsed         | 1649           |
|    total_timesteps      | 1177600        |
| train/                  |                |
|    approx_kl            | 0.08860798     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0172         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.014         |
|    cost_value_loss      | 1.19e-07       |
|    early_stop_epoch     | 8              |
|    entropy_loss         | -0.0542        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.0107        |
|    mean_cost_advantages | -0.00015191462 |
|    mean_reward_advan... | 0.00012659533  |
|    n_updates            | 1140           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000211      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.95e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 608          |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 116          |
|    time_elapsed         | 1662         |
|    total_timesteps      | 1187840      |
| train/                  |              |
|    approx_kl            | 0.04153941   |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.295        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.06e+03    |
|    cost_value_loss      | 0.00042      |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.337       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.0069      |
|    mean_cost_advantages | 0.001259152  |
|    mean_reward_advan... | -0.18246213  |
|    n_updates            | 1150         |
|    nu                   | 3.11         |
|    nu_loss              | -0.00304     |
|    policy_gradient_loss | -0.00867     |
|    reward_explained_... | -0.401       |
|    reward_value_loss    | 0.0166       |
|    total_cost           | 10.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 14           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 713          |
|    ep_len_mean          | 14.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 715          |
|    iterations           | 117          |
|    time_elapsed         | 1675         |
|    total_timesteps      | 1198080      |
| train/                  |              |
|    approx_kl            | 0.025796121  |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.242        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -41.4        |
|    cost_value_loss      | 0.000166     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.263       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00151      |
|    mean_cost_advantages | 0.0013946618 |
|    mean_reward_advan... | 0.049864195  |
|    n_updates            | 1160         |
|    nu                   | 3.11         |
|    nu_loss              | -0.00121     |
|    policy_gradient_loss | -0.00787     |
|    reward_explained_... | 0.533        |
|    reward_value_loss    | 0.0105       |
|    total_cost           | 4.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 784           |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 118           |
|    time_elapsed         | 1687          |
|    total_timesteps      | 1208320       |
| train/                  |               |
|    approx_kl            | 0.028237188   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.211         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.091        |
|    cost_value_loss      | 1.51e-06      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.178        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00838      |
|    mean_cost_advantages | 0.00069695234 |
|    mean_reward_advan... | 0.07106853    |
|    n_updates            | 1170          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00712      |
|    reward_explained_... | 0.845         |
|    reward_value_loss    | 0.00415       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 825          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 119          |
|    time_elapsed         | 1700         |
|    total_timesteps      | 1218560      |
| train/                  |              |
|    approx_kl            | 0.028487269  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.106        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -550         |
|    cost_value_loss      | 7.76e-05     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.1         |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00574     |
|    mean_cost_advantages | 0.0008608092 |
|    mean_reward_advan... | 0.050662808  |
|    n_updates            | 1180         |
|    nu                   | 3.11         |
|    nu_loss              | -0.000607    |
|    policy_gradient_loss | -0.00538     |
|    reward_explained_... | 0.926        |
|    reward_value_loss    | 0.00162      |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 717           |
|    iterations           | 120           |
|    time_elapsed         | 1713          |
|    total_timesteps      | 1228800       |
| train/                  |               |
|    approx_kl            | 0.015891287   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0697        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0151        |
|    cost_value_loss      | 4.5e-07       |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0736       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00373      |
|    mean_cost_advantages | 0.00033315198 |
|    mean_reward_advan... | 0.02065835    |
|    n_updates            | 1190          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00157      |
|    reward_explained_... | 0.985         |
|    reward_value_loss    | 0.000219      |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 121          |
|    time_elapsed         | 1728         |
|    total_timesteps      | 1239040      |
| train/                  |              |
|    approx_kl            | 0.008783037  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0504       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0982      |
|    cost_value_loss      | 1.58e-08     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0874      |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00122      |
|    mean_cost_advantages | -8.87887e-05 |
|    mean_reward_advan... | 0.0037642499 |
|    n_updates            | 1200         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00212     |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 5e-05        |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 716            |
|    iterations           | 122            |
|    time_elapsed         | 1742           |
|    total_timesteps      | 1249280        |
| train/                  |                |
|    approx_kl            | 0.0027994192   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0191         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00366       |
|    cost_value_loss      | 9.6e-09        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.088         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000227       |
|    mean_cost_advantages | -2.8669456e-05 |
|    mean_reward_advan... | -0.0018695904  |
|    n_updates            | 1210           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00103       |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 2.83e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 716            |
|    iterations           | 123            |
|    time_elapsed         | 1757           |
|    total_timesteps      | 1259520        |
| train/                  |                |
|    approx_kl            | 0.009135733    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0404         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0492        |
|    cost_value_loss      | 7.86e-09       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0552        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00104       |
|    mean_cost_advantages | 4.812059e-06   |
|    mean_reward_advan... | -2.5856873e-05 |
|    n_updates            | 1220           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000736      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 9.92e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 124           |
|    time_elapsed         | 1773          |
|    total_timesteps      | 1269760       |
| train/                  |               |
|    approx_kl            | 0.0005377751  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00837       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.041        |
|    cost_value_loss      | 9.6e-09       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0521       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00017       |
|    mean_cost_advantages | -3.68693e-06  |
|    mean_reward_advan... | -0.0005211736 |
|    n_updates            | 1230          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 7.35e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.8e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 125           |
|    time_elapsed         | 1790          |
|    total_timesteps      | 1280000       |
| train/                  |               |
|    approx_kl            | 0.0015647154  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0348       |
|    cost_value_loss      | 3.6e-08       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0575       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000181     |
|    mean_cost_advantages | 2.3892512e-06 |
|    mean_reward_advan... | -0.0017416368 |
|    n_updates            | 1240          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 7.45e-06      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.59e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 713           |
|    iterations           | 126           |
|    time_elapsed         | 1807          |
|    total_timesteps      | 1290240       |
| train/                  |               |
|    approx_kl            | 0.0020220012  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00791       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0152       |
|    cost_value_loss      | 7.24e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0531       |
|    learning_rate        | 0.0005        |
|    loss                 | 1.17e-05      |
|    mean_cost_advantages | 2.402754e-05  |
|    mean_reward_advan... | -6.854662e-05 |
|    n_updates            | 1250          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.91e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 7.42e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 127            |
|    time_elapsed         | 1822           |
|    total_timesteps      | 1300480        |
| train/                  |                |
|    approx_kl            | 0.0039326036   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.012          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0581         |
|    cost_value_loss      | 7.28e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0434        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000145      |
|    mean_cost_advantages | -5.453559e-05  |
|    mean_reward_advan... | -0.00041673315 |
|    n_updates            | 1260           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 7.01e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 3.62e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 128            |
|    time_elapsed         | 1838           |
|    total_timesteps      | 1310720        |
| train/                  |                |
|    approx_kl            | 0.0020917307   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00724        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00925       |
|    cost_value_loss      | 1.13e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0504        |
|    learning_rate        | 0.0005         |
|    loss                 | 0.00196        |
|    mean_cost_advantages | -1.0797158e-05 |
|    mean_reward_advan... | -0.001987021   |
|    n_updates            | 1270           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000134       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 3.39e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 712            |
|    iterations           | 129            |
|    time_elapsed         | 1852           |
|    total_timesteps      | 1320960        |
| train/                  |                |
|    approx_kl            | 0.004148039    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0291         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.00388        |
|    cost_value_loss      | 3.96e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0475        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00196       |
|    mean_cost_advantages | -1.9770094e-05 |
|    mean_reward_advan... | -0.0009275079  |
|    n_updates            | 1280           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000218      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 3.74e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 834          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 130          |
|    time_elapsed         | 1868         |
|    total_timesteps      | 1331200      |
| train/                  |              |
|    approx_kl            | 0.0025618114 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.134        |
|    cost_value_loss      | 7.65e-08     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0496      |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00155     |
|    mean_cost_advantages | 0.0002120748 |
|    mean_reward_advan... | -0.001361056 |
|    n_updates            | 1290         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000134    |
|    reward_explained_... | 1            |
|    reward_value_loss    | 4.14e-06     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 712           |
|    iterations           | 131           |
|    time_elapsed         | 1883          |
|    total_timesteps      | 1341440       |
| train/                  |               |
|    approx_kl            | 0.0010810832  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00692       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.117        |
|    cost_value_loss      | 7.55e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0517       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000157     |
|    mean_cost_advantages | 4.3443226e-05 |
|    mean_reward_advan... | -0.0004866782 |
|    n_updates            | 1300          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.79e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.96e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.22
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 60.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 293            |
|    ep_len_mean          | 32.5           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 711            |
|    iterations           | 132            |
|    time_elapsed         | 1898           |
|    total_timesteps      | 1351680        |
| train/                  |                |
|    approx_kl            | 0.2236104      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0328         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0609         |
|    cost_value_loss      | 6.67e-08       |
|    early_stop_epoch     | 9              |
|    entropy_loss         | -0.0616        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.0112        |
|    mean_cost_advantages | -6.0097867e-05 |
|    mean_reward_advan... | -0.00082877977 |
|    n_updates            | 1310           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0008        |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.11e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 547           |
|    ep_len_mean          | 17.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 712           |
|    iterations           | 133           |
|    time_elapsed         | 1911          |
|    total_timesteps      | 1361920       |
| train/                  |               |
|    approx_kl            | 0.03796818    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.202         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.000287     |
|    cost_value_loss      | 1.61e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.266        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.0125        |
|    mean_cost_advantages | -7.591608e-05 |
|    mean_reward_advan... | -0.31560957   |
|    n_updates            | 1320          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00624      |
|    reward_explained_... | -1.76         |
|    reward_value_loss    | 0.0385        |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 656           |
|    ep_len_mean          | 15.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 712           |
|    iterations           | 134           |
|    time_elapsed         | 1926          |
|    total_timesteps      | 1372160       |
| train/                  |               |
|    approx_kl            | 0.011835136   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.112         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.208        |
|    cost_value_loss      | 3.77e-09      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.23         |
|    learning_rate        | 0.0005        |
|    loss                 | 0.0142        |
|    mean_cost_advantages | -6.208854e-05 |
|    mean_reward_advan... | 0.058911286   |
|    n_updates            | 1330          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00523      |
|    reward_explained_... | -4.75         |
|    reward_value_loss    | 0.0319        |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 14.8           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 722            |
|    ep_len_mean          | 14.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 711            |
|    iterations           | 135            |
|    time_elapsed         | 1941           |
|    total_timesteps      | 1382400        |
| train/                  |                |
|    approx_kl            | 0.0072807306   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.187          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0161        |
|    cost_value_loss      | 3.9e-09        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.184         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.0014        |
|    mean_cost_advantages | -4.7599275e-05 |
|    mean_reward_advan... | 0.12048451     |
|    n_updates            | 1340           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00757       |
|    reward_explained_... | 0.315          |
|    reward_value_loss    | 0.0134         |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 813            |
|    ep_len_mean          | 12.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 712            |
|    iterations           | 136            |
|    time_elapsed         | 1954           |
|    total_timesteps      | 1392640        |
| train/                  |                |
|    approx_kl            | 0.048364222    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.153          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0159         |
|    cost_value_loss      | 7.03e-09       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.114         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00289       |
|    mean_cost_advantages | -2.6640593e-05 |
|    mean_reward_advan... | 0.07816332     |
|    n_updates            | 1350           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00625       |
|    reward_explained_... | 0.763          |
|    reward_value_loss    | 0.00606        |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 137            |
|    time_elapsed         | 1966           |
|    total_timesteps      | 1402880        |
| train/                  |                |
|    approx_kl            | 0.033472832    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.052          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0363        |
|    cost_value_loss      | 8.26e-09       |
|    early_stop_epoch     | 1              |
|    entropy_loss         | -0.0757        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00232       |
|    mean_cost_advantages | -2.3769384e-05 |
|    mean_reward_advan... | 0.05833689     |
|    n_updates            | 1360           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00335       |
|    reward_explained_... | 0.968          |
|    reward_value_loss    | 0.000737       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 17.2           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 481            |
|    ep_len_mean          | 21.9           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 138            |
|    time_elapsed         | 1979           |
|    total_timesteps      | 1413120        |
| train/                  |                |
|    approx_kl            | 0.02540961     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0237         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0207        |
|    cost_value_loss      | 1.82e-08       |
|    early_stop_epoch     | 3              |
|    entropy_loss         | -0.0879        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00145       |
|    mean_cost_advantages | -2.9536322e-06 |
|    mean_reward_advan... | 0.020916881    |
|    n_updates            | 1370           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000744      |
|    reward_explained_... | 0.994          |
|    reward_value_loss    | 4.35e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 656          |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 139          |
|    time_elapsed         | 1991         |
|    total_timesteps      | 1423360      |
| train/                  |              |
|    approx_kl            | 0.041775506  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.227        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.000944    |
|    cost_value_loss      | 2.59e-08     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.402       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00217      |
|    mean_cost_advantages | 3.068879e-07 |
|    mean_reward_advan... | -0.16760571  |
|    n_updates            | 1380         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00798     |
|    reward_explained_... | -0.8         |
|    reward_value_loss    | 0.0246       |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 737           |
|    ep_len_mean          | 13.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 140           |
|    time_elapsed         | 2006          |
|    total_timesteps      | 1433600       |
| train/                  |               |
|    approx_kl            | 0.013760952   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.164         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0107       |
|    cost_value_loss      | 1.53e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.257        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000987      |
|    mean_cost_advantages | 6.7645756e-06 |
|    mean_reward_advan... | 0.04554235    |
|    n_updates            | 1390          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00667      |
|    reward_explained_... | 0.184         |
|    reward_value_loss    | 0.0123        |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 809            |
|    ep_len_mean          | 12.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 715            |
|    iterations           | 141            |
|    time_elapsed         | 2018           |
|    total_timesteps      | 1443840        |
| train/                  |                |
|    approx_kl            | 0.025654832    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.125          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0593        |
|    cost_value_loss      | 2.57e-08       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.185         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00209       |
|    mean_cost_advantages | -2.7280412e-05 |
|    mean_reward_advan... | 0.054664515    |
|    n_updates            | 1400           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00608       |
|    reward_explained_... | 0.804          |
|    reward_value_loss    | 0.0046         |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 829           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 142           |
|    time_elapsed         | 2031          |
|    total_timesteps      | 1454080       |
| train/                  |               |
|    approx_kl            | 0.016111642   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.056         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0382        |
|    cost_value_loss      | 2.89e-08      |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.103        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00572      |
|    mean_cost_advantages | 3.9179653e-05 |
|    mean_reward_advan... | 0.040046707   |
|    n_updates            | 1410          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00329      |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 0.000763      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 808          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 715          |
|    iterations           | 143          |
|    time_elapsed         | 2046         |
|    total_timesteps      | 1464320      |
| train/                  |              |
|    approx_kl            | 0.021465193  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0144      |
|    cost_value_loss      | 1.25e-07     |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.0917      |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00768     |
|    mean_cost_advantages | 4.739777e-06 |
|    mean_reward_advan... | 0.016081003  |
|    n_updates            | 1420         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00129     |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 9.85e-05     |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 831            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 715            |
|    iterations           | 144            |
|    time_elapsed         | 2060           |
|    total_timesteps      | 1474560        |
| train/                  |                |
|    approx_kl            | 0.012343818    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0296         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.547          |
|    cost_value_loss      | 5.41e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.103         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00784       |
|    mean_cost_advantages | -4.1706364e-05 |
|    mean_reward_advan... | -0.0082721785  |
|    n_updates            | 1430           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00406       |
|    reward_explained_... | 0.79           |
|    reward_value_loss    | 0.00147        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 145           |
|    time_elapsed         | 2075          |
|    total_timesteps      | 1484800       |
| train/                  |               |
|    approx_kl            | 0.00326183    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.011         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.202        |
|    cost_value_loss      | 2.9e-08       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0828       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000924      |
|    mean_cost_advantages | 0.00015404014 |
|    mean_reward_advan... | 0.0037964836  |
|    n_updates            | 1440          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000565     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 5.29e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 146           |
|    time_elapsed         | 2090          |
|    total_timesteps      | 1495040       |
| train/                  |               |
|    approx_kl            | 0.008673983   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0511        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.176         |
|    cost_value_loss      | 1.66e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0762       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00073      |
|    mean_cost_advantages | 2.7173088e-05 |
|    mean_reward_advan... | 0.0010040717  |
|    n_updates            | 1450          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000389     |
|    reward_explained_... | 0.993         |
|    reward_value_loss    | 0.000117      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 147            |
|    time_elapsed         | 2105           |
|    total_timesteps      | 1505280        |
| train/                  |                |
|    approx_kl            | 0.011598555    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0323         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0716        |
|    cost_value_loss      | 2.52e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.107         |
|    learning_rate        | 0.0005         |
|    loss                 | -1.76e-05      |
|    mean_cost_advantages | -2.5862095e-05 |
|    mean_reward_advan... | -0.0023664956  |
|    n_updates            | 1460           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000867      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 2.31e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 148            |
|    time_elapsed         | 2120           |
|    total_timesteps      | 1515520        |
| train/                  |                |
|    approx_kl            | 0.0005529445   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0113         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0526         |
|    cost_value_loss      | 7.77e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0972        |
|    learning_rate        | 0.0005         |
|    loss                 | 0.000272       |
|    mean_cost_advantages | -0.00014203634 |
|    mean_reward_advan... | -0.0020100418  |
|    n_updates            | 1470           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000136      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.56e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 149           |
|    time_elapsed         | 2134          |
|    total_timesteps      | 1525760       |
| train/                  |               |
|    approx_kl            | 0.015339166   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0342        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0433        |
|    cost_value_loss      | 9.3e-08       |
|    early_stop_epoch     | 6             |
|    entropy_loss         | -0.0742       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00286      |
|    mean_cost_advantages | 4.458501e-05  |
|    mean_reward_advan... | -0.0015802343 |
|    n_updates            | 1480          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000505     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.05e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 150            |
|    time_elapsed         | 2149           |
|    total_timesteps      | 1536000        |
| train/                  |                |
|    approx_kl            | 0.0010388384   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0056         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0219         |
|    cost_value_loss      | 7.67e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0679        |
|    learning_rate        | 0.0005         |
|    loss                 | -6.26e-05      |
|    mean_cost_advantages | -2.7821303e-05 |
|    mean_reward_advan... | -0.00109987    |
|    n_updates            | 1490           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.9e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.68e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 151            |
|    time_elapsed         | 2164           |
|    total_timesteps      | 1546240        |
| train/                  |                |
|    approx_kl            | 0.0037983      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0166         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.00319        |
|    cost_value_loss      | 3.18e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0746        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00168       |
|    mean_cost_advantages | 2.8491762e-05  |
|    mean_reward_advan... | -0.00080469827 |
|    n_updates            | 1500           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000115      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 2.85e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 833            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 152            |
|    time_elapsed         | 2180           |
|    total_timesteps      | 1556480        |
| train/                  |                |
|    approx_kl            | 0.011561771    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0615         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0486        |
|    cost_value_loss      | 9.51e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0938        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00202       |
|    mean_cost_advantages | -2.0088819e-05 |
|    mean_reward_advan... | -0.0019688886  |
|    n_updates            | 1510           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00187       |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 3.04e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 153           |
|    time_elapsed         | 2192          |
|    total_timesteps      | 1566720       |
| train/                  |               |
|    approx_kl            | 0.015276192   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.071         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00376      |
|    cost_value_loss      | 3.33e-08      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0812       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00263       |
|    mean_cost_advantages | 1.6283968e-05 |
|    mean_reward_advan... | -0.0028060735 |
|    n_updates            | 1520          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00102      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.6e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 154            |
|    time_elapsed         | 2206           |
|    total_timesteps      | 1576960        |
| train/                  |                |
|    approx_kl            | 0.015887296    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0248         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0136        |
|    cost_value_loss      | 8.31e-08       |
|    early_stop_epoch     | 4              |
|    entropy_loss         | -0.0508        |
|    learning_rate        | 0.0005         |
|    loss                 | 0.00174        |
|    mean_cost_advantages | -2.2972781e-05 |
|    mean_reward_advan... | -0.0021102163  |
|    n_updates            | 1530           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000207      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 3.78e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 155           |
|    time_elapsed         | 2221          |
|    total_timesteps      | 1587200       |
| train/                  |               |
|    approx_kl            | 0.0011831668  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00828       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00888      |
|    cost_value_loss      | 8.78e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0344       |
|    learning_rate        | 0.0005        |
|    loss                 | 6.88e-05      |
|    mean_cost_advantages | 8.831295e-06  |
|    mean_reward_advan... | -0.0010730156 |
|    n_updates            | 1540          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.85e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 2.27e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 156           |
|    time_elapsed         | 2236          |
|    total_timesteps      | 1597440       |
| train/                  |               |
|    approx_kl            | 0.0008031976  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00847       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0881        |
|    cost_value_loss      | 4.1e-08       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0382       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00055       |
|    mean_cost_advantages | 4.0808558e-05 |
|    mean_reward_advan... | 0.00036574856 |
|    n_updates            | 1550          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.06e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 2.74e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 157            |
|    time_elapsed         | 2250           |
|    total_timesteps      | 1607680        |
| train/                  |                |
|    approx_kl            | 0.001427666    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00487        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0182         |
|    cost_value_loss      | 7.43e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0271        |
|    learning_rate        | 0.0005         |
|    loss                 | -7.15e-05      |
|    mean_cost_advantages | -3.52627e-05   |
|    mean_reward_advan... | -0.00068769854 |
|    n_updates            | 1560           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.32e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 8.36e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 713            |
|    iterations           | 158            |
|    time_elapsed         | 2266           |
|    total_timesteps      | 1617920        |
| train/                  |                |
|    approx_kl            | 0.0019091083   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00566        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0683        |
|    cost_value_loss      | 5.61e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.026         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000171      |
|    mean_cost_advantages | 8.360829e-06   |
|    mean_reward_advan... | -0.00043659462 |
|    n_updates            | 1570           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 3.68e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 3.32e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.12
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 16.8           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 605            |
|    ep_len_mean          | 16.5           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 714            |
|    iterations           | 159            |
|    time_elapsed         | 2278           |
|    total_timesteps      | 1628160        |
| train/                  |                |
|    approx_kl            | 0.121767916    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0368         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0478        |
|    cost_value_loss      | 6.06e-08       |
|    early_stop_epoch     | 2              |
|    entropy_loss         | -0.0394        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.0104        |
|    mean_cost_advantages | 7.570012e-07   |
|    mean_reward_advan... | -0.00073735084 |
|    n_updates            | 1580           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00151       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 2.27e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 16.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 662           |
|    ep_len_mean          | 14.8          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 160           |
|    time_elapsed         | 2293          |
|    total_timesteps      | 1638400       |
| train/                  |               |
|    approx_kl            | 0.0081070755  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.214         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0809        |
|    cost_value_loss      | 3.4e-07       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.219        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00605      |
|    mean_cost_advantages | 0.00023218643 |
|    mean_reward_advan... | -0.09926362   |
|    n_updates            | 1590          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00787      |
|    reward_explained_... | 0.523         |
|    reward_value_loss    | 0.00795       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 735           |
|    ep_len_mean          | 13.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 161           |
|    time_elapsed         | 2305          |
|    total_timesteps      | 1648640       |
| train/                  |               |
|    approx_kl            | 0.02095001    |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.201         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.69e+03     |
|    cost_value_loss      | 0.000119      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.18         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00241      |
|    mean_cost_advantages | 0.00013614519 |
|    mean_reward_advan... | 0.0073944414  |
|    n_updates            | 1600          |
|    nu                   | 3.11          |
|    nu_loss              | -0.000607     |
|    policy_gradient_loss | -0.00711      |
|    reward_explained_... | 0.423         |
|    reward_value_loss    | 0.00773       |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 811           |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 162           |
|    time_elapsed         | 2318          |
|    total_timesteps      | 1658880       |
| train/                  |               |
|    approx_kl            | 0.03161742    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.181         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0862        |
|    cost_value_loss      | 4.73e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.117        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00589      |
|    mean_cost_advantages | 0.00070769974 |
|    mean_reward_advan... | 0.039423715   |
|    n_updates            | 1610          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00715      |
|    reward_explained_... | 0.797         |
|    reward_value_loss    | 0.00357       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 163           |
|    time_elapsed         | 2331          |
|    total_timesteps      | 1669120       |
| train/                  |               |
|    approx_kl            | 0.015933417   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0274        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0211       |
|    cost_value_loss      | 4.6e-09       |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.071        |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00195       |
|    mean_cost_advantages | 1.0429712e-05 |
|    mean_reward_advan... | 0.05325514    |
|    n_updates            | 1620          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00278      |
|    reward_explained_... | 0.964         |
|    reward_value_loss    | 0.000657      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 715            |
|    iterations           | 164            |
|    time_elapsed         | 2346           |
|    total_timesteps      | 1679360        |
| train/                  |                |
|    approx_kl            | 0.00070248824  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0209         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0495        |
|    cost_value_loss      | 2.88e-09       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0639        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00181       |
|    mean_cost_advantages | -2.4792642e-05 |
|    mean_reward_advan... | 0.023015374    |
|    n_updates            | 1630           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000262      |
|    reward_explained_... | 0.995          |
|    reward_value_loss    | 6.26e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 165           |
|    time_elapsed         | 2361          |
|    total_timesteps      | 1689600       |
| train/                  |               |
|    approx_kl            | 0.0061032744  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0239        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0389       |
|    cost_value_loss      | 3.32e-09      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0772       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.00113       |
|    mean_cost_advantages | 3.1543539e-06 |
|    mean_reward_advan... | 0.0041358164  |
|    n_updates            | 1640          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000516     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 5.99e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 166           |
|    time_elapsed         | 2375          |
|    total_timesteps      | 1699840       |
| train/                  |               |
|    approx_kl            | 0.019468024   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0781        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0284       |
|    cost_value_loss      | 5.4e-09       |
|    early_stop_epoch     | 6             |
|    entropy_loss         | -0.101        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00337      |
|    mean_cost_advantages | 1.7064834e-06 |
|    mean_reward_advan... | 0.0013869934  |
|    n_updates            | 1650          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00189      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 4.84e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 167           |
|    time_elapsed         | 2389          |
|    total_timesteps      | 1710080       |
| train/                  |               |
|    approx_kl            | 0.0020432805  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0171        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0477       |
|    cost_value_loss      | 5.78e-09      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0942       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000156     |
|    mean_cost_advantages | 5.0698704e-06 |
|    mean_reward_advan... | 0.00069155474 |
|    n_updates            | 1660          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000223     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.52e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 833          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 715          |
|    iterations           | 168          |
|    time_elapsed         | 2404         |
|    total_timesteps      | 1720320      |
| train/                  |              |
|    approx_kl            | 0.0056874063 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0393      |
|    cost_value_loss      | 6.52e-08     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.1         |
|    learning_rate        | 0.0005       |
|    loss                 | 0.000149     |
|    mean_cost_advantages | 9.827503e-06 |
|    mean_reward_advan... | 0.0012555979 |
|    n_updates            | 1670         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000132    |
|    reward_explained_... | 1            |
|    reward_value_loss    | 7.75e-06     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 169           |
|    time_elapsed         | 2419          |
|    total_timesteps      | 1730560       |
| train/                  |               |
|    approx_kl            | 0.0005054437  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.012         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.319         |
|    cost_value_loss      | 1.71e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.09         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.000189     |
|    mean_cost_advantages | 1.2676425e-05 |
|    mean_reward_advan... | -0.0010996215 |
|    n_updates            | 1680          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 3.87e-05      |
|    reward_explained_... | 0.988         |
|    reward_value_loss    | 0.000134      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.05
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 16.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 637            |
|    ep_len_mean          | 16.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 715            |
|    iterations           | 170            |
|    time_elapsed         | 2433           |
|    total_timesteps      | 1740800        |
| train/                  |                |
|    approx_kl            | 0.04587991     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0271         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.382          |
|    cost_value_loss      | 1.05e-07       |
|    early_stop_epoch     | 5              |
|    entropy_loss         | -0.0951        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.0166        |
|    mean_cost_advantages | -0.00010431148 |
|    mean_reward_advan... | -0.004310906   |
|    n_updates            | 1690           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00129       |
|    reward_explained_... | 0.978          |
|    reward_value_loss    | 0.000336       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 720           |
|    ep_len_mean          | 14.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 171           |
|    time_elapsed         | 2446          |
|    total_timesteps      | 1751040       |
| train/                  |               |
|    approx_kl            | 0.041155994   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.245         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0436       |
|    cost_value_loss      | 1.95e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.23         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00531      |
|    mean_cost_advantages | 1.3260813e-05 |
|    mean_reward_advan... | -0.08279358   |
|    n_updates            | 1700          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00718      |
|    reward_explained_... | 0.613         |
|    reward_value_loss    | 0.00681       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 807            |
|    ep_len_mean          | 12.4           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 716            |
|    iterations           | 172            |
|    time_elapsed         | 2458           |
|    total_timesteps      | 1761280        |
| train/                  |                |
|    approx_kl            | 0.050281525    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.215          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0129        |
|    cost_value_loss      | 3.8e-08        |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.165         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00734       |
|    mean_cost_advantages | -2.9297442e-07 |
|    mean_reward_advan... | 0.0010096075   |
|    n_updates            | 1710           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00774       |
|    reward_explained_... | 0.768          |
|    reward_value_loss    | 0.00378        |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 822          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 173          |
|    time_elapsed         | 2472         |
|    total_timesteps      | 1771520      |
| train/                  |              |
|    approx_kl            | 0.015814135  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.16         |
|    cost_value_loss      | 9.52e-09     |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.121       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00343     |
|    mean_cost_advantages | 0.0001213133 |
|    mean_reward_advan... | 0.036368378  |
|    n_updates            | 1720         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00344     |
|    reward_explained_... | 0.953        |
|    reward_value_loss    | 0.000815     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 751           |
|    ep_len_mean          | 13.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 174           |
|    time_elapsed         | 2486          |
|    total_timesteps      | 1781760       |
| train/                  |               |
|    approx_kl            | 0.0140710175  |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0432        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.58e+03     |
|    cost_value_loss      | 0.00012       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.12         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00282      |
|    mean_cost_advantages | 0.00032575228 |
|    mean_reward_advan... | 0.017457005   |
|    n_updates            | 1730          |
|    nu                   | 3.11          |
|    nu_loss              | -0.000607     |
|    policy_gradient_loss | -0.00129      |
|    reward_explained_... | 0.984         |
|    reward_value_loss    | 0.000248      |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 809          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 175          |
|    time_elapsed         | 2499         |
|    total_timesteps      | 1792000      |
| train/                  |              |
|    approx_kl            | 0.03239902   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.171        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.107        |
|    cost_value_loss      | 2.65e-07     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.191       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00646     |
|    mean_cost_advantages | 0.000396836  |
|    mean_reward_advan... | -0.022980774 |
|    n_updates            | 1740         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00647     |
|    reward_explained_... | 0.821        |
|    reward_value_loss    | 0.00288      |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 828          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 176          |
|    time_elapsed         | 2511         |
|    total_timesteps      | 1802240      |
| train/                  |              |
|    approx_kl            | 0.018793587  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0803       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.00843      |
|    cost_value_loss      | 2.76e-08     |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.113       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00617     |
|    mean_cost_advantages | 7.590192e-05 |
|    mean_reward_advan... | 0.015803264  |
|    n_updates            | 1750         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00517     |
|    reward_explained_... | 0.957        |
|    reward_value_loss    | 0.000701     |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 717           |
|    iterations           | 177           |
|    time_elapsed         | 2526          |
|    total_timesteps      | 1812480       |
| train/                  |               |
|    approx_kl            | 0.010852543   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0295        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.033        |
|    cost_value_loss      | 8.93e-09      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0789       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00364      |
|    mean_cost_advantages | 1.0201185e-05 |
|    mean_reward_advan... | 0.008593855   |
|    n_updates            | 1760          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00138      |
|    reward_explained_... | 0.991         |
|    reward_value_loss    | 0.000114      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 717            |
|    iterations           | 178            |
|    time_elapsed         | 2541           |
|    total_timesteps      | 1822720        |
| train/                  |                |
|    approx_kl            | 0.0036157449   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0154         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0471        |
|    cost_value_loss      | 4.3e-09        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0562        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00302       |
|    mean_cost_advantages | -3.3824974e-06 |
|    mean_reward_advan... | 0.0047256667   |
|    n_updates            | 1770           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000603      |
|    reward_explained_... | 0.995          |
|    reward_value_loss    | 7.83e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 179           |
|    time_elapsed         | 2556          |
|    total_timesteps      | 1832960       |
| train/                  |               |
|    approx_kl            | 0.0035384975  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00972       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.044        |
|    cost_value_loss      | 3.86e-09      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0657       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.0011       |
|    mean_cost_advantages | -8.374752e-08 |
|    mean_reward_advan... | 0.00062328146 |
|    n_updates            | 1780          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -3.96e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.14e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 831            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 716            |
|    iterations           | 180            |
|    time_elapsed         | 2571           |
|    total_timesteps      | 1843200        |
| train/                  |                |
|    approx_kl            | 0.00076842244  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00739        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0398        |
|    cost_value_loss      | 4.55e-09       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0711        |
|    learning_rate        | 0.0005         |
|    loss                 | 6e-05          |
|    mean_cost_advantages | -2.5616214e-06 |
|    mean_reward_advan... | -0.0006663339  |
|    n_updates            | 1790           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 1.33e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 6.95e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 716            |
|    iterations           | 181            |
|    time_elapsed         | 2586           |
|    total_timesteps      | 1853440        |
| train/                  |                |
|    approx_kl            | 0.00025066276  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00592        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0332         |
|    cost_value_loss      | 6.2e-09        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0728        |
|    learning_rate        | 0.0005         |
|    loss                 | -0.000208      |
|    mean_cost_advantages | -2.4647068e-05 |
|    mean_reward_advan... | 0.0010308485   |
|    n_updates            | 1800           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000171      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.81e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 622           |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 182           |
|    time_elapsed         | 2600          |
|    total_timesteps      | 1863680       |
| train/                  |               |
|    approx_kl            | 0.021725476   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0208        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0506        |
|    cost_value_loss      | 2.52e-08      |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.0853       |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00154      |
|    mean_cost_advantages | -3.066765e-05 |
|    mean_reward_advan... | -0.0012122495 |
|    n_updates            | 1810          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000303     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 3.3e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 793           |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 717           |
|    iterations           | 183           |
|    time_elapsed         | 2612          |
|    total_timesteps      | 1873920       |
| train/                  |               |
|    approx_kl            | 0.08567605    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.218         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.108        |
|    cost_value_loss      | 6.3e-09       |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.265        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00567      |
|    mean_cost_advantages | 2.6978892e-05 |
|    mean_reward_advan... | -0.0837277    |
|    n_updates            | 1820          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00823      |
|    reward_explained_... | 0.251         |
|    reward_value_loss    | 0.00921       |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 828          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 184          |
|    time_elapsed         | 2624         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.021381836  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0871       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.00466     |
|    cost_value_loss      | 1.62e-08     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.105       |
|    learning_rate        | 0.0005       |
|    loss                 | -0.00361     |
|    mean_cost_advantages | 1.067288e-05 |
|    mean_reward_advan... | 0.041125476  |
|    n_updates            | 1830         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00578     |
|    reward_explained_... | 0.893        |
|    reward_value_loss    | 0.00148      |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 717           |
|    iterations           | 185           |
|    time_elapsed         | 2639          |
|    total_timesteps      | 1894400       |
| train/                  |               |
|    approx_kl            | 0.0054158876  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0331        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.123         |
|    cost_value_loss      | 5.48e-08      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0726       |
|    learning_rate        | 0.0005        |
|    loss                 | -3.92e-05     |
|    mean_cost_advantages | -7.591014e-05 |
|    mean_reward_advan... | 0.02669768    |
|    n_updates            | 1840          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00084      |
|    reward_explained_... | 0.986         |
|    reward_value_loss    | 0.000128      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 832            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 717            |
|    iterations           | 186            |
|    time_elapsed         | 2653           |
|    total_timesteps      | 1904640        |
| train/                  |                |
|    approx_kl            | 0.0008440431   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0109         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0112        |
|    cost_value_loss      | 4.26e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0676        |
|    learning_rate        | 0.0005         |
|    loss                 | -8.81e-05      |
|    mean_cost_advantages | -2.1833004e-05 |
|    mean_reward_advan... | 0.0046997024   |
|    n_updates            | 1850           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000194      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 2.17e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 187          |
|    time_elapsed         | 2669         |
|    total_timesteps      | 1914880      |
| train/                  |              |
|    approx_kl            | 0.0013478143 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0533      |
|    cost_value_loss      | 4.25e-08     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0653      |
|    learning_rate        | 0.0005       |
|    loss                 | -0.000111    |
|    mean_cost_advantages | 3.523181e-06 |
|    mean_reward_advan... | 0.0001181292 |
|    n_updates            | 1860         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000361    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 2.54e-05     |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 717            |
|    iterations           | 188            |
|    time_elapsed         | 2683           |
|    total_timesteps      | 1925120        |
| train/                  |                |
|    approx_kl            | 0.00092707167  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0154         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0612        |
|    cost_value_loss      | 4.58e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0615        |
|    learning_rate        | 0.0005         |
|    loss                 | 9.21e-06       |
|    mean_cost_advantages | 2.2219992e-05  |
|    mean_reward_advan... | -0.00077076606 |
|    n_updates            | 1870           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -4.57e-05      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 9.19e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 831            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 717            |
|    iterations           | 189            |
|    time_elapsed         | 2698           |
|    total_timesteps      | 1935360        |
| train/                  |                |
|    approx_kl            | 0.002340575    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.015          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0318        |
|    cost_value_loss      | 5.24e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0618        |
|    learning_rate        | 0.0005         |
|    loss                 | -5.38e-05      |
|    mean_cost_advantages | 9.14142e-06    |
|    mean_reward_advan... | -0.00031524975 |
|    n_updates            | 1880           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000101      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.23e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 716            |
|    iterations           | 190            |
|    time_elapsed         | 2714           |
|    total_timesteps      | 1945600        |
| train/                  |                |
|    approx_kl            | 0.0009668533   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0193         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.151          |
|    cost_value_loss      | 3.01e-08       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0605        |
|    learning_rate        | 0.0005         |
|    loss                 | 6.69e-05       |
|    mean_cost_advantages | 5.881301e-05   |
|    mean_reward_advan... | -0.00014219806 |
|    n_updates            | 1890           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 3.54e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.6e-06        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 834           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 191           |
|    time_elapsed         | 2729          |
|    total_timesteps      | 1955840       |
| train/                  |               |
|    approx_kl            | 0.003356108   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0177        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0402       |
|    cost_value_loss      | 4e-08         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0601       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.000124      |
|    mean_cost_advantages | -9.003805e-06 |
|    mean_reward_advan... | -0.0008790203 |
|    n_updates            | 1900          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.57e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.75e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.05
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 85.7           |
|    ep_len_mean          | 77.8           |
|    ep_rew_mean          | 6.49e+03       |
| time/                   |                |
|    fps                  | 716            |
|    iterations           | 192            |
|    time_elapsed         | 2743           |
|    total_timesteps      | 1966080        |
| train/                  |                |
|    approx_kl            | 0.050678752    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.038          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0357         |
|    cost_value_loss      | 8.92e-08       |
|    early_stop_epoch     | 6              |
|    entropy_loss         | -0.081         |
|    learning_rate        | 0.0005         |
|    loss                 | 0.015          |
|    mean_cost_advantages | -4.2489177e-05 |
|    mean_reward_advan... | -0.0013416761  |
|    n_updates            | 1910           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0011        |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.95e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.19
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 757           |
|    ep_len_mean          | 13.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 716           |
|    iterations           | 193           |
|    time_elapsed         | 2757          |
|    total_timesteps      | 1976320       |
| train/                  |               |
|    approx_kl            | 0.18584037    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0547        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.519         |
|    cost_value_loss      | 1e-05         |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.0653       |
|    learning_rate        | 0.0005        |
|    loss                 | 0.112         |
|    mean_cost_advantages | -0.0009941601 |
|    mean_reward_advan... | -0.7809748    |
|    n_updates            | 1920          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00108      |
|    reward_explained_... | -42.6         |
|    reward_value_loss    | 0.322         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 820          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 194          |
|    time_elapsed         | 2770         |
|    total_timesteps      | 1986560      |
| train/                  |              |
|    approx_kl            | 0.02175966   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.012        |
|    cost_value_loss      | 8.33e-07     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.137       |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00342      |
|    mean_cost_advantages | -0.001103001 |
|    mean_reward_advan... | 0.12115516   |
|    n_updates            | 1930         |
|    nu                   | 3.11         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00382     |
|    reward_explained_... | -1.06        |
|    reward_value_loss    | 0.0306       |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 834            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 717            |
|    iterations           | 195            |
|    time_elapsed         | 2784           |
|    total_timesteps      | 1996800        |
| train/                  |                |
|    approx_kl            | 0.016176116    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0306         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00401       |
|    cost_value_loss      | 1.8e-07        |
|    early_stop_epoch     | 6              |
|    entropy_loss         | -0.105         |
|    learning_rate        | 0.0005         |
|    loss                 | -0.00202       |
|    mean_cost_advantages | -0.00017447151 |
|    mean_reward_advan... | 0.06484702     |
|    n_updates            | 1940           |
|    nu                   | 3.11           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00245       |
|    reward_explained_... | 0.921          |
|    reward_value_loss    | 0.00113        |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 13.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 733           |
|    ep_len_mean          | 13.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 717           |
|    iterations           | 196           |
|    time_elapsed         | 2798          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.024693774   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0529        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0185       |
|    cost_value_loss      | 4.61e-08      |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.119        |
|    learning_rate        | 0.0005        |
|    loss                 | -0.00244      |
|    mean_cost_advantages | -3.788488e-05 |
|    mean_reward_advan... | 0.022309467   |
|    n_updates            | 1950          |
|    nu                   | 3.11          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00154      |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 3.51e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Mean reward: 9994.500000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 46.87 minutes[0m
