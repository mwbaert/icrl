[32;1mConfigured folder ./cpg/wandb/run-20220627_121126-38m5sron/files for saving[0m
[32;1mName: JTL-v0_CJTL-v0_s_20_sid_-1[0m
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
----------------------------------
| eval/               |          |
|    best_mean_reward | 7.91e+03 |
|    mean_ep_length   | 121      |
|    mean_reward      | 7.91e+03 |
|    true_cost        | 0.494    |
| infos/              |          |
|    cost             | 0.3      |
| rollout/            |          |
|    adjusted_reward  | 29       |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 4.58e+03 |
| time/               |          |
|    fps              | 1031     |
|    iterations       | 1        |
|    time_elapsed     | 9        |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 7.91e+03   |
|    mean_ep_length       | 169        |
|    mean_reward          | 3.89e+03   |
|    true_cost            | 0.391      |
| infos/                  |            |
|    cost                 | 0.64       |
| rollout/                |            |
|    adjusted_reward      | 31.1       |
|    ep_len_mean          | 143        |
|    ep_rew_mean          | 4.5e+03    |
| time/                   |            |
|    fps                  | 488        |
|    iterations           | 2          |
|    time_elapsed         | 41         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.01586532 |
|    average_cost         | 0.49365234 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -437       |
|    cost_value_loss      | 21.8       |
|    early_stop_epoch     | 7          |
|    entropy_loss         | -1.38      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.57e+06   |
|    mean_cost_advantages | 7.72922    |
|    mean_reward_advan... | 463.61572  |
|    n_updates            | 10         |
|    nu                   | 1.06       |
|    nu_loss              | -0.494     |
|    policy_gradient_loss | -0.024     |
|    reward_explained_... | -2.05e+07  |
|    reward_value_loss    | 2.56e+06   |
|    total_cost           | 5055.0     |
----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 9.97e+03   |
|    mean_ep_length       | 65.6       |
|    mean_reward          | 9.97e+03   |
|    true_cost            | 0.287      |
| infos/                  |            |
|    cost                 | 0.14       |
| rollout/                |            |
|    adjusted_reward      | 45.9       |
|    ep_len_mean          | 137        |
|    ep_rew_mean          | 5.72e+03   |
| time/                   |            |
|    fps                  | 590        |
|    iterations           | 3          |
|    time_elapsed         | 52         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.02095626 |
|    average_cost         | 0.39101562 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -3.78      |
|    cost_value_loss      | 27.1       |
|    early_stop_epoch     | 4          |
|    entropy_loss         | -1.34      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13e+06   |
|    mean_cost_advantages | 4.2547903  |
|    mean_reward_advan... | 472.03534  |
|    n_updates            | 20         |
|    nu                   | 1.13       |
|    nu_loss              | -0.416     |
|    policy_gradient_loss | -0.0223    |
|    reward_explained_... | -1.94e+08  |
|    reward_value_loss    | 2.68e+06   |
|    total_cost           | 4004.0     |
----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.97e+03    |
|    mean_ep_length       | 121         |
|    mean_reward          | 7.94e+03    |
|    true_cost            | 0.217       |
| infos/                  |             |
|    cost                 | 0.22        |
| rollout/                |             |
|    adjusted_reward      | 61.7        |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 7.13e+03    |
| time/                   |             |
|    fps                  | 622         |
|    iterations           | 4           |
|    time_elapsed         | 65          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.017947838 |
|    average_cost         | 0.2870117   |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.98       |
|    cost_value_loss      | 28.5        |
|    early_stop_epoch     | 6           |
|    entropy_loss         | -1.26       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.62e+06    |
|    mean_cost_advantages | 1.3921121   |
|    mean_reward_advan... | 718.60803   |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.324      |
|    policy_gradient_loss | -0.0218     |
|    reward_explained_... | -1.23e+09   |
|    reward_value_loss    | 3.94e+06    |
|    total_cost           | 2939.0      |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.97e+03    |
|    mean_ep_length       | 115         |
|    mean_reward          | 5.93e+03    |
|    true_cost            | 0.178       |
| infos/                  |             |
|    cost                 | 0.21        |
| rollout/                |             |
|    adjusted_reward      | 58.8        |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | 7.03e+03    |
| time/                   |             |
|    fps                  | 649         |
|    iterations           | 5           |
|    time_elapsed         | 78          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.018191094 |
|    average_cost         | 0.2171875   |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.59       |
|    cost_value_loss      | 27.3        |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -1.15       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.3e+06     |
|    mean_cost_advantages | 0.034032144 |
|    mean_reward_advan... | 984.79767   |
|    n_updates            | 40          |
|    nu                   | 1.26        |
|    nu_loss              | -0.26       |
|    policy_gradient_loss | -0.017      |
|    reward_explained_... | -1.46e+10   |
|    reward_value_loss    | 5.28e+06    |
|    total_cost           | 2224.0      |
-----------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.97e+03    |
|    mean_ep_length       | 122         |
|    mean_reward          | 5.93e+03    |
|    true_cost            | 0.0972      |
| infos/                  |             |
|    cost                 | 0.07        |
| rollout/                |             |
|    adjusted_reward      | 34.5        |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 5.82e+03    |
| time/                   |             |
|    fps                  | 655         |
|    iterations           | 6           |
|    time_elapsed         | 93          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.017654972 |
|    average_cost         | 0.17792968  |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.25       |
|    cost_value_loss      | 22.3        |
|    early_stop_epoch     | 7           |
|    entropy_loss         | -1.02       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.91e+06    |
|    mean_cost_advantages | -0.52202064 |
|    mean_reward_advan... | 918.965     |
|    n_updates            | 50          |
|    nu                   | 1.33        |
|    nu_loss              | -0.225      |
|    policy_gradient_loss | -0.0142     |
|    reward_explained_... | -3.07e+10   |
|    reward_value_loss    | 4.97e+06    |
|    total_cost           | 1822.0      |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.07
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.97e+03    |
|    mean_ep_length       | 144         |
|    mean_reward          | 3.92e+03    |
|    true_cost            | 0.15        |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | 34.3        |
|    ep_len_mean          | 149         |
|    ep_rew_mean          | 5.11e+03    |
| time/                   |             |
|    fps                  | 684         |
|    iterations           | 7           |
|    time_elapsed         | 104         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.074229464 |
|    average_cost         | 0.09716797  |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.825      |
|    cost_value_loss      | 11.1        |
|    early_stop_epoch     | 6           |
|    entropy_loss         | -0.88       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57e+06    |
|    mean_cost_advantages | -1.1975054  |
|    mean_reward_advan... | 534.2199    |
|    n_updates            | 60          |
|    nu                   | 1.39        |
|    nu_loss              | -0.129      |
|    policy_gradient_loss | -0.0032     |
|    reward_explained_... | -9.25e+10   |
|    reward_value_loss    | 2.93e+06    |
|    total_cost           | 995.0       |
-----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.97e+03     |
|    mean_ep_length       | 119          |
|    mean_reward          | 7.94e+03     |
|    true_cost            | 0.0854       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 60.8         |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | 6.83e+03     |
| time/                   |              |
|    fps                  | 700          |
|    iterations           | 8            |
|    time_elapsed         | 116          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.015066755  |
|    average_cost         | 0.14951172   |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.3         |
|    cost_value_loss      | 16.1         |
|    early_stop_epoch     | 9            |
|    entropy_loss         | -1           |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+06     |
|    mean_cost_advantages | -0.026034212 |
|    mean_reward_advan... | 531.9077     |
|    n_updates            | 70           |
|    nu                   | 1.46         |
|    nu_loss              | -0.208       |
|    policy_gradient_loss | -0.016       |
|    reward_explained_... | -1.25e+11    |
|    reward_value_loss    | 2.93e+06     |
|    total_cost           | 1531.0       |
------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.97e+03    |
|    mean_ep_length       | 58.4        |
|    mean_reward          | 9.97e+03    |
|    true_cost            | 0.103       |
| infos/                  |             |
|    cost                 | 0.21        |
| rollout/                |             |
|    adjusted_reward      | 73.5        |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 8.04e+03    |
| time/                   |             |
|    fps                  | 722         |
|    iterations           | 9           |
|    time_elapsed         | 127         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.015511435 |
|    average_cost         | 0.085351564 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.229      |
|    cost_value_loss      | 9.11        |
|    early_stop_epoch     | 9           |
|    entropy_loss         | -0.879      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.94e+06    |
|    mean_cost_advantages | -1.0878661  |
|    mean_reward_advan... | 939.2367    |
|    n_updates            | 80          |
|    nu                   | 1.52        |
|    nu_loss              | -0.124      |
|    policy_gradient_loss | -0.011      |
|    reward_explained_... | -4.91e+11   |
|    reward_value_loss    | 5.04e+06    |
|    total_cost           | 874.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.97e+03    |
|    mean_ep_length       | 94.2        |
|    mean_reward          | 9.96e+03    |
|    true_cost            | 0.0707      |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | 81.4        |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 8.15e+03    |
| time/                   |             |
|    fps                  | 742         |
|    iterations           | 10          |
|    time_elapsed         | 137         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.006432505 |
|    average_cost         | 0.103027344 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.56       |
|    cost_value_loss      | 12.2        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.819      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.86e+06    |
|    mean_cost_advantages | -0.50545865 |
|    mean_reward_advan... | 1116.1791   |
|    n_updates            | 90          |
|    nu                   | 1.58        |
|    nu_loss              | -0.156      |
|    policy_gradient_loss | -0.0098     |
|    reward_explained_... | -5.04e+12   |
|    reward_value_loss    | 6.04e+06    |
|    total_cost           | 1055.0      |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.98e+03    |
|    mean_ep_length       | 56.2        |
|    mean_reward          | 9.98e+03    |
|    true_cost            | 0.0631      |
| infos/                  |             |
|    cost                 | 0.08        |
| rollout/                |             |
|    adjusted_reward      | 117         |
|    ep_len_mean          | 79.5        |
|    ep_rew_mean          | 8.76e+03    |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 11          |
|    time_elapsed         | 149         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.004990688 |
|    average_cost         | 0.07070313  |
|    clip_fraction        | 0.0603      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0466     |
|    cost_value_loss      | 8.14        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.758      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.98e+06    |
|    mean_cost_advantages | -0.90814245 |
|    mean_reward_advan... | 1230.7313   |
|    n_updates            | 100         |
|    nu                   | 1.64        |
|    nu_loss              | -0.112      |
|    policy_gradient_loss | -0.0107     |
|    reward_explained_... | -2.28e+13   |
|    reward_value_loss    | 6.65e+06    |
|    total_cost           | 724.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.98e+03    |
|    mean_ep_length       | 74.6        |
|    mean_reward          | 7.95e+03    |
|    true_cost            | 0.083       |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 170         |
|    ep_len_mean          | 56.1        |
|    ep_rew_mean          | 9.37e+03    |
| time/                   |             |
|    fps                  | 765         |
|    iterations           | 12          |
|    time_elapsed         | 160         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.004861551 |
|    average_cost         | 0.063085936 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0415      |
|    cost_value_loss      | 6.1         |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.716      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.72e+06    |
|    mean_cost_advantages | -0.87372655 |
|    mean_reward_advan... | 1682.0764   |
|    n_updates            | 110         |
|    nu                   | 1.69        |
|    nu_loss              | -0.103      |
|    policy_gradient_loss | -0.00935    |
|    reward_explained_... | -7.05e+13   |
|    reward_value_loss    | 9.2e+06     |
|    total_cost           | 646.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.98e+03     |
|    mean_ep_length       | 112          |
|    mean_reward          | 5.93e+03     |
|    true_cost            | 0.0858       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | 216          |
|    ep_len_mean          | 47.5         |
|    ep_rew_mean          | 9.88e+03     |
| time/                   |              |
|    fps                  | 780          |
|    iterations           | 13           |
|    time_elapsed         | 170          |
|    total_timesteps      | 133120       |
| train/                  |              |
|    approx_kl            | 0.0029515806 |
|    average_cost         | 0.08300781   |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.482       |
|    cost_value_loss      | 7.72         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.686       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.65e+06     |
|    mean_cost_advantages | -0.60306084  |
|    mean_reward_advan... | 2429.02      |
|    n_updates            | 120          |
|    nu                   | 1.75         |
|    nu_loss              | -0.141       |
|    policy_gradient_loss | -0.00478     |
|    reward_explained_... | -2.42e+14    |
|    reward_value_loss    | 1.34e+07     |
|    total_cost           | 850.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.98e+03    |
|    mean_ep_length       | 63.4        |
|    mean_reward          | 9.98e+03    |
|    true_cost            | 0.0554      |
| infos/                  |             |
|    cost                 | 0.18        |
| rollout/                |             |
|    adjusted_reward      | 193         |
|    ep_len_mean          | 53.4        |
|    ep_rew_mean          | 9.47e+03    |
| time/                   |             |
|    fps                  | 795         |
|    iterations           | 14          |
|    time_elapsed         | 180         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.006345234 |
|    average_cost         | 0.085839845 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.191      |
|    cost_value_loss      | 6.79        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.722      |
|    learning_rate        | 0.0003      |
|    loss                 | 7.16e+06    |
|    mean_cost_advantages | -0.48335558 |
|    mean_reward_advan... | 3046.2214   |
|    n_updates            | 130         |
|    nu                   | 1.81        |
|    nu_loss              | -0.15       |
|    policy_gradient_loss | -0.00761    |
|    reward_explained_... | -7.35e+14   |
|    reward_value_loss    | 1.69e+07    |
|    total_cost           | 879.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.074        |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 198          |
|    ep_len_mean          | 46.6         |
|    ep_rew_mean          | 9.88e+03     |
| time/                   |              |
|    fps                  | 802          |
|    iterations           | 15           |
|    time_elapsed         | 191          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0043220697 |
|    average_cost         | 0.055371094  |
|    clip_fraction        | 0.00643      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0257      |
|    cost_value_loss      | 3.39         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.653       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.97e+06     |
|    mean_cost_advantages | -0.5003694   |
|    mean_reward_advan... | 2669.3176    |
|    n_updates            | 140          |
|    nu                   | 1.86         |
|    nu_loss              | -0.1         |
|    policy_gradient_loss | -0.00374     |
|    reward_explained_... | -5.24e+14    |
|    reward_value_loss    | 1.49e+07     |
|    total_cost           | 567.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 41.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0525        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | 229           |
|    ep_len_mean          | 44.1          |
|    ep_rew_mean          | 9.68e+03      |
| time/                   |               |
|    fps                  | 815           |
|    iterations           | 16            |
|    time_elapsed         | 200           |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.0058919205  |
|    average_cost         | 0.07402344    |
|    clip_fraction        | 0.0235        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.663        |
|    cost_value_loss      | 6.24          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.634        |
|    learning_rate        | 0.0003        |
|    loss                 | 8.38e+06      |
|    mean_cost_advantages | 0.00093705655 |
|    mean_reward_advan... | 2748.621      |
|    n_updates            | 150           |
|    nu                   | 1.92          |
|    nu_loss              | -0.138        |
|    policy_gradient_loss | -0.0058       |
|    reward_explained_... | -1.13e+15     |
|    reward_value_loss    | 1.52e+07      |
|    total_cost           | 758.0         |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 28.8        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.05        |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | 250         |
|    ep_len_mean          | 38.1        |
|    ep_rew_mean          | 9.78e+03    |
| time/                   |             |
|    fps                  | 826         |
|    iterations           | 17          |
|    time_elapsed         | 210         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.004326853 |
|    average_cost         | 0.05253906  |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.00205     |
|    cost_value_loss      | 2.62        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.606      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.62e+06    |
|    mean_cost_advantages | -0.35223565 |
|    mean_reward_advan... | 3073.0703   |
|    n_updates            | 160         |
|    nu                   | 1.97        |
|    nu_loss              | -0.101      |
|    policy_gradient_loss | -0.00471    |
|    reward_explained_... | -5.62e+14   |
|    reward_value_loss    | 1.73e+07    |
|    total_cost           | 538.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0427       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 279          |
|    ep_len_mean          | 35.1         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 832          |
|    iterations           | 18           |
|    time_elapsed         | 221          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0013318844 |
|    average_cost         | 0.05         |
|    clip_fraction        | 0.00235      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.384       |
|    cost_value_loss      | 2.25         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.602       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.36e+07     |
|    mean_cost_advantages | -0.25459993  |
|    mean_reward_advan... | 3329.8079    |
|    n_updates            | 170          |
|    nu                   | 2.02         |
|    nu_loss              | -0.0985      |
|    policy_gradient_loss | -0.00191     |
|    reward_explained_... | -5.55e+14    |
|    reward_value_loss    | 1.88e+07     |
|    total_cost           | 512.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 39.6        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0423      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | 216         |
|    ep_len_mean          | 43.5        |
|    ep_rew_mean          | 9.48e+03    |
| time/                   |             |
|    fps                  | 837         |
|    iterations           | 19          |
|    time_elapsed         | 232         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.002899383 |
|    average_cost         | 0.042675782 |
|    clip_fraction        | 0.00638     |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0172     |
|    cost_value_loss      | 1.08        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.607      |
|    learning_rate        | 0.0003      |
|    loss                 | 8.89e+06    |
|    mean_cost_advantages | -0.26571926 |
|    mean_reward_advan... | 3626.2007   |
|    n_updates            | 180         |
|    nu                   | 2.07        |
|    nu_loss              | -0.0863     |
|    policy_gradient_loss | -0.00162    |
|    reward_explained_... | -2.51e+15   |
|    reward_value_loss    | 2.07e+07    |
|    total_cost           | 437.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 38.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0476       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 289          |
|    ep_len_mean          | 36.7         |
|    ep_rew_mean          | 9.58e+03     |
| time/                   |              |
|    fps                  | 842          |
|    iterations           | 20           |
|    time_elapsed         | 243          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0039302595 |
|    average_cost         | 0.042285156  |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.923       |
|    cost_value_loss      | 1.93         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.536       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.19e+06     |
|    mean_cost_advantages | 0.006703435  |
|    mean_reward_advan... | 2842.1353    |
|    n_updates            | 190          |
|    nu                   | 2.12         |
|    nu_loss              | -0.0876      |
|    policy_gradient_loss | -0.00244     |
|    reward_explained_... | -1.69e+15    |
|    reward_value_loss    | 1.6e+07      |
|    total_cost           | 433.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 39.2         |
|    mean_reward          | 9.98e+03     |
|    true_cost            | 0.0503       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 293          |
|    ep_len_mean          | 36.2         |
|    ep_rew_mean          | 9.79e+03     |
| time/                   |              |
|    fps                  | 848          |
|    iterations           | 21           |
|    time_elapsed         | 253          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.0024540522 |
|    average_cost         | 0.047558594  |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.447       |
|    cost_value_loss      | 1.27         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.574       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.01e+06     |
|    mean_cost_advantages | -0.035861284 |
|    mean_reward_advan... | 3675.2515    |
|    n_updates            | 200          |
|    nu                   | 2.17         |
|    nu_loss              | -0.101       |
|    policy_gradient_loss | -0.00156     |
|    reward_explained_... | -1.42e+15    |
|    reward_value_loss    | 2.1e+07      |
|    total_cost           | 487.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 33           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0407       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 264          |
|    ep_len_mean          | 38.9         |
|    ep_rew_mean          | 9.58e+03     |
| time/                   |              |
|    fps                  | 858          |
|    iterations           | 22           |
|    time_elapsed         | 262          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.0022488318 |
|    average_cost         | 0.05029297   |
|    clip_fraction        | 0.000908     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.573       |
|    cost_value_loss      | 1.45         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.595       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+07     |
|    mean_cost_advantages | 0.0020569726 |
|    mean_reward_advan... | 3805.982     |
|    n_updates            | 210          |
|    nu                   | 2.22         |
|    nu_loss              | -0.109       |
|    policy_gradient_loss | -0.00144     |
|    reward_explained_... | -1.59e+15    |
|    reward_value_loss    | 2.13e+07     |
|    total_cost           | 515.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0401       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | 308          |
|    ep_len_mean          | 32.6         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 864          |
|    iterations           | 23           |
|    time_elapsed         | 272          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.0020995145 |
|    average_cost         | 0.040722657  |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.591       |
|    cost_value_loss      | 1.47         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.537       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.72e+06     |
|    mean_cost_advantages | -0.048232783 |
|    mean_reward_advan... | 3360.8286    |
|    n_updates            | 220          |
|    nu                   | 2.27         |
|    nu_loss              | -0.0903      |
|    policy_gradient_loss | -0.00182     |
|    reward_explained_... | -3.3e+14     |
|    reward_value_loss    | 1.9e+07      |
|    total_cost           | 417.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 25.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0441        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | 323           |
|    ep_len_mean          | 28.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 870           |
|    iterations           | 24            |
|    time_elapsed         | 282           |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.00020270803 |
|    average_cost         | 0.040136717   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.434        |
|    cost_value_loss      | 1.01          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.547        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.07e+07      |
|    mean_cost_advantages | -0.07276581   |
|    mean_reward_advan... | 3853.1577     |
|    n_updates            | 230           |
|    nu                   | 2.31          |
|    nu_loss              | -0.0909       |
|    policy_gradient_loss | -0.000798     |
|    reward_explained_... | -1.58e+15     |
|    reward_value_loss    | 2.19e+07      |
|    total_cost           | 411.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0362       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 277          |
|    ep_len_mean          | 37.3         |
|    ep_rew_mean          | 9.68e+03     |
| time/                   |              |
|    fps                  | 876          |
|    iterations           | 25           |
|    time_elapsed         | 292          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.0032957452 |
|    average_cost         | 0.044140626  |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.538       |
|    cost_value_loss      | 0.758        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.551       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.17e+07     |
|    mean_cost_advantages | -0.012579164 |
|    mean_reward_advan... | 3990.9836    |
|    n_updates            | 240          |
|    nu                   | 2.36         |
|    nu_loss              | -0.102       |
|    policy_gradient_loss | -0.00141     |
|    reward_explained_... | -5.03e+14    |
|    reward_value_loss    | 2.27e+07     |
|    total_cost           | 452.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 24           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0358       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 307          |
|    ep_len_mean          | 30.7         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 882          |
|    iterations           | 26           |
|    time_elapsed         | 301          |
|    total_timesteps      | 266240       |
| train/                  |              |
|    approx_kl            | 0.0012366375 |
|    average_cost         | 0.036230467  |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.729       |
|    cost_value_loss      | 0.857        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.517       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02e+07     |
|    mean_cost_advantages | -0.04365557  |
|    mean_reward_advan... | 3474.3638    |
|    n_updates            | 250          |
|    nu                   | 2.4          |
|    nu_loss              | -0.0854      |
|    policy_gradient_loss | -0.000662    |
|    reward_explained_... | -1.54e+15    |
|    reward_value_loss    | 1.95e+07     |
|    total_cost           | 371.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 26.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0365        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 282           |
|    ep_len_mean          | 34.6          |
|    ep_rew_mean          | 9.68e+03      |
| time/                   |               |
|    fps                  | 885           |
|    iterations           | 27            |
|    time_elapsed         | 312           |
|    total_timesteps      | 276480        |
| train/                  |               |
|    approx_kl            | 0.00050935545 |
|    average_cost         | 0.035839844   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.348        |
|    cost_value_loss      | 0.515         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.518        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.3e+07       |
|    mean_cost_advantages | -0.05774309   |
|    mean_reward_advan... | 3761.6138     |
|    n_updates            | 260           |
|    nu                   | 2.44          |
|    nu_loss              | -0.086        |
|    policy_gradient_loss | -0.000535     |
|    reward_explained_... | -3.52e+14     |
|    reward_value_loss    | 2.13e+07      |
|    total_cost           | 367.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 28.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.046        |
| infos/                  |              |
|    cost                 | 0.13         |
| rollout/                |              |
|    adjusted_reward      | 363          |
|    ep_len_mean          | 26           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 885          |
|    iterations           | 28           |
|    time_elapsed         | 323          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.0029351898 |
|    average_cost         | 0.03652344   |
|    clip_fraction        | 0.00998      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.31        |
|    cost_value_loss      | 0.949        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.47        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.89e+06     |
|    mean_cost_advantages | 0.025097486  |
|    mean_reward_advan... | 3401.894     |
|    n_updates            | 270          |
|    nu                   | 2.49         |
|    nu_loss              | -0.0893      |
|    policy_gradient_loss | -0.00133     |
|    reward_explained_... | -8.5e+14     |
|    reward_value_loss    | 1.94e+07     |
|    total_cost           | 374.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0357       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 332          |
|    ep_len_mean          | 31.6         |
|    ep_rew_mean          | 9.69e+03     |
| time/                   |              |
|    fps                  | 885          |
|    iterations           | 29           |
|    time_elapsed         | 335          |
|    total_timesteps      | 296960       |
| train/                  |              |
|    approx_kl            | 0.0028189684 |
|    average_cost         | 0.045996092  |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.83        |
|    cost_value_loss      | 0.721        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.537       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.75e+06     |
|    mean_cost_advantages | 0.019674305  |
|    mean_reward_advan... | 4335.3975    |
|    n_updates            | 280          |
|    nu                   | 2.53         |
|    nu_loss              | -0.114       |
|    policy_gradient_loss | -0.00134     |
|    reward_explained_... | -3.96e+15    |
|    reward_value_loss    | 2.46e+07     |
|    total_cost           | 471.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0414       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 353          |
|    ep_len_mean          | 26           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 881          |
|    iterations           | 30           |
|    time_elapsed         | 348          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0018102167 |
|    average_cost         | 0.035742186  |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.19        |
|    cost_value_loss      | 0.841        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.494       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.06e+07     |
|    mean_cost_advantages | -0.044896998 |
|    mean_reward_advan... | 3917.3625    |
|    n_updates            | 290          |
|    nu                   | 2.57         |
|    nu_loss              | -0.0904      |
|    policy_gradient_loss | -0.00137     |
|    reward_explained_... | -1.6e+15     |
|    reward_value_loss    | 2.23e+07     |
|    total_cost           | 366.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 29.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0362       |
| infos/                  |              |
|    cost                 | 0.09         |
| rollout/                |              |
|    adjusted_reward      | 343          |
|    ep_len_mean          | 26.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 886          |
|    iterations           | 31           |
|    time_elapsed         | 358          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0028565268 |
|    average_cost         | 0.04140625   |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.466       |
|    cost_value_loss      | 0.427        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.499       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.24e+07     |
|    mean_cost_advantages | 0.0065944865 |
|    mean_reward_advan... | 4105.3525    |
|    n_updates            | 300          |
|    nu                   | 2.61         |
|    nu_loss              | -0.106       |
|    policy_gradient_loss | -0.00114     |
|    reward_explained_... | -1.19e+14    |
|    reward_value_loss    | 2.35e+07     |
|    total_cost           | 424.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0394       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 344          |
|    ep_len_mean          | 28.3         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 32           |
|    time_elapsed         | 370          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0028150072 |
|    average_cost         | 0.036230467  |
|    clip_fraction        | 0.00512      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.711       |
|    cost_value_loss      | 0.574        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.503       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.19e+07     |
|    mean_cost_advantages | -0.020530853 |
|    mean_reward_advan... | 4089.1948    |
|    n_updates            | 310          |
|    nu                   | 2.66         |
|    nu_loss              | -0.0947      |
|    policy_gradient_loss | -0.00106     |
|    reward_explained_... | -1.1e+14     |
|    reward_value_loss    | 2.29e+07     |
|    total_cost           | 371.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 28           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0464       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 407          |
|    ep_len_mean          | 25.3         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 887          |
|    iterations           | 33           |
|    time_elapsed         | 380          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0017392315 |
|    average_cost         | 0.039355468  |
|    clip_fraction        | 0.000215     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.17        |
|    cost_value_loss      | 0.639        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.474       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.1e+07      |
|    mean_cost_advantages | 0.013974145  |
|    mean_reward_advan... | 3968.484     |
|    n_updates            | 320          |
|    nu                   | 2.7          |
|    nu_loss              | -0.105       |
|    policy_gradient_loss | -0.000675    |
|    reward_explained_... | -1.78e+14    |
|    reward_value_loss    | 2.25e+07     |
|    total_cost           | 403.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0374       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 356          |
|    ep_len_mean          | 24.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 34           |
|    time_elapsed         | 390          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0015722638 |
|    average_cost         | 0.04638672   |
|    clip_fraction        | 0.000156     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.65        |
|    cost_value_loss      | 0.83         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.499       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.46e+07     |
|    mean_cost_advantages | 0.025715953  |
|    mean_reward_advan... | 4564.3228    |
|    n_updates            | 330          |
|    nu                   | 2.74         |
|    nu_loss              | -0.125       |
|    policy_gradient_loss | -0.000805    |
|    reward_explained_... | -9.9e+14     |
|    reward_value_loss    | 2.61e+07     |
|    total_cost           | 475.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0443       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 400          |
|    ep_len_mean          | 23.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 894          |
|    iterations           | 35           |
|    time_elapsed         | 400          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.0012167952 |
|    average_cost         | 0.037402343  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.52        |
|    cost_value_loss      | 0.439        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.461       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.31e+07     |
|    mean_cost_advantages | -0.04634758  |
|    mean_reward_advan... | 4056.6343    |
|    n_updates            | 340          |
|    nu                   | 2.78         |
|    nu_loss              | -0.102       |
|    policy_gradient_loss | -0.000512    |
|    reward_explained_... | -3.9e+15     |
|    reward_value_loss    | 2.3e+07      |
|    total_cost           | 383.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 24.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0361        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 381           |
|    ep_len_mean          | 26.6          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 896           |
|    iterations           | 36            |
|    time_elapsed         | 411           |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.00062697707 |
|    average_cost         | 0.04433594    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.17         |
|    cost_value_loss      | 0.886         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.479        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.08e+07      |
|    mean_cost_advantages | -0.0029269985 |
|    mean_reward_advan... | 4500.4136     |
|    n_updates            | 350           |
|    nu                   | 2.82          |
|    nu_loss              | -0.123        |
|    policy_gradient_loss | -0.000686     |
|    reward_explained_... | -1.66e+14     |
|    reward_value_loss    | 2.55e+07      |
|    total_cost           | 454.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0388       |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | 404          |
|    ep_len_mean          | 25           |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 896          |
|    iterations           | 37           |
|    time_elapsed         | 422          |
|    total_timesteps      | 378880       |
| train/                  |              |
|    approx_kl            | 0.0024706207 |
|    average_cost         | 0.036132812  |
|    clip_fraction        | 0.00345      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.12        |
|    cost_value_loss      | 0.748        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.456       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02e+07     |
|    mean_cost_advantages | -0.047097594 |
|    mean_reward_advan... | 4260.4087    |
|    n_updates            | 360          |
|    nu                   | 2.87         |
|    nu_loss              | -0.102       |
|    policy_gradient_loss | -0.000783    |
|    reward_explained_... | -4.21e+14    |
|    reward_value_loss    | 2.41e+07     |
|    total_cost           | 370.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 21            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0366        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 400           |
|    ep_len_mean          | 24.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 892           |
|    iterations           | 38            |
|    time_elapsed         | 436           |
|    total_timesteps      | 389120        |
| train/                  |               |
|    approx_kl            | 0.00026496546 |
|    average_cost         | 0.038769532   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.22         |
|    cost_value_loss      | 0.657         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.46         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.52e+07      |
|    mean_cost_advantages | -0.021414742  |
|    mean_reward_advan... | 4470.756      |
|    n_updates            | 370           |
|    nu                   | 2.91          |
|    nu_loss              | -0.111        |
|    policy_gradient_loss | -0.000544     |
|    reward_explained_... | -3.79e+14     |
|    reward_value_loss    | 2.53e+07      |
|    total_cost           | 397.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.033        |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 377          |
|    ep_len_mean          | 25.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 896          |
|    iterations           | 39           |
|    time_elapsed         | 445          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.0012488016 |
|    average_cost         | 0.036621094  |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.991       |
|    cost_value_loss      | 0.504        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.451       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.28e+07     |
|    mean_cost_advantages | -0.013669881 |
|    mean_reward_advan... | 4418.167     |
|    n_updates            | 380          |
|    nu                   | 2.95         |
|    nu_loss              | -0.107       |
|    policy_gradient_loss | -0.000549    |
|    reward_explained_... | -4.96e+15    |
|    reward_value_loss    | 2.49e+07     |
|    total_cost           | 375.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 24.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0368       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 409          |
|    ep_len_mean          | 23.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 898          |
|    iterations           | 40           |
|    time_elapsed         | 455          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0011377253 |
|    average_cost         | 0.03300781   |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.545       |
|    cost_value_loss      | 0.377        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.442       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.26e+07     |
|    mean_cost_advantages | -0.017784502 |
|    mean_reward_advan... | 4209.868     |
|    n_updates            | 390          |
|    nu                   | 2.99         |
|    nu_loss              | -0.0974      |
|    policy_gradient_loss | -0.000366    |
|    reward_explained_... | -3.21e+14    |
|    reward_value_loss    | 2.34e+07     |
|    total_cost           | 338.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0327        |
| infos/                  |               |
|    cost                 | 0.05          |
| rollout/                |               |
|    adjusted_reward      | 384           |
|    ep_len_mean          | 25.9          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 899           |
|    iterations           | 41            |
|    time_elapsed         | 467           |
|    total_timesteps      | 419840        |
| train/                  |               |
|    approx_kl            | 0.00037651305 |
|    average_cost         | 0.036816407   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.788        |
|    cost_value_loss      | 0.397         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.449        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.27e+07      |
|    mean_cost_advantages | -0.0017646775 |
|    mean_reward_advan... | 4462.327      |
|    n_updates            | 400           |
|    nu                   | 3.03          |
|    nu_loss              | -0.11         |
|    policy_gradient_loss | -0.000482     |
|    reward_explained_... | -9.02e+13     |
|    reward_value_loss    | 2.5e+07       |
|    total_cost           | 377.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 24.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0295        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 381           |
|    ep_len_mean          | 25.7          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 900           |
|    iterations           | 42            |
|    time_elapsed         | 477           |
|    total_timesteps      | 430080        |
| train/                  |               |
|    approx_kl            | 0.00034227865 |
|    average_cost         | 0.032714844   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.764        |
|    cost_value_loss      | 0.324         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.42         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.21e+07      |
|    mean_cost_advantages | -0.02154219   |
|    mean_reward_advan... | 4146.6523     |
|    n_updates            | 410           |
|    nu                   | 3.08          |
|    nu_loss              | -0.0993       |
|    policy_gradient_loss | -0.000328     |
|    reward_explained_... | -5.2e+15      |
|    reward_value_loss    | 2.33e+07      |
|    total_cost           | 335.0         |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 20.2           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.0354         |
| infos/                  |                |
|    cost                 | 0.02           |
| rollout/                |                |
|    adjusted_reward      | 416            |
|    ep_len_mean          | 23.5           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 903            |
|    iterations           | 43             |
|    time_elapsed         | 487            |
|    total_timesteps      | 440320         |
| train/                  |                |
|    approx_kl            | 0.000121587866 |
|    average_cost         | 0.029492188    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.716         |
|    cost_value_loss      | 0.27           |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.416         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.16e+07       |
|    mean_cost_advantages | -0.032640193   |
|    mean_reward_advan... | 4102.621       |
|    n_updates            | 420            |
|    nu                   | 3.12           |
|    nu_loss              | -0.0907        |
|    policy_gradient_loss | -0.00022       |
|    reward_explained_... | -1.1e+14       |
|    reward_value_loss    | 2.29e+07       |
|    total_cost           | 302.0          |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0354       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 424          |
|    ep_len_mean          | 24.7         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 905          |
|    iterations           | 44           |
|    time_elapsed         | 497          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.0018376752 |
|    average_cost         | 0.035449218  |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.07        |
|    cost_value_loss      | 0.41         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.435       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.19e+07     |
|    mean_cost_advantages | 0.016984925  |
|    mean_reward_advan... | 4490.599     |
|    n_updates            | 430          |
|    nu                   | 3.16         |
|    nu_loss              | -0.11        |
|    policy_gradient_loss | -0.000763    |
|    reward_explained_... | -2.75e+14    |
|    reward_value_loss    | 2.49e+07     |
|    total_cost           | 363.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 20.2        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0313      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | 426         |
|    ep_len_mean          | 23.6        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 907         |
|    iterations           | 45          |
|    time_elapsed         | 507         |
|    total_timesteps      | 460800      |
| train/                  |             |
|    approx_kl            | 0.004050737 |
|    average_cost         | 0.035449218 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.53       |
|    cost_value_loss      | 0.407       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.422      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07e+07    |
|    mean_cost_advantages | 0.006238119 |
|    mean_reward_advan... | 4445.345    |
|    n_updates            | 440         |
|    nu                   | 3.2         |
|    nu_loss              | -0.112      |
|    policy_gradient_loss | -0.000969   |
|    reward_explained_... | -6.06e+15   |
|    reward_value_loss    | 2.49e+07    |
|    total_cost           | 363.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0303       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 404          |
|    ep_len_mean          | 23.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 909          |
|    iterations           | 46           |
|    time_elapsed         | 518          |
|    total_timesteps      | 471040       |
| train/                  |              |
|    approx_kl            | 0.0023254151 |
|    average_cost         | 0.031347655  |
|    clip_fraction        | 0.00102      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.598       |
|    cost_value_loss      | 0.242        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.438       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+07     |
|    mean_cost_advantages | -0.034311473 |
|    mean_reward_advan... | 4542.121     |
|    n_updates            | 450          |
|    nu                   | 3.24         |
|    nu_loss              | -0.1         |
|    policy_gradient_loss | -0.00049     |
|    reward_explained_... | -3.11e+14    |
|    reward_value_loss    | 2.5e+07      |
|    total_cost           | 321.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.037        |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 440          |
|    ep_len_mean          | 22.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 911          |
|    iterations           | 47           |
|    time_elapsed         | 527          |
|    total_timesteps      | 481280       |
| train/                  |              |
|    approx_kl            | 0.0015800191 |
|    average_cost         | 0.030273438  |
|    clip_fraction        | 0.00284      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.578       |
|    cost_value_loss      | 0.258        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.419       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.01e+07     |
|    mean_cost_advantages | -0.010242773 |
|    mean_reward_advan... | 4279.5386    |
|    n_updates            | 460          |
|    nu                   | 3.28         |
|    nu_loss              | -0.098       |
|    policy_gradient_loss | -0.000506    |
|    reward_explained_... | -9.41e+13    |
|    reward_value_loss    | 2.36e+07     |
|    total_cost           | 310.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 21.6        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.033       |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | 425         |
|    ep_len_mean          | 22.7        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 48          |
|    time_elapsed         | 537         |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.001510398 |
|    average_cost         | 0.03701172  |
|    clip_fraction        | 8.79e-05    |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.13       |
|    cost_value_loss      | 0.35        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.428      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.29e+07    |
|    mean_cost_advantages | 0.03669602  |
|    mean_reward_advan... | 4601.0444   |
|    n_updates            | 470         |
|    nu                   | 3.32        |
|    nu_loss              | -0.121      |
|    policy_gradient_loss | -0.000334   |
|    reward_explained_... | -2.51e+14   |
|    reward_value_loss    | 2.53e+07    |
|    total_cost           | 379.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0323       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 427          |
|    ep_len_mean          | 22.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 915          |
|    iterations           | 49           |
|    time_elapsed         | 548          |
|    total_timesteps      | 501760       |
| train/                  |              |
|    approx_kl            | 0.0028385993 |
|    average_cost         | 0.03300781   |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.5         |
|    cost_value_loss      | 0.368        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.402       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.27e+07     |
|    mean_cost_advantages | -0.006667529 |
|    mean_reward_advan... | 4361.746     |
|    n_updates            | 480          |
|    nu                   | 3.36         |
|    nu_loss              | -0.11        |
|    policy_gradient_loss | -0.000732    |
|    reward_explained_... | -9.22e+13    |
|    reward_value_loss    | 2.42e+07     |
|    total_cost           | 338.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0412       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | 459          |
|    ep_len_mean          | 21.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 50           |
|    time_elapsed         | 558          |
|    total_timesteps      | 512000       |
| train/                  |              |
|    approx_kl            | 0.002672595  |
|    average_cost         | 0.032324217  |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.877       |
|    cost_value_loss      | 0.286        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.407       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.75e+06     |
|    mean_cost_advantages | -0.016774869 |
|    mean_reward_advan... | 4414.0986    |
|    n_updates            | 490          |
|    nu                   | 3.4          |
|    nu_loss              | -0.109       |
|    policy_gradient_loss | -0.000848    |
|    reward_explained_... | -8.39e+13    |
|    reward_value_loss    | 2.42e+07     |
|    total_cost           | 331.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0316       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 407          |
|    ep_len_mean          | 22.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 51           |
|    time_elapsed         | 569          |
|    total_timesteps      | 522240       |
| train/                  |              |
|    approx_kl            | 0.0061661303 |
|    average_cost         | 0.04121094   |
|    clip_fraction        | 0.0633       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.84        |
|    cost_value_loss      | 0.442        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.407       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.19e+07     |
|    mean_cost_advantages | 0.046678305  |
|    mean_reward_advan... | 4631.9253    |
|    n_updates            | 500          |
|    nu                   | 3.44         |
|    nu_loss              | -0.14        |
|    policy_gradient_loss | -0.00222     |
|    reward_explained_... | -6.81e+13    |
|    reward_value_loss    | 2.56e+07     |
|    total_cost           | 422.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 23.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0401        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 397           |
|    ep_len_mean          | 26            |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 52            |
|    time_elapsed         | 578           |
|    total_timesteps      | 532480        |
| train/                  |               |
|    approx_kl            | 0.00084153016 |
|    average_cost         | 0.031640626   |
|    clip_fraction        | 0.000273      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.17         |
|    cost_value_loss      | 0.366         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.392        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.9e+06       |
|    mean_cost_advantages | -0.023465287  |
|    mean_reward_advan... | 4183.7437     |
|    n_updates            | 510           |
|    nu                   | 3.49          |
|    nu_loss              | -0.109        |
|    policy_gradient_loss | -0.000499     |
|    reward_explained_... | -3.75e+14     |
|    reward_value_loss    | 2.27e+07      |
|    total_cost           | 324.0         |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 19.2        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0355      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | 447         |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 922         |
|    iterations           | 53          |
|    time_elapsed         | 588         |
|    total_timesteps      | 542720      |
| train/                  |             |
|    approx_kl            | 0.006190347 |
|    average_cost         | 0.040136717 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.27       |
|    cost_value_loss      | 0.774       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.383      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22e+07    |
|    mean_cost_advantages | 0.085466295 |
|    mean_reward_advan... | 4098.1167   |
|    n_updates            | 520         |
|    nu                   | 3.53        |
|    nu_loss              | -0.14       |
|    policy_gradient_loss | -0.00255    |
|    reward_explained_... | -9.45e+13   |
|    reward_value_loss    | 2.21e+07    |
|    total_cost           | 411.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0364        |
| infos/                  |               |
|    cost                 | 0.07          |
| rollout/                |               |
|    adjusted_reward      | 456           |
|    ep_len_mean          | 21.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 54            |
|    time_elapsed         | 599           |
|    total_timesteps      | 552960        |
| train/                  |               |
|    approx_kl            | 0.00070775347 |
|    average_cost         | 0.035546876   |
|    clip_fraction        | 1.95e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.622        |
|    cost_value_loss      | 0.32          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.39         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.04e+07      |
|    mean_cost_advantages | -0.056607515  |
|    mean_reward_advan... | 4489.4893     |
|    n_updates            | 530           |
|    nu                   | 3.57          |
|    nu_loss              | -0.125        |
|    policy_gradient_loss | -0.000312     |
|    reward_explained_... | -2.54e+14     |
|    reward_value_loss    | 2.44e+07      |
|    total_cost           | 364.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 23.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0297        |
| infos/                  |               |
|    cost                 | 0.06          |
| rollout/                |               |
|    adjusted_reward      | 461           |
|    ep_len_mean          | 22.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 924           |
|    iterations           | 55            |
|    time_elapsed         | 608           |
|    total_timesteps      | 563200        |
| train/                  |               |
|    approx_kl            | 0.0009354472  |
|    average_cost         | 0.03642578    |
|    clip_fraction        | 0.00174       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.72         |
|    cost_value_loss      | 0.693         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.381        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.2e+07       |
|    mean_cost_advantages | -0.0092588235 |
|    mean_reward_advan... | 4521.568      |
|    n_updates            | 540           |
|    nu                   | 3.61          |
|    nu_loss              | -0.13         |
|    policy_gradient_loss | -0.000631     |
|    reward_explained_... | -5.51e+15     |
|    reward_value_loss    | 2.46e+07      |
|    total_cost           | 373.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 25            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0327        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 457           |
|    ep_len_mean          | 21.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 56            |
|    time_elapsed         | 619           |
|    total_timesteps      | 573440        |
| train/                  |               |
|    approx_kl            | 4.6007394e-06 |
|    average_cost         | 0.0296875     |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.275        |
|    cost_value_loss      | 0.197         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.387        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.34e+07      |
|    mean_cost_advantages | -0.07572396   |
|    mean_reward_advan... | 4581.4253     |
|    n_updates            | 550           |
|    nu                   | 3.66          |
|    nu_loss              | -0.107        |
|    policy_gradient_loss | -0.000136     |
|    reward_explained_... | -2.64e+14     |
|    reward_value_loss    | 2.47e+07      |
|    total_cost           | 304.0         |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 18.6           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.031          |
| infos/                  |                |
|    cost                 | 0.06           |
| rollout/                |                |
|    adjusted_reward      | 464            |
|    ep_len_mean          | 21.8           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 926            |
|    iterations           | 57             |
|    time_elapsed         | 630            |
|    total_timesteps      | 583680         |
| train/                  |                |
|    approx_kl            | -1.3524317e-05 |
|    average_cost         | 0.032714844    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.585         |
|    cost_value_loss      | 0.25           |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.379         |
|    learning_rate        | 0.0003         |
|    loss                 | 9.93e+06       |
|    mean_cost_advantages | -0.013633658   |
|    mean_reward_advan... | 4504.532       |
|    n_updates            | 560            |
|    nu                   | 3.7            |
|    nu_loss              | -0.12          |
|    policy_gradient_loss | -0.00022       |
|    reward_explained_... | -3.03e+13      |
|    reward_value_loss    | 2.43e+07       |
|    total_cost           | 335.0          |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0363       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 486          |
|    ep_len_mean          | 20.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 58           |
|    time_elapsed         | 642          |
|    total_timesteps      | 593920       |
| train/                  |              |
|    approx_kl            | 0.0042374525 |
|    average_cost         | 0.030957032  |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.714       |
|    cost_value_loss      | 0.262        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.38        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.27e+07     |
|    mean_cost_advantages | -0.012652169 |
|    mean_reward_advan... | 4559.125     |
|    n_updates            | 570          |
|    nu                   | 3.74         |
|    nu_loss              | -0.115       |
|    policy_gradient_loss | -0.000686    |
|    reward_explained_... | -2.6e+14     |
|    reward_value_loss    | 2.45e+07     |
|    total_cost           | 317.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0331       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 476          |
|    ep_len_mean          | 21.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 59           |
|    time_elapsed         | 653          |
|    total_timesteps      | 604160       |
| train/                  |              |
|    approx_kl            | 0.0023156912 |
|    average_cost         | 0.036328126  |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.21        |
|    cost_value_loss      | 0.292        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.372       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.49e+07     |
|    mean_cost_advantages | 0.02366682   |
|    mean_reward_advan... | 4643.2607    |
|    n_updates            | 580          |
|    nu                   | 3.79         |
|    nu_loss              | -0.136       |
|    policy_gradient_loss | -0.000707    |
|    reward_explained_... | -6.28e+13    |
|    reward_value_loss    | 2.51e+07     |
|    total_cost           | 372.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0311       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 464          |
|    ep_len_mean          | 22           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 60           |
|    time_elapsed         | 663          |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0031169257 |
|    average_cost         | 0.03310547   |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.836       |
|    cost_value_loss      | 0.236        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.374       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.42e+07     |
|    mean_cost_advantages | -0.018099021 |
|    mean_reward_advan... | 4580.952     |
|    n_updates            | 590          |
|    nu                   | 3.83         |
|    nu_loss              | -0.125       |
|    policy_gradient_loss | -0.00063     |
|    reward_explained_... | -1.53e+13    |
|    reward_value_loss    | 2.46e+07     |
|    total_cost           | 339.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0342       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 471          |
|    ep_len_mean          | 20.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 61           |
|    time_elapsed         | 673          |
|    total_timesteps      | 624640       |
| train/                  |              |
|    approx_kl            | 0.0013997153 |
|    average_cost         | 0.031054687  |
|    clip_fraction        | 0.00456      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.814       |
|    cost_value_loss      | 0.248        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.375       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.09e+07     |
|    mean_cost_advantages | -0.004650368 |
|    mean_reward_advan... | 4490.4766    |
|    n_updates            | 600          |
|    nu                   | 3.87         |
|    nu_loss              | -0.119       |
|    policy_gradient_loss | -0.000385    |
|    reward_explained_... | -9.79e+15    |
|    reward_value_loss    | 2.38e+07     |
|    total_cost           | 318.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0315       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 441          |
|    ep_len_mean          | 21.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 929          |
|    iterations           | 62           |
|    time_elapsed         | 683          |
|    total_timesteps      | 634880       |
| train/                  |              |
|    approx_kl            | 0.0026082534 |
|    average_cost         | 0.034179688  |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.37        |
|    cost_value_loss      | 0.308        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.369       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.05e+07     |
|    mean_cost_advantages | 0.027390227  |
|    mean_reward_advan... | 4524.816     |
|    n_updates            | 610          |
|    nu                   | 3.92         |
|    nu_loss              | -0.132       |
|    policy_gradient_loss | -0.000656    |
|    reward_explained_... | -2.47e+14    |
|    reward_value_loss    | 2.4e+07      |
|    total_cost           | 350.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 26.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0315       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 461          |
|    ep_len_mean          | 21.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 930          |
|    iterations           | 63           |
|    time_elapsed         | 693          |
|    total_timesteps      | 645120       |
| train/                  |              |
|    approx_kl            | 0.0023876778 |
|    average_cost         | 0.031542968  |
|    clip_fraction        | 0.000352     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.723       |
|    cost_value_loss      | 0.246        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.355       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.13e+07     |
|    mean_cost_advantages | 0.008248337  |
|    mean_reward_advan... | 4250.3237    |
|    n_updates            | 620          |
|    nu                   | 3.96         |
|    nu_loss              | -0.124       |
|    policy_gradient_loss | -0.000266    |
|    reward_explained_... | -1.17e+13    |
|    reward_value_loss    | 2.24e+07     |
|    total_cost           | 323.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 22.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0315        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 458           |
|    ep_len_mean          | 21.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 64            |
|    time_elapsed         | 707           |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.00045908708 |
|    average_cost         | 0.031542968   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.991        |
|    cost_value_loss      | 0.223         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.359        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.37e+07      |
|    mean_cost_advantages | -0.014266314  |
|    mean_reward_advan... | 4385.495      |
|    n_updates            | 630           |
|    nu                   | 4.01          |
|    nu_loss              | -0.125        |
|    policy_gradient_loss | -0.000218     |
|    reward_explained_... | -2.5e+14      |
|    reward_value_loss    | 2.31e+07      |
|    total_cost           | 323.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0292       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 441          |
|    ep_len_mean          | 22.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 65           |
|    time_elapsed         | 719          |
|    total_timesteps      | 665600       |
| train/                  |              |
|    approx_kl            | 0.0015959736 |
|    average_cost         | 0.031542968  |
|    clip_fraction        | 0.00041      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.91        |
|    cost_value_loss      | 0.235        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.359       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.21e+07     |
|    mean_cost_advantages | -0.005882988 |
|    mean_reward_advan... | 4350.9497    |
|    n_updates            | 640          |
|    nu                   | 4.05         |
|    nu_loss              | -0.126       |
|    policy_gradient_loss | -0.000326    |
|    reward_explained_... | -2.1e+16     |
|    reward_value_loss    | 2.28e+07     |
|    total_cost           | 323.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0354       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 467          |
|    ep_len_mean          | 22.8         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 66           |
|    time_elapsed         | 729          |
|    total_timesteps      | 675840       |
| train/                  |              |
|    approx_kl            | 0.0028071573 |
|    average_cost         | 0.029199218  |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.901       |
|    cost_value_loss      | 0.251        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.351       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.03e+07     |
|    mean_cost_advantages | -0.011335662 |
|    mean_reward_advan... | 4192.8467    |
|    n_updates            | 650          |
|    nu                   | 4.09         |
|    nu_loss              | -0.118       |
|    policy_gradient_loss | -0.000865    |
|    reward_explained_... | -1.84e+13    |
|    reward_value_loss    | 2.18e+07     |
|    total_cost           | 299.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0325        |
| infos/                  |               |
|    cost                 | 0.06          |
| rollout/                |               |
|    adjusted_reward      | 463           |
|    ep_len_mean          | 23.1          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 924           |
|    iterations           | 67            |
|    time_elapsed         | 742           |
|    total_timesteps      | 686080        |
| train/                  |               |
|    approx_kl            | 0.00030976394 |
|    average_cost         | 0.035351563   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.25         |
|    cost_value_loss      | 0.257         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.353        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.26e+07      |
|    mean_cost_advantages | 0.017326165   |
|    mean_reward_advan... | 4358.925      |
|    n_updates            | 660           |
|    nu                   | 4.14          |
|    nu_loss              | -0.145        |
|    policy_gradient_loss | -0.000159     |
|    reward_explained_... | -6.4e+13      |
|    reward_value_loss    | 2.27e+07      |
|    total_cost           | 362.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0296        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 474           |
|    ep_len_mean          | 20.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 923           |
|    iterations           | 68            |
|    time_elapsed         | 753           |
|    total_timesteps      | 696320        |
| train/                  |               |
|    approx_kl            | 0.00039256114 |
|    average_cost         | 0.03251953    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.15         |
|    cost_value_loss      | 0.3           |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.349        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.27e+07      |
|    mean_cost_advantages | 0.00019107162 |
|    mean_reward_advan... | 4310.3193     |
|    n_updates            | 670           |
|    nu                   | 4.18          |
|    nu_loss              | -0.135        |
|    policy_gradient_loss | -0.00033      |
|    reward_explained_... | -2.66e+14     |
|    reward_value_loss    | 2.24e+07      |
|    total_cost           | 333.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0288       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 477          |
|    ep_len_mean          | 20.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 69           |
|    time_elapsed         | 764          |
|    total_timesteps      | 706560       |
| train/                  |              |
|    approx_kl            | 4.561923e-05 |
|    average_cost         | 0.029589843  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.612       |
|    cost_value_loss      | 0.17         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.345       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.12e+07     |
|    mean_cost_advantages | -0.038192242 |
|    mean_reward_advan... | 4352.566     |
|    n_updates            | 680          |
|    nu                   | 4.23         |
|    nu_loss              | -0.124       |
|    policy_gradient_loss | -0.000113    |
|    reward_explained_... | -2.84e+13    |
|    reward_value_loss    | 2.26e+07     |
|    total_cost           | 303.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0322       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 481          |
|    ep_len_mean          | 21.9         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 70           |
|    time_elapsed         | 775          |
|    total_timesteps      | 716800       |
| train/                  |              |
|    approx_kl            | 0.0013024125 |
|    average_cost         | 0.028808594  |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.969       |
|    cost_value_loss      | 0.209        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.345       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.31e+07     |
|    mean_cost_advantages | -0.012353243 |
|    mean_reward_advan... | 4363.2017    |
|    n_updates            | 690          |
|    nu                   | 4.27         |
|    nu_loss              | -0.122       |
|    policy_gradient_loss | -0.000295    |
|    reward_explained_... | -5.47e+15    |
|    reward_value_loss    | 2.25e+07     |
|    total_cost           | 295.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0278       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 470          |
|    ep_len_mean          | 23.1         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 71           |
|    time_elapsed         | 786          |
|    total_timesteps      | 727040       |
| train/                  |              |
|    approx_kl            | 0.0021132238 |
|    average_cost         | 0.032226562  |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.21        |
|    cost_value_loss      | 0.198        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.34        |
|    learning_rate        | 0.0003       |
|    loss                 | 9.71e+06     |
|    mean_cost_advantages | 0.012919037  |
|    mean_reward_advan... | 4343.078     |
|    n_updates            | 700          |
|    nu                   | 4.32         |
|    nu_loss              | -0.138       |
|    policy_gradient_loss | -0.000524    |
|    reward_explained_... | -6.4e+13     |
|    reward_value_loss    | 2.25e+07     |
|    total_cost           | 330.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 21.6        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0305      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | 479         |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 923         |
|    iterations           | 72          |
|    time_elapsed         | 798         |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.000996717 |
|    average_cost         | 0.027832031 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.2        |
|    cost_value_loss      | 0.246       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.334      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.64e+06    |
|    mean_cost_advantages | -0.00948122 |
|    mean_reward_advan... | 4269.0664   |
|    n_updates            | 710         |
|    nu                   | 4.36        |
|    nu_loss              | -0.12       |
|    policy_gradient_loss | -0.000184   |
|    reward_explained_... | -7.12e+15   |
|    reward_value_loss    | 2.19e+07    |
|    total_cost           | 285.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0308       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 441          |
|    ep_len_mean          | 20.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 73           |
|    time_elapsed         | 810          |
|    total_timesteps      | 747520       |
| train/                  |              |
|    approx_kl            | 0.0020636185 |
|    average_cost         | 0.03046875   |
|    clip_fraction        | 0.00465      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.01        |
|    cost_value_loss      | 0.19         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.333       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.15e+07     |
|    mean_cost_advantages | 0.0058555305 |
|    mean_reward_advan... | 4303.019     |
|    n_updates            | 720          |
|    nu                   | 4.4          |
|    nu_loss              | -0.133       |
|    policy_gradient_loss | -0.000336    |
|    reward_explained_... | -6.96e+15    |
|    reward_value_loss    | 2.2e+07      |
|    total_cost           | 312.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0268        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 487           |
|    ep_len_mean          | 20.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 923           |
|    iterations           | 74            |
|    time_elapsed         | 820           |
|    total_timesteps      | 757760        |
| train/                  |               |
|    approx_kl            | 0.00093182235 |
|    average_cost         | 0.030761719   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.819        |
|    cost_value_loss      | 0.195         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.316        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.19e+07      |
|    mean_cost_advantages | 0.010993985   |
|    mean_reward_advan... | 3941.1204     |
|    n_updates            | 730           |
|    nu                   | 4.45          |
|    nu_loss              | -0.136        |
|    policy_gradient_loss | -0.000159     |
|    reward_explained_... | -3.48e+15     |
|    reward_value_loss    | 2.02e+07      |
|    total_cost           | 315.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0286        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 492           |
|    ep_len_mean          | 20.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 924           |
|    iterations           | 75            |
|    time_elapsed         | 830           |
|    total_timesteps      | 768000        |
| train/                  |               |
|    approx_kl            | 0.00012416574 |
|    average_cost         | 0.026757812   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.841        |
|    cost_value_loss      | 0.175         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.334        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.91e+06      |
|    mean_cost_advantages | -0.054454762  |
|    mean_reward_advan... | 4348.304      |
|    n_updates            | 740           |
|    nu                   | 4.49          |
|    nu_loss              | -0.119        |
|    policy_gradient_loss | -9.15e-05     |
|    reward_explained_... | -5.34e+13     |
|    reward_value_loss    | 2.2e+07       |
|    total_cost           | 274.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0291       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 483          |
|    ep_len_mean          | 20.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 76           |
|    time_elapsed         | 841          |
|    total_timesteps      | 778240       |
| train/                  |              |
|    approx_kl            | 1.839836e-05 |
|    average_cost         | 0.02861328   |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.87        |
|    cost_value_loss      | 0.167        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.329       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.4e+06      |
|    mean_cost_advantages | -0.003166994 |
|    mean_reward_advan... | 4354.638     |
|    n_updates            | 750          |
|    nu                   | 4.54         |
|    nu_loss              | -0.129       |
|    policy_gradient_loss | -6.68e-05    |
|    reward_explained_... | -1.33e+13    |
|    reward_value_loss    | 2.2e+07      |
|    total_cost           | 293.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0246       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 461          |
|    ep_len_mean          | 22.7         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 77           |
|    time_elapsed         | 850          |
|    total_timesteps      | 788480       |
| train/                  |              |
|    approx_kl            | 0.0029325604 |
|    average_cost         | 0.029101562  |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.09        |
|    cost_value_loss      | 0.184        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.324       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.42e+06     |
|    mean_cost_advantages | 0.0117317755 |
|    mean_reward_advan... | 4243.502     |
|    n_updates            | 760          |
|    nu                   | 4.58         |
|    nu_loss              | -0.132       |
|    policy_gradient_loss | -0.000723    |
|    reward_explained_... | -5.98e+13    |
|    reward_value_loss    | 2.14e+07     |
|    total_cost           | 298.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0278       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 495          |
|    ep_len_mean          | 20.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 78           |
|    time_elapsed         | 860          |
|    total_timesteps      | 798720       |
| train/                  |              |
|    approx_kl            | 0.0040349397 |
|    average_cost         | 0.024609376  |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.213       |
|    cost_value_loss      | 0.0956       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.318       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.12e+07     |
|    mean_cost_advantages | -0.024843302 |
|    mean_reward_advan... | 4043.7246    |
|    n_updates            | 770          |
|    nu                   | 4.63         |
|    nu_loss              | -0.113       |
|    policy_gradient_loss | -0.000618    |
|    reward_explained_... | -6.97e+13    |
|    reward_value_loss    | 2.03e+07     |
|    total_cost           | 252.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0291       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 488          |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 928          |
|    iterations           | 79           |
|    time_elapsed         | 871          |
|    total_timesteps      | 808960       |
| train/                  |              |
|    approx_kl            | 0.001725855  |
|    average_cost         | 0.027832031  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.677       |
|    cost_value_loss      | 0.117        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.319       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.11e+07     |
|    mean_cost_advantages | 0.0010865771 |
|    mean_reward_advan... | 4280.299     |
|    n_updates            | 780          |
|    nu                   | 4.67         |
|    nu_loss              | -0.129       |
|    policy_gradient_loss | -0.000148    |
|    reward_explained_... | -5.42e+13    |
|    reward_value_loss    | 2.15e+07     |
|    total_cost           | 285.0        |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 19.4           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.0312         |
| infos/                  |                |
|    cost                 | 0.02           |
| rollout/                |                |
|    adjusted_reward      | 482            |
|    ep_len_mean          | 22.1           |
|    ep_rew_mean          | 9.89e+03       |
| time/                   |                |
|    fps                  | 928            |
|    iterations           | 80             |
|    time_elapsed         | 882            |
|    total_timesteps      | 819200         |
| train/                  |                |
|    approx_kl            | -0.00011313309 |
|    average_cost         | 0.029101562    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.644         |
|    cost_value_loss      | 0.136          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.327         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.18e+07       |
|    mean_cost_advantages | 0.009832499    |
|    mean_reward_advan... | 4251.8755      |
|    n_updates            | 790            |
|    nu                   | 4.71           |
|    nu_loss              | -0.136         |
|    policy_gradient_loss | -3.19e-05      |
|    reward_explained_... | -1.27e+16      |
|    reward_value_loss    | 2.11e+07       |
|    total_cost           | 298.0          |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0319       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 511          |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 929          |
|    iterations           | 81           |
|    time_elapsed         | 892          |
|    total_timesteps      | 829440       |
| train/                  |              |
|    approx_kl            | 0.0038192314 |
|    average_cost         | 0.031152343  |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.58        |
|    cost_value_loss      | 0.248        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.318       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.32e+06     |
|    mean_cost_advantages | 0.024952007  |
|    mean_reward_advan... | 4155.317     |
|    n_updates            | 800          |
|    nu                   | 4.76         |
|    nu_loss              | -0.147       |
|    policy_gradient_loss | -0.000802    |
|    reward_explained_... | -5.83e+13    |
|    reward_value_loss    | 2.06e+07     |
|    total_cost           | 319.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0328        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 503           |
|    ep_len_mean          | 20.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 930           |
|    iterations           | 82            |
|    time_elapsed         | 902           |
|    total_timesteps      | 839680        |
| train/                  |               |
|    approx_kl            | 7.7849256e-07 |
|    average_cost         | 0.031933594   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.814        |
|    cost_value_loss      | 0.164         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.321        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.07e+07      |
|    mean_cost_advantages | -0.006830401  |
|    mean_reward_advan... | 4334.256      |
|    n_updates            | 810           |
|    nu                   | 4.8           |
|    nu_loss              | -0.152        |
|    policy_gradient_loss | -4.86e-05     |
|    reward_explained_... | -4.77e+13     |
|    reward_value_loss    | 2.15e+07      |
|    total_cost           | 327.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0278       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 482          |
|    ep_len_mean          | 22.8         |
|    ep_rew_mean          | 9.89e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 83           |
|    time_elapsed         | 912          |
|    total_timesteps      | 849920       |
| train/                  |              |
|    approx_kl            | 0.0029587583 |
|    average_cost         | 0.0328125    |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.997       |
|    cost_value_loss      | 0.198        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.318       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.27e+06     |
|    mean_cost_advantages | 0.005447628  |
|    mean_reward_advan... | 4271.5864    |
|    n_updates            | 820          |
|    nu                   | 4.85         |
|    nu_loss              | -0.158       |
|    policy_gradient_loss | -0.000632    |
|    reward_explained_... | -1.29e+16    |
|    reward_value_loss    | 2.1e+07      |
|    total_cost           | 336.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.026        |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 487          |
|    ep_len_mean          | 20.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 932          |
|    iterations           | 84           |
|    time_elapsed         | 922          |
|    total_timesteps      | 860160       |
| train/                  |              |
|    approx_kl            | 1.089304e-05 |
|    average_cost         | 0.027832031  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.645       |
|    cost_value_loss      | 0.163        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.31        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.76e+06     |
|    mean_cost_advantages | -0.02885512  |
|    mean_reward_advan... | 4096.133     |
|    n_updates            | 830          |
|    nu                   | 4.89         |
|    nu_loss              | -0.135       |
|    policy_gradient_loss | -0.000114    |
|    reward_explained_... | -1.41e+13    |
|    reward_value_loss    | 2e+07        |
|    total_cost           | 285.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0276        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 477           |
|    ep_len_mean          | 20.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 933           |
|    iterations           | 85            |
|    time_elapsed         | 932           |
|    total_timesteps      | 870400        |
| train/                  |               |
|    approx_kl            | 3.2480537e-05 |
|    average_cost         | 0.025976563   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.471        |
|    cost_value_loss      | 0.116         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.316        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.09e+07      |
|    mean_cost_advantages | -0.030456182  |
|    mean_reward_advan... | 4147.3623     |
|    n_updates            | 840           |
|    nu                   | 4.94          |
|    nu_loss              | -0.127        |
|    policy_gradient_loss | -6.05e-05     |
|    reward_explained_... | -5.09e+13     |
|    reward_value_loss    | 2.01e+07      |
|    total_cost           | 266.0         |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 19.4        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0334      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | 497         |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 9.89e+03    |
| time/                   |             |
|    fps                  | 934         |
|    iterations           | 86          |
|    time_elapsed         | 942         |
|    total_timesteps      | 880640      |
| train/                  |             |
|    approx_kl            | 0.004174087 |
|    average_cost         | 0.027636718 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.29       |
|    cost_value_loss      | 0.208       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.307      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.09e+06    |
|    mean_cost_advantages | 0.018693045 |
|    mean_reward_advan... | 4013.7942   |
|    n_updates            | 850         |
|    nu                   | 4.98        |
|    nu_loss              | -0.136      |
|    policy_gradient_loss | -0.000905   |
|    reward_explained_... | -6.2e+15    |
|    reward_value_loss    | 1.94e+07    |
|    total_cost           | 283.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0285       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 500          |
|    ep_len_mean          | 19.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 87           |
|    time_elapsed         | 952          |
|    total_timesteps      | 890880       |
| train/                  |              |
|    approx_kl            | 0.0027223225 |
|    average_cost         | 0.03339844   |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.93        |
|    cost_value_loss      | 0.236        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.306       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.55e+06     |
|    mean_cost_advantages | 0.03352905   |
|    mean_reward_advan... | 4106.0786    |
|    n_updates            | 860          |
|    nu                   | 5.03         |
|    nu_loss              | -0.166       |
|    policy_gradient_loss | -0.000566    |
|    reward_explained_... | -5.49e+13    |
|    reward_value_loss    | 2e+07        |
|    total_cost           | 342.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 21.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0286        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 491           |
|    ep_len_mean          | 22.1          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 936           |
|    iterations           | 88            |
|    time_elapsed         | 962           |
|    total_timesteps      | 901120        |
| train/                  |               |
|    approx_kl            | 0.00013755911 |
|    average_cost         | 0.028515626   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.631        |
|    cost_value_loss      | 0.147         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.312        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.31e+06      |
|    mean_cost_advantages | -0.039095767  |
|    mean_reward_advan... | 4148.92       |
|    n_updates            | 870           |
|    nu                   | 5.07          |
|    nu_loss              | -0.143        |
|    policy_gradient_loss | -6.18e-05     |
|    reward_explained_... | -1.21e+13     |
|    reward_value_loss    | 2e+07         |
|    total_cost           | 292.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0267       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 472          |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 937          |
|    iterations           | 89           |
|    time_elapsed         | 972          |
|    total_timesteps      | 911360       |
| train/                  |              |
|    approx_kl            | 0.0013122401 |
|    average_cost         | 0.02861328   |
|    clip_fraction        | 0.000186     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.508       |
|    cost_value_loss      | 0.126        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.306       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02e+07     |
|    mean_cost_advantages | -0.011760798 |
|    mean_reward_advan... | 4036.8704    |
|    n_updates            | 880          |
|    nu                   | 5.12         |
|    nu_loss              | -0.145       |
|    policy_gradient_loss | -0.000147    |
|    reward_explained_... | -1.43e+16    |
|    reward_value_loss    | 1.94e+07     |
|    total_cost           | 293.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0264       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 506          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 90           |
|    time_elapsed         | 982          |
|    total_timesteps      | 921600       |
| train/                  |              |
|    approx_kl            | 0.000941783  |
|    average_cost         | 0.026660156  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.811       |
|    cost_value_loss      | 0.152        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.3         |
|    learning_rate        | 0.0003       |
|    loss                 | 8.73e+06     |
|    mean_cost_advantages | -0.006561504 |
|    mean_reward_advan... | 3878.3933    |
|    n_updates            | 890          |
|    nu                   | 5.16         |
|    nu_loss              | -0.136       |
|    policy_gradient_loss | -9.21e-05    |
|    reward_explained_... | -7.65e+15    |
|    reward_value_loss    | 1.85e+07     |
|    total_cost           | 273.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0289        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 490           |
|    ep_len_mean          | 19.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 939           |
|    iterations           | 91            |
|    time_elapsed         | 992           |
|    total_timesteps      | 931840        |
| train/                  |               |
|    approx_kl            | 0.00016774845 |
|    average_cost         | 0.026367188   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.472        |
|    cost_value_loss      | 0.105         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.308        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.02e+06      |
|    mean_cost_advantages | -0.026134187  |
|    mean_reward_advan... | 4120.722      |
|    n_updates            | 900           |
|    nu                   | 5.21          |
|    nu_loss              | -0.136        |
|    policy_gradient_loss | -2.89e-05     |
|    reward_explained_... | -4.58e+13     |
|    reward_value_loss    | 1.96e+07      |
|    total_cost           | 270.0         |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 19.6        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0245      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | 455         |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 9.89e+03    |
| time/                   |             |
|    fps                  | 939         |
|    iterations           | 92          |
|    time_elapsed         | 1002        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.001500434 |
|    average_cost         | 0.02890625  |
|    clip_fraction        | 9.77e-06    |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.822      |
|    cost_value_loss      | 0.138       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.306      |
|    learning_rate        | 0.0003      |
|    loss                 | 8.73e+06    |
|    mean_cost_advantages | 0.018204298 |
|    mean_reward_advan... | 3971.052    |
|    n_updates            | 910         |
|    nu                   | 5.25        |
|    nu_loss              | -0.151      |
|    policy_gradient_loss | -0.000118   |
|    reward_explained_... | -1.34e+13   |
|    reward_value_loss    | 1.88e+07    |
|    total_cost           | 296.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0299       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 496          |
|    ep_len_mean          | 20.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 93           |
|    time_elapsed         | 1013         |
|    total_timesteps      | 952320       |
| train/                  |              |
|    approx_kl            | 0.002541155  |
|    average_cost         | 0.02451172   |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.474       |
|    cost_value_loss      | 0.122        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.291       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.11e+07     |
|    mean_cost_advantages | -0.009318819 |
|    mean_reward_advan... | 3694.892     |
|    n_updates            | 920          |
|    nu                   | 5.3          |
|    nu_loss              | -0.129       |
|    policy_gradient_loss | -0.000383    |
|    reward_explained_... | -5.18e+15    |
|    reward_value_loss    | 1.75e+07     |
|    total_cost           | 251.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0287       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 484          |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 94           |
|    time_elapsed         | 1024         |
|    total_timesteps      | 962560       |
| train/                  |              |
|    approx_kl            | 0.0022342694 |
|    average_cost         | 0.029882813  |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.73        |
|    cost_value_loss      | 0.206        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.298       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.55e+06     |
|    mean_cost_advantages | 0.027332747  |
|    mean_reward_advan... | 3948.4016    |
|    n_updates            | 930          |
|    nu                   | 5.34         |
|    nu_loss              | -0.158       |
|    policy_gradient_loss | -0.000654    |
|    reward_explained_... | -5.27e+13    |
|    reward_value_loss    | 1.86e+07     |
|    total_cost           | 306.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0299       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 507          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 95           |
|    time_elapsed         | 1035         |
|    total_timesteps      | 972800       |
| train/                  |              |
|    approx_kl            | 0.0009534663 |
|    average_cost         | 0.028710937  |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.707       |
|    cost_value_loss      | 0.153        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.302       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.12e+07     |
|    mean_cost_advantages | 0.0034077652 |
|    mean_reward_advan... | 3887.7625    |
|    n_updates            | 940          |
|    nu                   | 5.39         |
|    nu_loss              | -0.153       |
|    policy_gradient_loss | -0.000181    |
|    reward_explained_... | -5.11e+13    |
|    reward_value_loss    | 1.81e+07     |
|    total_cost           | 294.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0302        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | 512           |
|    ep_len_mean          | 19.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 938           |
|    iterations           | 96            |
|    time_elapsed         | 1047          |
|    total_timesteps      | 983040        |
| train/                  |               |
|    approx_kl            | 0.0020396928  |
|    average_cost         | 0.029882813   |
|    clip_fraction        | 0.00806       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.751        |
|    cost_value_loss      | 0.151         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.304        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.02e+06      |
|    mean_cost_advantages | -0.0015503988 |
|    mean_reward_advan... | 4021.8723     |
|    n_updates            | 950           |
|    nu                   | 5.44          |
|    nu_loss              | -0.161        |
|    policy_gradient_loss | -0.000253     |
|    reward_explained_... | -1.39e+16     |
|    reward_value_loss    | 1.87e+07      |
|    total_cost           | 306.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0264       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 507          |
|    ep_len_mean          | 19.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 97           |
|    time_elapsed         | 1058         |
|    total_timesteps      | 993280       |
| train/                  |              |
|    approx_kl            | 0.0019296646 |
|    average_cost         | 0.03017578   |
|    clip_fraction        | 0.00653      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.19        |
|    cost_value_loss      | 0.185        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.301       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.3e+06      |
|    mean_cost_advantages | 0.0069667324 |
|    mean_reward_advan... | 4015.461     |
|    n_updates            | 960          |
|    nu                   | 5.48         |
|    nu_loss              | -0.164       |
|    policy_gradient_loss | -0.000316    |
|    reward_explained_... | -1.11e+13    |
|    reward_value_loss    | 1.86e+07     |
|    total_cost           | 309.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0288       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | 492          |
|    ep_len_mean          | 20           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 98           |
|    time_elapsed         | 1069         |
|    total_timesteps      | 1003520      |
| train/                  |              |
|    approx_kl            | 0.0010589268 |
|    average_cost         | 0.026367188  |
|    clip_fraction        | 9.77e-06     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.461       |
|    cost_value_loss      | 0.115        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.301       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.99e+06     |
|    mean_cost_advantages | -0.028132271 |
|    mean_reward_advan... | 3980.6167    |
|    n_updates            | 970          |
|    nu                   | 5.53         |
|    nu_loss              | -0.145       |
|    policy_gradient_loss | -0.000125    |
|    reward_explained_... | -4.32e+13    |
|    reward_value_loss    | 1.83e+07     |
|    total_cost           | 270.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0285       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 503          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 99           |
|    time_elapsed         | 1082         |
|    total_timesteps      | 1013760      |
| train/                  |              |
|    approx_kl            | 0.0024353932 |
|    average_cost         | 0.028808594  |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.772       |
|    cost_value_loss      | 0.139        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.297       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.55e+06     |
|    mean_cost_advantages | 0.014365504  |
|    mean_reward_advan... | 3864.776     |
|    n_updates            | 980          |
|    nu                   | 5.57         |
|    nu_loss              | -0.159       |
|    policy_gradient_loss | -0.000346    |
|    reward_explained_... | -4.79e+13    |
|    reward_value_loss    | 1.77e+07     |
|    total_cost           | 295.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0308       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 515          |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 100          |
|    time_elapsed         | 1093         |
|    total_timesteps      | 1024000      |
| train/                  |              |
|    approx_kl            | 0.0006640926 |
|    average_cost         | 0.028515626  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.648       |
|    cost_value_loss      | 0.125        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.295       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.78e+06     |
|    mean_cost_advantages | -0.002998216 |
|    mean_reward_advan... | 3898.0151    |
|    n_updates            | 990          |
|    nu                   | 5.62         |
|    nu_loss              | -0.159       |
|    policy_gradient_loss | -7.14e-05    |
|    reward_explained_... | -1.15e+13    |
|    reward_value_loss    | 1.78e+07     |
|    total_cost           | 292.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 19.6        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0264      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | 503         |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 101         |
|    time_elapsed         | 1104        |
|    total_timesteps      | 1034240     |
| train/                  |             |
|    approx_kl            | 6.33982e-05 |
|    average_cost         | 0.030761719 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.665      |
|    cost_value_loss      | 0.141       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.297      |
|    learning_rate        | 0.0003      |
|    loss                 | 1e+07       |
|    mean_cost_advantages | 0.013582962 |
|    mean_reward_advan... | 3947.688    |
|    n_updates            | 1000        |
|    nu                   | 5.67        |
|    nu_loss              | -0.173      |
|    policy_gradient_loss | -8.37e-05   |
|    reward_explained_... | -1.07e+13   |
|    reward_value_loss    | 1.8e+07     |
|    total_cost           | 315.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0265       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 496          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 102          |
|    time_elapsed         | 1117         |
|    total_timesteps      | 1044480      |
| train/                  |              |
|    approx_kl            | 0.002139309  |
|    average_cost         | 0.026367188  |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.479       |
|    cost_value_loss      | 0.106        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.291       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.04e+06     |
|    mean_cost_advantages | -0.021134116 |
|    mean_reward_advan... | 3825.6946    |
|    n_updates            | 1010         |
|    nu                   | 5.72         |
|    nu_loss              | -0.149       |
|    policy_gradient_loss | -0.00051     |
|    reward_explained_... | -4.86e+13    |
|    reward_value_loss    | 1.74e+07     |
|    total_cost           | 270.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 22            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0282        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 492           |
|    ep_len_mean          | 19.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 103           |
|    time_elapsed         | 1128          |
|    total_timesteps      | 1054720       |
| train/                  |               |
|    approx_kl            | 6.1033446e-05 |
|    average_cost         | 0.026464844   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.67         |
|    cost_value_loss      | 0.124         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.296        |
|    learning_rate        | 0.0003        |
|    loss                 | 7.79e+06      |
|    mean_cost_advantages | 0.002089071   |
|    mean_reward_advan... | 3785.9714     |
|    n_updates            | 1020          |
|    nu                   | 5.76          |
|    nu_loss              | -0.151        |
|    policy_gradient_loss | -2.25e-05     |
|    reward_explained_... | -4.83e+13     |
|    reward_value_loss    | 1.71e+07      |
|    total_cost           | 271.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.028        |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 483          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 104          |
|    time_elapsed         | 1139         |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0010164741 |
|    average_cost         | 0.028222656  |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.981       |
|    cost_value_loss      | 0.146        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.295       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.73e+06     |
|    mean_cost_advantages | 0.010497324  |
|    mean_reward_advan... | 3727.819     |
|    n_updates            | 1030         |
|    nu                   | 5.81         |
|    nu_loss              | -0.163       |
|    policy_gradient_loss | -0.000154    |
|    reward_explained_... | -1.24e+13    |
|    reward_value_loss    | 1.67e+07     |
|    total_cost           | 289.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 18.8        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0282      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | 510         |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 935         |
|    iterations           | 105         |
|    time_elapsed         | 1149        |
|    total_timesteps      | 1075200     |
| train/                  |             |
|    approx_kl            | 0.001609131 |
|    average_cost         | 0.028027344 |
|    clip_fraction        | 0.0522      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.773      |
|    cost_value_loss      | 0.152       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.29       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.67e+06    |
|    mean_cost_advantages | 0.002951    |
|    mean_reward_advan... | 3650.1719   |
|    n_updates            | 1040        |
|    nu                   | 5.86        |
|    nu_loss              | -0.163      |
|    policy_gradient_loss | -0.000775   |
|    reward_explained_... | -7.21e+15   |
|    reward_value_loss    | 1.64e+07    |
|    total_cost           | 287.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0263        |
| infos/                  |               |
|    cost                 | 0.06          |
| rollout/                |               |
|    adjusted_reward      | 499           |
|    ep_len_mean          | 19.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 106           |
|    time_elapsed         | 1161          |
|    total_timesteps      | 1085440       |
| train/                  |               |
|    approx_kl            | 0.0013984913  |
|    average_cost         | 0.028222656   |
|    clip_fraction        | 0.0148        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.911        |
|    cost_value_loss      | 0.15          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.295        |
|    learning_rate        | 0.0003        |
|    loss                 | 7.95e+06      |
|    mean_cost_advantages | -0.0039717723 |
|    mean_reward_advan... | 3820.4414     |
|    n_updates            | 1050          |
|    nu                   | 5.9           |
|    nu_loss              | -0.165        |
|    policy_gradient_loss | -0.000401     |
|    reward_explained_... | -1.03e+13     |
|    reward_value_loss    | 1.69e+07      |
|    total_cost           | 289.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0309       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 523          |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 107          |
|    time_elapsed         | 1172         |
|    total_timesteps      | 1095680      |
| train/                  |              |
|    approx_kl            | 0.0049052276 |
|    average_cost         | 0.02626953   |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.596       |
|    cost_value_loss      | 0.13         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.293       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.11e+06     |
|    mean_cost_advantages | -0.017734364 |
|    mean_reward_advan... | 3765.7144    |
|    n_updates            | 1060         |
|    nu                   | 5.95         |
|    nu_loss              | -0.155       |
|    policy_gradient_loss | -0.000782    |
|    reward_explained_... | -4.12e+13    |
|    reward_value_loss    | 1.65e+07     |
|    total_cost           | 269.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0284      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | 490         |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 9.89e+03    |
| time/                   |             |
|    fps                  | 934         |
|    iterations           | 108         |
|    time_elapsed         | 1182        |
|    total_timesteps      | 1105920     |
| train/                  |             |
|    approx_kl            | 0.003913621 |
|    average_cost         | 0.030859375 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.997      |
|    cost_value_loss      | 0.14        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.292      |
|    learning_rate        | 0.0003      |
|    loss                 | 9.47e+06    |
|    mean_cost_advantages | 0.013385015 |
|    mean_reward_advan... | 3843.4624   |
|    n_updates            | 1070        |
|    nu                   | 6           |
|    nu_loss              | -0.184      |
|    policy_gradient_loss | -0.00073    |
|    reward_explained_... | -9.88e+12   |
|    reward_value_loss    | 1.7e+07     |
|    total_cost           | 316.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.028         |
| infos/                  |               |
|    cost                 | 0.05          |
| rollout/                |               |
|    adjusted_reward      | 509           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 109           |
|    time_elapsed         | 1193          |
|    total_timesteps      | 1116160       |
| train/                  |               |
|    approx_kl            | 0.0017578764  |
|    average_cost         | 0.02841797    |
|    clip_fraction        | 0.0344        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.552        |
|    cost_value_loss      | 0.132         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.281        |
|    learning_rate        | 0.0003        |
|    loss                 | 8.4e+06       |
|    mean_cost_advantages | -0.0030749596 |
|    mean_reward_advan... | 3585.6567     |
|    n_updates            | 1080          |
|    nu                   | 6.05          |
|    nu_loss              | -0.17         |
|    policy_gradient_loss | -0.000427     |
|    reward_explained_... | -1.29e+13     |
|    reward_value_loss    | 1.58e+07      |
|    total_cost           | 291.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0279       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 504          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 933          |
|    iterations           | 110          |
|    time_elapsed         | 1206         |
|    total_timesteps      | 1126400      |
| train/                  |              |
|    approx_kl            | 0.0018690903 |
|    average_cost         | 0.028027344  |
|    clip_fraction        | 0.00654      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.553       |
|    cost_value_loss      | 0.118        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.283       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.24e+06     |
|    mean_cost_advantages | -0.010227203 |
|    mean_reward_advan... | 3695.0835    |
|    n_updates            | 1090         |
|    nu                   | 6.1          |
|    nu_loss              | -0.169       |
|    policy_gradient_loss | -0.000212    |
|    reward_explained_... | -9.52e+15    |
|    reward_value_loss    | 1.63e+07     |
|    total_cost           | 287.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 21            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0284        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | 503           |
|    ep_len_mean          | 21.5          |
|    ep_rew_mean          | 9.89e+03      |
| time/                   |               |
|    fps                  | 933           |
|    iterations           | 111           |
|    time_elapsed         | 1217          |
|    total_timesteps      | 1136640       |
| train/                  |               |
|    approx_kl            | 0.00061014533 |
|    average_cost         | 0.027929688   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.418        |
|    cost_value_loss      | 0.106         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.291        |
|    learning_rate        | 0.0003        |
|    loss                 | 8.25e+06      |
|    mean_cost_advantages | -0.007993633  |
|    mean_reward_advan... | 3690.3171     |
|    n_updates            | 1100          |
|    nu                   | 6.14          |
|    nu_loss              | -0.17         |
|    policy_gradient_loss | -4.29e-05     |
|    reward_explained_... | -2.54e+12     |
|    reward_value_loss    | 1.59e+07      |
|    total_cost           | 286.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0312       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 525          |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 933          |
|    iterations           | 112          |
|    time_elapsed         | 1229         |
|    total_timesteps      | 1146880      |
| train/                  |              |
|    approx_kl            | 0.0027751836 |
|    average_cost         | 0.02841797   |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.824       |
|    cost_value_loss      | 0.139        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.279       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.11e+06     |
|    mean_cost_advantages | 0.004082177  |
|    mean_reward_advan... | 3627.8237    |
|    n_updates            | 1110         |
|    nu                   | 6.19         |
|    nu_loss              | -0.175       |
|    policy_gradient_loss | -0.000334    |
|    reward_explained_... | -1.05e+16    |
|    reward_value_loss    | 1.57e+07     |
|    total_cost           | 291.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0284       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 519          |
|    ep_len_mean          | 19.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 932          |
|    iterations           | 113          |
|    time_elapsed         | 1241         |
|    total_timesteps      | 1157120      |
| train/                  |              |
|    approx_kl            | 0.0030478053 |
|    average_cost         | 0.031152343  |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.87        |
|    cost_value_loss      | 0.134        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.283       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.76e+06     |
|    mean_cost_advantages | 0.0113024665 |
|    mean_reward_advan... | 3743.606     |
|    n_updates            | 1120         |
|    nu                   | 6.24         |
|    nu_loss              | -0.193       |
|    policy_gradient_loss | -0.000685    |
|    reward_explained_... | -2.39e+12    |
|    reward_value_loss    | 1.62e+07     |
|    total_cost           | 319.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0299        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 506           |
|    ep_len_mean          | 19.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 931           |
|    iterations           | 114           |
|    time_elapsed         | 1252          |
|    total_timesteps      | 1167360       |
| train/                  |               |
|    approx_kl            | 0.0006887485  |
|    average_cost         | 0.02841797    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.479        |
|    cost_value_loss      | 0.123         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.288        |
|    learning_rate        | 0.0003        |
|    loss                 | 7.67e+06      |
|    mean_cost_advantages | -0.0072655543 |
|    mean_reward_advan... | 3701.222      |
|    n_updates            | 1130          |
|    nu                   | 6.29          |
|    nu_loss              | -0.177        |
|    policy_gradient_loss | -9.77e-05     |
|    reward_explained_... | -1.21e+16     |
|    reward_value_loss    | 1.58e+07      |
|    total_cost           | 291.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0274       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 496          |
|    ep_len_mean          | 20.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 115          |
|    time_elapsed         | 1263         |
|    total_timesteps      | 1177600      |
| train/                  |              |
|    approx_kl            | 0.0014984027 |
|    average_cost         | 0.029882813  |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.12        |
|    cost_value_loss      | 0.17         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.288       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.39e+06     |
|    mean_cost_advantages | 0.027484339  |
|    mean_reward_advan... | 3624.455     |
|    n_updates            | 1140         |
|    nu                   | 6.34         |
|    nu_loss              | -0.188       |
|    policy_gradient_loss | -0.000362    |
|    reward_explained_... | -9.78e+12    |
|    reward_value_loss    | 1.53e+07     |
|    total_cost           | 306.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0273       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 511          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 116          |
|    time_elapsed         | 1274         |
|    total_timesteps      | 1187840      |
| train/                  |              |
|    approx_kl            | 0.0017047317 |
|    average_cost         | 0.027441407  |
|    clip_fraction        | 0.000459     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.603       |
|    cost_value_loss      | 0.129        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.287       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.9e+06      |
|    mean_cost_advantages | -0.005501896 |
|    mean_reward_advan... | 3558.3274    |
|    n_updates            | 1150         |
|    nu                   | 6.39         |
|    nu_loss              | -0.174       |
|    policy_gradient_loss | -0.00014     |
|    reward_explained_... | -9.84e+12    |
|    reward_value_loss    | 1.49e+07     |
|    total_cost           | 281.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0283       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 511          |
|    ep_len_mean          | 18.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 932          |
|    iterations           | 117          |
|    time_elapsed         | 1285         |
|    total_timesteps      | 1198080      |
| train/                  |              |
|    approx_kl            | 0.0031367883 |
|    average_cost         | 0.02734375   |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.642       |
|    cost_value_loss      | 0.117        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.287       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.38e+06     |
|    mean_cost_advantages | -0.007576776 |
|    mean_reward_advan... | 3601.979     |
|    n_updates            | 1160         |
|    nu                   | 6.44         |
|    nu_loss              | -0.175       |
|    policy_gradient_loss | -0.000289    |
|    reward_explained_... | -3.8e+13     |
|    reward_value_loss    | 1.51e+07     |
|    total_cost           | 280.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.028        |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 503          |
|    ep_len_mean          | 19.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 118          |
|    time_elapsed         | 1297         |
|    total_timesteps      | 1208320      |
| train/                  |              |
|    approx_kl            | 0.0028588648 |
|    average_cost         | 0.028320312  |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1           |
|    cost_value_loss      | 0.131        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.281       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.33e+06     |
|    mean_cost_advantages | 0.002020695  |
|    mean_reward_advan... | 3539.6812    |
|    n_updates            | 1170         |
|    nu                   | 6.49         |
|    nu_loss              | -0.182       |
|    policy_gradient_loss | -0.000606    |
|    reward_explained_... | -4.7e+12     |
|    reward_value_loss    | 1.49e+07     |
|    total_cost           | 290.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0299       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 523          |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 930          |
|    iterations           | 119          |
|    time_elapsed         | 1310         |
|    total_timesteps      | 1218560      |
| train/                  |              |
|    approx_kl            | 0.006380086  |
|    average_cost         | 0.028027344  |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.38        |
|    cost_value_loss      | 0.193        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.281       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.28e+06     |
|    mean_cost_advantages | 0.0059082666 |
|    mean_reward_advan... | 3531.1792    |
|    n_updates            | 1180         |
|    nu                   | 6.54         |
|    nu_loss              | -0.182       |
|    policy_gradient_loss | -0.000815    |
|    reward_explained_... | -3.81e+13    |
|    reward_value_loss    | 1.46e+07     |
|    total_cost           | 287.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0271       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 511          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 930          |
|    iterations           | 120          |
|    time_elapsed         | 1320         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0033556249 |
|    average_cost         | 0.029882813  |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.65        |
|    cost_value_loss      | 0.112        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.28        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.17e+06     |
|    mean_cost_advantages | -0.010217743 |
|    mean_reward_advan... | 3585.2332    |
|    n_updates            | 1190         |
|    nu                   | 6.59         |
|    nu_loss              | -0.195       |
|    policy_gradient_loss | -0.000669    |
|    reward_explained_... | -9.17e+12    |
|    reward_value_loss    | 1.49e+07     |
|    total_cost           | 306.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0268       |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | 520          |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 121          |
|    time_elapsed         | 1330         |
|    total_timesteps      | 1239040      |
| train/                  |              |
|    approx_kl            | 0.0017920479 |
|    average_cost         | 0.027050782  |
|    clip_fraction        | 0.00321      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.35        |
|    cost_value_loss      | 0.106        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.282       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.82e+06     |
|    mean_cost_advantages | -0.01161772  |
|    mean_reward_advan... | 3517.7358    |
|    n_updates            | 1200         |
|    nu                   | 6.64         |
|    nu_loss              | -0.178       |
|    policy_gradient_loss | -0.000152    |
|    reward_explained_... | -3.71e+13    |
|    reward_value_loss    | 1.45e+07     |
|    total_cost           | 277.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0267       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 521          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 929          |
|    iterations           | 122          |
|    time_elapsed         | 1343         |
|    total_timesteps      | 1249280      |
| train/                  |              |
|    approx_kl            | 0.0004409834 |
|    average_cost         | 0.026757812  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.699       |
|    cost_value_loss      | 0.114        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.283       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.04e+06     |
|    mean_cost_advantages | -0.013074519 |
|    mean_reward_advan... | 3535.3228    |
|    n_updates            | 1210         |
|    nu                   | 6.69         |
|    nu_loss              | -0.178       |
|    policy_gradient_loss | -9.21e-05    |
|    reward_explained_... | -3.99e+12    |
|    reward_value_loss    | 1.45e+07     |
|    total_cost           | 274.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0263        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 511           |
|    ep_len_mean          | 19.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 929           |
|    iterations           | 123           |
|    time_elapsed         | 1355          |
|    total_timesteps      | 1259520       |
| train/                  |               |
|    approx_kl            | 0.0024071727  |
|    average_cost         | 0.026660156   |
|    clip_fraction        | 0.02          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.827        |
|    cost_value_loss      | 0.116         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.28         |
|    learning_rate        | 0.0003        |
|    loss                 | 6.46e+06      |
|    mean_cost_advantages | -0.0036954195 |
|    mean_reward_advan... | 3526.7954     |
|    n_updates            | 1220          |
|    nu                   | 6.74          |
|    nu_loss              | -0.178        |
|    policy_gradient_loss | -0.000473     |
|    reward_explained_... | -3.48e+13     |
|    reward_value_loss    | 1.44e+07      |
|    total_cost           | 273.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0259        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 521           |
|    ep_len_mean          | 19.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 928           |
|    iterations           | 124           |
|    time_elapsed         | 1367          |
|    total_timesteps      | 1269760       |
| train/                  |               |
|    approx_kl            | 0.00022561094 |
|    average_cost         | 0.02626953    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.603        |
|    cost_value_loss      | 0.104         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.285        |
|    learning_rate        | 0.0003        |
|    loss                 | 7.42e+06      |
|    mean_cost_advantages | -0.0044300053 |
|    mean_reward_advan... | 3455.472      |
|    n_updates            | 1230          |
|    nu                   | 6.79          |
|    nu_loss              | -0.177        |
|    policy_gradient_loss | -9.93e-05     |
|    reward_explained_... | -9.14e+12     |
|    reward_value_loss    | 1.4e+07       |
|    total_cost           | 269.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0248        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 505           |
|    ep_len_mean          | 19.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 928           |
|    iterations           | 125           |
|    time_elapsed         | 1378          |
|    total_timesteps      | 1280000       |
| train/                  |               |
|    approx_kl            | 0.00087559596 |
|    average_cost         | 0.025878906   |
|    clip_fraction        | 0.000234      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.502        |
|    cost_value_loss      | 0.0954        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.284        |
|    learning_rate        | 0.0003        |
|    loss                 | 7.21e+06      |
|    mean_cost_advantages | -0.010025032  |
|    mean_reward_advan... | 3474.3179     |
|    n_updates            | 1240          |
|    nu                   | 6.84          |
|    nu_loss              | -0.176        |
|    policy_gradient_loss | -0.000124     |
|    reward_explained_... | -8.68e+12     |
|    reward_value_loss    | 1.4e+07       |
|    total_cost           | 265.0         |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 20.6           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.0243         |
| infos/                  |                |
|    cost                 | 0.03           |
| rollout/                |                |
|    adjusted_reward      | 502            |
|    ep_len_mean          | 21.4           |
|    ep_rew_mean          | 9.89e+03       |
| time/                   |                |
|    fps                  | 927            |
|    iterations           | 126            |
|    time_elapsed         | 1390           |
|    total_timesteps      | 1290240        |
| train/                  |                |
|    approx_kl            | -2.5929883e-06 |
|    average_cost         | 0.024804687    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.78          |
|    cost_value_loss      | 0.122          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.286         |
|    learning_rate        | 0.0003         |
|    loss                 | 7.12e+06       |
|    mean_cost_advantages | -0.001927883   |
|    mean_reward_advan... | 3394.5913      |
|    n_updates            | 1250           |
|    nu                   | 6.89           |
|    nu_loss              | -0.17          |
|    policy_gradient_loss | -7.8e-05       |
|    reward_explained_... | -1.08e+16      |
|    reward_value_loss    | 1.36e+07       |
|    total_cost           | 254.0          |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0262        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 510           |
|    ep_len_mean          | 19.8          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 927           |
|    iterations           | 127           |
|    time_elapsed         | 1402          |
|    total_timesteps      | 1300480       |
| train/                  |               |
|    approx_kl            | 1.8482207e-05 |
|    average_cost         | 0.024316406   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.867        |
|    cost_value_loss      | 0.114         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.28         |
|    learning_rate        | 0.0003        |
|    loss                 | 6.08e+06      |
|    mean_cost_advantages | 0.004087953   |
|    mean_reward_advan... | 3324.4224     |
|    n_updates            | 1260          |
|    nu                   | 6.94          |
|    nu_loss              | -0.168        |
|    policy_gradient_loss | -3.77e-05     |
|    reward_explained_... | -2.49e+12     |
|    reward_value_loss    | 1.33e+07      |
|    total_cost           | 249.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0264       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 525          |
|    ep_len_mean          | 18.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 128          |
|    time_elapsed         | 1416         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 0.0050532636 |
|    average_cost         | 0.026171874  |
|    clip_fraction        | 0.0512       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.996       |
|    cost_value_loss      | 0.125        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.282       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.62e+06     |
|    mean_cost_advantages | 0.0059397453 |
|    mean_reward_advan... | 3375.89      |
|    n_updates            | 1270         |
|    nu                   | 6.99         |
|    nu_loss              | -0.182       |
|    policy_gradient_loss | -0.000658    |
|    reward_explained_... | -1.11e+16    |
|    reward_value_loss    | 1.34e+07     |
|    total_cost           | 268.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0239       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 515          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 129          |
|    time_elapsed         | 1429         |
|    total_timesteps      | 1320960      |
| train/                  |              |
|    approx_kl            | 0.0017929662 |
|    average_cost         | 0.026367188  |
|    clip_fraction        | 0.00533      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.923       |
|    cost_value_loss      | 0.123        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.283       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.64e+06     |
|    mean_cost_advantages | 0.0023566824 |
|    mean_reward_advan... | 3408.7703    |
|    n_updates            | 1280         |
|    nu                   | 7.04         |
|    nu_loss              | -0.184       |
|    policy_gradient_loss | -0.000255    |
|    reward_explained_... | -2.1e+12     |
|    reward_value_loss    | 1.35e+07     |
|    total_cost           | 270.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0262       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 533          |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 130          |
|    time_elapsed         | 1440         |
|    total_timesteps      | 1331200      |
| train/                  |              |
|    approx_kl            | 0.0023209658 |
|    average_cost         | 0.023925781  |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.544       |
|    cost_value_loss      | 0.0898       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.286       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.71e+06     |
|    mean_cost_advantages | -0.013245525 |
|    mean_reward_advan... | 3350.1738    |
|    n_updates            | 1290         |
|    nu                   | 7.08         |
|    nu_loss              | -0.168       |
|    policy_gradient_loss | -0.000345    |
|    reward_explained_... | -1.52e+16    |
|    reward_value_loss    | 1.31e+07     |
|    total_cost           | 245.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.025        |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 515          |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 131          |
|    time_elapsed         | 1453         |
|    total_timesteps      | 1341440      |
| train/                  |              |
|    approx_kl            | 0.006839226  |
|    average_cost         | 0.026171874  |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.22        |
|    cost_value_loss      | 0.13         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.284       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.34e+06     |
|    mean_cost_advantages | 0.0122617055 |
|    mean_reward_advan... | 3401.5024    |
|    n_updates            | 1300         |
|    nu                   | 7.13         |
|    nu_loss              | -0.185       |
|    policy_gradient_loss | -0.00102     |
|    reward_explained_... | -8.07e+12    |
|    reward_value_loss    | 1.34e+07     |
|    total_cost           | 268.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0249       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 513          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 921          |
|    iterations           | 132          |
|    time_elapsed         | 1466         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.000358807  |
|    average_cost         | 0.025        |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.56        |
|    cost_value_loss      | 0.09         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.291       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.61e+06     |
|    mean_cost_advantages | -0.002333168 |
|    mean_reward_advan... | 3309.9       |
|    n_updates            | 1310         |
|    nu                   | 7.18         |
|    nu_loss              | -0.178       |
|    policy_gradient_loss | -0.000125    |
|    reward_explained_... | -8.38e+12    |
|    reward_value_loss    | 1.29e+07     |
|    total_cost           | 256.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0257       |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | 524          |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 133          |
|    time_elapsed         | 1477         |
|    total_timesteps      | 1361920      |
| train/                  |              |
|    approx_kl            | 0.002151513  |
|    average_cost         | 0.024902344  |
|    clip_fraction        | 0.00354      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.604       |
|    cost_value_loss      | 0.0945       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.291       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.79e+06     |
|    mean_cost_advantages | -0.002045843 |
|    mean_reward_advan... | 3277.106     |
|    n_updates            | 1320         |
|    nu                   | 7.23         |
|    nu_loss              | -0.179       |
|    policy_gradient_loss | -0.000189    |
|    reward_explained_... | -3.36e+13    |
|    reward_value_loss    | 1.26e+07     |
|    total_cost           | 255.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.024        |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 514          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 134          |
|    time_elapsed         | 1487         |
|    total_timesteps      | 1372160      |
| train/                  |              |
|    approx_kl            | 0.0023453329 |
|    average_cost         | 0.025683593  |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.17        |
|    cost_value_loss      | 0.124        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.29        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.09e+06     |
|    mean_cost_advantages | 0.004595075  |
|    mean_reward_advan... | 3295.9263    |
|    n_updates            | 1330         |
|    nu                   | 7.28         |
|    nu_loss              | -0.186       |
|    policy_gradient_loss | -0.000632    |
|    reward_explained_... | -3.62e+12    |
|    reward_value_loss    | 1.27e+07     |
|    total_cost           | 263.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0232       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 498          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 135          |
|    time_elapsed         | 1498         |
|    total_timesteps      | 1382400      |
| train/                  |              |
|    approx_kl            | 0.0024175341 |
|    average_cost         | 0.024023438  |
|    clip_fraction        | 0.0029       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.5         |
|    cost_value_loss      | 0.0838       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.297       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.53e+06     |
|    mean_cost_advantages | -0.011687962 |
|    mean_reward_advan... | 3238.7163    |
|    n_updates            | 1340         |
|    nu                   | 7.33         |
|    nu_loss              | -0.175       |
|    policy_gradient_loss | -0.000176    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 1.23e+07     |
|    total_cost           | 246.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0243        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 504           |
|    ep_len_mean          | 20.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 136           |
|    time_elapsed         | 1510          |
|    total_timesteps      | 1392640       |
| train/                  |               |
|    approx_kl            | 0.002811674   |
|    average_cost         | 0.023242187   |
|    clip_fraction        | 0.0211        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.587        |
|    cost_value_loss      | 0.0921        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.301        |
|    learning_rate        | 0.0003        |
|    loss                 | 5.38e+06      |
|    mean_cost_advantages | -0.0031896844 |
|    mean_reward_advan... | 3156.468      |
|    n_updates            | 1350          |
|    nu                   | 7.38          |
|    nu_loss              | -0.17         |
|    policy_gradient_loss | -0.000358     |
|    reward_explained_... | -2.09e+12     |
|    reward_value_loss    | 1.19e+07      |
|    total_cost           | 238.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0234        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 516           |
|    ep_len_mean          | 19.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 921           |
|    iterations           | 137           |
|    time_elapsed         | 1521          |
|    total_timesteps      | 1402880       |
| train/                  |               |
|    approx_kl            | 0.00071878755 |
|    average_cost         | 0.024316406   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.32         |
|    cost_value_loss      | 0.139         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.299        |
|    learning_rate        | 0.0003        |
|    loss                 | 6.85e+06      |
|    mean_cost_advantages | 0.012708232   |
|    mean_reward_advan... | 3149.3582     |
|    n_updates            | 1360          |
|    nu                   | 7.42          |
|    nu_loss              | -0.179        |
|    policy_gradient_loss | -0.00019      |
|    reward_explained_... | -1.37e+15     |
|    reward_value_loss    | 1.19e+07      |
|    total_cost           | 249.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0214       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 508          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 138          |
|    time_elapsed         | 1536         |
|    total_timesteps      | 1413120      |
| train/                  |              |
|    approx_kl            | 0.0012939412 |
|    average_cost         | 0.0234375    |
|    clip_fraction        | 0.00673      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.11        |
|    cost_value_loss      | 0.107        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.302       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.86e+06     |
|    mean_cost_advantages | -0.012487981 |
|    mean_reward_advan... | 3189.3318    |
|    n_updates            | 1370         |
|    nu                   | 7.47         |
|    nu_loss              | -0.174       |
|    policy_gradient_loss | -0.000375    |
|    reward_explained_... | -7.89e+12    |
|    reward_value_loss    | 1.2e+07      |
|    total_cost           | 240.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0226       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 508          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 139          |
|    time_elapsed         | 1547         |
|    total_timesteps      | 1423360      |
| train/                  |              |
|    approx_kl            | 0.0007307702 |
|    average_cost         | 0.021386718  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.66        |
|    cost_value_loss      | 0.0853       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.307       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.99e+06     |
|    mean_cost_advantages | -0.008921443 |
|    mean_reward_advan... | 3141.714     |
|    n_updates            | 1380         |
|    nu                   | 7.52         |
|    nu_loss              | -0.16        |
|    policy_gradient_loss | -0.00022     |
|    reward_explained_... | -7.89e+12    |
|    reward_value_loss    | 1.17e+07     |
|    total_cost           | 219.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0209        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 509           |
|    ep_len_mean          | 19.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 920           |
|    iterations           | 140           |
|    time_elapsed         | 1558          |
|    total_timesteps      | 1433600       |
| train/                  |               |
|    approx_kl            | 1.5033164e-05 |
|    average_cost         | 0.022558594   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.14         |
|    cost_value_loss      | 0.109         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.31         |
|    learning_rate        | 0.0003        |
|    loss                 | 5.92e+06      |
|    mean_cost_advantages | 0.009288339   |
|    mean_reward_advan... | 3118.9204     |
|    n_updates            | 1390          |
|    nu                   | 7.56          |
|    nu_loss              | -0.17         |
|    policy_gradient_loss | -0.000199     |
|    reward_explained_... | -1.98e+12     |
|    reward_value_loss    | 1.15e+07      |
|    total_cost           | 231.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 23           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0197       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 494          |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 141          |
|    time_elapsed         | 1569         |
|    total_timesteps      | 1443840      |
| train/                  |              |
|    approx_kl            | 0.002153762  |
|    average_cost         | 0.020898437  |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.819       |
|    cost_value_loss      | 0.0809       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.31        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.65e+06     |
|    mean_cost_advantages | -0.003162006 |
|    mean_reward_advan... | 3092.7788    |
|    n_updates            | 1400         |
|    nu                   | 7.61         |
|    nu_loss              | -0.158       |
|    policy_gradient_loss | -0.000294    |
|    reward_explained_... | -8.01e+12    |
|    reward_value_loss    | 1.14e+07     |
|    total_cost           | 214.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0203       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 490          |
|    ep_len_mean          | 20.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 142          |
|    time_elapsed         | 1580         |
|    total_timesteps      | 1454080      |
| train/                  |              |
|    approx_kl            | 0.0011026434 |
|    average_cost         | 0.019726563  |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.33        |
|    cost_value_loss      | 0.111        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.312       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.17e+06     |
|    mean_cost_advantages | 0.002520742  |
|    mean_reward_advan... | 2980.866     |
|    n_updates            | 1410         |
|    nu                   | 7.66         |
|    nu_loss              | -0.15        |
|    policy_gradient_loss | -0.000319    |
|    reward_explained_... | -9.05e+12    |
|    reward_value_loss    | 1.09e+07     |
|    total_cost           | 202.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 19          |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0187      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | 511         |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 917         |
|    iterations           | 143         |
|    time_elapsed         | 1595        |
|    total_timesteps      | 1464320     |
| train/                  |             |
|    approx_kl            | 0.004716049 |
|    average_cost         | 0.0203125   |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.43       |
|    cost_value_loss      | 0.115       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.324      |
|    learning_rate        | 0.0003      |
|    loss                 | 5.34e+06    |
|    mean_cost_advantages | 0.00749802  |
|    mean_reward_advan... | 2985.7407   |
|    n_updates            | 1420        |
|    nu                   | 7.7         |
|    nu_loss              | -0.156      |
|    policy_gradient_loss | -0.00093    |
|    reward_explained_... | -8.1e+12    |
|    reward_value_loss    | 1.08e+07    |
|    total_cost           | 208.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0175       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 500          |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 144          |
|    time_elapsed         | 1608         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.000643212  |
|    average_cost         | 0.01875      |
|    clip_fraction        | 0.00771      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.17        |
|    cost_value_loss      | 0.0903       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.32        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.9e+06      |
|    mean_cost_advantages | -0.012569258 |
|    mean_reward_advan... | 3038.2417    |
|    n_updates            | 1430         |
|    nu                   | 7.74         |
|    nu_loss              | -0.144       |
|    policy_gradient_loss | -0.000348    |
|    reward_explained_... | -3.32e+15    |
|    reward_value_loss    | 1.1e+07      |
|    total_cost           | 192.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0172       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 490          |
|    ep_len_mean          | 20.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 145          |
|    time_elapsed         | 1620         |
|    total_timesteps      | 1484800      |
| train/                  |              |
|    approx_kl            | 0.001989864  |
|    average_cost         | 0.017480468  |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.24        |
|    cost_value_loss      | 0.0873       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.324       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.57e+06     |
|    mean_cost_advantages | -0.009428084 |
|    mean_reward_advan... | 2985.3726    |
|    n_updates            | 1440         |
|    nu                   | 7.79         |
|    nu_loss              | -0.135       |
|    policy_gradient_loss | -0.000445    |
|    reward_explained_... | -4.17e+15    |
|    reward_value_loss    | 1.07e+07     |
|    total_cost           | 179.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0166       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 502          |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 915          |
|    iterations           | 146          |
|    time_elapsed         | 1632         |
|    total_timesteps      | 1495040      |
| train/                  |              |
|    approx_kl            | 0.002944254  |
|    average_cost         | 0.0171875    |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.56        |
|    cost_value_loss      | 0.102        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.327       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.18e+06     |
|    mean_cost_advantages | 0.0010259456 |
|    mean_reward_advan... | 2915.7996    |
|    n_updates            | 1450         |
|    nu                   | 7.83         |
|    nu_loss              | -0.134       |
|    policy_gradient_loss | -0.000519    |
|    reward_explained_... | -2.74e+15    |
|    reward_value_loss    | 1.03e+07     |
|    total_cost           | 176.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0187       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 506          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 147          |
|    time_elapsed         | 1643         |
|    total_timesteps      | 1505280      |
| train/                  |              |
|    approx_kl            | 0.0001209294 |
|    average_cost         | 0.016601562  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.62        |
|    cost_value_loss      | 0.09         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.329       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.75e+06     |
|    mean_cost_advantages | -0.010911097 |
|    mean_reward_advan... | 2957.5005    |
|    n_updates            | 1460         |
|    nu                   | 7.87         |
|    nu_loss              | -0.13        |
|    policy_gradient_loss | -0.000278    |
|    reward_explained_... | -7.56e+12    |
|    reward_value_loss    | 1.05e+07     |
|    total_cost           | 170.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0197       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 511          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 915          |
|    iterations           | 148          |
|    time_elapsed         | 1654         |
|    total_timesteps      | 1515520      |
| train/                  |              |
|    approx_kl            | 0.0035749576 |
|    average_cost         | 0.01875      |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.87        |
|    cost_value_loss      | 0.0874       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.329       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.3e+06      |
|    mean_cost_advantages | 0.010568605  |
|    mean_reward_advan... | 2950.1052    |
|    n_updates            | 1470         |
|    nu                   | 7.91         |
|    nu_loss              | -0.148       |
|    policy_gradient_loss | -0.000643    |
|    reward_explained_... | -1.85e+12    |
|    reward_value_loss    | 1.04e+07     |
|    total_cost           | 192.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.015        |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 503          |
|    ep_len_mean          | 20           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 149          |
|    time_elapsed         | 1665         |
|    total_timesteps      | 1525760      |
| train/                  |              |
|    approx_kl            | 0.0010763283 |
|    average_cost         | 0.019726563  |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.96        |
|    cost_value_loss      | 0.108        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.332       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.73e+06     |
|    mean_cost_advantages | 0.003109884  |
|    mean_reward_advan... | 2945.5085    |
|    n_updates            | 1480         |
|    nu                   | 7.95         |
|    nu_loss              | -0.156       |
|    policy_gradient_loss | -0.000765    |
|    reward_explained_... | -7.34e+12    |
|    reward_value_loss    | 1.03e+07     |
|    total_cost           | 202.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0141       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 497          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 150          |
|    time_elapsed         | 1675         |
|    total_timesteps      | 1536000      |
| train/                  |              |
|    approx_kl            | 0.0003939038 |
|    average_cost         | 0.015039062  |
|    clip_fraction        | 0.000381     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.27        |
|    cost_value_loss      | 0.0795       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.339       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.66e+06     |
|    mean_cost_advantages | -0.022524228 |
|    mean_reward_advan... | 2897.265     |
|    n_updates            | 1490         |
|    nu                   | 7.99         |
|    nu_loss              | -0.12        |
|    policy_gradient_loss | -0.000254    |
|    reward_explained_... | -7.34e+12    |
|    reward_value_loss    | 1.01e+07     |
|    total_cost           | 154.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0184       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 507          |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 151          |
|    time_elapsed         | 1686         |
|    total_timesteps      | 1546240      |
| train/                  |              |
|    approx_kl            | 0.0019280377 |
|    average_cost         | 0.0140625    |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.66        |
|    cost_value_loss      | 0.0765       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.341       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.12e+06     |
|    mean_cost_advantages | -0.011265513 |
|    mean_reward_advan... | 2852.6062    |
|    n_updates            | 1500         |
|    nu                   | 8.03         |
|    nu_loss              | -0.112       |
|    policy_gradient_loss | -0.000432    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 9.82e+06     |
|    total_cost           | 144.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 20.2        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0141      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | 498         |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 916         |
|    iterations           | 152         |
|    time_elapsed         | 1698        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.003889583 |
|    average_cost         | 0.018359374 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.24       |
|    cost_value_loss      | 0.112       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.341      |
|    learning_rate        | 0.0003      |
|    loss                 | 5.15e+06    |
|    mean_cost_advantages | 0.021709112 |
|    mean_reward_advan... | 2867.3403   |
|    n_updates            | 1510        |
|    nu                   | 8.07        |
|    nu_loss              | -0.147      |
|    policy_gradient_loss | -0.000847   |
|    reward_explained_... | -7.43e+16   |
|    reward_value_loss    | 9.86e+06    |
|    total_cost           | 188.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 22            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0134        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | 491           |
|    ep_len_mean          | 21.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 916           |
|    iterations           | 153           |
|    time_elapsed         | 1708          |
|    total_timesteps      | 1566720       |
| train/                  |               |
|    approx_kl            | 0.00095259084 |
|    average_cost         | 0.0140625     |
|    clip_fraction        | 0.000986      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.96         |
|    cost_value_loss      | 0.0952        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.348        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.72e+06      |
|    mean_cost_advantages | -0.01922638   |
|    mean_reward_advan... | 2811.7996     |
|    n_updates            | 1520          |
|    nu                   | 8.11          |
|    nu_loss              | -0.113        |
|    policy_gradient_loss | -0.000373     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 9.55e+06      |
|    total_cost           | 144.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.012        |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 496          |
|    ep_len_mean          | 19.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 154          |
|    time_elapsed         | 1720         |
|    total_timesteps      | 1576960      |
| train/                  |              |
|    approx_kl            | 0.0014861912 |
|    average_cost         | 0.013378906  |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.94        |
|    cost_value_loss      | 0.0858       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.352       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.05e+06     |
|    mean_cost_advantages | -0.01294698  |
|    mean_reward_advan... | 2765.9468    |
|    n_updates            | 1530         |
|    nu                   | 8.14         |
|    nu_loss              | -0.108       |
|    policy_gradient_loss | -0.000452    |
|    reward_explained_... | -7.35e+12    |
|    reward_value_loss    | 9.32e+06     |
|    total_cost           | 137.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0128       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 494          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 155          |
|    time_elapsed         | 1730         |
|    total_timesteps      | 1587200      |
| train/                  |              |
|    approx_kl            | 0.0008671152 |
|    average_cost         | 0.012011719  |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.94        |
|    cost_value_loss      | 0.0564       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.348       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.93e+06     |
|    mean_cost_advantages | -0.013329199 |
|    mean_reward_advan... | 2765.1538    |
|    n_updates            | 1540         |
|    nu                   | 8.18         |
|    nu_loss              | -0.0978      |
|    policy_gradient_loss | -0.000209    |
|    reward_explained_... | -7.19e+12    |
|    reward_value_loss    | 9.27e+06     |
|    total_cost           | 123.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0128       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 495          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 156          |
|    time_elapsed         | 1740         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0004759039 |
|    average_cost         | 0.012792969  |
|    clip_fraction        | 0.00187      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.96        |
|    cost_value_loss      | 0.089        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.348       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.41e+06     |
|    mean_cost_advantages | 0.0072866315 |
|    mean_reward_advan... | 2733.0913    |
|    n_updates            | 1550         |
|    nu                   | 8.21         |
|    nu_loss              | -0.105       |
|    policy_gradient_loss | -0.000212    |
|    reward_explained_... | -1.81e+12    |
|    reward_value_loss    | 9.11e+06     |
|    total_cost           | 131.0        |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 18             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.0118         |
| infos/                  |                |
|    cost                 | 0.02           |
| rollout/                |                |
|    adjusted_reward      | 483            |
|    ep_len_mean          | 20.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 918            |
|    iterations           | 157            |
|    time_elapsed         | 1751           |
|    total_timesteps      | 1607680        |
| train/                  |                |
|    approx_kl            | 0.0001381063   |
|    average_cost         | 0.012792969    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.51          |
|    cost_value_loss      | 0.0796         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.351         |
|    learning_rate        | 0.0003         |
|    loss                 | 4.32e+06       |
|    mean_cost_advantages | -5.6384877e-05 |
|    mean_reward_advan... | 2715.3508      |
|    n_updates            | 1560           |
|    nu                   | 8.25           |
|    nu_loss              | -0.105         |
|    policy_gradient_loss | -0.000257      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 8.99e+06       |
|    total_cost           | 131.0          |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 18.6           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.0133         |
| infos/                  |                |
|    cost                 | 0.02           |
| rollout/                |                |
|    adjusted_reward      | 497            |
|    ep_len_mean          | 20.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 917            |
|    iterations           | 158            |
|    time_elapsed         | 1763           |
|    total_timesteps      | 1617920        |
| train/                  |                |
|    approx_kl            | 0.004175319    |
|    average_cost         | 0.011816407    |
|    clip_fraction        | 0.0445         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -3.11          |
|    cost_value_loss      | 0.0852         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.342         |
|    learning_rate        | 0.0003         |
|    loss                 | 2.91e+06       |
|    mean_cost_advantages | -0.00053398625 |
|    mean_reward_advan... | 2620.858       |
|    n_updates            | 1570           |
|    nu                   | 8.28           |
|    nu_loss              | -0.0975        |
|    policy_gradient_loss | -0.000676      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 8.71e+06       |
|    total_cost           | 121.0          |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0106       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 499          |
|    ep_len_mean          | 20.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 159          |
|    time_elapsed         | 1774         |
|    total_timesteps      | 1628160      |
| train/                  |              |
|    approx_kl            | 0.0014978198 |
|    average_cost         | 0.01328125   |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.46        |
|    cost_value_loss      | 0.0835       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.345       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.77e+06     |
|    mean_cost_advantages | 0.006681365  |
|    mean_reward_advan... | 2692.9395    |
|    n_updates            | 1580         |
|    nu                   | 8.31         |
|    nu_loss              | -0.11        |
|    policy_gradient_loss | -0.000562    |
|    reward_explained_... | -6.89e+12    |
|    reward_value_loss    | 8.81e+06     |
|    total_cost           | 136.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0103       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 507          |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 160          |
|    time_elapsed         | 1785         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.001977257  |
|    average_cost         | 0.010644531  |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.08        |
|    cost_value_loss      | 0.0569       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.345       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.08e+06     |
|    mean_cost_advantages | -0.018650811 |
|    mean_reward_advan... | 2673.2861    |
|    n_updates            | 1590         |
|    nu                   | 8.34         |
|    nu_loss              | -0.0885      |
|    policy_gradient_loss | -0.00048     |
|    reward_explained_... | nan          |
|    reward_value_loss    | 8.7e+06      |
|    total_cost           | 109.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0101        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 492           |
|    ep_len_mean          | 20.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 917           |
|    iterations           | 161           |
|    time_elapsed         | 1796          |
|    total_timesteps      | 1648640       |
| train/                  |               |
|    approx_kl            | 0.0063372133  |
|    average_cost         | 0.010253906   |
|    clip_fraction        | 0.071         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.59         |
|    cost_value_loss      | 0.0753        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.344        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.01e+06      |
|    mean_cost_advantages | -0.0014523773 |
|    mean_reward_advan... | 2683.8765     |
|    n_updates            | 1600          |
|    nu                   | 8.37          |
|    nu_loss              | -0.0856       |
|    policy_gradient_loss | -0.00107      |
|    reward_explained_... | -6.68e+12     |
|    reward_value_loss    | 8.71e+06      |
|    total_cost           | 105.0         |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 21             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00996        |
| infos/                  |                |
|    cost                 | 0.02           |
| rollout/                |                |
|    adjusted_reward      | 495            |
|    ep_len_mean          | 20.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 918            |
|    iterations           | 162            |
|    time_elapsed         | 1806           |
|    total_timesteps      | 1658880        |
| train/                  |                |
|    approx_kl            | -0.00015558477 |
|    average_cost         | 0.010058594    |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.98          |
|    cost_value_loss      | 0.0602         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.345         |
|    learning_rate        | 0.0003         |
|    loss                 | 4.57e+06       |
|    mean_cost_advantages | 0.0039053552   |
|    mean_reward_advan... | 2611.1785      |
|    n_updates            | 1610           |
|    nu                   | 8.4            |
|    nu_loss              | -0.0842        |
|    policy_gradient_loss | -8.41e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 8.35e+06       |
|    total_cost           | 103.0          |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00879      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 484          |
|    ep_len_mean          | 20.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 918          |
|    iterations           | 163          |
|    time_elapsed         | 1816         |
|    total_timesteps      | 1669120      |
| train/                  |              |
|    approx_kl            | 0.0009108543 |
|    average_cost         | 0.009960937  |
|    clip_fraction        | 0.00434      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.72        |
|    cost_value_loss      | 0.0757       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.343       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.95e+06     |
|    mean_cost_advantages | -0.0023244   |
|    mean_reward_advan... | 2596.5347    |
|    n_updates            | 1620         |
|    nu                   | 8.43         |
|    nu_loss              | -0.0837      |
|    policy_gradient_loss | -0.000262    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 8.28e+06     |
|    total_cost           | 102.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 21.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00889       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 492           |
|    ep_len_mean          | 20.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 164           |
|    time_elapsed         | 1827          |
|    total_timesteps      | 1679360       |
| train/                  |               |
|    approx_kl            | -9.612939e-05 |
|    average_cost         | 0.0087890625  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.9          |
|    cost_value_loss      | 0.0559        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.345        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.66e+06      |
|    mean_cost_advantages | -0.0076946607 |
|    mean_reward_advan... | 2536.3357     |
|    n_updates            | 1630          |
|    nu                   | 8.46          |
|    nu_loss              | -0.0741       |
|    policy_gradient_loss | -6.14e-05     |
|    reward_explained_... | -1.72e+12     |
|    reward_value_loss    | 7.99e+06      |
|    total_cost           | 90.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00908      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 498          |
|    ep_len_mean          | 20.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 165          |
|    time_elapsed         | 1837         |
|    total_timesteps      | 1689600      |
| train/                  |              |
|    approx_kl            | 0.0019902606 |
|    average_cost         | 0.008886719  |
|    clip_fraction        | 0.00666      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.69        |
|    cost_value_loss      | 0.0721       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.338       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.97e+06     |
|    mean_cost_advantages | 0.005720622  |
|    mean_reward_advan... | 2549.0564    |
|    n_updates            | 1640         |
|    nu                   | 8.49         |
|    nu_loss              | -0.0752      |
|    policy_gradient_loss | -0.000202    |
|    reward_explained_... | -6.61e+12    |
|    reward_value_loss    | 8e+06        |
|    total_cost           | 91.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0108       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 486          |
|    ep_len_mean          | 20.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 166          |
|    time_elapsed         | 1848         |
|    total_timesteps      | 1699840      |
| train/                  |              |
|    approx_kl            | 0.0013059468 |
|    average_cost         | 0.009082031  |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.61        |
|    cost_value_loss      | 0.0613       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.338       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.67e+06     |
|    mean_cost_advantages | 0.0019071319 |
|    mean_reward_advan... | 2554.5142    |
|    n_updates            | 1650         |
|    nu                   | 8.51         |
|    nu_loss              | -0.0771      |
|    policy_gradient_loss | -0.000369    |
|    reward_explained_... | -6.44e+12    |
|    reward_value_loss    | 7.98e+06     |
|    total_cost           | 93.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 20          |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.0102      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 501         |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 167         |
|    time_elapsed         | 1860        |
|    total_timesteps      | 1710080     |
| train/                  |             |
|    approx_kl            | 0.001388935 |
|    average_cost         | 0.010839844 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.69       |
|    cost_value_loss      | 0.0819      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.337      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.99e+06    |
|    mean_cost_advantages | 0.007910075 |
|    mean_reward_advan... | 2489.137    |
|    n_updates            | 1660        |
|    nu                   | 8.54        |
|    nu_loss              | -0.0923     |
|    policy_gradient_loss | -0.000313   |
|    reward_explained_... | nan         |
|    reward_value_loss    | 7.69e+06    |
|    total_cost           | 111.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00947      |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 494          |
|    ep_len_mean          | 20.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 168          |
|    time_elapsed         | 1870         |
|    total_timesteps      | 1720320      |
| train/                  |              |
|    approx_kl            | 0.0009721834 |
|    average_cost         | 0.01015625   |
|    clip_fraction        | 1.95e-05     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.4         |
|    cost_value_loss      | 0.0674       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.324       |
|    learning_rate        | 0.0003       |
|    loss                 | 4.01e+06     |
|    mean_cost_advantages | -0.005155253 |
|    mean_reward_advan... | 2523.2383    |
|    n_updates            | 1670         |
|    nu                   | 8.56         |
|    nu_loss              | -0.0867      |
|    policy_gradient_loss | -0.00016     |
|    reward_explained_... | nan          |
|    reward_value_loss    | 7.79e+06     |
|    total_cost           | 104.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 21.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00732       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 507           |
|    ep_len_mean          | 19.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 920           |
|    iterations           | 169           |
|    time_elapsed         | 1880          |
|    total_timesteps      | 1730560       |
| train/                  |               |
|    approx_kl            | 0.0030172404  |
|    average_cost         | 0.009472656   |
|    clip_fraction        | 0.0253        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.35         |
|    cost_value_loss      | 0.0535        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.32         |
|    learning_rate        | 0.0003        |
|    loss                 | 3.6e+06       |
|    mean_cost_advantages | -0.0060016103 |
|    mean_reward_advan... | 2479.0933     |
|    n_updates            | 1680          |
|    nu                   | 8.59          |
|    nu_loss              | -0.0811       |
|    policy_gradient_loss | -0.000307     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 7.58e+06      |
|    total_cost           | 97.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 21.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.0112        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 498           |
|    ep_len_mean          | 19.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 920           |
|    iterations           | 170           |
|    time_elapsed         | 1890          |
|    total_timesteps      | 1740800       |
| train/                  |               |
|    approx_kl            | 0.00014410238 |
|    average_cost         | 0.0073242188  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.77         |
|    cost_value_loss      | 0.0453        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.317        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.63e+06      |
|    mean_cost_advantages | -0.016999759  |
|    mean_reward_advan... | 2501.9783     |
|    n_updates            | 1690          |
|    nu                   | 8.61          |
|    nu_loss              | -0.0629       |
|    policy_gradient_loss | -4e-05        |
|    reward_explained_... | -6.14e+12     |
|    reward_value_loss    | 7.65e+06      |
|    total_cost           | 75.0          |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 18.4        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.00732     |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | 493         |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 921         |
|    iterations           | 171         |
|    time_elapsed         | 1900        |
|    total_timesteps      | 1751040     |
| train/                  |             |
|    approx_kl            | 0.003351438 |
|    average_cost         | 0.011230469 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.83       |
|    cost_value_loss      | 0.0725      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 3.83e+06    |
|    mean_cost_advantages | 0.03087248  |
|    mean_reward_advan... | 2447.6335   |
|    n_updates            | 1700        |
|    nu                   | 8.64        |
|    nu_loss              | -0.0967     |
|    policy_gradient_loss | -0.000718   |
|    reward_explained_... | -6.21e+12   |
|    reward_value_loss    | 7.4e+06     |
|    total_cost           | 115.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00625      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 503          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 172          |
|    time_elapsed         | 1910         |
|    total_timesteps      | 1761280      |
| train/                  |              |
|    approx_kl            | 0.0026637907 |
|    average_cost         | 0.0073242188 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.9         |
|    cost_value_loss      | 0.045        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.32        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.87e+06     |
|    mean_cost_advantages | -0.016881622 |
|    mean_reward_advan... | 2407.966     |
|    n_updates            | 1710         |
|    nu                   | 8.66         |
|    nu_loss              | -0.0633      |
|    policy_gradient_loss | -0.000186    |
|    reward_explained_... | -1.57e+12    |
|    reward_value_loss    | 7.22e+06     |
|    total_cost           | 75.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00937      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 508          |
|    ep_len_mean          | 19.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 173          |
|    time_elapsed         | 1920         |
|    total_timesteps      | 1771520      |
| train/                  |              |
|    approx_kl            | 0.002150293  |
|    average_cost         | 0.00625      |
|    clip_fraction        | 0.00954      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.04        |
|    cost_value_loss      | 0.0319       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.315       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.72e+06     |
|    mean_cost_advantages | -0.016747681 |
|    mean_reward_advan... | 2427.474     |
|    n_updates            | 1720         |
|    nu                   | 8.68         |
|    nu_loss              | -0.0541      |
|    policy_gradient_loss | -0.000154    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 7.25e+06     |
|    total_cost           | 64.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00723      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 503          |
|    ep_len_mean          | 20.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 174          |
|    time_elapsed         | 1929         |
|    total_timesteps      | 1781760      |
| train/                  |              |
|    approx_kl            | 0.0038994458 |
|    average_cost         | 0.009375     |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.31        |
|    cost_value_loss      | 0.075        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.316       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.59e+06     |
|    mean_cost_advantages | 0.018272031  |
|    mean_reward_advan... | 2418.9224    |
|    n_updates            | 1730         |
|    nu                   | 8.7          |
|    nu_loss              | -0.0814      |
|    policy_gradient_loss | -0.000604    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 7.2e+06      |
|    total_cost           | 96.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 22.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00967      |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 504          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 175          |
|    time_elapsed         | 1940         |
|    total_timesteps      | 1792000      |
| train/                  |              |
|    approx_kl            | 0.0026309458 |
|    average_cost         | 0.0072265626 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.97        |
|    cost_value_loss      | 0.0392       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.311       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.77e+06     |
|    mean_cost_advantages | -0.00890579  |
|    mean_reward_advan... | 2383.1223    |
|    n_updates            | 1740         |
|    nu                   | 8.73         |
|    nu_loss              | -0.0629      |
|    policy_gradient_loss | -0.000292    |
|    reward_explained_... | -5.91e+12    |
|    reward_value_loss    | 7.01e+06     |
|    total_cost           | 74.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00947      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 495          |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 176          |
|    time_elapsed         | 1950         |
|    total_timesteps      | 1802240      |
| train/                  |              |
|    approx_kl            | 0.0047639078 |
|    average_cost         | 0.009667968  |
|    clip_fraction        | 0.061        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.42        |
|    cost_value_loss      | 0.0892       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.312       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.18e+06     |
|    mean_cost_advantages | 0.018063322  |
|    mean_reward_advan... | 2373.5854    |
|    n_updates            | 1750         |
|    nu                   | 8.75         |
|    nu_loss              | -0.0844      |
|    policy_gradient_loss | -0.00097     |
|    reward_explained_... | -1.45e+12    |
|    reward_value_loss    | 6.94e+06     |
|    total_cost           | 99.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00762      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 490          |
|    ep_len_mean          | 20.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 177          |
|    time_elapsed         | 1962         |
|    total_timesteps      | 1812480      |
| train/                  |              |
|    approx_kl            | 0.0020057026 |
|    average_cost         | 0.009472656  |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.55        |
|    cost_value_loss      | 0.0831       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.312       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.36e+06     |
|    mean_cost_advantages | -0.00222043  |
|    mean_reward_advan... | 2318.7925    |
|    n_updates            | 1760         |
|    nu                   | 8.77         |
|    nu_loss              | -0.0829      |
|    policy_gradient_loss | -0.000412    |
|    reward_explained_... | -5.87e+12    |
|    reward_value_loss    | 6.7e+06      |
|    total_cost           | 97.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00674      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 501          |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 178          |
|    time_elapsed         | 1975         |
|    total_timesteps      | 1822720      |
| train/                  |              |
|    approx_kl            | 0.006111241  |
|    average_cost         | 0.0076171877 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.89        |
|    cost_value_loss      | 0.0371       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.308       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.78e+06     |
|    mean_cost_advantages | -0.016838674 |
|    mean_reward_advan... | 2283.6794    |
|    n_updates            | 1770         |
|    nu                   | 8.79         |
|    nu_loss              | -0.0668      |
|    policy_gradient_loss | -0.000818    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 6.55e+06     |
|    total_cost           | 78.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00859      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 508          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 179          |
|    time_elapsed         | 1985         |
|    total_timesteps      | 1832960      |
| train/                  |              |
|    approx_kl            | 0.0022246363 |
|    average_cost         | 0.0067382813 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.28        |
|    cost_value_loss      | 0.0339       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.299       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.03e+06     |
|    mean_cost_advantages | -0.015377134 |
|    mean_reward_advan... | 2293.562     |
|    n_updates            | 1780         |
|    nu                   | 8.81         |
|    nu_loss              | -0.0592      |
|    policy_gradient_loss | -0.000173    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 6.55e+06     |
|    total_cost           | 69.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 18.8        |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.00723     |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | 506         |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 923         |
|    iterations           | 180         |
|    time_elapsed         | 1995        |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.003058723 |
|    average_cost         | 0.00859375  |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.5        |
|    cost_value_loss      | 0.063       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.296      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.9e+06     |
|    mean_cost_advantages | 0.011966323 |
|    mean_reward_advan... | 2299.5967   |
|    n_updates            | 1790        |
|    nu                   | 8.83        |
|    nu_loss              | -0.0757     |
|    policy_gradient_loss | -0.00045    |
|    reward_explained_... | -1.4e+12    |
|    reward_value_loss    | 6.55e+06    |
|    total_cost           | 88.0        |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00635       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 507           |
|    ep_len_mean          | 20            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 923           |
|    iterations           | 181           |
|    time_elapsed         | 2005          |
|    total_timesteps      | 1853440       |
| train/                  |               |
|    approx_kl            | -1.169471e-05 |
|    average_cost         | 0.0072265626  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.38         |
|    cost_value_loss      | 0.0381        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.292        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.31e+06      |
|    mean_cost_advantages | -0.006528354  |
|    mean_reward_advan... | 2272.7017     |
|    n_updates            | 1800          |
|    nu                   | 8.85          |
|    nu_loss              | -0.0638       |
|    policy_gradient_loss | -7.84e-05     |
|    reward_explained_... | -1.38e+12     |
|    reward_value_loss    | 6.41e+06      |
|    total_cost           | 74.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 20             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00732        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 500            |
|    ep_len_mean          | 19.8           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 924            |
|    iterations           | 182            |
|    time_elapsed         | 2016           |
|    total_timesteps      | 1863680        |
| train/                  |                |
|    approx_kl            | 0.000108361186 |
|    average_cost         | 0.0063476562   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.59          |
|    cost_value_loss      | 0.031          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.287         |
|    learning_rate        | 0.0003         |
|    loss                 | 2.97e+06       |
|    mean_cost_advantages | -0.010540521   |
|    mean_reward_advan... | 2263.0698      |
|    n_updates            | 1810           |
|    nu                   | 8.87           |
|    nu_loss              | -0.0562        |
|    policy_gradient_loss | -4.45e-05      |
|    reward_explained_... | -5.38e+12      |
|    reward_value_loss    | 6.33e+06       |
|    total_cost           | 65.0           |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 18.2           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.0085         |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 507            |
|    ep_len_mean          | 19.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 924            |
|    iterations           | 183            |
|    time_elapsed         | 2026           |
|    total_timesteps      | 1873920        |
| train/                  |                |
|    approx_kl            | -0.00010975059 |
|    average_cost         | 0.0073242188   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -6.2           |
|    cost_value_loss      | 0.0614         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.286         |
|    learning_rate        | 0.0003         |
|    loss                 | 2.42e+06       |
|    mean_cost_advantages | 0.0075628497   |
|    mean_reward_advan... | 2220.7637      |
|    n_updates            | 1820           |
|    nu                   | 8.89           |
|    nu_loss              | -0.065         |
|    policy_gradient_loss | -7.13e-05      |
|    reward_explained_... | nan            |
|    reward_value_loss    | 6.16e+06       |
|    total_cost           | 75.0           |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00576      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 500          |
|    ep_len_mean          | 20           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 184          |
|    time_elapsed         | 2036         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0018070493 |
|    average_cost         | 0.008496094  |
|    clip_fraction        | 0.0097       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.55        |
|    cost_value_loss      | 0.0489       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.284       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.88e+06     |
|    mean_cost_advantages | 0.008153165  |
|    mean_reward_advan... | 2215.725     |
|    n_updates            | 1830         |
|    nu                   | 8.91         |
|    nu_loss              | -0.0755      |
|    policy_gradient_loss | -0.00025     |
|    reward_explained_... | nan          |
|    reward_value_loss    | 6.13e+06     |
|    total_cost           | 87.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00859       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 498           |
|    ep_len_mean          | 19.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 185           |
|    time_elapsed         | 2047          |
|    total_timesteps      | 1894400       |
| train/                  |               |
|    approx_kl            | 0.00027292885 |
|    average_cost         | 0.005761719   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.3          |
|    cost_value_loss      | 0.0318        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.29         |
|    learning_rate        | 0.0003        |
|    loss                 | 2.93e+06      |
|    mean_cost_advantages | -0.013881211  |
|    mean_reward_advan... | 2172.8823     |
|    n_updates            | 1840          |
|    nu                   | 8.92          |
|    nu_loss              | -0.0513       |
|    policy_gradient_loss | -4.53e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 5.93e+06      |
|    total_cost           | 59.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0085       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 500          |
|    ep_len_mean          | 20.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 186          |
|    time_elapsed         | 2058         |
|    total_timesteps      | 1904640      |
| train/                  |              |
|    approx_kl            | 5.715803e-05 |
|    average_cost         | 0.00859375   |
|    clip_fraction        | 8.79e-05     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.79        |
|    cost_value_loss      | 0.0653       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.283       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.56e+06     |
|    mean_cost_advantages | 0.015463462  |
|    mean_reward_advan... | 2143.5808    |
|    n_updates            | 1850         |
|    nu                   | 8.94         |
|    nu_loss              | -0.0767      |
|    policy_gradient_loss | -0.000108    |
|    reward_explained_... | -5.4e+12     |
|    reward_value_loss    | 5.82e+06     |
|    total_cost           | 88.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00664       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 501           |
|    ep_len_mean          | 20            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 187           |
|    time_elapsed         | 2068          |
|    total_timesteps      | 1914880       |
| train/                  |               |
|    approx_kl            | 0.00068767834 |
|    average_cost         | 0.008496094   |
|    clip_fraction        | 0.00408       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.69         |
|    cost_value_loss      | 0.0517        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.283        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.51e+06      |
|    mean_cost_advantages | -0.003713204  |
|    mean_reward_advan... | 2127.5747     |
|    n_updates            | 1860          |
|    nu                   | 8.96          |
|    nu_loss              | -0.076        |
|    policy_gradient_loss | -0.000233     |
|    reward_explained_... | -1.33e+12     |
|    reward_value_loss    | 5.73e+06      |
|    total_cost           | 87.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00791      |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 511          |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 188          |
|    time_elapsed         | 2078         |
|    total_timesteps      | 1925120      |
| train/                  |              |
|    approx_kl            | 0.0019731899 |
|    average_cost         | 0.006640625  |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.2         |
|    cost_value_loss      | 0.0504       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.274       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.89e+06     |
|    mean_cost_advantages | -0.00567628  |
|    mean_reward_advan... | 2117.9292    |
|    n_updates            | 1870         |
|    nu                   | 8.98         |
|    nu_loss              | -0.0595      |
|    policy_gradient_loss | -0.000272    |
|    reward_explained_... | -1.3e+12     |
|    reward_value_loss    | 5.66e+06     |
|    total_cost           | 68.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00576      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 501          |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 189          |
|    time_elapsed         | 2087         |
|    total_timesteps      | 1935360      |
| train/                  |              |
|    approx_kl            | 0.0007019084 |
|    average_cost         | 0.007910157  |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.57        |
|    cost_value_loss      | 0.066        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.272       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.14e+06     |
|    mean_cost_advantages | 0.012926942  |
|    mean_reward_advan... | 2126.789     |
|    n_updates            | 1880         |
|    nu                   | 9            |
|    nu_loss              | -0.071       |
|    policy_gradient_loss | -0.000339    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 5.66e+06     |
|    total_cost           | 81.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00605       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 500           |
|    ep_len_mean          | 20.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 926           |
|    iterations           | 190           |
|    time_elapsed         | 2098          |
|    total_timesteps      | 1945600       |
| train/                  |               |
|    approx_kl            | 0.00030228673 |
|    average_cost         | 0.005761719   |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.57         |
|    cost_value_loss      | 0.0432        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.269        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.74e+06      |
|    mean_cost_advantages | -0.015095604  |
|    mean_reward_advan... | 2076.586      |
|    n_updates            | 1890          |
|    nu                   | 9.02          |
|    nu_loss              | -0.0518       |
|    policy_gradient_loss | -5.73e-05     |
|    reward_explained_... | -5.1e+12      |
|    reward_value_loss    | 5.46e+06      |
|    total_cost           | 59.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00576      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 503          |
|    ep_len_mean          | 20.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 191          |
|    time_elapsed         | 2110         |
|    total_timesteps      | 1955840      |
| train/                  |              |
|    approx_kl            | 0.001246165  |
|    average_cost         | 0.0060546873 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.11        |
|    cost_value_loss      | 0.0418       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.263       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.55e+06     |
|    mean_cost_advantages | 0.0016259302 |
|    mean_reward_advan... | 2049.2297    |
|    n_updates            | 1900         |
|    nu                   | 9.03         |
|    nu_loss              | -0.0546      |
|    policy_gradient_loss | -0.000196    |
|    reward_explained_... | -1.27e+12    |
|    reward_value_loss    | 5.35e+06     |
|    total_cost           | 62.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00635      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 512          |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 192          |
|    time_elapsed         | 2121         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0012829477 |
|    average_cost         | 0.005761719  |
|    clip_fraction        | 0.00318      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.18        |
|    cost_value_loss      | 0.0393       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.26        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.33e+06     |
|    mean_cost_advantages | -0.006793104 |
|    mean_reward_advan... | 2043.8982    |
|    n_updates            | 1910         |
|    nu                   | 9.05         |
|    nu_loss              | -0.052       |
|    policy_gradient_loss | -0.00011     |
|    reward_explained_... | -1.24e+12    |
|    reward_value_loss    | 5.3e+06      |
|    total_cost           | 59.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00811      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 503          |
|    ep_len_mean          | 20.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 193          |
|    time_elapsed         | 2131         |
|    total_timesteps      | 1976320      |
| train/                  |              |
|    approx_kl            | 0.0022243164 |
|    average_cost         | 0.0063476562 |
|    clip_fraction        | 0.00559      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.68        |
|    cost_value_loss      | 0.0625       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.259       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.39e+06     |
|    mean_cost_advantages | 0.0030894934 |
|    mean_reward_advan... | 2049.0957    |
|    n_updates            | 1920         |
|    nu                   | 9.07         |
|    nu_loss              | -0.0574      |
|    policy_gradient_loss | -0.000206    |
|    reward_explained_... | -1.21e+12    |
|    reward_value_loss    | 5.29e+06     |
|    total_cost           | 65.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00625      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 509          |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 194          |
|    time_elapsed         | 2142         |
|    total_timesteps      | 1986560      |
| train/                  |              |
|    approx_kl            | 0.0012520201 |
|    average_cost         | 0.008105469  |
|    clip_fraction        | 0.00652      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.53        |
|    cost_value_loss      | 0.0917       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.256       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.2e+06      |
|    mean_cost_advantages | 0.017687634  |
|    mean_reward_advan... | 1999.2649    |
|    n_updates            | 1930         |
|    nu                   | 9.08         |
|    nu_loss              | -0.0735      |
|    policy_gradient_loss | -0.000194    |
|    reward_explained_... | -4.88e+12    |
|    reward_value_loss    | 5.1e+06      |
|    total_cost           | 83.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00518      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 504          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 195          |
|    time_elapsed         | 2153         |
|    total_timesteps      | 1996800      |
| train/                  |              |
|    approx_kl            | 0.0012079881 |
|    average_cost         | 0.00625      |
|    clip_fraction        | 0.00978      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.41        |
|    cost_value_loss      | 0.0603       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.253       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.51e+06     |
|    mean_cost_advantages | -0.009572588 |
|    mean_reward_advan... | 1998.8695    |
|    n_updates            | 1940         |
|    nu                   | 9.1          |
|    nu_loss              | -0.0568      |
|    policy_gradient_loss | -0.000203    |
|    reward_explained_... | -4.78e+12    |
|    reward_value_loss    | 5.07e+06     |
|    total_cost           | 64.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00508       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 509           |
|    ep_len_mean          | 19.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 927           |
|    iterations           | 196           |
|    time_elapsed         | 2163          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.00021039431 |
|    average_cost         | 0.0051757814  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.22         |
|    cost_value_loss      | 0.0231        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.251        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.96e+06      |
|    mean_cost_advantages | -0.015688585  |
|    mean_reward_advan... | 1963.2701     |
|    n_updates            | 1950          |
|    nu                   | 9.12          |
|    nu_loss              | -0.0471       |
|    policy_gradient_loss | -5.58e-05     |
|    reward_explained_... | -1.19e+12     |
|    reward_value_loss    | 4.93e+06      |
|    total_cost           | 53.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00605       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 514           |
|    ep_len_mean          | 19.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 927           |
|    iterations           | 197           |
|    time_elapsed         | 2175          |
|    total_timesteps      | 2017280       |
| train/                  |               |
|    approx_kl            | 0.0019448254  |
|    average_cost         | 0.005078125   |
|    clip_fraction        | 0.0148        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.91         |
|    cost_value_loss      | 0.023         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.248        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.54e+06      |
|    mean_cost_advantages | -0.0063893264 |
|    mean_reward_advan... | 1953.0625     |
|    n_updates            | 1960          |
|    nu                   | 9.13          |
|    nu_loss              | -0.0463       |
|    policy_gradient_loss | -0.000175     |
|    reward_explained_... | -1.18e+12     |
|    reward_value_loss    | 4.88e+06      |
|    total_cost           | 52.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00576      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 506          |
|    ep_len_mean          | 20           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 198          |
|    time_elapsed         | 2185         |
|    total_timesteps      | 2027520      |
| train/                  |              |
|    approx_kl            | 0.001774227  |
|    average_cost         | 0.0060546873 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.91        |
|    cost_value_loss      | 0.0451       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.246       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.55e+06     |
|    mean_cost_advantages | 0.005683872  |
|    mean_reward_advan... | 1949.4238    |
|    n_updates            | 1970         |
|    nu                   | 9.15         |
|    nu_loss              | -0.0553      |
|    policy_gradient_loss | -0.000595    |
|    reward_explained_... | -4.58e+12    |
|    reward_value_loss    | 4.83e+06     |
|    total_cost           | 62.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00449      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 507          |
|    ep_len_mean          | 20.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 199          |
|    time_elapsed         | 2198         |
|    total_timesteps      | 2037760      |
| train/                  |              |
|    approx_kl            | 0.0007857297 |
|    average_cost         | 0.005761719  |
|    clip_fraction        | 0.00163      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.3         |
|    cost_value_loss      | 0.0383       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.245       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.67e+06     |
|    mean_cost_advantages | -0.002153603 |
|    mean_reward_advan... | 1906.3483    |
|    n_updates            | 1980         |
|    nu                   | 9.16         |
|    nu_loss              | -0.0527      |
|    policy_gradient_loss | -0.000138    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 4.68e+06     |
|    total_cost           | 59.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00645      |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | 511          |
|    ep_len_mean          | 19.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 200          |
|    time_elapsed         | 2208         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.004740367  |
|    average_cost         | 0.0044921874 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.23        |
|    cost_value_loss      | 0.0267       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.244       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.92e+06     |
|    mean_cost_advantages | -0.014299834 |
|    mean_reward_advan... | 1891.5258    |
|    n_updates            | 1990         |
|    nu                   | 9.18         |
|    nu_loss              | -0.0412      |
|    policy_gradient_loss | -0.000514    |
|    reward_explained_... | -4.61e+16    |
|    reward_value_loss    | 4.59e+06     |
|    total_cost           | 46.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00479      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 506          |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 201          |
|    time_elapsed         | 2219         |
|    total_timesteps      | 2058240      |
| train/                  |              |
|    approx_kl            | 0.0012822169 |
|    average_cost         | 0.0064453124 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.62        |
|    cost_value_loss      | 0.0446       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.24        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.15e+06     |
|    mean_cost_advantages | 0.010345249  |
|    mean_reward_advan... | 1882.8512    |
|    n_updates            | 2000         |
|    nu                   | 9.19         |
|    nu_loss              | -0.0591      |
|    policy_gradient_loss | -0.00025     |
|    reward_explained_... | nan          |
|    reward_value_loss    | 4.55e+06     |
|    total_cost           | 66.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00488       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 516           |
|    ep_len_mean          | 19.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 927           |
|    iterations           | 202           |
|    time_elapsed         | 2229          |
|    total_timesteps      | 2068480       |
| train/                  |               |
|    approx_kl            | 0.002073478   |
|    average_cost         | 0.0047851563  |
|    clip_fraction        | 0.0256        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.09         |
|    cost_value_loss      | 0.0304        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.241        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.14e+06      |
|    mean_cost_advantages | -0.0104777245 |
|    mean_reward_advan... | 1841.4277     |
|    n_updates            | 2010          |
|    nu                   | 9.21          |
|    nu_loss              | -0.044        |
|    policy_gradient_loss | -0.0003       |
|    reward_explained_... | -4.51e+12     |
|    reward_value_loss    | 4.41e+06      |
|    total_cost           | 49.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 20.2           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00371        |
| infos/                  |                |
|    cost                 | 0.01           |
| rollout/                |                |
|    adjusted_reward      | 518            |
|    ep_len_mean          | 19.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 927            |
|    iterations           | 203            |
|    time_elapsed         | 2240           |
|    total_timesteps      | 2078720        |
| train/                  |                |
|    approx_kl            | 0.00041309622  |
|    average_cost         | 0.0048828125   |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    cost_explained_va... | -4.42          |
|    cost_value_loss      | 0.0495         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.24          |
|    learning_rate        | 0.0003         |
|    loss                 | 1.97e+06       |
|    mean_cost_advantages | -0.00017994875 |
|    mean_reward_advan... | 1849.3962      |
|    n_updates            | 2020           |
|    nu                   | 9.22           |
|    nu_loss              | -0.045         |
|    policy_gradient_loss | -9.28e-05      |
|    reward_explained_... | -1.08e+12      |
|    reward_value_loss    | 4.4e+06        |
|    total_cost           | 50.0           |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00596      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 518          |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 928          |
|    iterations           | 204          |
|    time_elapsed         | 2250         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0020839046 |
|    average_cost         | 0.0037109375 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.95        |
|    cost_value_loss      | 0.0173       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.236       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.02e+06     |
|    mean_cost_advantages | -0.009971054 |
|    mean_reward_advan... | 1834.3398    |
|    n_updates            | 2030         |
|    nu                   | 9.23         |
|    nu_loss              | -0.0342      |
|    policy_gradient_loss | -0.000202    |
|    reward_explained_... | -1.08e+12    |
|    reward_value_loss    | 4.34e+06     |
|    total_cost           | 38.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00459      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 512          |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 928          |
|    iterations           | 205          |
|    time_elapsed         | 2260         |
|    total_timesteps      | 2099200      |
| train/                  |              |
|    approx_kl            | 0.0065760934 |
|    average_cost         | 0.005957031  |
|    clip_fraction        | 0.0734       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -13.8        |
|    cost_value_loss      | 0.139        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.231       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.22e+06     |
|    mean_cost_advantages | 0.025196513  |
|    mean_reward_advan... | 1812.8334    |
|    n_updates            | 2040         |
|    nu                   | 9.25         |
|    nu_loss              | -0.055       |
|    policy_gradient_loss | -0.0019      |
|    reward_explained_... | -4.26e+12    |
|    reward_value_loss    | 4.25e+06     |
|    total_cost           | 61.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00605       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 518           |
|    ep_len_mean          | 19.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 928           |
|    iterations           | 206           |
|    time_elapsed         | 2271          |
|    total_timesteps      | 2109440       |
| train/                  |               |
|    approx_kl            | 0.0012579702  |
|    average_cost         | 0.0045898436  |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.71         |
|    cost_value_loss      | 0.0282        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.228        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.06e+06      |
|    mean_cost_advantages | -0.0036143183 |
|    mean_reward_advan... | 1779.096      |
|    n_updates            | 2050          |
|    nu                   | 9.26          |
|    nu_loss              | -0.0424       |
|    policy_gradient_loss | -0.000202     |
|    reward_explained_... | -4.22e+12     |
|    reward_value_loss    | 4.12e+06      |
|    total_cost           | 47.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00371      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 508          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 929          |
|    iterations           | 207          |
|    time_elapsed         | 2281         |
|    total_timesteps      | 2119680      |
| train/                  |              |
|    approx_kl            | 0.0050538047 |
|    average_cost         | 0.0060546873 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.42        |
|    cost_value_loss      | 0.0644       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.226       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.77e+06     |
|    mean_cost_advantages | 0.010927928  |
|    mean_reward_advan... | 1776.9553    |
|    n_updates            | 2060         |
|    nu                   | 9.28         |
|    nu_loss              | -0.0561      |
|    policy_gradient_loss | -0.000624    |
|    reward_explained_... | -1.03e+12    |
|    reward_value_loss    | 4.08e+06     |
|    total_cost           | 62.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00508      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 518          |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 929          |
|    iterations           | 208          |
|    time_elapsed         | 2291         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0039627277 |
|    average_cost         | 0.0037109375 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.57        |
|    cost_value_loss      | 0.0183       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.227       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.82e+06     |
|    mean_cost_advantages | -0.015975252 |
|    mean_reward_advan... | 1730.0443    |
|    n_updates            | 2070         |
|    nu                   | 9.29         |
|    nu_loss              | -0.0344      |
|    policy_gradient_loss | -0.000519    |
|    reward_explained_... | -1.04e+12    |
|    reward_value_loss    | 3.93e+06     |
|    total_cost           | 38.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00537       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 507           |
|    ep_len_mean          | 20.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 930           |
|    iterations           | 209           |
|    time_elapsed         | 2300          |
|    total_timesteps      | 2140160       |
| train/                  |               |
|    approx_kl            | 0.00085957395 |
|    average_cost         | 0.005078125   |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.78         |
|    cost_value_loss      | 0.0308        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.229        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.9e+06       |
|    mean_cost_advantages | 0.003783631   |
|    mean_reward_advan... | 1735.1248     |
|    n_updates            | 2080          |
|    nu                   | 9.3           |
|    nu_loss              | -0.0472       |
|    policy_gradient_loss | -0.000164     |
|    reward_explained_... | -1.01e+12     |
|    reward_value_loss    | 3.92e+06      |
|    total_cost           | 52.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00254      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 513          |
|    ep_len_mean          | 19.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 930          |
|    iterations           | 210          |
|    time_elapsed         | 2311         |
|    total_timesteps      | 2150400      |
| train/                  |              |
|    approx_kl            | 0.0006676734 |
|    average_cost         | 0.0053710938 |
|    clip_fraction        | 0.00394      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.89        |
|    cost_value_loss      | 0.0482       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.229       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.88e+06     |
|    mean_cost_advantages | 0.008320942  |
|    mean_reward_advan... | 1684.652     |
|    n_updates            | 2090         |
|    nu                   | 9.31         |
|    nu_loss              | -0.05        |
|    policy_gradient_loss | -0.000163    |
|    reward_explained_... | -4.09e+12    |
|    reward_value_loss    | 3.76e+06     |
|    total_cost           | 55.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00186      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 517          |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 930          |
|    iterations           | 211          |
|    time_elapsed         | 2321         |
|    total_timesteps      | 2160640      |
| train/                  |              |
|    approx_kl            | 2.284433e-05 |
|    average_cost         | 0.0025390624 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.23        |
|    cost_value_loss      | 0.0186       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.226       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.07e+06     |
|    mean_cost_advantages | -0.013943252 |
|    mean_reward_advan... | 1678.1002    |
|    n_updates            | 2100         |
|    nu                   | 9.33         |
|    nu_loss              | -0.0236      |
|    policy_gradient_loss | -5.44e-05    |
|    reward_explained_... | -4.01e+12    |
|    reward_value_loss    | 3.72e+06     |
|    total_cost           | 26.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00215      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 519          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 212          |
|    time_elapsed         | 2331         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0008094323 |
|    average_cost         | 0.0018554687 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.66        |
|    cost_value_loss      | 0.01         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.219       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.79e+06     |
|    mean_cost_advantages | -0.003771749 |
|    mean_reward_advan... | 1667.861     |
|    n_updates            | 2110         |
|    nu                   | 9.34         |
|    nu_loss              | -0.0173      |
|    policy_gradient_loss | -0.000138    |
|    reward_explained_... | -9.8e+11     |
|    reward_value_loss    | 3.67e+06     |
|    total_cost           | 19.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00342      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 516          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 213          |
|    time_elapsed         | 2341         |
|    total_timesteps      | 2181120      |
| train/                  |              |
|    approx_kl            | 0.0024272061 |
|    average_cost         | 0.0021484375 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3           |
|    cost_value_loss      | 0.0127       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.215       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.49e+06     |
|    mean_cost_advantages | 0.0005288073 |
|    mean_reward_advan... | 1653.8395    |
|    n_updates            | 2120         |
|    nu                   | 9.35         |
|    nu_loss              | -0.0201      |
|    policy_gradient_loss | -0.000229    |
|    reward_explained_... | -9.64e+11    |
|    reward_value_loss    | 3.6e+06      |
|    total_cost           | 22.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00322      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 516          |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 214          |
|    time_elapsed         | 2351         |
|    total_timesteps      | 2191360      |
| train/                  |              |
|    approx_kl            | 0.0028774806 |
|    average_cost         | 0.0034179688 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.29        |
|    cost_value_loss      | 0.0338       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.218       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.1e+06      |
|    mean_cost_advantages | 0.011990765  |
|    mean_reward_advan... | 1625.709     |
|    n_updates            | 2130         |
|    nu                   | 9.36         |
|    nu_loss              | -0.032       |
|    policy_gradient_loss | -0.000559    |
|    reward_explained_... | -3.85e+12    |
|    reward_value_loss    | 3.51e+06     |
|    total_cost           | 35.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00322       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 511           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 932           |
|    iterations           | 215           |
|    time_elapsed         | 2362          |
|    total_timesteps      | 2201600       |
| train/                  |               |
|    approx_kl            | 0.000599903   |
|    average_cost         | 0.0032226562  |
|    clip_fraction        | 0.0181        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.83         |
|    cost_value_loss      | 0.0519        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.219        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.04e+06      |
|    mean_cost_advantages | -0.0026838523 |
|    mean_reward_advan... | 1603.9319     |
|    n_updates            | 2140          |
|    nu                   | 9.37          |
|    nu_loss              | -0.0302       |
|    policy_gradient_loss | -0.000346     |
|    reward_explained_... | -3.8e+12      |
|    reward_value_loss    | 3.43e+06      |
|    total_cost           | 33.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00234      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 518          |
|    ep_len_mean          | 19.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 931          |
|    iterations           | 216          |
|    time_elapsed         | 2373         |
|    total_timesteps      | 2211840      |
| train/                  |              |
|    approx_kl            | 0.0008078022 |
|    average_cost         | 0.0032226562 |
|    clip_fraction        | 0.00907      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.81        |
|    cost_value_loss      | 0.0248       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.217       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.62e+06     |
|    mean_cost_advantages | 0.006387043  |
|    mean_reward_advan... | 1571.8818    |
|    n_updates            | 2150         |
|    nu                   | 9.38         |
|    nu_loss              | -0.0302      |
|    policy_gradient_loss | -0.000151    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 3.33e+06     |
|    total_cost           | 33.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 514           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 932           |
|    iterations           | 217           |
|    time_elapsed         | 2383          |
|    total_timesteps      | 2222080       |
| train/                  |               |
|    approx_kl            | 0.001495091   |
|    average_cost         | 0.00234375    |
|    clip_fraction        | 0.0106        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.17         |
|    cost_value_loss      | 0.0269        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.215        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.34e+06      |
|    mean_cost_advantages | -0.0019236967 |
|    mean_reward_advan... | 1569.5739     |
|    n_updates            | 2160          |
|    nu                   | 9.39          |
|    nu_loss              | -0.022        |
|    policy_gradient_loss | -0.00021      |
|    reward_explained_... | -3.7e+12      |
|    reward_value_loss    | 3.3e+06       |
|    total_cost           | 24.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00234      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 514          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 932          |
|    iterations           | 218          |
|    time_elapsed         | 2394         |
|    total_timesteps      | 2232320      |
| train/                  |              |
|    approx_kl            | 0.0014412173 |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.8         |
|    cost_value_loss      | 0.0118       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.213       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.49e+06     |
|    mean_cost_advantages | -0.005400955 |
|    mean_reward_advan... | 1536.5459    |
|    n_updates            | 2170         |
|    nu                   | 9.4          |
|    nu_loss              | -0.0183      |
|    policy_gradient_loss | -0.000142    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 3.19e+06     |
|    total_cost           | 20.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00254      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 520          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 932          |
|    iterations           | 219          |
|    time_elapsed         | 2404         |
|    total_timesteps      | 2242560      |
| train/                  |              |
|    approx_kl            | 0.0019663866 |
|    average_cost         | 0.00234375   |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.03        |
|    cost_value_loss      | 0.0192       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.213       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.73e+06     |
|    mean_cost_advantages | 0.0011377425 |
|    mean_reward_advan... | 1513.5973    |
|    n_updates            | 2180         |
|    nu                   | 9.41         |
|    nu_loss              | -0.022       |
|    policy_gradient_loss | -0.000125    |
|    reward_explained_... | -3.67e+12    |
|    reward_value_loss    | 3.12e+06     |
|    total_cost           | 24.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00176       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 514           |
|    ep_len_mean          | 19.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 933           |
|    iterations           | 220           |
|    time_elapsed         | 2414          |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 0.00010695293 |
|    average_cost         | 0.0025390624  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.35         |
|    cost_value_loss      | 0.023         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.207        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.46e+06      |
|    mean_cost_advantages | -0.0034608368 |
|    mean_reward_advan... | 1510.5344     |
|    n_updates            | 2190          |
|    nu                   | 9.42          |
|    nu_loss              | -0.0239       |
|    policy_gradient_loss | -6.3e-05      |
|    reward_explained_... | -8.93e+11     |
|    reward_value_loss    | 3.09e+06      |
|    total_cost           | 26.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 511           |
|    ep_len_mean          | 19.8          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 933           |
|    iterations           | 221           |
|    time_elapsed         | 2423          |
|    total_timesteps      | 2263040       |
| train/                  |               |
|    approx_kl            | 0.0006492007  |
|    average_cost         | 0.0017578125  |
|    clip_fraction        | 0.00774       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.67         |
|    cost_value_loss      | 0.0228        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.214        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.38e+06      |
|    mean_cost_advantages | -0.0016969105 |
|    mean_reward_advan... | 1476.9479     |
|    n_updates            | 2200          |
|    nu                   | 9.42          |
|    nu_loss              | -0.0166       |
|    policy_gradient_loss | -0.000116     |
|    reward_explained_... | -8.86e+11     |
|    reward_value_loss    | 2.98e+06      |
|    total_cost           | 18.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00254       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 526           |
|    ep_len_mean          | 19.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 933           |
|    iterations           | 222           |
|    time_elapsed         | 2434          |
|    total_timesteps      | 2273280       |
| train/                  |               |
|    approx_kl            | 0.0026091312  |
|    average_cost         | 0.0012695312  |
|    clip_fraction        | 0.0302        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.32         |
|    cost_value_loss      | 0.0101        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.206        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.2e+06       |
|    mean_cost_advantages | -0.0024592294 |
|    mean_reward_advan... | 1449.8718     |
|    n_updates            | 2210          |
|    nu                   | 9.43          |
|    nu_loss              | -0.012        |
|    policy_gradient_loss | -0.000251     |
|    reward_explained_... | -3.92e+11     |
|    reward_value_loss    | 2.9e+06       |
|    total_cost           | 13.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00107      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 515          |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 223          |
|    time_elapsed         | 2444         |
|    total_timesteps      | 2283520      |
| train/                  |              |
|    approx_kl            | 0.0021440987 |
|    average_cost         | 0.0025390624 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.89        |
|    cost_value_loss      | 0.0268       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.204       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.51e+06     |
|    mean_cost_advantages | 0.0066503705 |
|    mean_reward_advan... | 1462.8512    |
|    n_updates            | 2220         |
|    nu                   | 9.44         |
|    nu_loss              | -0.0239      |
|    policy_gradient_loss | -0.000367    |
|    reward_explained_... | -2.12e+11    |
|    reward_value_loss    | 2.91e+06     |
|    total_cost           | 26.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 517           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 224           |
|    time_elapsed         | 2454          |
|    total_timesteps      | 2293760       |
| train/                  |               |
|    approx_kl            | 0.00026787614 |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 9.77e-06      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.34         |
|    cost_value_loss      | 0.0062        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.202        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.67e+06      |
|    mean_cost_advantages | -0.008697881  |
|    mean_reward_advan... | 1416.86       |
|    n_updates            | 2230          |
|    nu                   | 9.45          |
|    nu_loss              | -0.0101       |
|    policy_gradient_loss | -2.53e-05     |
|    reward_explained_... | -8.59e+11     |
|    reward_value_loss    | 2.78e+06      |
|    total_cost           | 11.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 22.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00234       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 514           |
|    ep_len_mean          | 18.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 225           |
|    time_elapsed         | 2464          |
|    total_timesteps      | 2304000       |
| train/                  |               |
|    approx_kl            | -0.0002177183 |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.000674      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.61         |
|    cost_value_loss      | 0.0213        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.2          |
|    learning_rate        | 0.0003        |
|    loss                 | 1.44e+06      |
|    mean_cost_advantages | 0.005285727   |
|    mean_reward_advan... | 1400.8489     |
|    n_updates            | 2240          |
|    nu                   | 9.45          |
|    nu_loss              | -0.0185       |
|    policy_gradient_loss | -4.89e-05     |
|    reward_explained_... | -2.1e+11      |
|    reward_value_loss    | 2.72e+06      |
|    total_cost           | 20.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 510           |
|    ep_len_mean          | 19.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 226           |
|    time_elapsed         | 2474          |
|    total_timesteps      | 2314240       |
| train/                  |               |
|    approx_kl            | 4.2168052e-05 |
|    average_cost         | 0.00234375    |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.96         |
|    cost_value_loss      | 0.023         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.203        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.17e+06      |
|    mean_cost_advantages | 0.0019001266  |
|    mean_reward_advan... | 1373.9083     |
|    n_updates            | 2250          |
|    nu                   | 9.46          |
|    nu_loss              | -0.0222       |
|    policy_gradient_loss | -5.2e-05      |
|    reward_explained_... | -3.72e+11     |
|    reward_value_loss    | 2.64e+06      |
|    total_cost           | 24.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 519           |
|    ep_len_mean          | 19            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 227           |
|    time_elapsed         | 2484          |
|    total_timesteps      | 2324480       |
| train/                  |               |
|    approx_kl            | 0.0007895778  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.67         |
|    cost_value_loss      | 0.0076        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.204        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.25e+06      |
|    mean_cost_advantages | -0.008810317  |
|    mean_reward_advan... | 1344.6609     |
|    n_updates            | 2260          |
|    nu                   | 9.47          |
|    nu_loss              | -0.00647      |
|    policy_gradient_loss | -1.22e-05     |
|    reward_explained_... | -8.32e+11     |
|    reward_value_loss    | 2.56e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 518           |
|    ep_len_mean          | 19.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 228           |
|    time_elapsed         | 2495          |
|    total_timesteps      | 2334720       |
| train/                  |               |
|    approx_kl            | 0.001183162   |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0.0226        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.16         |
|    cost_value_loss      | 0.013         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.197        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.63e+06      |
|    mean_cost_advantages | 0.00093828765 |
|    mean_reward_advan... | 1345.9987     |
|    n_updates            | 2270          |
|    nu                   | 9.47          |
|    nu_loss              | -0.0139       |
|    policy_gradient_loss | -0.000197     |
|    reward_explained_... | -8.06e+11     |
|    reward_value_loss    | 2.54e+06      |
|    total_cost           | 15.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00166       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 518           |
|    ep_len_mean          | 19.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 936           |
|    iterations           | 229           |
|    time_elapsed         | 2504          |
|    total_timesteps      | 2344960       |
| train/                  |               |
|    approx_kl            | 0.002167872   |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0244        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -7.23         |
|    cost_value_loss      | 0.00567       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.198        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.85e+05      |
|    mean_cost_advantages | -0.0024019617 |
|    mean_reward_advan... | 1318.9209     |
|    n_updates            | 2280          |
|    nu                   | 9.48          |
|    nu_loss              | -0.00925      |
|    policy_gradient_loss | -0.000193     |
|    reward_explained_... | -3.23e+12     |
|    reward_value_loss    | 2.47e+06      |
|    total_cost           | 10.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 512           |
|    ep_len_mean          | 19.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 936           |
|    iterations           | 230           |
|    time_elapsed         | 2514          |
|    total_timesteps      | 2355200       |
| train/                  |               |
|    approx_kl            | 0.0006269798  |
|    average_cost         | 0.0016601563  |
|    clip_fraction        | 0.000127      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.98         |
|    cost_value_loss      | 0.0137        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.197        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.21e+06      |
|    mean_cost_advantages | -5.881153e-05 |
|    mean_reward_advan... | 1298.5745     |
|    n_updates            | 2290          |
|    nu                   | 9.48          |
|    nu_loss              | -0.0157       |
|    policy_gradient_loss | -4.48e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 2.4e+06       |
|    total_cost           | 17.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 19.2           |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.00107        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 517            |
|    ep_len_mean          | 19.2           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 937            |
|    iterations           | 231            |
|    time_elapsed         | 2523           |
|    total_timesteps      | 2365440        |
| train/                  |                |
|    approx_kl            | 0.0013106304   |
|    average_cost         | 0.0010742188   |
|    clip_fraction        | 0.0121         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -6.26          |
|    cost_value_loss      | 0.0217         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.195         |
|    learning_rate        | 0.0003         |
|    loss                 | 1.08e+06       |
|    mean_cost_advantages | -0.00052532146 |
|    mean_reward_advan... | 1266.8611      |
|    n_updates            | 2300           |
|    nu                   | 9.49           |
|    nu_loss              | -0.0102        |
|    policy_gradient_loss | -0.000164      |
|    reward_explained_... | -3.16e+12      |
|    reward_value_loss    | 2.32e+06       |
|    total_cost           | 11.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00166       |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 517           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 937           |
|    iterations           | 232           |
|    time_elapsed         | 2534          |
|    total_timesteps      | 2375680       |
| train/                  |               |
|    approx_kl            | 0.001494652   |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0.0166        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.12         |
|    cost_value_loss      | 0.00925       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.192        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.04e+06      |
|    mean_cost_advantages | -0.0077242157 |
|    mean_reward_advan... | 1255.768      |
|    n_updates            | 2310          |
|    nu                   | 9.49          |
|    nu_loss              | -0.0102       |
|    policy_gradient_loss | -0.000107     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 2.28e+06      |
|    total_cost           | 11.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00127      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 518          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 937          |
|    iterations           | 233          |
|    time_elapsed         | 2544         |
|    total_timesteps      | 2385920      |
| train/                  |              |
|    approx_kl            | 0.0017702006 |
|    average_cost         | 0.0016601563 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.89        |
|    cost_value_loss      | 0.0205       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.193       |
|    learning_rate        | 0.0003       |
|    loss                 | 8.68e+05     |
|    mean_cost_advantages | 0.001800277  |
|    mean_reward_advan... | 1233.2316    |
|    n_updates            | 2320         |
|    nu                   | 9.5          |
|    nu_loss              | -0.0158      |
|    policy_gradient_loss | -0.000166    |
|    reward_explained_... | -3.07e+12    |
|    reward_value_loss    | 2.21e+06     |
|    total_cost           | 17.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00127      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 520          |
|    ep_len_mean          | 19.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 937          |
|    iterations           | 234          |
|    time_elapsed         | 2554         |
|    total_timesteps      | 2396160      |
| train/                  |              |
|    approx_kl            | 0.0012389651 |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.59        |
|    cost_value_loss      | 0.0108       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.189       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.28e+06     |
|    mean_cost_advantages | 0.0021617434 |
|    mean_reward_advan... | 1218.6927    |
|    n_updates            | 2330         |
|    nu                   | 9.5          |
|    nu_loss              | -0.0121      |
|    policy_gradient_loss | -0.000208    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 2.16e+06     |
|    total_cost           | 13.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00225       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 520           |
|    ep_len_mean          | 18.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 938           |
|    iterations           | 235           |
|    time_elapsed         | 2565          |
|    total_timesteps      | 2406400       |
| train/                  |               |
|    approx_kl            | 0.0015693028  |
|    average_cost         | 0.0012695312  |
|    clip_fraction        | 0.00114       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.95         |
|    cost_value_loss      | 0.0106        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.189        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.12e+06      |
|    mean_cost_advantages | -0.0011626728 |
|    mean_reward_advan... | 1202.008      |
|    n_updates            | 2340          |
|    nu                   | 9.51          |
|    nu_loss              | -0.0121       |
|    policy_gradient_loss | -3.87e-05     |
|    reward_explained_... | -2.94e+12     |
|    reward_value_loss    | 2.11e+06      |
|    total_cost           | 13.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 520           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 937           |
|    iterations           | 236           |
|    time_elapsed         | 2576          |
|    total_timesteps      | 2416640       |
| train/                  |               |
|    approx_kl            | 0.00064811145 |
|    average_cost         | 0.0022460937  |
|    clip_fraction        | 0.00697       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.01         |
|    cost_value_loss      | 0.024         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.187        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.07e+06      |
|    mean_cost_advantages | 0.010355781   |
|    mean_reward_advan... | 1182.0195     |
|    n_updates            | 2350          |
|    nu                   | 9.51          |
|    nu_loss              | -0.0214       |
|    policy_gradient_loss | -0.00015      |
|    reward_explained_... | -7.3e+11      |
|    reward_value_loss    | 2.06e+06      |
|    total_cost           | 23.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000781     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 513          |
|    ep_len_mean          | 19.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 237          |
|    time_elapsed         | 2587         |
|    total_timesteps      | 2426880      |
| train/                  |              |
|    approx_kl            | 0.0009819744 |
|    average_cost         | 0.0013671875 |
|    clip_fraction        | 0.00207      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.58        |
|    cost_value_loss      | 0.0137       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.191       |
|    learning_rate        | 0.0003       |
|    loss                 | 9.36e+05     |
|    mean_cost_advantages | -0.004171776 |
|    mean_reward_advan... | 1158.6559    |
|    n_updates            | 2360         |
|    nu                   | 9.52         |
|    nu_loss              | -0.013       |
|    policy_gradient_loss | -4.37e-05    |
|    reward_explained_... | -7.22e+11    |
|    reward_value_loss    | 2e+06        |
|    total_cost           | 14.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00205      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | 515          |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 238          |
|    time_elapsed         | 2597         |
|    total_timesteps      | 2437120      |
| train/                  |              |
|    approx_kl            | 0.0025720268 |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.84        |
|    cost_value_loss      | 0.00421      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.189       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02e+06     |
|    mean_cost_advantages | -0.004965876 |
|    mean_reward_advan... | 1123.3967    |
|    n_updates            | 2370         |
|    nu                   | 9.52         |
|    nu_loss              | -0.00744     |
|    policy_gradient_loss | -0.000215    |
|    reward_explained_... | -3.2e+11     |
|    reward_value_loss    | 1.92e+06     |
|    total_cost           | 8.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00156      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 526          |
|    ep_len_mean          | 18.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 239          |
|    time_elapsed         | 2607         |
|    total_timesteps      | 2447360      |
| train/                  |              |
|    approx_kl            | 0.0011112762 |
|    average_cost         | 0.0020507812 |
|    clip_fraction        | 0.00717      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.64        |
|    cost_value_loss      | 0.035        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.19        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.04e+06     |
|    mean_cost_advantages | 0.0110005345 |
|    mean_reward_advan... | 1107.3806    |
|    n_updates            | 2380         |
|    nu                   | 9.53         |
|    nu_loss              | -0.0195      |
|    policy_gradient_loss | -9.12e-05    |
|    reward_explained_... | -1.78e+11    |
|    reward_value_loss    | 1.87e+06     |
|    total_cost           | 21.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00146       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 520           |
|    ep_len_mean          | 19.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 938           |
|    iterations           | 240           |
|    time_elapsed         | 2618          |
|    total_timesteps      | 2457600       |
| train/                  |               |
|    approx_kl            | 0.00044078525 |
|    average_cost         | 0.0015625     |
|    clip_fraction        | 0.000908      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.93         |
|    cost_value_loss      | 0.0342        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.185        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.79e+05      |
|    mean_cost_advantages | -0.0011296903 |
|    mean_reward_advan... | 1108.8756     |
|    n_updates            | 2390          |
|    nu                   | 9.53          |
|    nu_loss              | -0.0149       |
|    policy_gradient_loss | -5.37e-05     |
|    reward_explained_... | -1.1e+11      |
|    reward_value_loss    | 1.85e+06      |
|    total_cost           | 16.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 520          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 241          |
|    time_elapsed         | 2628         |
|    total_timesteps      | 2467840      |
| train/                  |              |
|    approx_kl            | 0.0015083144 |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.79        |
|    cost_value_loss      | 0.0128       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.18        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.05e+06     |
|    mean_cost_advantages | -0.007630094 |
|    mean_reward_advan... | 1076.8174    |
|    n_updates            | 2400         |
|    nu                   | 9.53         |
|    nu_loss              | -0.014       |
|    policy_gradient_loss | -0.000211    |
|    reward_explained_... | -1.71e+11    |
|    reward_value_loss    | 1.78e+06     |
|    total_cost           | 15.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 525           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 939           |
|    iterations           | 242           |
|    time_elapsed         | 2638          |
|    total_timesteps      | 2478080       |
| train/                  |               |
|    approx_kl            | 0.0007824069  |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0.00287       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.18         |
|    cost_value_loss      | 0.0138        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.179        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.43e+05      |
|    mean_cost_advantages | -0.0027897398 |
|    mean_reward_advan... | 1058.9933     |
|    n_updates            | 2410          |
|    nu                   | 9.54          |
|    nu_loss              | -0.014        |
|    policy_gradient_loss | -8.25e-05     |
|    reward_explained_... | -2.98e+11     |
|    reward_value_loss    | 1.73e+06      |
|    total_cost           | 15.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00127      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 521          |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 243          |
|    time_elapsed         | 2649         |
|    total_timesteps      | 2488320      |
| train/                  |              |
|    approx_kl            | 0.0013967227 |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.00885      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.19        |
|    cost_value_loss      | 0.00834      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.173       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.43e+05     |
|    mean_cost_advantages | -0.008153634 |
|    mean_reward_advan... | 1047.5797    |
|    n_updates            | 2420         |
|    nu                   | 9.54         |
|    nu_loss              | -0.00932     |
|    policy_gradient_loss | -6.97e-05    |
|    reward_explained_... | -6.58e+11    |
|    reward_value_loss    | 1.69e+06     |
|    total_cost           | 10.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 21.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00186      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 521          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 244          |
|    time_elapsed         | 2659         |
|    total_timesteps      | 2498560      |
| train/                  |              |
|    approx_kl            | 0.0015229569 |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.88        |
|    cost_value_loss      | 0.0184       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.175       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.05e+05     |
|    mean_cost_advantages | 0.002710388  |
|    mean_reward_advan... | 1016.5126    |
|    n_updates            | 2430         |
|    nu                   | 9.55         |
|    nu_loss              | -0.0121      |
|    policy_gradient_loss | -0.000216    |
|    reward_explained_... | -6.58e+11    |
|    reward_value_loss    | 1.63e+06     |
|    total_cost           | 13.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00117       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 523           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 939           |
|    iterations           | 245           |
|    time_elapsed         | 2669          |
|    total_timesteps      | 2508800       |
| train/                  |               |
|    approx_kl            | 0.00034061805 |
|    average_cost         | 0.0018554687  |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.85         |
|    cost_value_loss      | 0.0225        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.174        |
|    learning_rate        | 0.0003        |
|    loss                 | 9.18e+05      |
|    mean_cost_advantages | 0.0071046934  |
|    mean_reward_advan... | 996.67596     |
|    n_updates            | 2440          |
|    nu                   | 9.55          |
|    nu_loss              | -0.0177       |
|    policy_gradient_loss | -4.96e-05     |
|    reward_explained_... | -2.58e+12     |
|    reward_value_loss    | 1.58e+06      |
|    total_cost           | 19.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 518           |
|    ep_len_mean          | 19.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 940           |
|    iterations           | 246           |
|    time_elapsed         | 2679          |
|    total_timesteps      | 2519040       |
| train/                  |               |
|    approx_kl            | 0.0015558059  |
|    average_cost         | 0.001171875   |
|    clip_fraction        | 0.00504       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.87         |
|    cost_value_loss      | 0.00804       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.176        |
|    learning_rate        | 0.0003        |
|    loss                 | 8.25e+05      |
|    mean_cost_advantages | -0.0067035602 |
|    mean_reward_advan... | 981.0656      |
|    n_updates            | 2450          |
|    nu                   | 9.56          |
|    nu_loss              | -0.0112       |
|    policy_gradient_loss | -8.67e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.54e+06      |
|    total_cost           | 12.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 520           |
|    ep_len_mean          | 18.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 940           |
|    iterations           | 247           |
|    time_elapsed         | 2688          |
|    total_timesteps      | 2529280       |
| train/                  |               |
|    approx_kl            | 0.0016708585  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0131        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.16         |
|    cost_value_loss      | 0.0199        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.178        |
|    learning_rate        | 0.0003        |
|    loss                 | 6.06e+05      |
|    mean_cost_advantages | -0.0018534705 |
|    mean_reward_advan... | 949.1769      |
|    n_updates            | 2460          |
|    nu                   | 9.56          |
|    nu_loss              | -0.00933      |
|    policy_gradient_loss | -0.000146     |
|    reward_explained_... | -2.52e+12     |
|    reward_value_loss    | 1.47e+06      |
|    total_cost           | 10.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00127      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 524          |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 248          |
|    time_elapsed         | 2698         |
|    total_timesteps      | 2539520      |
| train/                  |              |
|    approx_kl            | 0.0056746635 |
|    average_cost         | 0.001953125  |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.9         |
|    cost_value_loss      | 0.0319       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.17        |
|    learning_rate        | 0.0003       |
|    loss                 | 7.02e+05     |
|    mean_cost_advantages | 0.0061229668 |
|    mean_reward_advan... | 933.38635    |
|    n_updates            | 2470         |
|    nu                   | 9.56         |
|    nu_loss              | -0.0187      |
|    policy_gradient_loss | -0.000751    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 1.43e+06     |
|    total_cost           | 20.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 516           |
|    ep_len_mean          | 19.8          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 249           |
|    time_elapsed         | 2708          |
|    total_timesteps      | 2549760       |
| train/                  |               |
|    approx_kl            | -5.574147e-05 |
|    average_cost         | 0.0012695312  |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.24         |
|    cost_value_loss      | 0.0095        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.173        |
|    learning_rate        | 0.0003        |
|    loss                 | 7.17e+05      |
|    mean_cost_advantages | 0.0020466982  |
|    mean_reward_advan... | 922.35284     |
|    n_updates            | 2480          |
|    nu                   | 9.57          |
|    nu_loss              | -0.0121       |
|    policy_gradient_loss | -3.32e-05     |
|    reward_explained_... | -2.41e+12     |
|    reward_value_loss    | 1.4e+06       |
|    total_cost           | 13.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 524           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 250           |
|    time_elapsed         | 2719          |
|    total_timesteps      | 2560000       |
| train/                  |               |
|    approx_kl            | 0.0010189969  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.016         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.27         |
|    cost_value_loss      | 0.0105        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.169        |
|    learning_rate        | 0.0003        |
|    loss                 | 7.29e+05      |
|    mean_cost_advantages | -0.0028337303 |
|    mean_reward_advan... | 884.8434      |
|    n_updates            | 2490          |
|    nu                   | 9.57          |
|    nu_loss              | -0.00934      |
|    policy_gradient_loss | -0.000122     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 1.34e+06      |
|    total_cost           | 10.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 523          |
|    ep_len_mean          | 18.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 251          |
|    time_elapsed         | 2729         |
|    total_timesteps      | 2570240      |
| train/                  |              |
|    approx_kl            | 0.0005125367 |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.24        |
|    cost_value_loss      | 0.0175       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.171       |
|    learning_rate        | 0.0003       |
|    loss                 | 6.56e+05     |
|    mean_cost_advantages | -0.002004955 |
|    mean_reward_advan... | 877.7874     |
|    n_updates            | 2500         |
|    nu                   | 9.57         |
|    nu_loss              | -0.0122      |
|    policy_gradient_loss | -0.000176    |
|    reward_explained_... | -2.36e+12    |
|    reward_value_loss    | 1.31e+06     |
|    total_cost           | 13.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 524          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 942          |
|    iterations           | 252          |
|    time_elapsed         | 2739         |
|    total_timesteps      | 2580480      |
| train/                  |              |
|    approx_kl            | 0.0017556412 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.5         |
|    cost_value_loss      | 0.00228      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.168       |
|    learning_rate        | 0.0003       |
|    loss                 | 7.48e+05     |
|    mean_cost_advantages | -0.009291837 |
|    mean_reward_advan... | 854.82697    |
|    n_updates            | 2510         |
|    nu                   | 9.58         |
|    nu_loss              | -0.00374     |
|    policy_gradient_loss | -0.000107    |
|    reward_explained_... | -5.85e+11    |
|    reward_value_loss    | 1.26e+06     |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 524           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 942           |
|    iterations           | 253           |
|    time_elapsed         | 2749          |
|    total_timesteps      | 2590720       |
| train/                  |               |
|    approx_kl            | 0.00033286834 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.52         |
|    cost_value_loss      | 0.00174       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.167        |
|    learning_rate        | 0.0003        |
|    loss                 | 6.38e+05      |
|    mean_cost_advantages | -0.0034689754 |
|    mean_reward_advan... | 835.14856     |
|    n_updates            | 2520          |
|    nu                   | 9.58          |
|    nu_loss              | -0.00281      |
|    policy_gradient_loss | -1.38e-05     |
|    reward_explained_... | -5.77e+11     |
|    reward_value_loss    | 1.22e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 526          |
|    ep_len_mean          | 18.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 942          |
|    iterations           | 254          |
|    time_elapsed         | 2760         |
|    total_timesteps      | 2600960      |
| train/                  |              |
|    approx_kl            | 0.0021196639 |
|    average_cost         | 0.0010742188 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.37        |
|    cost_value_loss      | 0.0151       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.168       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.36e+05     |
|    mean_cost_advantages | 0.0066482536 |
|    mean_reward_advan... | 815.49335    |
|    n_updates            | 2530         |
|    nu                   | 9.58         |
|    nu_loss              | -0.0103      |
|    policy_gradient_loss | -0.000104    |
|    reward_explained_... | -2.52e+11    |
|    reward_value_loss    | 1.18e+06     |
|    total_cost           | 11.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 519           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 255           |
|    time_elapsed         | 2772          |
|    total_timesteps      | 2611200       |
| train/                  |               |
|    approx_kl            | 0.00016440285 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.99         |
|    cost_value_loss      | 0.0128        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.165        |
|    learning_rate        | 0.0003        |
|    loss                 | 6.87e+05      |
|    mean_cost_advantages | -0.0022623346 |
|    mean_reward_advan... | 800.0749      |
|    n_updates            | 2540          |
|    nu                   | 9.59          |
|    nu_loss              | -0.00655      |
|    policy_gradient_loss | -5.14e-05     |
|    reward_explained_... | -1.38e+11     |
|    reward_value_loss    | 1.14e+06      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000293      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 525           |
|    ep_len_mean          | 19.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 942           |
|    iterations           | 256           |
|    time_elapsed         | 2782          |
|    total_timesteps      | 2621440       |
| train/                  |               |
|    approx_kl            | 0.00019615341 |
|    average_cost         | 0.0012695312  |
|    clip_fraction        | 0.00539       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.42         |
|    cost_value_loss      | 0.0127        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.166        |
|    learning_rate        | 0.0003        |
|    loss                 | 6.73e+05      |
|    mean_cost_advantages | 0.0021466692  |
|    mean_reward_advan... | 766.7339      |
|    n_updates            | 2550          |
|    nu                   | 9.59          |
|    nu_loss              | -0.0122       |
|    policy_gradient_loss | -7.74e-05     |
|    reward_explained_... | -8.85e+10     |
|    reward_value_loss    | 1.09e+06      |
|    total_cost           | 13.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000879      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 526           |
|    ep_len_mean          | 18.8          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 942           |
|    iterations           | 257           |
|    time_elapsed         | 2792          |
|    total_timesteps      | 2631680       |
| train/                  |               |
|    approx_kl            | 0.0004994081  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.000166      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.89         |
|    cost_value_loss      | 0.0016        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.159        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.95e+05      |
|    mean_cost_advantages | -0.0079855705 |
|    mean_reward_advan... | 755.74664     |
|    n_updates            | 2560          |
|    nu                   | 9.59          |
|    nu_loss              | -0.00281      |
|    policy_gradient_loss | -7.47e-06     |
|    reward_explained_... | -1.35e+11     |
|    reward_value_loss    | 1.06e+06      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 524           |
|    ep_len_mean          | 18.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 942           |
|    iterations           | 258           |
|    time_elapsed         | 2803          |
|    total_timesteps      | 2641920       |
| train/                  |               |
|    approx_kl            | 0.0019204676  |
|    average_cost         | 0.0008789062  |
|    clip_fraction        | 0.0119        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -7.21         |
|    cost_value_loss      | 0.0111        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.163        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.69e+05      |
|    mean_cost_advantages | 0.00094481965 |
|    mean_reward_advan... | 736.9839      |
|    n_updates            | 2570          |
|    nu                   | 9.6           |
|    nu_loss              | -0.00843      |
|    policy_gradient_loss | -0.000138     |
|    reward_explained_... | -2.35e+11     |
|    reward_value_loss    | 1.03e+06      |
|    total_cost           | 9.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 527           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 942           |
|    iterations           | 259           |
|    time_elapsed         | 2812          |
|    total_timesteps      | 2652160       |
| train/                  |               |
|    approx_kl            | 0.0019596354  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.0307        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -7.75         |
|    cost_value_loss      | 0.00351       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.16         |
|    learning_rate        | 0.0003        |
|    loss                 | 5.6e+05       |
|    mean_cost_advantages | -0.0025409993 |
|    mean_reward_advan... | 713.70447     |
|    n_updates            | 2580          |
|    nu                   | 9.6           |
|    nu_loss              | -0.00656      |
|    policy_gradient_loss | -0.000166     |
|    reward_explained_... | -5.23e+11     |
|    reward_value_loss    | 9.87e+05      |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 529          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 943          |
|    iterations           | 260          |
|    time_elapsed         | 2822         |
|    total_timesteps      | 2662400      |
| train/                  |              |
|    approx_kl            | 0.0007024836 |
|    average_cost         | 0.0010742188 |
|    clip_fraction        | 0.000234     |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.05        |
|    cost_value_loss      | 0.0119       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.161       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.22e+05     |
|    mean_cost_advantages | 0.0040009855 |
|    mean_reward_advan... | 697.9891     |
|    n_updates            | 2590         |
|    nu                   | 9.6          |
|    nu_loss              | -0.0103      |
|    policy_gradient_loss | -6.63e-05    |
|    reward_explained_... | -5.11e+11    |
|    reward_value_loss    | 9.53e+05     |
|    total_cost           | 11.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 20.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 523           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 261           |
|    time_elapsed         | 2833          |
|    total_timesteps      | 2672640       |
| train/                  |               |
|    approx_kl            | 0.00025656229 |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.000957      |
|    clip_range           | 0.2           |
|    cost_explained_va... | -9.68         |
|    cost_value_loss      | 0.00313       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.16         |
|    learning_rate        | 0.0003        |
|    loss                 | 4.22e+05      |
|    mean_cost_advantages | -0.0014812653 |
|    mean_reward_advan... | 681.3673      |
|    n_updates            | 2600          |
|    nu                   | 9.6           |
|    nu_loss              | -0.00563      |
|    policy_gradient_loss | -2.58e-05     |
|    reward_explained_... | -2.01e+12     |
|    reward_value_loss    | 9.22e+05      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00137       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 521           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 262           |
|    time_elapsed         | 2843          |
|    total_timesteps      | 2682880       |
| train/                  |               |
|    approx_kl            | 0.0011560146  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0122        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.08         |
|    cost_value_loss      | 0.0104        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.164        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.06e+05      |
|    mean_cost_advantages | 0.0007104894  |
|    mean_reward_advan... | 649.70795     |
|    n_updates            | 2610          |
|    nu                   | 9.61          |
|    nu_loss              | -0.00469      |
|    policy_gradient_loss | -7.56e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 8.79e+05      |
|    total_cost           | 5.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 528          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 943          |
|    iterations           | 263          |
|    time_elapsed         | 2854         |
|    total_timesteps      | 2693120      |
| train/                  |              |
|    approx_kl            | 0.0013444191 |
|    average_cost         | 0.0013671875 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.08        |
|    cost_value_loss      | 0.0193       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.158       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.66e+05     |
|    mean_cost_advantages | 0.008098326  |
|    mean_reward_advan... | 627.04       |
|    n_updates            | 2620         |
|    nu                   | 9.61         |
|    nu_loss              | -0.0131      |
|    policy_gradient_loss | -7.34e-05    |
|    reward_explained_... | -1.97e+12    |
|    reward_value_loss    | 8.44e+05     |
|    total_cost           | 14.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 520           |
|    ep_len_mean          | 19.3          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 264           |
|    time_elapsed         | 2864          |
|    total_timesteps      | 2703360       |
| train/                  |               |
|    approx_kl            | 0.00061136996 |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.00292       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.43         |
|    cost_value_loss      | 0.00159       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.159        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.31e+05      |
|    mean_cost_advantages | -0.0019331243 |
|    mean_reward_advan... | 617.4574      |
|    n_updates            | 2630          |
|    nu                   | 9.61          |
|    nu_loss              | -0.00282      |
|    policy_gradient_loss | -1.94e-05     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 8.19e+05      |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00117       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 530           |
|    ep_len_mean          | 18.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 265           |
|    time_elapsed         | 2874          |
|    total_timesteps      | 2713600       |
| train/                  |               |
|    approx_kl            | 0.0017805544  |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.00733       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -9.7          |
|    cost_value_loss      | 0.00239       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.156        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.35e+05      |
|    mean_cost_advantages | 0.00016196056 |
|    mean_reward_advan... | 581.7149      |
|    n_updates            | 2640          |
|    nu                   | 9.61          |
|    nu_loss              | -0.00375      |
|    policy_gradient_loss | -3.76e-05     |
|    reward_explained_... | -1.93e+12     |
|    reward_value_loss    | 7.8e+05       |
|    total_cost           | 4.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00107      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 521          |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 944          |
|    iterations           | 266          |
|    time_elapsed         | 2884         |
|    total_timesteps      | 2723840      |
| train/                  |              |
|    approx_kl            | 0.0009799094 |
|    average_cost         | 0.001171875  |
|    clip_fraction        | 0.00935      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.22        |
|    cost_value_loss      | 0.0102       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.152       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.93e+05     |
|    mean_cost_advantages | 0.0046194373 |
|    mean_reward_advan... | 577.8713     |
|    n_updates            | 2650         |
|    nu                   | 9.62         |
|    nu_loss              | -0.0113      |
|    policy_gradient_loss | -0.000154    |
|    reward_explained_... | nan          |
|    reward_value_loss    | 7.59e+05     |
|    total_cost           | 12.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.2         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 528          |
|    ep_len_mean          | 18.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 944          |
|    iterations           | 267          |
|    time_elapsed         | 2895         |
|    total_timesteps      | 2734080      |
| train/                  |              |
|    approx_kl            | 0.0014002371 |
|    average_cost         | 0.0010742188 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.71        |
|    cost_value_loss      | 0.0103       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.153       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.68e+05     |
|    mean_cost_advantages | 0.0021228632 |
|    mean_reward_advan... | 544.45715    |
|    n_updates            | 2660         |
|    nu                   | 9.62         |
|    nu_loss              | -0.0103      |
|    policy_gradient_loss | -0.000172    |
|    reward_explained_... | -1.85e+12    |
|    reward_value_loss    | 7.2e+05      |
|    total_cost           | 11.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 21.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 523           |
|    ep_len_mean          | 19.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 944           |
|    iterations           | 268           |
|    time_elapsed         | 2905          |
|    total_timesteps      | 2744320       |
| train/                  |               |
|    approx_kl            | 0.0007757052  |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.00494       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.4          |
|    cost_value_loss      | 0.00736       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.151        |
|    learning_rate        | 0.0003        |
|    loss                 | 3.8e+05       |
|    mean_cost_advantages | -0.0010257408 |
|    mean_reward_advan... | 533.6852      |
|    n_updates            | 2670          |
|    nu                   | 9.62          |
|    nu_loss              | -0.00564      |
|    policy_gradient_loss | -2.93e-05     |
|    reward_explained_... | -4.5e+11      |
|    reward_value_loss    | 6.98e+05      |
|    total_cost           | 6.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 527           |
|    ep_len_mean          | 18.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 944           |
|    iterations           | 269           |
|    time_elapsed         | 2916          |
|    total_timesteps      | 2754560       |
| train/                  |               |
|    approx_kl            | 0.0010255153  |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0.00688       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.74         |
|    cost_value_loss      | 0.0106        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.153        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.22e+05      |
|    mean_cost_advantages | 0.00022474825 |
|    mean_reward_advan... | 505.93433     |
|    n_updates            | 2680          |
|    nu                   | 9.62          |
|    nu_loss              | -0.00752      |
|    policy_gradient_loss | -9.58e-05     |
|    reward_explained_... | -4.44e+11     |
|    reward_value_loss    | 6.64e+05      |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 523           |
|    ep_len_mean          | 19.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 944           |
|    iterations           | 270           |
|    time_elapsed         | 2927          |
|    total_timesteps      | 2764800       |
| train/                  |               |
|    approx_kl            | 0.0009818021  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.00409       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.55         |
|    cost_value_loss      | 0.00414       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.15         |
|    learning_rate        | 0.0003        |
|    loss                 | 3.39e+05      |
|    mean_cost_advantages | 0.0022117526  |
|    mean_reward_advan... | 493.29803     |
|    n_updates            | 2690          |
|    nu                   | 9.63          |
|    nu_loss              | -0.0047       |
|    policy_gradient_loss | -4.26e-05     |
|    reward_explained_... | -4.33e+11     |
|    reward_value_loss    | 6.41e+05      |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00107       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 524           |
|    ep_len_mean          | 19            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 944           |
|    iterations           | 271           |
|    time_elapsed         | 2939          |
|    total_timesteps      | 2775040       |
| train/                  |               |
|    approx_kl            | 0.001117018   |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.0251        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.13         |
|    cost_value_loss      | 0.00628       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.148        |
|    learning_rate        | 0.0003        |
|    loss                 | 4.7e+05       |
|    mean_cost_advantages | -0.0013066422 |
|    mean_reward_advan... | 464.48282     |
|    n_updates            | 2700          |
|    nu                   | 9.63          |
|    nu_loss              | -0.00658      |
|    policy_gradient_loss | -0.000142     |
|    reward_explained_... | nan           |
|    reward_value_loss    | 6.12e+05      |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000586     |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | 531          |
|    ep_len_mean          | 18.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 943          |
|    iterations           | 272          |
|    time_elapsed         | 2951         |
|    total_timesteps      | 2785280      |
| train/                  |              |
|    approx_kl            | 0.0019611095 |
|    average_cost         | 0.0010742188 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3.33        |
|    cost_value_loss      | 0.017        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.147       |
|    learning_rate        | 0.0003       |
|    loss                 | 3.03e+05     |
|    mean_cost_advantages | 0.0009235971 |
|    mean_reward_advan... | 444.72052    |
|    n_updates            | 2710         |
|    nu                   | 9.63         |
|    nu_loss              | -0.0103      |
|    policy_gradient_loss | -0.000332    |
|    reward_explained_... | -4.23e+11    |
|    reward_value_loss    | 5.87e+05     |
|    total_cost           | 11.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 532           |
|    ep_len_mean          | 18.8          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 273           |
|    time_elapsed         | 2963          |
|    total_timesteps      | 2795520       |
| train/                  |               |
|    approx_kl            | 0.0002993699  |
|    average_cost         | 0.0005859375  |
|    clip_fraction        | 0.00187       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.11         |
|    cost_value_loss      | 0.00449       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.142        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.37e+05      |
|    mean_cost_advantages | -0.0065381215 |
|    mean_reward_advan... | 436.6817      |
|    n_updates            | 2720          |
|    nu                   | 9.63          |
|    nu_loss              | -0.00564      |
|    policy_gradient_loss | -1.69e-06     |
|    reward_explained_... | -4.08e+11     |
|    reward_value_loss    | 5.66e+05      |
|    total_cost           | 6.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00176      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 529          |
|    ep_len_mean          | 18.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 943          |
|    iterations           | 274          |
|    time_elapsed         | 2974         |
|    total_timesteps      | 2805760      |
| train/                  |              |
|    approx_kl            | 0.0006944089 |
|    average_cost         | 0.0015625    |
|    clip_fraction        | 0.00251      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.84        |
|    cost_value_loss      | 0.0191       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.144       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.68e+05     |
|    mean_cost_advantages | 0.004315196  |
|    mean_reward_advan... | 415.8703     |
|    n_updates            | 2730         |
|    nu                   | 9.63         |
|    nu_loss              | -0.0151      |
|    policy_gradient_loss | -5.82e-05    |
|    reward_explained_... | -4e+11       |
|    reward_value_loss    | 5.42e+05     |
|    total_cost           | 16.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 525          |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 942          |
|    iterations           | 275          |
|    time_elapsed         | 2986         |
|    total_timesteps      | 2816000      |
| train/                  |              |
|    approx_kl            | 0.001153961  |
|    average_cost         | 0.0017578125 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.2         |
|    cost_value_loss      | 0.0219       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.145       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.49e+05     |
|    mean_cost_advantages | 0.0060855094 |
|    mean_reward_advan... | 389.47876    |
|    n_updates            | 2740         |
|    nu                   | 9.64         |
|    nu_loss              | -0.0169      |
|    policy_gradient_loss | -0.000153    |
|    reward_explained_... | -3.98e+11    |
|    reward_value_loss    | 5.19e+05     |
|    total_cost           | 18.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 16.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 533          |
|    ep_len_mean          | 18.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 943          |
|    iterations           | 276          |
|    time_elapsed         | 2996         |
|    total_timesteps      | 2826240      |
| train/                  |              |
|    approx_kl            | 0.000669401  |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.00965      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.45        |
|    cost_value_loss      | 0.00228      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.145       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.34e+05     |
|    mean_cost_advantages | -0.004463962 |
|    mean_reward_advan... | 360.80972    |
|    n_updates            | 2750         |
|    nu                   | 9.64         |
|    nu_loss              | -0.00376     |
|    policy_gradient_loss | -8.74e-05    |
|    reward_explained_... | -3.95e+11    |
|    reward_value_loss    | 4.95e+05     |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 530           |
|    ep_len_mean          | 18.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 277           |
|    time_elapsed         | 3006          |
|    total_timesteps      | 2836480       |
| train/                  |               |
|    approx_kl            | 0.0010932668  |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.00424       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.15         |
|    cost_value_loss      | 0.00586       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.15         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.7e+05       |
|    mean_cost_advantages | 0.00011418888 |
|    mean_reward_advan... | 352.367       |
|    n_updates            | 2760          |
|    nu                   | 9.64          |
|    nu_loss              | -0.00659      |
|    policy_gradient_loss | -9.33e-05     |
|    reward_explained_... | -3.8e+11      |
|    reward_value_loss    | 4.76e+05      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 533           |
|    ep_len_mean          | 19            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 278           |
|    time_elapsed         | 3017          |
|    total_timesteps      | 2846720       |
| train/                  |               |
|    approx_kl            | 0.0009552905  |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0137        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.31         |
|    cost_value_loss      | 0.00109       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.146        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.87e+05      |
|    mean_cost_advantages | -0.0056103924 |
|    mean_reward_advan... | 326.9518      |
|    n_updates            | 2770          |
|    nu                   | 9.64          |
|    nu_loss              | -0.00188      |
|    policy_gradient_loss | -5.86e-05     |
|    reward_explained_... | -3.75e+11     |
|    reward_value_loss    | 4.55e+05      |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 530           |
|    ep_len_mean          | 18.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 944           |
|    iterations           | 279           |
|    time_elapsed         | 3026          |
|    total_timesteps      | 2856960       |
| train/                  |               |
|    approx_kl            | 0.00050842535 |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.00399       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.54         |
|    cost_value_loss      | 0.000582      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.141        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.89e+05      |
|    mean_cost_advantages | -0.0008709917 |
|    mean_reward_advan... | 308.7928      |
|    n_updates            | 2780          |
|    nu                   | 9.65          |
|    nu_loss              | -0.000942     |
|    policy_gradient_loss | -2.5e-05      |
|    reward_explained_... | -3.67e+11     |
|    reward_value_loss    | 4.36e+05      |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000684      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 533           |
|    ep_len_mean          | 18.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 944           |
|    iterations           | 280           |
|    time_elapsed         | 3036          |
|    total_timesteps      | 2867200       |
| train/                  |               |
|    approx_kl            | 0.0008064681  |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.019         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -15.5         |
|    cost_value_loss      | 0.00219       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.143        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.06e+05      |
|    mean_cost_advantages | 0.00032290127 |
|    mean_reward_advan... | 284.79453     |
|    n_updates            | 2790          |
|    nu                   | 9.65          |
|    nu_loss              | -0.00377      |
|    policy_gradient_loss | -7.87e-05     |
|    reward_explained_... | -3.61e+11     |
|    reward_value_loss    | 4.17e+05      |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 535           |
|    ep_len_mean          | 18.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 943           |
|    iterations           | 281           |
|    time_elapsed         | 3048          |
|    total_timesteps      | 2877440       |
| train/                  |               |
|    approx_kl            | 0.00062918226 |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.00394       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.18         |
|    cost_value_loss      | 0.00431       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.144        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.97e+05      |
|    mean_cost_advantages | 0.00013125103 |
|    mean_reward_advan... | 266.287       |
|    n_updates            | 2800          |
|    nu                   | 9.65          |
|    nu_loss              | -0.0066       |
|    policy_gradient_loss | -6.99e-05     |
|    reward_explained_... | -3.55e+11     |
|    reward_value_loss    | 4.01e+05      |
|    total_cost           | 7.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00234       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 534           |
|    ep_len_mean          | 18.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 944           |
|    iterations           | 282           |
|    time_elapsed         | 3058          |
|    total_timesteps      | 2887680       |
| train/                  |               |
|    approx_kl            | 0.0012209438  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.00925       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.95         |
|    cost_value_loss      | 0.007         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.142        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.98e+05      |
|    mean_cost_advantages | 0.004949064   |
|    mean_reward_advan... | 247.6523      |
|    n_updates            | 2810          |
|    nu                   | 9.65          |
|    nu_loss              | -0.00471      |
|    policy_gradient_loss | -6.97e-06     |
|    reward_explained_... | -3.46e+11     |
|    reward_value_loss    | 3.84e+05      |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.07
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 9.99e+03    |
|    mean_ep_length       | 17          |
|    mean_reward          | 9.99e+03    |
|    true_cost            | 0.00293     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | 511         |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 944         |
|    iterations           | 283         |
|    time_elapsed         | 3068        |
|    total_timesteps      | 2897920     |
| train/                  |             |
|    approx_kl            | 0.072195075 |
|    average_cost         | 0.00234375  |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -25.9       |
|    cost_value_loss      | 0.146       |
|    early_stop_epoch     | 9           |
|    entropy_loss         | -0.145      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.53e+05    |
|    mean_cost_advantages | 0.022312902 |
|    mean_reward_advan... | 224.31406   |
|    n_updates            | 2820        |
|    nu                   | 9.66        |
|    nu_loss              | -0.0226     |
|    policy_gradient_loss | -0.000924   |
|    reward_explained_... | -3.41e+11   |
|    reward_value_loss    | 3.69e+05    |
|    total_cost           | 24.0        |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 532          |
|    ep_len_mean          | 18.8         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 946          |
|    iterations           | 284          |
|    time_elapsed         | 3073         |
|    total_timesteps      | 2908160      |
| train/                  |              |
|    approx_kl            | 0.02499847   |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.43        |
|    cost_value_loss      | 0.0746       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.125       |
|    learning_rate        | 0.0003       |
|    loss                 | 2.18e+05     |
|    mean_cost_advantages | 0.0006629502 |
|    mean_reward_advan... | 171.22147    |
|    n_updates            | 2830         |
|    nu                   | 9.66         |
|    nu_loss              | -0.0283      |
|    policy_gradient_loss | -0.0026      |
|    reward_explained_... | nan          |
|    reward_value_loss    | 3.59e+05     |
|    total_cost           | 30.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 538           |
|    ep_len_mean          | 18.6          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 946           |
|    iterations           | 285           |
|    time_elapsed         | 3083          |
|    total_timesteps      | 2918400       |
| train/                  |               |
|    approx_kl            | 0.0024541547  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0297        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.322        |
|    cost_value_loss      | 0.00205       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.134        |
|    learning_rate        | 0.0003        |
|    loss                 | 2.36e+05      |
|    mean_cost_advantages | -0.11380763   |
|    mean_reward_advan... | 199.47954     |
|    n_updates            | 2840          |
|    nu                   | 9.66          |
|    nu_loss              | -0.00283      |
|    policy_gradient_loss | -0.000137     |
|    reward_explained_... | -3.33e+11     |
|    reward_value_loss    | 3.51e+05      |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 20.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 533          |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 946          |
|    iterations           | 286          |
|    time_elapsed         | 3093         |
|    total_timesteps      | 2928640      |
| train/                  |              |
|    approx_kl            | 0.0009467959 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.00896      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.224        |
|    cost_value_loss      | 0.00392      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.141       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.87e+05     |
|    mean_cost_advantages | -0.16722438  |
|    mean_reward_advan... | 185.53831    |
|    n_updates            | 2850         |
|    nu                   | 9.66         |
|    nu_loss              | -0.00377     |
|    policy_gradient_loss | -2.29e-07    |
|    reward_explained_... | -3.34e+15    |
|    reward_value_loss    | 3.41e+05     |
|    total_cost           | 4.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.6          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000781      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 543           |
|    ep_len_mean          | 18.4          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 946           |
|    iterations           | 287           |
|    time_elapsed         | 3104          |
|    total_timesteps      | 2938880       |
| train/                  |               |
|    approx_kl            | 0.01257656    |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.0316        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.67         |
|    cost_value_loss      | 0.0107        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.142        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.61e+05      |
|    mean_cost_advantages | -0.047900736  |
|    mean_reward_advan... | 159.05136     |
|    n_updates            | 2860          |
|    nu                   | 9.67          |
|    nu_loss              | -0.00661      |
|    policy_gradient_loss | -0.000183     |
|    reward_explained_... | -8e+10        |
|    reward_value_loss    | 3.26e+05      |
|    total_cost           | 7.0           |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 18.2          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00303       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 554           |
|    ep_len_mean          | 18.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 947           |
|    iterations           | 288           |
|    time_elapsed         | 3111          |
|    total_timesteps      | 2949120       |
| train/                  |               |
|    approx_kl            | 0.015674198   |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0.0237        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.143        |
|    cost_value_loss      | 0.00991       |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.144        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.48e+05      |
|    mean_cost_advantages | -0.0138810575 |
|    mean_reward_advan... | 151.77086     |
|    n_updates            | 2870          |
|    nu                   | 9.67          |
|    nu_loss              | -0.00755      |
|    policy_gradient_loss | 7.96e-05      |
|    reward_explained_... | nan           |
|    reward_value_loss    | 3.17e+05      |
|    total_cost           | 8.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.6         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000781     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 522          |
|    ep_len_mean          | 18.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 949          |
|    iterations           | 289          |
|    time_elapsed         | 3115         |
|    total_timesteps      | 2959360      |
| train/                  |              |
|    approx_kl            | 0.03950972   |
|    average_cost         | 0.0030273437 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.57        |
|    cost_value_loss      | 0.0643       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.142       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.79e+05     |
|    mean_cost_advantages | 0.04200972   |
|    mean_reward_advan... | 158.40044    |
|    n_updates            | 2880         |
|    nu                   | 9.67         |
|    nu_loss              | -0.0293      |
|    policy_gradient_loss | -0.000272    |
|    reward_explained_... | -3e+11       |
|    reward_value_loss    | 3.11e+05     |
|    total_cost           | 31.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 17.8          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 530           |
|    ep_len_mean          | 18.7          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 951           |
|    iterations           | 290           |
|    time_elapsed         | 3120          |
|    total_timesteps      | 2969600       |
| train/                  |               |
|    approx_kl            | 0.06744017    |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0.135         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.02         |
|    cost_value_loss      | 0.025         |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.12         |
|    learning_rate        | 0.0003        |
|    loss                 | 1.83e+05      |
|    mean_cost_advantages | -0.0029689686 |
|    mean_reward_advan... | 81.76764      |
|    n_updates            | 2890          |
|    nu                   | 9.67          |
|    nu_loss              | -0.00756      |
|    policy_gradient_loss | 0.001         |
|    reward_explained_... | -4.45e+11     |
|    reward_value_loss    | 4.31e+05      |
|    total_cost           | 8.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 19.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00146      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 544          |
|    ep_len_mean          | 17.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 951          |
|    iterations           | 291          |
|    time_elapsed         | 3130         |
|    total_timesteps      | 2979840      |
| train/                  |              |
|    approx_kl            | 0.004388531  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.186        |
|    cost_value_loss      | 9.06e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.139       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.17e+05     |
|    mean_cost_advantages | -0.010369053 |
|    mean_reward_advan... | 121.881516   |
|    n_updates            | 2900         |
|    nu                   | 9.68         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.000164     |
|    reward_explained_... | -3.1e+11     |
|    reward_value_loss    | 3.08e+05     |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 543          |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 952          |
|    iterations           | 292          |
|    time_elapsed         | 3138         |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.03600459   |
|    average_cost         | 0.0014648438 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.81        |
|    cost_value_loss      | 0.0242       |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.143       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.36e+05     |
|    mean_cost_advantages | 0.008076762  |
|    mean_reward_advan... | 121.37842    |
|    n_updates            | 2910         |
|    nu                   | 9.68         |
|    nu_loss              | -0.0142      |
|    policy_gradient_loss | 0.000102     |
|    reward_explained_... | -2.98e+11    |
|    reward_value_loss    | 2.97e+05     |
|    total_cost           | 15.0         |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 19.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000391      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 544           |
|    ep_len_mean          | 18.5          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 954           |
|    iterations           | 293           |
|    time_elapsed         | 3144          |
|    total_timesteps      | 3000320       |
| train/                  |               |
|    approx_kl            | 0.06211578    |
|    average_cost         | 0.000390625   |
|    clip_fraction        | 0.0535        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.17         |
|    cost_value_loss      | 0.00783       |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.146        |
|    learning_rate        | 0.0003        |
|    loss                 | 1.46e+05      |
|    mean_cost_advantages | -0.0013247669 |
|    mean_reward_advan... | 106.46135     |
|    n_updates            | 2920          |
|    nu                   | 9.68          |
|    nu_loss              | -0.00378      |
|    policy_gradient_loss | 0.000878      |
|    reward_explained_... | nan           |
|    reward_value_loss    | 2.94e+05      |
|    total_cost           | 4.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Mean reward: 6614.527778 +/- 4769.691266.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 53.60 minutes[0m
