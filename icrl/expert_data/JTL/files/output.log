[32;1mConfigured folder ./cpg/wandb/run-20220530_113954-25ehikdt/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -1.29e+06 |
|    mean_ep_length   | 186       |
|    mean_reward      | -1.29e+06 |
|    true_cost        | 0.64      |
| infos/              |           |
|    cost             | 0.0412    |
| rollout/            |           |
|    adjusted_reward  | -7.16     |
|    ep_len_mean      | 192       |
|    ep_rew_mean      | 838       |
| time/               |           |
|    fps              | 428       |
|    iterations       | 1         |
|    time_elapsed     | 4         |
|    total_timesteps  | 2048      |
-----------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.29e+06   |
|    mean_ep_length       | 190         |
|    mean_reward          | -1.34e+06   |
|    true_cost            | 0.536       |
| infos/                  |             |
|    cost                 | 0.0202      |
| rollout/                |             |
|    adjusted_reward      | 3.55        |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 1.68e+03    |
| time/                   |             |
|    fps                  | 372         |
|    iterations           | 2           |
|    time_elapsed         | 10          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012186477 |
|    average_cost         | 0.63964844  |
|    clip_fraction        | 0.0966      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.69       |
|    cost_value_loss      | 0.437       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.413       |
|    mean_cost_advantages | 0.595348    |
|    mean_reward_advan... | -0.42985392 |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.64       |
|    policy_gradient_loss | -0.00351    |
|    reward_explained_... | -7.13       |
|    reward_value_loss    | 0.641       |
|    total_cost           | 1310.0      |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.12e+05   |
|    mean_ep_length       | 59          |
|    mean_reward          | -4.12e+05   |
|    true_cost            | 0.555       |
| infos/                  |             |
|    cost                 | 0.0281      |
| rollout/                |             |
|    adjusted_reward      | 103         |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 6.03e+03    |
| time/                   |             |
|    fps                  | 395         |
|    iterations           | 3           |
|    time_elapsed         | 15          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.013713535 |
|    average_cost         | 0.5361328   |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.16       |
|    cost_value_loss      | 0.0844      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.981       |
|    mean_cost_advantages | 0.29008937  |
|    mean_reward_advan... | 0.27990252  |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.571      |
|    policy_gradient_loss | -0.00581    |
|    reward_explained_... | -1.17e+03   |
|    reward_value_loss    | 1.22        |
|    total_cost           | 1098.0      |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -2.38e+05  |
|    mean_ep_length       | 53.8       |
|    mean_reward          | -2.38e+05  |
|    true_cost            | 0.56       |
| infos/                  |            |
|    cost                 | 0.0332     |
| rollout/                |            |
|    adjusted_reward      | 112        |
|    ep_len_mean          | 97.4       |
|    ep_rew_mean          | 7.08e+03   |
| time/                   |            |
|    fps                  | 421        |
|    iterations           | 4          |
|    time_elapsed         | 19         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01864502 |
|    average_cost         | 0.5551758  |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -13.3      |
|    cost_value_loss      | 0.112      |
|    early_stop_epoch     | 4          |
|    entropy_loss         | -1.31      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.56       |
|    mean_cost_advantages | 0.09623524 |
|    mean_reward_advan... | 2.066991   |
|    n_updates            | 30         |
|    nu                   | 1.2        |
|    nu_loss              | -0.628     |
|    policy_gradient_loss | -0.0093    |
|    reward_explained_... | -88.4      |
|    reward_value_loss    | 6.46       |
|    total_cost           | 1137.0     |
----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.44e+05   |
|    mean_ep_length       | 28.2        |
|    mean_reward          | -1.44e+05   |
|    true_cost            | 0.544       |
| infos/                  |             |
|    cost                 | 0.0252      |
| rollout/                |             |
|    adjusted_reward      | 297         |
|    ep_len_mean          | 44.3        |
|    ep_rew_mean          | 9.58e+03    |
| time/                   |             |
|    fps                  | 438         |
|    iterations           | 5           |
|    time_elapsed         | 23          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.019392082 |
|    average_cost         | 0.5600586   |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -5.02       |
|    cost_value_loss      | 0.149       |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -1.29       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.04        |
|    mean_cost_advantages | 0.08470726  |
|    mean_reward_advan... | 1.3686023   |
|    n_updates            | 40          |
|    nu                   | 1.27        |
|    nu_loss              | -0.672      |
|    policy_gradient_loss | -0.00889    |
|    reward_explained_... | -0.457      |
|    reward_value_loss    | 4.38        |
|    total_cost           | 1147.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -8.6e+04    |
|    mean_ep_length       | 18.8        |
|    mean_reward          | -8.6e+04    |
|    true_cost            | 0.689       |
| infos/                  |             |
|    cost                 | 0.0352      |
| rollout/                |             |
|    adjusted_reward      | 460         |
|    ep_len_mean          | 26.4        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 459         |
|    iterations           | 6           |
|    time_elapsed         | 26          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.016203534 |
|    average_cost         | 0.5444336   |
|    clip_fraction        | 0.0799      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.31       |
|    cost_value_loss      | 0.0963      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.18       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.91        |
|    mean_cost_advantages | -0.12771139 |
|    mean_reward_advan... | 1.8294628   |
|    n_updates            | 50          |
|    nu                   | 1.34        |
|    nu_loss              | -0.691      |
|    policy_gradient_loss | -0.00742    |
|    reward_explained_... | 0.135       |
|    reward_value_loss    | 3.42        |
|    total_cost           | 1115.0      |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -8.6e+04    |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -9e+04      |
|    true_cost            | 0.591       |
| infos/                  |             |
|    cost                 | 0.0309      |
| rollout/                |             |
|    adjusted_reward      | 465         |
|    ep_len_mean          | 22.4        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 473         |
|    iterations           | 7           |
|    time_elapsed         | 30          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.01974911  |
|    average_cost         | 0.68896484  |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.663      |
|    cost_value_loss      | 0.0657      |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.01       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    mean_cost_advantages | -0.13158812 |
|    mean_reward_advan... | 0.7492094   |
|    n_updates            | 60          |
|    nu                   | 1.42        |
|    nu_loss              | -0.925      |
|    policy_gradient_loss | -0.00742    |
|    reward_explained_... | 0.497       |
|    reward_value_loss    | 0.68        |
|    total_cost           | 1411.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -8.6e+04    |
|    mean_ep_length       | 20.2        |
|    mean_reward          | -9e+04      |
|    true_cost            | 0.541       |
| infos/                  |             |
|    cost                 | 0.0426      |
| rollout/                |             |
|    adjusted_reward      | 597         |
|    ep_len_mean          | 17.3        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 8           |
|    time_elapsed         | 33          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.019460334 |
|    average_cost         | 0.5908203   |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.417       |
|    cost_value_loss      | 0.0458      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.01       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0985      |
|    mean_cost_advantages | -0.1075211  |
|    mean_reward_advan... | -0.2152718  |
|    n_updates            | 70          |
|    nu                   | 1.5         |
|    nu_loss              | -0.838      |
|    policy_gradient_loss | -0.00855    |
|    reward_explained_... | 0.524       |
|    reward_value_loss    | 0.224       |
|    total_cost           | 1210.0      |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -7.2e+04     |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -7.2e+04     |
|    true_cost            | 0.623        |
| infos/                  |              |
|    cost                 | 0.0445       |
| rollout/                |              |
|    adjusted_reward      | 646          |
|    ep_len_mean          | 15.9         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 496          |
|    iterations           | 9            |
|    time_elapsed         | 37           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.011689     |
|    average_cost         | 0.54052734   |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.621        |
|    cost_value_loss      | 0.0206       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.917       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00642      |
|    mean_cost_advantages | -0.122997016 |
|    mean_reward_advan... | -0.41301376  |
|    n_updates            | 80           |
|    nu                   | 1.57         |
|    nu_loss              | -0.808       |
|    policy_gradient_loss | -0.00723     |
|    reward_explained_... | 0.543        |
|    reward_value_loss    | 0.0439       |
|    total_cost           | 1107.0       |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -7.2e+04     |
|    mean_ep_length       | 18.2         |
|    mean_reward          | -7.6e+04     |
|    true_cost            | 0.549        |
| infos/                  |              |
|    cost                 | 0.0341       |
| rollout/                |              |
|    adjusted_reward      | 687          |
|    ep_len_mean          | 15.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 504          |
|    iterations           | 10           |
|    time_elapsed         | 40           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.01598197   |
|    average_cost         | 0.6230469    |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.576        |
|    cost_value_loss      | 0.0166       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.854       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00111     |
|    mean_cost_advantages | -0.024807932 |
|    mean_reward_advan... | -0.4545583   |
|    n_updates            | 90           |
|    nu                   | 1.66         |
|    nu_loss              | -0.981       |
|    policy_gradient_loss | -0.00551     |
|    reward_explained_... | 0.551        |
|    reward_value_loss    | 0.0462       |
|    total_cost           | 1276.0       |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -6.4e+04    |
|    mean_ep_length       | 13          |
|    mean_reward          | -6.4e+04    |
|    true_cost            | 0.616       |
| infos/                  |             |
|    cost                 | 0.0437      |
| rollout/                |             |
|    adjusted_reward      | 729         |
|    ep_len_mean          | 13.7        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 512         |
|    iterations           | 11          |
|    time_elapsed         | 43          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.015177656 |
|    average_cost         | 0.5488281   |
|    clip_fraction        | 0.0976      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.741       |
|    cost_value_loss      | 0.0112      |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.797      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00186     |
|    mean_cost_advantages | -0.05010452 |
|    mean_reward_advan... | -0.3865965  |
|    n_updates            | 100         |
|    nu                   | 1.74        |
|    nu_loss              | -0.909      |
|    policy_gradient_loss | -0.0074     |
|    reward_explained_... | 0.491       |
|    reward_value_loss    | 0.0213      |
|    total_cost           | 1124.0      |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.4e+04     |
|    mean_ep_length       | 13.2         |
|    mean_reward          | -8.6e+04     |
|    true_cost            | 0.621        |
| infos/                  |              |
|    cost                 | 0.0347       |
| rollout/                |              |
|    adjusted_reward      | 781          |
|    ep_len_mean          | 13.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 515          |
|    iterations           | 12           |
|    time_elapsed         | 47           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.015403431  |
|    average_cost         | 0.61621094   |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.804        |
|    cost_value_loss      | 0.0068       |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.717       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00214      |
|    mean_cost_advantages | 0.0017852646 |
|    mean_reward_advan... | -0.28113323  |
|    n_updates            | 110          |
|    nu                   | 1.82         |
|    nu_loss              | -1.07        |
|    policy_gradient_loss | -0.00525     |
|    reward_explained_... | 0.681        |
|    reward_value_loss    | 0.0112       |
|    total_cost           | 1262.0       |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -6.4e+04    |
|    mean_ep_length       | 13          |
|    mean_reward          | -7.4e+04    |
|    true_cost            | 0.632       |
| infos/                  |             |
|    cost                 | 0.0475      |
| rollout/                |             |
|    adjusted_reward      | 799         |
|    ep_len_mean          | 12.7        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 516         |
|    iterations           | 13          |
|    time_elapsed         | 51          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.004936738 |
|    average_cost         | 0.62109375  |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.891       |
|    cost_value_loss      | 0.00406     |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.658      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00285     |
|    mean_cost_advantages | -0.01307988 |
|    mean_reward_advan... | -0.21499261 |
|    n_updates            | 120         |
|    nu                   | 1.91        |
|    nu_loss              | -1.13       |
|    policy_gradient_loss | -0.00428    |
|    reward_explained_... | 0.73        |
|    reward_value_loss    | 0.0068      |
|    total_cost           | 1272.0      |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.4e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -6.8e+04     |
|    true_cost            | 0.598        |
| infos/                  |              |
|    cost                 | 0.0429       |
| rollout/                |              |
|    adjusted_reward      | 811          |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 514          |
|    iterations           | 14           |
|    time_elapsed         | 55           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.010157652  |
|    average_cost         | 0.63183594   |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.913        |
|    cost_value_loss      | 0.00334      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.617       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0074      |
|    mean_cost_advantages | -0.002736625 |
|    mean_reward_advan... | -0.17522907  |
|    n_updates            | 130          |
|    nu                   | 2            |
|    nu_loss              | -1.21        |
|    policy_gradient_loss | -0.00298     |
|    reward_explained_... | 0.849        |
|    reward_value_loss    | 0.0037       |
|    total_cost           | 1294.0       |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.4e+04    |
|    mean_ep_length       | 12.6        |
|    mean_reward          | -5.4e+04    |
|    true_cost            | 0.562       |
| infos/                  |             |
|    cost                 | 0.041       |
| rollout/                |             |
|    adjusted_reward      | 819         |
|    ep_len_mean          | 12.4        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 15          |
|    time_elapsed         | 59          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.015092589 |
|    average_cost         | 0.59765625  |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.889       |
|    cost_value_loss      | 0.00431     |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -0.583      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00849    |
|    mean_cost_advantages | 0.002587164 |
|    mean_reward_advan... | -0.15280038 |
|    n_updates            | 140         |
|    nu                   | 2.09        |
|    nu_loss              | -1.2        |
|    policy_gradient_loss | -0.00544    |
|    reward_explained_... | 0.878       |
|    reward_value_loss    | 0.00344     |
|    total_cost           | 1224.0      |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.2e+04     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -5.2e+04     |
|    true_cost            | 0.532        |
| infos/                  |              |
|    cost                 | 0.0411       |
| rollout/                |              |
|    adjusted_reward      | 823          |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 510          |
|    iterations           | 16           |
|    time_elapsed         | 64           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.011277979  |
|    average_cost         | 0.5625       |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.885        |
|    cost_value_loss      | 0.00389      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.555       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00541     |
|    mean_cost_advantages | -0.008324805 |
|    mean_reward_advan... | -0.11937982  |
|    n_updates            | 150          |
|    nu                   | 2.18         |
|    nu_loss              | -1.18        |
|    policy_gradient_loss | -0.00396     |
|    reward_explained_... | 0.883        |
|    reward_value_loss    | 0.00239      |
|    total_cost           | 1152.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -5e+04       |
|    true_cost            | 0.516        |
| infos/                  |              |
|    cost                 | 0.0381       |
| rollout/                |              |
|    adjusted_reward      | 827          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 498          |
|    iterations           | 17           |
|    time_elapsed         | 69           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.011412113  |
|    average_cost         | 0.5317383    |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.913        |
|    cost_value_loss      | 0.00298      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.526       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000102     |
|    mean_cost_advantages | -0.006637157 |
|    mean_reward_advan... | -0.09858183  |
|    n_updates            | 160          |
|    nu                   | 2.27         |
|    nu_loss              | -1.16        |
|    policy_gradient_loss | -0.0027      |
|    reward_explained_... | 0.904        |
|    reward_value_loss    | 0.00199      |
|    total_cost           | 1089.0       |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.6e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -4.6e+04     |
|    true_cost            | 0.476        |
| infos/                  |              |
|    cost                 | 0.0358       |
| rollout/                |              |
|    adjusted_reward      | 826          |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 501          |
|    iterations           | 18           |
|    time_elapsed         | 73           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.018923594  |
|    average_cost         | 0.515625     |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.887        |
|    cost_value_loss      | 0.00333      |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.503       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00374      |
|    mean_cost_advantages | 0.010089011  |
|    mean_reward_advan... | -0.085706696 |
|    n_updates            | 170          |
|    nu                   | 2.37         |
|    nu_loss              | -1.17        |
|    policy_gradient_loss | -0.00324     |
|    reward_explained_... | 0.937        |
|    reward_value_loss    | 0.0016       |
|    total_cost           | 1056.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.6e+04     |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -5.6e+04     |
|    true_cost            | 0.447        |
| infos/                  |              |
|    cost                 | 0.0325       |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 497          |
|    iterations           | 19           |
|    time_elapsed         | 78           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.008900888  |
|    average_cost         | 0.47607422   |
|    clip_fraction        | 0.0726       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.897        |
|    cost_value_loss      | 0.00303      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.476       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00686      |
|    mean_cost_advantages | -0.011586782 |
|    mean_reward_advan... | -0.067858115 |
|    n_updates            | 180          |
|    nu                   | 2.46         |
|    nu_loss              | -1.13        |
|    policy_gradient_loss | -0.00212     |
|    reward_explained_... | 0.96         |
|    reward_value_loss    | 0.00081      |
|    total_cost           | 975.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -3.8e+04     |
|    true_cost            | 0.399        |
| infos/                  |              |
|    cost                 | 0.0355       |
| rollout/                |              |
|    adjusted_reward      | 819          |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 495          |
|    iterations           | 20           |
|    time_elapsed         | 82           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.010372631  |
|    average_cost         | 0.44677734   |
|    clip_fraction        | 0.152        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.901        |
|    cost_value_loss      | 0.00296      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.436       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0038      |
|    mean_cost_advantages | -0.009825395 |
|    mean_reward_advan... | -0.056814246 |
|    n_updates            | 190          |
|    nu                   | 2.55         |
|    nu_loss              | -1.1         |
|    policy_gradient_loss | -0.00283     |
|    reward_explained_... | 0.979        |
|    reward_value_loss    | 0.000425     |
|    total_cost           | 915.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.8e+04     |
|    true_cost            | 0.357        |
| infos/                  |              |
|    cost                 | 0.0303       |
| rollout/                |              |
|    adjusted_reward      | 824          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 496          |
|    iterations           | 21           |
|    time_elapsed         | 86           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.023776375  |
|    average_cost         | 0.39892578   |
|    clip_fraction        | 0.146        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.916        |
|    cost_value_loss      | 0.00239      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.405       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00575      |
|    mean_cost_advantages | -0.010591242 |
|    mean_reward_advan... | -0.06569757  |
|    n_updates            | 200          |
|    nu                   | 2.65         |
|    nu_loss              | -1.02        |
|    policy_gradient_loss | -0.0017      |
|    reward_explained_... | 0.723        |
|    reward_value_loss    | 0.00544      |
|    total_cost           | 817.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -2e+04       |
|    true_cost            | 0.322        |
| infos/                  |              |
|    cost                 | 0.0291       |
| rollout/                |              |
|    adjusted_reward      | 828          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 492          |
|    iterations           | 22           |
|    time_elapsed         | 91           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.013743069  |
|    average_cost         | 0.35742188   |
|    clip_fraction        | 0.0969       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.89         |
|    cost_value_loss      | 0.00247      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.385       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00983     |
|    mean_cost_advantages | -0.008471689 |
|    mean_reward_advan... | -0.04478485  |
|    n_updates            | 210          |
|    nu                   | 2.74         |
|    nu_loss              | -0.946       |
|    policy_gradient_loss | -0.0021      |
|    reward_explained_... | 0.943        |
|    reward_value_loss    | 0.000684     |
|    total_cost           | 732.0        |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -2.4e+04     |
|    true_cost            | 0.303        |
| infos/                  |              |
|    cost                 | 0.0279       |
| rollout/                |              |
|    adjusted_reward      | 828          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 489          |
|    iterations           | 23           |
|    time_elapsed         | 96           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.016003259  |
|    average_cost         | 0.32177734   |
|    clip_fraction        | 0.13         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.849        |
|    cost_value_loss      | 0.00305      |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.365       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00728     |
|    mean_cost_advantages | -0.011739107 |
|    mean_reward_advan... | -0.037883133 |
|    n_updates            | 220          |
|    nu                   | 2.83         |
|    nu_loss              | -0.881       |
|    policy_gradient_loss | -0.00305     |
|    reward_explained_... | 0.919        |
|    reward_value_loss    | 0.00156      |
|    total_cost           | 659.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -1.8e+04     |
|    true_cost            | 0.249        |
| infos/                  |              |
|    cost                 | 0.0206       |
| rollout/                |              |
|    adjusted_reward      | 833          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 487          |
|    iterations           | 24           |
|    time_elapsed         | 100          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.008754578  |
|    average_cost         | 0.30273438   |
|    clip_fraction        | 0.0995       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.778        |
|    cost_value_loss      | 0.00392      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.335       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00139     |
|    mean_cost_advantages | -0.010982311 |
|    mean_reward_advan... | -0.032818697 |
|    n_updates            | 230          |
|    nu                   | 2.92         |
|    nu_loss              | -0.856       |
|    policy_gradient_loss | -0.00202     |
|    reward_explained_... | 0.973        |
|    reward_value_loss    | 0.00035      |
|    total_cost           | 620.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.2e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -1.2e+04     |
|    true_cost            | 0.241        |
| infos/                  |              |
|    cost                 | 0.0243       |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 484          |
|    iterations           | 25           |
|    time_elapsed         | 105          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.009240482  |
|    average_cost         | 0.24902344   |
|    clip_fraction        | 0.0812       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.686        |
|    cost_value_loss      | 0.00449      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.297       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000993     |
|    mean_cost_advantages | -0.017499406 |
|    mean_reward_advan... | -0.031592507 |
|    n_updates            | 240          |
|    nu                   | 3            |
|    nu_loss              | -0.726       |
|    policy_gradient_loss | -0.0011      |
|    reward_explained_... | 0.979        |
|    reward_value_loss    | 0.000403     |
|    total_cost           | 510.0        |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4e+03        |
|    mean_ep_length       | 12            |
|    mean_reward          | -4e+03        |
|    true_cost            | 0.187         |
| infos/                  |               |
|    cost                 | 0.0194        |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 482           |
|    iterations           | 26            |
|    time_elapsed         | 110           |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.016357139   |
|    average_cost         | 0.24072266    |
|    clip_fraction        | 0.096         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.76          |
|    cost_value_loss      | 0.00313       |
|    early_stop_epoch     | 7             |
|    entropy_loss         | -0.309        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00292      |
|    mean_cost_advantages | -0.0057920655 |
|    mean_reward_advan... | -0.028873052  |
|    n_updates            | 250           |
|    nu                   | 3.08          |
|    nu_loss              | -0.722        |
|    policy_gradient_loss | -0.00147      |
|    reward_explained_... | 0.99          |
|    reward_value_loss    | 0.000183      |
|    total_cost           | 493.0         |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2e+03       |
|    mean_ep_length       | 12           |
|    mean_reward          | -2e+03       |
|    true_cost            | 0.135        |
| infos/                  |              |
|    cost                 | 0.0152       |
| rollout/                |              |
|    adjusted_reward      | 837          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 27           |
|    time_elapsed         | 114          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.024147574  |
|    average_cost         | 0.18701172   |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.557        |
|    cost_value_loss      | 0.00549      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.301       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0141       |
|    mean_cost_advantages | -0.024831936 |
|    mean_reward_advan... | -0.02567476  |
|    n_updates            | 260          |
|    nu                   | 3.17         |
|    nu_loss              | -0.577       |
|    policy_gradient_loss | -0.00196     |
|    reward_explained_... | 0.991        |
|    reward_value_loss    | 0.000213     |
|    total_cost           | 383.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 5.99e+03     |
|    true_cost            | 0.0742       |
| infos/                  |              |
|    cost                 | 0.00903      |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 484          |
|    iterations           | 28           |
|    time_elapsed         | 118          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.027050698  |
|    average_cost         | 0.1352539    |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.345        |
|    cost_value_loss      | 0.00528      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.29        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00032     |
|    mean_cost_advantages | -0.027033985 |
|    mean_reward_advan... | -0.02762724  |
|    n_updates            | 270          |
|    nu                   | 3.24         |
|    nu_loss              | -0.428       |
|    policy_gradient_loss | -0.0025      |
|    reward_explained_... | 0.991        |
|    reward_value_loss    | 0.000211     |
|    total_cost           | 277.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0396       |
| infos/                  |              |
|    cost                 | 0.00457      |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 29           |
|    time_elapsed         | 123          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.009686141  |
|    average_cost         | 0.07421875   |
|    clip_fraction        | 0.0997       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.137        |
|    cost_value_loss      | 0.00439      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.251       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00126     |
|    mean_cost_advantages | -0.033062894 |
|    mean_reward_advan... | -0.022853259 |
|    n_updates            | 280          |
|    nu                   | 3.31         |
|    nu_loss              | -0.241       |
|    policy_gradient_loss | -0.00145     |
|    reward_explained_... | 0.99         |
|    reward_value_loss    | 0.000153     |
|    total_cost           | 152.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.0322       |
| infos/                  |              |
|    cost                 | 0.00185      |
| rollout/                |              |
|    adjusted_reward      | 797          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 30           |
|    time_elapsed         | 128          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.004137507  |
|    average_cost         | 0.03955078   |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.16        |
|    cost_value_loss      | 0.00266      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.226       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000551     |
|    mean_cost_advantages | -0.016483648 |
|    mean_reward_advan... | -0.019055087 |
|    n_updates            | 290          |
|    nu                   | 3.38         |
|    nu_loss              | -0.131       |
|    policy_gradient_loss | -0.000449    |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 5.31e-05     |
|    total_cost           | 81.0         |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.019        |
| infos/                  |              |
|    cost                 | 0.00467      |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 31           |
|    time_elapsed         | 132          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.033155743  |
|    average_cost         | 0.032226562  |
|    clip_fraction        | 0.0696       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.04        |
|    cost_value_loss      | 0.00207      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.218       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00102      |
|    mean_cost_advantages | -0.005073307 |
|    mean_reward_advan... | -0.046306692 |
|    n_updates            | 300          |
|    nu                   | 3.44         |
|    nu_loss              | -0.109       |
|    policy_gradient_loss | -0.00483     |
|    reward_explained_... | 0.137        |
|    reward_value_loss    | 0.0125       |
|    total_cost           | 66.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00879      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 32           |
|    time_elapsed         | 137          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.002016024  |
|    average_cost         | 0.019042969  |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.44        |
|    cost_value_loss      | 0.00149      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.186       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00433     |
|    mean_cost_advantages | -0.006808731 |
|    mean_reward_advan... | -0.013472436 |
|    n_updates            | 310          |
|    nu                   | 3.5          |
|    nu_loss              | -0.0656      |
|    policy_gradient_loss | -0.00229     |
|    reward_explained_... | 0.766        |
|    reward_value_loss    | 0.000536     |
|    total_cost           | 39.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0.00191       |
| rollout/                |               |
|    adjusted_reward      | 826           |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 33            |
|    time_elapsed         | 141           |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 0.002454026   |
|    average_cost         | 0.0087890625  |
|    clip_fraction        | 0.0141        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.89         |
|    cost_value_loss      | 0.000705      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.167        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000493     |
|    mean_cost_advantages | -0.008834115  |
|    mean_reward_advan... | -0.0077321082 |
|    n_updates            | 320           |
|    nu                   | 3.55          |
|    nu_loss              | -0.0308       |
|    policy_gradient_loss | -3.47e-05     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 3.93e-05      |
|    total_cost           | 18.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 473           |
|    iterations           | 34            |
|    time_elapsed         | 147           |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.0051998403  |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0346        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.547        |
|    cost_value_loss      | 0.00021       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.161        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00465      |
|    mean_cost_advantages | -0.0063092182 |
|    mean_reward_advan... | -0.018749448  |
|    n_updates            | 330           |
|    nu                   | 3.6           |
|    nu_loss              | -0.00694      |
|    policy_gradient_loss | -0.00213      |
|    reward_explained_... | 0.866         |
|    reward_value_loss    | 0.00223       |
|    total_cost           | 4.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00195       |
| infos/                  |               |
|    cost                 | 0.00195       |
| rollout/                |               |
|    adjusted_reward      | 796           |
|    ep_len_mean          | 12.9          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 471           |
|    iterations           | 35            |
|    time_elapsed         | 152           |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 0.0063263206  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0528        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0721        |
|    cost_value_loss      | 2.9e-05       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.145        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00267      |
|    mean_cost_advantages | -0.0029516155 |
|    mean_reward_advan... | -0.005182919  |
|    n_updates            | 340           |
|    nu                   | 3.64          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000857     |
|    reward_explained_... | 0.955         |
|    reward_value_loss    | 0.000181      |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 470           |
|    iterations           | 36            |
|    time_elapsed         | 156           |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.036800552   |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0684        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.12         |
|    cost_value_loss      | 0.000195      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.163        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000118     |
|    mean_cost_advantages | 0.00063403824 |
|    mean_reward_advan... | -0.032904264  |
|    n_updates            | 350           |
|    nu                   | 3.68          |
|    nu_loss              | -0.00712      |
|    policy_gradient_loss | -0.00529      |
|    reward_explained_... | 0.186         |
|    reward_value_loss    | 0.0124        |
|    total_cost           | 4.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 37           |
|    time_elapsed         | 161          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0024637484 |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.29        |
|    cost_value_loss      | 9.96e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.145       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000865    |
|    mean_cost_advantages | 0.0015128034 |
|    mean_reward_advan... | 0.0019883462 |
|    n_updates            | 360          |
|    nu                   | 3.72         |
|    nu_loss              | -0.0036      |
|    policy_gradient_loss | -0.000443    |
|    reward_explained_... | 0.53         |
|    reward_value_loss    | 0.000518     |
|    total_cost           | 2.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 836            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 466            |
|    iterations           | 38             |
|    time_elapsed         | 166            |
|    total_timesteps      | 77824          |
| train/                  |                |
|    approx_kl            | 0.0034756106   |
|    average_cost         | 0.0009765625   |
|    clip_fraction        | 0.0271         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -4.99          |
|    cost_value_loss      | 0.0001         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.133         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000744       |
|    mean_cost_advantages | -0.0009414112  |
|    mean_reward_advan... | -0.00043374998 |
|    n_updates            | 370            |
|    nu                   | 3.75           |
|    nu_loss              | -0.00363       |
|    policy_gradient_loss | -0.000452      |
|    reward_explained_... | 0.992          |
|    reward_value_loss    | 0.000102       |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12.4          |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0.00203       |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 465           |
|    iterations           | 39            |
|    time_elapsed         | 171           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.0065353666  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0267        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.92         |
|    cost_value_loss      | 9.89e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.115        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000284     |
|    mean_cost_advantages | 4.484784e-06  |
|    mean_reward_advan... | -0.0065478487 |
|    n_updates            | 380           |
|    nu                   | 3.78          |
|    nu_loss              | -0.00366      |
|    policy_gradient_loss | -0.000608     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 2.66e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.000977     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 40           |
|    time_elapsed         | 176          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0015161708 |
|    average_cost         | 0.0009765625 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -10.6        |
|    cost_value_loss      | 9.87e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.119       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000576     |
|    mean_cost_advantages | 0.0009053447 |
|    mean_reward_advan... | -0.010527017 |
|    n_updates            | 390          |
|    nu                   | 3.81         |
|    nu_loss              | -0.00369     |
|    policy_gradient_loss | -0.000368    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 5.99e-05     |
|    total_cost           | 2.0          |
------------------------------------------
Mean reward: 9994.500000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 03.18 minutes[0m
