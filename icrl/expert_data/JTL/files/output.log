[32;1mConfigured folder ./cpg/wandb/run-20220607_091057-2tlguotl/files for saving[0m
[32;1mName: D2B-v0_CDD2B-v0_tk_0.01_s_20_sid_0_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
Using cpu device
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -1.22e+06 |
|    mean_ep_length   | 171       |
|    mean_reward      | -1.22e+06 |
|    true_cost        | 0.629     |
| infos/              |           |
|    cost             | 0.0352    |
| rollout/            |           |
|    adjusted_reward  | -2.69     |
|    ep_len_mean      | 174       |
|    ep_rew_mean      | 2.57e+03  |
| time/               |           |
|    fps              | 399       |
|    iterations       | 1         |
|    time_elapsed     | 5         |
|    total_timesteps  | 2048      |
-----------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.6e+05     |
|    mean_ep_length       | 110          |
|    mean_reward          | -6.6e+05     |
|    true_cost            | 0.712        |
| infos/                  |              |
|    cost                 | 0.0357       |
| rollout/                |              |
|    adjusted_reward      | 23.1         |
|    ep_len_mean          | 147          |
|    ep_rew_mean          | 4.33e+03     |
| time/                   |              |
|    fps                  | 348          |
|    iterations           | 2            |
|    time_elapsed         | 11           |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0129365865 |
|    average_cost         | 0.62890625   |
|    clip_fraction        | 0.0875       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0802      |
|    cost_value_loss      | 0.414        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.38        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.469        |
|    mean_cost_advantages | 0.6748688    |
|    mean_reward_advan... | -0.05816908  |
|    n_updates            | 10           |
|    nu                   | 1.06         |
|    nu_loss              | -0.629       |
|    policy_gradient_loss | -0.00824     |
|    reward_explained_... | -6.23        |
|    reward_value_loss    | 1.38         |
|    total_cost           | 1288.0       |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.76e+05   |
|    mean_ep_length       | 63          |
|    mean_reward          | -3.76e+05   |
|    true_cost            | 0.663       |
| infos/                  |             |
|    cost                 | 0.0396      |
| rollout/                |             |
|    adjusted_reward      | 172         |
|    ep_len_mean          | 92.3        |
|    ep_rew_mean          | 7.66e+03    |
| time/                   |             |
|    fps                  | 366         |
|    iterations           | 3           |
|    time_elapsed         | 16          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.016581941 |
|    average_cost         | 0.71240234  |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.00605    |
|    cost_value_loss      | 0.0934      |
|    early_stop_epoch     | 8           |
|    entropy_loss         | -1.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.89        |
|    mean_cost_advantages | 0.39043906  |
|    mean_reward_advan... | 0.64262843  |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.758      |
|    policy_gradient_loss | -0.00924    |
|    reward_explained_... | -11.8       |
|    reward_value_loss    | 2.5         |
|    total_cost           | 1459.0      |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -3.46e+05   |
|    mean_ep_length       | 58.4        |
|    mean_reward          | -3.46e+05   |
|    true_cost            | 0.521       |
| infos/                  |             |
|    cost                 | 0.0241      |
| rollout/                |             |
|    adjusted_reward      | 313         |
|    ep_len_mean          | 44.4        |
|    ep_rew_mean          | 9.88e+03    |
| time/                   |             |
|    fps                  | 386         |
|    iterations           | 4           |
|    time_elapsed         | 21          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.017197099 |
|    average_cost         | 0.66308594  |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.7        |
|    cost_value_loss      | 0.119       |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.54        |
|    mean_cost_advantages | 0.08333197  |
|    mean_reward_advan... | 2.4229946   |
|    n_updates            | 30          |
|    nu                   | 1.2         |
|    nu_loss              | -0.75       |
|    policy_gradient_loss | -0.00941    |
|    reward_explained_... | -5.29       |
|    reward_value_loss    | 6.35        |
|    total_cost           | 1358.0      |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.46e+05   |
|    mean_ep_length       | 27          |
|    mean_reward          | -1.46e+05   |
|    true_cost            | 0.535       |
| infos/                  |             |
|    cost                 | 0.0495      |
| rollout/                |             |
|    adjusted_reward      | 383         |
|    ep_len_mean          | 28          |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 397         |
|    iterations           | 5           |
|    time_elapsed         | 25          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.020202909 |
|    average_cost         | 0.5205078   |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.29       |
|    cost_value_loss      | 0.115       |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -1.23       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.934       |
|    mean_cost_advantages | -0.1890882  |
|    mean_reward_advan... | 2.4626067   |
|    n_updates            | 40          |
|    nu                   | 1.27        |
|    nu_loss              | -0.625      |
|    policy_gradient_loss | -0.0139     |
|    reward_explained_... | 0.26        |
|    reward_value_loss    | 3.58        |
|    total_cost           | 1066.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.12e+05   |
|    mean_ep_length       | 19.2        |
|    mean_reward          | -1.12e+05   |
|    true_cost            | 0.585       |
| infos/                  |             |
|    cost                 | 0.0217      |
| rollout/                |             |
|    adjusted_reward      | 461         |
|    ep_len_mean          | 24.2        |
|    ep_rew_mean          | 9.89e+03    |
| time/                   |             |
|    fps                  | 412         |
|    iterations           | 6           |
|    time_elapsed         | 29          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.023011228 |
|    average_cost         | 0.53466797  |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.44       |
|    cost_value_loss      | 0.0945      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.377       |
|    mean_cost_advantages | -0.15960856 |
|    mean_reward_advan... | 0.98647434  |
|    n_updates            | 50          |
|    nu                   | 1.34        |
|    nu_loss              | -0.679      |
|    policy_gradient_loss | -0.00843    |
|    reward_explained_... | 0.525       |
|    reward_value_loss    | 1.05        |
|    total_cost           | 1095.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.6e+04    |
|    mean_ep_length       | 14.8        |
|    mean_reward          | -5.6e+04    |
|    true_cost            | 0.508       |
| infos/                  |             |
|    cost                 | 0.021       |
| rollout/                |             |
|    adjusted_reward      | 548         |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 422         |
|    iterations           | 7           |
|    time_elapsed         | 33          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.019345425 |
|    average_cost         | 0.58496094  |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.699      |
|    cost_value_loss      | 0.0577      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.03       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    mean_cost_advantages | -0.0848663  |
|    mean_reward_advan... | -0.14664853 |
|    n_updates            | 60          |
|    nu                   | 1.42        |
|    nu_loss              | -0.785      |
|    policy_gradient_loss | -0.00638    |
|    reward_explained_... | 0.463       |
|    reward_value_loss    | 0.442       |
|    total_cost           | 1198.0      |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.6e+04    |
|    mean_ep_length       | 17          |
|    mean_reward          | -9.6e+04    |
|    true_cost            | 0.545       |
| infos/                  |             |
|    cost                 | 0.0359      |
| rollout/                |             |
|    adjusted_reward      | 631         |
|    ep_len_mean          | 16.3        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 425         |
|    iterations           | 8           |
|    time_elapsed         | 38          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.015884982 |
|    average_cost         | 0.5083008   |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.142       |
|    cost_value_loss      | 0.0168      |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -0.925      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00371     |
|    mean_cost_advantages | -0.12559369 |
|    mean_reward_advan... | -0.38033283 |
|    n_updates            | 70          |
|    nu                   | 1.49        |
|    nu_loss              | -0.72       |
|    policy_gradient_loss | -0.00838    |
|    reward_explained_... | 0.498       |
|    reward_value_loss    | 0.0557      |
|    total_cost           | 1041.0      |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.6e+04    |
|    mean_ep_length       | 14.2        |
|    mean_reward          | -6.4e+04    |
|    true_cost            | 0.535       |
| infos/                  |             |
|    cost                 | 0.0362      |
| rollout/                |             |
|    adjusted_reward      | 671         |
|    ep_len_mean          | 15.3        |
|    ep_rew_mean          | 9.99e+03    |
| time/                   |             |
|    fps                  | 427         |
|    iterations           | 9           |
|    time_elapsed         | 43          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.01615566  |
|    average_cost         | 0.54541016  |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.73        |
|    cost_value_loss      | 0.0105      |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -0.833      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00241     |
|    mean_cost_advantages | -0.06895037 |
|    mean_reward_advan... | -0.37232453 |
|    n_updates            | 80          |
|    nu                   | 1.57        |
|    nu_loss              | -0.814      |
|    policy_gradient_loss | -0.00694    |
|    reward_explained_... | 0.502       |
|    reward_value_loss    | 0.0315      |
|    total_cost           | 1117.0      |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.4e+04     |
|    mean_ep_length       | 13.2         |
|    mean_reward          | -5.4e+04     |
|    true_cost            | 0.546        |
| infos/                  |              |
|    cost                 | 0.0292       |
| rollout/                |              |
|    adjusted_reward      | 749          |
|    ep_len_mean          | 13.7         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 10           |
|    time_elapsed         | 47           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.016764907  |
|    average_cost         | 0.53466797   |
|    clip_fraction        | 0.177        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.763        |
|    cost_value_loss      | 0.00788      |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.766       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00146      |
|    mean_cost_advantages | -0.026126651 |
|    mean_reward_advan... | -0.33698052  |
|    n_updates            | 90           |
|    nu                   | 1.65         |
|    nu_loss              | -0.84        |
|    policy_gradient_loss | -0.0094      |
|    reward_explained_... | 0.522        |
|    reward_value_loss    | 0.0206       |
|    total_cost           | 1095.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.4e+04     |
|    mean_ep_length       | 13.4         |
|    mean_reward          | -6.2e+04     |
|    true_cost            | 0.565        |
| infos/                  |              |
|    cost                 | 0.0326       |
| rollout/                |              |
|    adjusted_reward      | 766          |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 427          |
|    iterations           | 11           |
|    time_elapsed         | 52           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.009824237  |
|    average_cost         | 0.5463867    |
|    clip_fraction        | 0.132        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.847        |
|    cost_value_loss      | 0.00449      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.699       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0081      |
|    mean_cost_advantages | -0.028912572 |
|    mean_reward_advan... | -0.24627456  |
|    n_updates            | 100          |
|    nu                   | 1.73         |
|    nu_loss              | -0.902       |
|    policy_gradient_loss | -0.00599     |
|    reward_explained_... | 0.687        |
|    reward_value_loss    | 0.00927      |
|    total_cost           | 1119.0       |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.4e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -6e+04       |
|    true_cost            | 0.547        |
| infos/                  |              |
|    cost                 | 0.0336       |
| rollout/                |              |
|    adjusted_reward      | 783          |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 12           |
|    time_elapsed         | 56           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.015684012  |
|    average_cost         | 0.5649414    |
|    clip_fraction        | 0.0723       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.85         |
|    cost_value_loss      | 0.0043       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.657       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000347    |
|    mean_cost_advantages | 0.0020645102 |
|    mean_reward_advan... | -0.21148787  |
|    n_updates            | 110          |
|    nu                   | 1.81         |
|    nu_loss              | -0.978       |
|    policy_gradient_loss | -0.00481     |
|    reward_explained_... | 0.758        |
|    reward_value_loss    | 0.0104       |
|    total_cost           | 1157.0       |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.4e+04     |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -5.6e+04     |
|    true_cost            | 0.511        |
| infos/                  |              |
|    cost                 | 0.0346       |
| rollout/                |              |
|    adjusted_reward      | 806          |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 430          |
|    iterations           | 13           |
|    time_elapsed         | 61           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.015114028  |
|    average_cost         | 0.546875     |
|    clip_fraction        | 0.0908       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.881        |
|    cost_value_loss      | 0.00328      |
|    early_stop_epoch     | 8            |
|    entropy_loss         | -0.59        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0037      |
|    mean_cost_advantages | -0.009553298 |
|    mean_reward_advan... | -0.16041099  |
|    n_updates            | 120          |
|    nu                   | 1.9          |
|    nu_loss              | -0.993       |
|    policy_gradient_loss | -0.0046      |
|    reward_explained_... | 0.78         |
|    reward_value_loss    | 0.00491      |
|    total_cost           | 1120.0       |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.4e+04     |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -5.8e+04     |
|    true_cost            | 0.526        |
| infos/                  |              |
|    cost                 | 0.0329       |
| rollout/                |              |
|    adjusted_reward      | 824          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 428          |
|    iterations           | 14           |
|    time_elapsed         | 66           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.010454588  |
|    average_cost         | 0.5107422    |
|    clip_fraction        | 0.0646       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.923        |
|    cost_value_loss      | 0.00206      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.538       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00653     |
|    mean_cost_advantages | -0.016099827 |
|    mean_reward_advan... | -0.12788951  |
|    n_updates            | 130          |
|    nu                   | 1.99         |
|    nu_loss              | -0.97        |
|    policy_gradient_loss | -0.00335     |
|    reward_explained_... | 0.844        |
|    reward_value_loss    | 0.00322      |
|    total_cost           | 1046.0       |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.4e+04     |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -5.4e+04     |
|    true_cost            | 0.499        |
| infos/                  |              |
|    cost                 | 0.0331       |
| rollout/                |              |
|    adjusted_reward      | 828          |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 15           |
|    time_elapsed         | 71           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.016168818  |
|    average_cost         | 0.5258789    |
|    clip_fraction        | 0.0774       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.931        |
|    cost_value_loss      | 0.00174      |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.519       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000828     |
|    mean_cost_advantages | 0.0002902845 |
|    mean_reward_advan... | -0.10747354  |
|    n_updates            | 140          |
|    nu                   | 2.07         |
|    nu_loss              | -1.04        |
|    policy_gradient_loss | -0.00222     |
|    reward_explained_... | 0.916        |
|    reward_value_loss    | 0.00223      |
|    total_cost           | 1077.0       |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.4e+04      |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -4.4e+04      |
|    true_cost            | 0.488         |
| infos/                  |               |
|    cost                 | 0.036         |
| rollout/                |               |
|    adjusted_reward      | 826           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 422           |
|    iterations           | 16            |
|    time_elapsed         | 77            |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 0.0070523205  |
|    average_cost         | 0.49902344    |
|    clip_fraction        | 0.0464        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.927         |
|    cost_value_loss      | 0.00197       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.481        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000916     |
|    mean_cost_advantages | -0.0070851673 |
|    mean_reward_advan... | -0.09324443   |
|    n_updates            | 150           |
|    nu                   | 2.16          |
|    nu_loss              | -1.04         |
|    policy_gradient_loss | -0.00239      |
|    reward_explained_... | 0.915         |
|    reward_value_loss    | 0.00164       |
|    total_cost           | 1022.0        |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -4.4e+04       |
|    mean_ep_length       | 12.2           |
|    mean_reward          | -4.8e+04       |
|    true_cost            | 0.476          |
| infos/                  |                |
|    cost                 | 0.0355         |
| rollout/                |                |
|    adjusted_reward      | 820            |
|    ep_len_mean          | 12.3           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 417            |
|    iterations           | 17             |
|    time_elapsed         | 83             |
|    total_timesteps      | 34816          |
| train/                  |                |
|    approx_kl            | 0.008358672    |
|    average_cost         | 0.48828125     |
|    clip_fraction        | 0.0725         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.931          |
|    cost_value_loss      | 0.00168        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.436         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00439       |
|    mean_cost_advantages | -0.00076959404 |
|    mean_reward_advan... | -0.080130056   |
|    n_updates            | 160            |
|    nu                   | 2.25           |
|    nu_loss              | -1.06          |
|    policy_gradient_loss | -0.00281       |
|    reward_explained_... | 0.919          |
|    reward_value_loss    | 0.00148        |
|    total_cost           | 1000.0         |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -4.4e+04       |
|    mean_ep_length       | 12             |
|    mean_reward          | -5.2e+04       |
|    true_cost            | 0.461          |
| infos/                  |                |
|    cost                 | 0.0334         |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12.1           |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 414            |
|    iterations           | 18             |
|    time_elapsed         | 88             |
|    total_timesteps      | 36864          |
| train/                  |                |
|    approx_kl            | 0.0033838642   |
|    average_cost         | 0.47558594     |
|    clip_fraction        | 0.0409         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.949          |
|    cost_value_loss      | 0.00127        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.413         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00386       |
|    mean_cost_advantages | -0.00050952693 |
|    mean_reward_advan... | -0.06757937    |
|    n_updates            | 170            |
|    nu                   | 2.34           |
|    nu_loss              | -1.07          |
|    policy_gradient_loss | -0.00186       |
|    reward_explained_... | 0.941          |
|    reward_value_loss    | 0.00103        |
|    total_cost           | 974.0          |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.4e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -4.4e+04      |
|    true_cost            | 0.469         |
| infos/                  |               |
|    cost                 | 0.0341        |
| rollout/                |               |
|    adjusted_reward      | 829           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 412           |
|    iterations           | 19            |
|    time_elapsed         | 94            |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 0.005123184   |
|    average_cost         | 0.46142578    |
|    clip_fraction        | 0.0458        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.957         |
|    cost_value_loss      | 0.00116       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.382        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00135      |
|    mean_cost_advantages | -0.0048768776 |
|    mean_reward_advan... | -0.05144857   |
|    n_updates            | 180           |
|    nu                   | 2.44          |
|    nu_loss              | -1.08         |
|    policy_gradient_loss | -0.000926     |
|    reward_explained_... | 0.97          |
|    reward_value_loss    | 0.000556      |
|    total_cost           | 945.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.4e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -4.6e+04     |
|    true_cost            | 0.453        |
| infos/                  |              |
|    cost                 | 0.0348       |
| rollout/                |              |
|    adjusted_reward      | 838          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 409          |
|    iterations           | 20           |
|    time_elapsed         | 100          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0071101105 |
|    average_cost         | 0.46875      |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.944        |
|    cost_value_loss      | 0.00155      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.347       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00214     |
|    mean_cost_advantages | 0.0033258777 |
|    mean_reward_advan... | -0.047239542 |
|    n_updates            | 190          |
|    nu                   | 2.53         |
|    nu_loss              | -1.14        |
|    policy_gradient_loss | -0.00166     |
|    reward_explained_... | 0.975        |
|    reward_value_loss    | 0.000492     |
|    total_cost           | 960.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.2e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -4.2e+04      |
|    true_cost            | 0.446         |
| infos/                  |               |
|    cost                 | 0.037         |
| rollout/                |               |
|    adjusted_reward      | 833           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 403           |
|    iterations           | 21            |
|    time_elapsed         | 106           |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | -0.0005225996 |
|    average_cost         | 0.453125      |
|    clip_fraction        | 0.0629        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.96          |
|    cost_value_loss      | 0.00109       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.331        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00122       |
|    mean_cost_advantages | -0.001129776  |
|    mean_reward_advan... | -0.039128166  |
|    n_updates            | 200           |
|    nu                   | 2.62          |
|    nu_loss              | -1.15         |
|    policy_gradient_loss | -0.00106      |
|    reward_explained_... | 0.985         |
|    reward_value_loss    | 0.000291      |
|    total_cost           | 928.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.8e+04     |
|    true_cost            | 0.428        |
| infos/                  |              |
|    cost                 | 0.0346       |
| rollout/                |              |
|    adjusted_reward      | 833          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 405          |
|    iterations           | 22           |
|    time_elapsed         | 111          |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.003802618  |
|    average_cost         | 0.44628906   |
|    clip_fraction        | 0.0742       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.966        |
|    cost_value_loss      | 0.000901     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.308       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00283      |
|    mean_cost_advantages | 0.0032459213 |
|    mean_reward_advan... | -0.03385242  |
|    n_updates            | 210          |
|    nu                   | 2.71         |
|    nu_loss              | -1.17        |
|    policy_gradient_loss | -0.000987    |
|    reward_explained_... | 0.988        |
|    reward_value_loss    | 0.000204     |
|    total_cost           | 914.0        |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -3.8e+04       |
|    mean_ep_length       | 12             |
|    mean_reward          | -4.4e+04       |
|    true_cost            | 0.423          |
| infos/                  |                |
|    cost                 | 0.032          |
| rollout/                |                |
|    adjusted_reward      | 837            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 407            |
|    iterations           | 23             |
|    time_elapsed         | 115            |
|    total_timesteps      | 47104          |
| train/                  |                |
|    approx_kl            | 0.002711848    |
|    average_cost         | 0.42822266     |
|    clip_fraction        | 0.0494         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.968          |
|    cost_value_loss      | 0.000844       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.288         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00253        |
|    mean_cost_advantages | -4.8340065e-05 |
|    mean_reward_advan... | -0.033616465   |
|    n_updates            | 220            |
|    nu                   | 2.81           |
|    nu_loss              | -1.16          |
|    policy_gradient_loss | -0.000892      |
|    reward_explained_... | 0.989          |
|    reward_value_loss    | 0.000198       |
|    total_cost           | 877.0          |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -4.2e+04      |
|    true_cost            | 0.421         |
| infos/                  |               |
|    cost                 | 0.0342        |
| rollout/                |               |
|    adjusted_reward      | 832           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 408           |
|    iterations           | 24            |
|    time_elapsed         | 120           |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.0069176834  |
|    average_cost         | 0.42285156    |
|    clip_fraction        | 0.0189        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.976         |
|    cost_value_loss      | 0.000655      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.261        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00114       |
|    mean_cost_advantages | 0.00042898412 |
|    mean_reward_advan... | -0.026274106  |
|    n_updates            | 230           |
|    nu                   | 2.9           |
|    nu_loss              | -1.19         |
|    policy_gradient_loss | -0.00035      |
|    reward_explained_... | 0.992         |
|    reward_value_loss    | 0.000131      |
|    total_cost           | 866.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -4.2e+04     |
|    true_cost            | 0.421        |
| infos/                  |              |
|    cost                 | 0.0364       |
| rollout/                |              |
|    adjusted_reward      | 832          |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 409          |
|    iterations           | 25           |
|    time_elapsed         | 124          |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0017745914 |
|    average_cost         | 0.42089844   |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.982        |
|    cost_value_loss      | 0.00049      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.242       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0024       |
|    mean_cost_advantages | 0.0017762923 |
|    mean_reward_advan... | -0.025969703 |
|    n_updates            | 240          |
|    nu                   | 2.99         |
|    nu_loss              | -1.22        |
|    policy_gradient_loss | -0.00076     |
|    reward_explained_... | 0.994        |
|    reward_value_loss    | 0.000114     |
|    total_cost           | 862.0        |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -4.2e+04     |
|    true_cost            | 0.404        |
| infos/                  |              |
|    cost                 | 0.0328       |
| rollout/                |              |
|    adjusted_reward      | 837          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 413          |
|    iterations           | 26           |
|    time_elapsed         | 128          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.017361676  |
|    average_cost         | 0.42089844   |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.977        |
|    cost_value_loss      | 0.00067      |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.244       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00323     |
|    mean_cost_advantages | 0.006425039  |
|    mean_reward_advan... | -0.021511719 |
|    n_updates            | 250          |
|    nu                   | 3.09         |
|    nu_loss              | -1.26        |
|    policy_gradient_loss | -0.00115     |
|    reward_explained_... | 0.99         |
|    reward_value_loss    | 0.000182     |
|    total_cost           | 862.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.8e+04     |
|    true_cost            | 0.413        |
| infos/                  |              |
|    cost                 | 0.0368       |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 415          |
|    iterations           | 27           |
|    time_elapsed         | 133          |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0061786124 |
|    average_cost         | 0.4038086    |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.977        |
|    cost_value_loss      | 0.00065      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.246       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000917    |
|    mean_cost_advantages | -0.002419543 |
|    mean_reward_advan... | -0.01869289  |
|    n_updates            | 260          |
|    nu                   | 3.18         |
|    nu_loss              | -1.25        |
|    policy_gradient_loss | -0.000445    |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 7.72e-05     |
|    total_cost           | 827.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.8e+04     |
|    true_cost            | 0.403        |
| infos/                  |              |
|    cost                 | 0.0347       |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 418          |
|    iterations           | 28           |
|    time_elapsed         | 137          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0012154797 |
|    average_cost         | 0.41259766   |
|    clip_fraction        | 0.0311       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.985        |
|    cost_value_loss      | 0.000464     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.242       |
|    learning_rate        | 0.0003       |
|    loss                 | -3.05e-05    |
|    mean_cost_advantages | 0.0038386865 |
|    mean_reward_advan... | -0.018968122 |
|    n_updates            | 270          |
|    nu                   | 3.28         |
|    nu_loss              | -1.31        |
|    policy_gradient_loss | -0.000378    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 7.26e-05     |
|    total_cost           | 845.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -4e+04       |
|    true_cost            | 0.397        |
| infos/                  |              |
|    cost                 | 0.0344       |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 420          |
|    iterations           | 29           |
|    time_elapsed         | 141          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0077903834 |
|    average_cost         | 0.4033203    |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.984        |
|    cost_value_loss      | 0.000492     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.242       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00302     |
|    mean_cost_advantages | 0.002246365  |
|    mean_reward_advan... | -0.01775155  |
|    n_updates            | 280          |
|    nu                   | 3.37         |
|    nu_loss              | -1.32        |
|    policy_gradient_loss | -0.000928    |
|    reward_explained_... | 0.996        |
|    reward_value_loss    | 7e-05        |
|    total_cost           | 826.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -4e+04       |
|    true_cost            | 0.395        |
| infos/                  |              |
|    cost                 | 0.0366       |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 422          |
|    iterations           | 30           |
|    time_elapsed         | 145          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0031916033 |
|    average_cost         | 0.39746094   |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.971        |
|    cost_value_loss      | 0.000902     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.238       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00168      |
|    mean_cost_advantages | 0.0010440093 |
|    mean_reward_advan... | -0.015931332 |
|    n_updates            | 290          |
|    nu                   | 3.46         |
|    nu_loss              | -1.34        |
|    policy_gradient_loss | -0.000808    |
|    reward_explained_... | 0.994        |
|    reward_value_loss    | 0.000125     |
|    total_cost           | 814.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.6e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3.6e+04      |
|    true_cost            | 0.386         |
| infos/                  |               |
|    cost                 | 0.0353        |
| rollout/                |               |
|    adjusted_reward      | 836           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 424           |
|    iterations           | 31            |
|    time_elapsed         | 149           |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | -0.0007687381 |
|    average_cost         | 0.39453125    |
|    clip_fraction        | 0.0959        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.977         |
|    cost_value_loss      | 0.000749      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.235        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00956       |
|    mean_cost_advantages | -0.0005632424 |
|    mean_reward_advan... | -0.016180865  |
|    n_updates            | 300           |
|    nu                   | 3.56          |
|    nu_loss              | -1.37         |
|    policy_gradient_loss | -0.000453     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.4e-05       |
|    total_cost           | 808.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.4e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.4e+04     |
|    true_cost            | 0.375        |
| infos/                  |              |
|    cost                 | 0.0358       |
| rollout/                |              |
|    adjusted_reward      | 831          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 426          |
|    iterations           | 32           |
|    time_elapsed         | 153          |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0021028381 |
|    average_cost         | 0.38623047   |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.979        |
|    cost_value_loss      | 0.000653     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.23        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00172      |
|    mean_cost_advantages | 0.0020215292 |
|    mean_reward_advan... | -0.01399584  |
|    n_updates            | 310          |
|    nu                   | 3.65         |
|    nu_loss              | -1.37        |
|    policy_gradient_loss | -0.000252    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 2.77e-05     |
|    total_cost           | 791.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.4e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.6e+04     |
|    true_cost            | 0.371        |
| infos/                  |              |
|    cost                 | 0.0316       |
| rollout/                |              |
|    adjusted_reward      | 836          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 428          |
|    iterations           | 33           |
|    time_elapsed         | 157          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0049383114 |
|    average_cost         | 0.37548828   |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.971        |
|    cost_value_loss      | 0.000835     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.234       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000705    |
|    mean_cost_advantages | 0.0042641405 |
|    mean_reward_advan... | -0.012834883 |
|    n_updates            | 320          |
|    nu                   | 3.74         |
|    nu_loss              | -1.37        |
|    policy_gradient_loss | -0.000178    |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 5.27e-05     |
|    total_cost           | 769.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.4e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3.6e+04      |
|    true_cost            | 0.369         |
| infos/                  |               |
|    cost                 | 0.0367        |
| rollout/                |               |
|    adjusted_reward      | 831           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 429           |
|    iterations           | 34            |
|    time_elapsed         | 161           |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.010684751   |
|    average_cost         | 0.37109375    |
|    clip_fraction        | 0.0615        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.976         |
|    cost_value_loss      | 0.00073       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.237        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0038       |
|    mean_cost_advantages | 0.00062396255 |
|    mean_reward_advan... | -0.012271131  |
|    n_updates            | 330           |
|    nu                   | 3.84          |
|    nu_loss              | -1.39         |
|    policy_gradient_loss | -0.000592     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 4.79e-05      |
|    total_cost           | 760.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3.2e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.2e+04     |
|    true_cost            | 0.358        |
| infos/                  |              |
|    cost                 | 0.0315       |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 35           |
|    time_elapsed         | 166          |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0053565986 |
|    average_cost         | 0.36865234   |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.977        |
|    cost_value_loss      | 0.000703     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.225       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000802    |
|    mean_cost_advantages | 0.0036593655 |
|    mean_reward_advan... | -0.010946487 |
|    n_updates            | 340          |
|    nu                   | 3.93         |
|    nu_loss              | -1.41        |
|    policy_gradient_loss | -0.00076     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 2.65e-05     |
|    total_cost           | 755.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.2e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3.2e+04      |
|    true_cost            | 0.348         |
| infos/                  |               |
|    cost                 | 0.0347        |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 432           |
|    iterations           | 36            |
|    time_elapsed         | 170           |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.0024369175  |
|    average_cost         | 0.35791016    |
|    clip_fraction        | 0.0248        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.977         |
|    cost_value_loss      | 0.000716      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.213        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000364     |
|    mean_cost_advantages | 0.00021471333 |
|    mean_reward_advan... | -0.010343254  |
|    n_updates            | 350           |
|    nu                   | 4.02          |
|    nu_loss              | -1.41         |
|    policy_gradient_loss | -0.000524     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.95e-05      |
|    total_cost           | 733.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3.2e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3.4e+04      |
|    true_cost            | 0.345         |
| infos/                  |               |
|    cost                 | 0.0332        |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 434           |
|    iterations           | 37            |
|    time_elapsed         | 174           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.005548974   |
|    average_cost         | 0.34765625    |
|    clip_fraction        | 0.0481        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.981         |
|    cost_value_loss      | 0.000446      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.201        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00238       |
|    mean_cost_advantages | -0.0015886347 |
|    mean_reward_advan... | -0.010288671  |
|    n_updates            | 360           |
|    nu                   | 4.12          |
|    nu_loss              | -1.4          |
|    policy_gradient_loss | -0.00052      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 7.65e-05      |
|    total_cost           | 712.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -3e+04       |
|    true_cost            | 0.344        |
| infos/                  |              |
|    cost                 | 0.0365       |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 38           |
|    time_elapsed         | 178          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.002883372  |
|    average_cost         | 0.34521484   |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.983        |
|    cost_value_loss      | 0.000551     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.186       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000787    |
|    mean_cost_advantages | 0.0025329958 |
|    mean_reward_advan... | -0.009858364 |
|    n_updates            | 370          |
|    nu                   | 4.21         |
|    nu_loss              | -1.42        |
|    policy_gradient_loss | -0.000331    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.9e-05      |
|    total_cost           | 707.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -3e+04       |
|    mean_ep_length       | 12           |
|    mean_reward          | -3.2e+04     |
|    true_cost            | 0.339        |
| infos/                  |              |
|    cost                 | 0.032        |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 435          |
|    iterations           | 39           |
|    time_elapsed         | 183          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0042101177 |
|    average_cost         | 0.34375      |
|    clip_fraction        | 0.0272       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.987        |
|    cost_value_loss      | 0.000385     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.183       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000886     |
|    mean_cost_advantages | 0.0022820842 |
|    mean_reward_advan... | -0.008735988 |
|    n_updates            | 380          |
|    nu                   | 4.3          |
|    nu_loss              | -1.45        |
|    policy_gradient_loss | -3.1e-05     |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.32e-05     |
|    total_cost           | 704.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -3e+04        |
|    mean_ep_length       | 12            |
|    mean_reward          | -3e+04        |
|    true_cost            | 0.338         |
| infos/                  |               |
|    cost                 | 0.0323        |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 436           |
|    iterations           | 40            |
|    time_elapsed         | 187           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 0.004034874   |
|    average_cost         | 0.3388672     |
|    clip_fraction        | 0.0439        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.989         |
|    cost_value_loss      | 0.000328      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.175        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000984     |
|    mean_cost_advantages | -0.0002264423 |
|    mean_reward_advan... | -0.008183492  |
|    n_updates            | 390           |
|    nu                   | 4.39          |
|    nu_loss              | -1.46         |
|    policy_gradient_loss | -0.000289     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 9.85e-06      |
|    total_cost           | 694.0         |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -2.8e+04       |
|    mean_ep_length       | 12             |
|    mean_reward          | -2.8e+04       |
|    true_cost            | 0.338          |
| infos/                  |                |
|    cost                 | 0.0347         |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 437            |
|    iterations           | 41             |
|    time_elapsed         | 191            |
|    total_timesteps      | 83968          |
| train/                  |                |
|    approx_kl            | 0.0014422976   |
|    average_cost         | 0.33789062     |
|    clip_fraction        | 0.0308         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.991          |
|    cost_value_loss      | 0.000281       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.164         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00314       |
|    mean_cost_advantages | -0.00055555796 |
|    mean_reward_advan... | -0.007458899   |
|    n_updates            | 400            |
|    nu                   | 4.48           |
|    nu_loss              | -1.48          |
|    policy_gradient_loss | -0.000372      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 9.05e-06       |
|    total_cost           | 692.0          |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3e+04        |
|    true_cost            | 0.335         |
| infos/                  |               |
|    cost                 | 0.033         |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 438           |
|    iterations           | 42            |
|    time_elapsed         | 195           |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 0.0028309277  |
|    average_cost         | 0.33789062    |
|    clip_fraction        | 0.0297        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.987         |
|    cost_value_loss      | 0.000344      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.152        |
|    learning_rate        | 0.0003        |
|    loss                 | -6.2e-05      |
|    mean_cost_advantages | 0.003652046   |
|    mean_reward_advan... | -0.0075681726 |
|    n_updates            | 410           |
|    nu                   | 4.57          |
|    nu_loss              | -1.51         |
|    policy_gradient_loss | -0.000465     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 3.69e-05      |
|    total_cost           | 692.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -2.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -3e+04       |
|    true_cost            | 0.336        |
| infos/                  |              |
|    cost                 | 0.0364       |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 440          |
|    iterations           | 43           |
|    time_elapsed         | 200          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0032047324 |
|    average_cost         | 0.33544922   |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.994        |
|    cost_value_loss      | 0.000146     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.153       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000347     |
|    mean_cost_advantages | 0.002388597  |
|    mean_reward_advan... | -0.006941455 |
|    n_updates            | 420          |
|    nu                   | 4.66         |
|    nu_loss              | -1.53        |
|    policy_gradient_loss | -0.000184    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 1.45e-05     |
|    total_cost           | 687.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3e+04        |
|    true_cost            | 0.335         |
| infos/                  |               |
|    cost                 | 0.0336        |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 441           |
|    iterations           | 44            |
|    time_elapsed         | 204           |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.0052587837  |
|    average_cost         | 0.33642578    |
|    clip_fraction        | 0.0408        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.995         |
|    cost_value_loss      | 0.000156      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.154        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000158      |
|    mean_cost_advantages | 0.0034738083  |
|    mean_reward_advan... | -0.0071178745 |
|    n_updates            | 430           |
|    nu                   | 4.75          |
|    nu_loss              | -1.57         |
|    policy_gradient_loss | -0.000111     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 8.69e-06      |
|    total_cost           | 689.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3e+04        |
|    true_cost            | 0.334         |
| infos/                  |               |
|    cost                 | 0.035         |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 441           |
|    iterations           | 45            |
|    time_elapsed         | 208           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.0018752421  |
|    average_cost         | 0.33496094    |
|    clip_fraction        | 0.0334        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.994         |
|    cost_value_loss      | 0.00018       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.158        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00427      |
|    mean_cost_advantages | 0.0030708015  |
|    mean_reward_advan... | -0.0052188025 |
|    n_updates            | 440           |
|    nu                   | 4.84          |
|    nu_loss              | -1.59         |
|    policy_gradient_loss | -9.01e-05     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.53e-05      |
|    total_cost           | 686.0         |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3e+04        |
|    true_cost            | 0.33          |
| infos/                  |               |
|    cost                 | 0.0354        |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 442           |
|    iterations           | 46            |
|    time_elapsed         | 213           |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 0.017346941   |
|    average_cost         | 0.33447266    |
|    clip_fraction        | 0.0477        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.994         |
|    cost_value_loss      | 0.000197      |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.169        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000952     |
|    mean_cost_advantages | -0.0018654345 |
|    mean_reward_advan... | -0.0031749925 |
|    n_updates            | 450           |
|    nu                   | 4.92          |
|    nu_loss              | -1.62         |
|    policy_gradient_loss | -0.000305     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.78e-05      |
|    total_cost           | 685.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3e+04        |
|    true_cost            | 0.333         |
| infos/                  |               |
|    cost                 | 0.0346        |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 441           |
|    iterations           | 47            |
|    time_elapsed         | 217           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 0.0015691896  |
|    average_cost         | 0.32958984    |
|    clip_fraction        | 0.0461        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.994         |
|    cost_value_loss      | 0.000223      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.182        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00152      |
|    mean_cost_advantages | 0.001869597   |
|    mean_reward_advan... | -0.0047515994 |
|    n_updates            | 460           |
|    nu                   | 5.01          |
|    nu_loss              | -1.62         |
|    policy_gradient_loss | -4.2e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.65e-05      |
|    total_cost           | 675.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -2.8e+04      |
|    true_cost            | 0.327         |
| infos/                  |               |
|    cost                 | 0.0371        |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 442           |
|    iterations           | 48            |
|    time_elapsed         | 222           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.0010320342  |
|    average_cost         | 0.33251953    |
|    clip_fraction        | 0.0196        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.998         |
|    cost_value_loss      | 5.83e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.191        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000844     |
|    mean_cost_advantages | 0.0061947736  |
|    mean_reward_advan... | -0.0058107628 |
|    n_updates            | 470           |
|    nu                   | 5.1           |
|    nu_loss              | -1.67         |
|    policy_gradient_loss | -0.000112     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 4.27e-06      |
|    total_cost           | 681.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -3e+04        |
|    true_cost            | 0.308         |
| infos/                  |               |
|    cost                 | 0.0297        |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 442           |
|    iterations           | 49            |
|    time_elapsed         | 226           |
|    total_timesteps      | 100352        |
| train/                  |               |
|    approx_kl            | 0.010287991   |
|    average_cost         | 0.32714844    |
|    clip_fraction        | 0.0399        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.988         |
|    cost_value_loss      | 0.000425      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.215        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0033       |
|    mean_cost_advantages | 0.00022689789 |
|    mean_reward_advan... | -0.004977371  |
|    n_updates            | 480           |
|    nu                   | 5.19          |
|    nu_loss              | -1.67         |
|    policy_gradient_loss | -0.000944     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 1.05e-05      |
|    total_cost           | 670.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.8e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -2.8e+04      |
|    true_cost            | 0.292         |
| infos/                  |               |
|    cost                 | 0.0356        |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 443           |
|    iterations           | 50            |
|    time_elapsed         | 230           |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.011450315   |
|    average_cost         | 0.30810547    |
|    clip_fraction        | 0.133         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.969         |
|    cost_value_loss      | 0.00104       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.235        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00392      |
|    mean_cost_advantages | -0.0045409305 |
|    mean_reward_advan... | -0.0036944593 |
|    n_updates            | 490           |
|    nu                   | 5.28          |
|    nu_loss              | -1.6          |
|    policy_gradient_loss | -0.00135      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 4.23e-05      |
|    total_cost           | 631.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.2e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -2.2e+04      |
|    true_cost            | 0.28          |
| infos/                  |               |
|    cost                 | 0.0336        |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 444           |
|    iterations           | 51            |
|    time_elapsed         | 235           |
|    total_timesteps      | 104448        |
| train/                  |               |
|    approx_kl            | 0.0034124316  |
|    average_cost         | 0.2919922     |
|    clip_fraction        | 0.0651        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.955         |
|    cost_value_loss      | 0.00131       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.232        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000211      |
|    mean_cost_advantages | -0.00934362   |
|    mean_reward_advan... | -0.0031344593 |
|    n_updates            | 500           |
|    nu                   | 5.36          |
|    nu_loss              | -1.54         |
|    policy_gradient_loss | -0.000588     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 7.63e-06      |
|    total_cost           | 598.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2.2e+04      |
|    mean_ep_length       | 12            |
|    mean_reward          | -2.4e+04      |
|    true_cost            | 0.266         |
| infos/                  |               |
|    cost                 | 0.0282        |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 445           |
|    iterations           | 52            |
|    time_elapsed         | 239           |
|    total_timesteps      | 106496        |
| train/                  |               |
|    approx_kl            | 0.007516681   |
|    average_cost         | 0.28027344    |
|    clip_fraction        | 0.0745        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.955         |
|    cost_value_loss      | 0.00129       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.23         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00119      |
|    mean_cost_advantages | -0.005909941  |
|    mean_reward_advan... | -0.0038568801 |
|    n_updates            | 510           |
|    nu                   | 5.45          |
|    nu_loss              | -1.5          |
|    policy_gradient_loss | -0.00121      |
|    reward_explained_... | 0.994         |
|    reward_value_loss    | 0.000113      |
|    total_cost           | 574.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -2e+04        |
|    mean_ep_length       | 12            |
|    mean_reward          | -2e+04        |
|    true_cost            | 0.249         |
| infos/                  |               |
|    cost                 | 0.0251        |
| rollout/                |               |
|    adjusted_reward      | 825           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 445           |
|    iterations           | 53            |
|    time_elapsed         | 243           |
|    total_timesteps      | 108544        |
| train/                  |               |
|    approx_kl            | 0.009367304   |
|    average_cost         | 0.265625      |
|    clip_fraction        | 0.0722        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.926         |
|    cost_value_loss      | 0.00193       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.221        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00109      |
|    mean_cost_advantages | -0.0018856926 |
|    mean_reward_advan... | -0.0030609434 |
|    n_updates            | 520           |
|    nu                   | 5.53          |
|    nu_loss              | -1.45         |
|    policy_gradient_loss | -0.000789     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 1.36e-05      |
|    total_cost           | 544.0         |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -1.8e+04     |
|    mean_ep_length       | 12           |
|    mean_reward          | -1.8e+04     |
|    true_cost            | 0.209        |
| infos/                  |              |
|    cost                 | 0.0253       |
| rollout/                |              |
|    adjusted_reward      | 830          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 447          |
|    iterations           | 54           |
|    time_elapsed         | 247          |
|    total_timesteps      | 110592       |
| train/                  |              |
|    approx_kl            | 0.01963327   |
|    average_cost         | 0.24853516   |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.943        |
|    cost_value_loss      | 0.00137      |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.232       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0066      |
|    mean_cost_advantages | -0.008061135 |
|    mean_reward_advan... | -0.007381931 |
|    n_updates            | 530          |
|    nu                   | 5.62         |
|    nu_loss              | -1.38        |
|    policy_gradient_loss | -0.00169     |
|    reward_explained_... | 0.94         |
|    reward_value_loss    | 0.0013       |
|    total_cost           | 509.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4e+03       |
|    mean_ep_length       | 12           |
|    mean_reward          | -4e+03       |
|    true_cost            | 0.115        |
| infos/                  |              |
|    cost                 | 0.0116       |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 449          |
|    iterations           | 55           |
|    time_elapsed         | 250          |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.04586704   |
|    average_cost         | 0.20898438   |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.866        |
|    cost_value_loss      | 0.00286      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.251       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00261     |
|    mean_cost_advantages | -0.028036147 |
|    mean_reward_advan... | 0.002514411  |
|    n_updates            | 540          |
|    nu                   | 5.7          |
|    nu_loss              | -1.17        |
|    policy_gradient_loss | -0.00261     |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 5.87e-05     |
|    total_cost           | 428.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 1.99e+03      |
|    true_cost            | 0.0483        |
| infos/                  |               |
|    cost                 | 0.00698       |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 56            |
|    time_elapsed         | 254           |
|    total_timesteps      | 114688        |
| train/                  |               |
|    approx_kl            | 0.018086772   |
|    average_cost         | 0.114746094   |
|    clip_fraction        | 0.149         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.49          |
|    cost_value_loss      | 0.0074        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.256        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00509      |
|    mean_cost_advantages | -0.061593797  |
|    mean_reward_advan... | -0.0023193888 |
|    n_updates            | 550           |
|    nu                   | 5.77          |
|    nu_loss              | -0.654        |
|    policy_gradient_loss | -0.00267      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.03e-05      |
|    total_cost           | 235.0         |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 5.99e+03     |
|    true_cost            | 0.022        |
| infos/                  |              |
|    cost                 | 0.00234      |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 451          |
|    iterations           | 57           |
|    time_elapsed         | 258          |
|    total_timesteps      | 116736       |
| train/                  |              |
|    approx_kl            | 0.016588159  |
|    average_cost         | 0.048339844  |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0962       |
|    cost_value_loss      | 0.0058       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.244       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00401     |
|    mean_cost_advantages | -0.055717863 |
|    mean_reward_advan... | -0.003499541 |
|    n_updates            | 560          |
|    nu                   | 5.84         |
|    nu_loss              | -0.279       |
|    policy_gradient_loss | -0.00169     |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 2.88e-05     |
|    total_cost           | 99.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00488       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 825           |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 451           |
|    iterations           | 58            |
|    time_elapsed         | 263           |
|    total_timesteps      | 118784        |
| train/                  |               |
|    approx_kl            | 0.0092913825  |
|    average_cost         | 0.021972656   |
|    clip_fraction        | 0.0941        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.288        |
|    cost_value_loss      | 0.00258       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.22         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00153       |
|    mean_cost_advantages | -0.02416914   |
|    mean_reward_advan... | -0.0010544639 |
|    n_updates            | 570           |
|    nu                   | 5.91          |
|    nu_loss              | -0.128        |
|    policy_gradient_loss | -0.000965     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 2.87e-05      |
|    total_cost           | 45.0          |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 9.99e+03     |
|    mean_ep_length       | 12           |
|    mean_reward          | 9.99e+03     |
|    true_cost            | 0.00391      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 835          |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 9.99e+03     |
| time/                   |              |
|    fps                  | 451          |
|    iterations           | 59           |
|    time_elapsed         | 267          |
|    total_timesteps      | 120832       |
| train/                  |              |
|    approx_kl            | 0.0047211437 |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.0401       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.33        |
|    cost_value_loss      | 0.000706     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.201       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00168      |
|    mean_cost_advantages | -0.014053166 |
|    mean_reward_advan... | -0.005559736 |
|    n_updates            | 580          |
|    nu                   | 5.97         |
|    nu_loss              | -0.0288      |
|    policy_gradient_loss | -0.000596    |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 0.000125     |
|    total_cost           | 10.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 60            |
|    time_elapsed         | 272           |
|    total_timesteps      | 122880        |
| train/                  |               |
|    approx_kl            | 0.0067979298  |
|    average_cost         | 0.00390625    |
|    clip_fraction        | 0.0682        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.18         |
|    cost_value_loss      | 0.000514      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.192        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00217      |
|    mean_cost_advantages | -0.0059717847 |
|    mean_reward_advan... | 0.00019171246 |
|    n_updates            | 590           |
|    nu                   | 6.02          |
|    nu_loss              | -0.0233       |
|    policy_gradient_loss | -0.000398     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 4.4e-05       |
|    total_cost           | 8.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 61            |
|    time_elapsed         | 277           |
|    total_timesteps      | 124928        |
| train/                  |               |
|    approx_kl            | 0.0016717025  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0203        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00712       |
|    cost_value_loss      | 2.21e-05      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.176        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000346      |
|    mean_cost_advantages | -0.0032020328 |
|    mean_reward_advan... | -0.0021427954 |
|    n_updates            | 600           |
|    nu                   | 6.07          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000369     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.06e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 62            |
|    time_elapsed         | 281           |
|    total_timesteps      | 126976        |
| train/                  |               |
|    approx_kl            | 0.0071521373  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0574        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -6.34         |
|    cost_value_loss      | 0.000144      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.167        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00101       |
|    mean_cost_advantages | -0.0008137254 |
|    mean_reward_advan... | -0.0016037268 |
|    n_updates            | 610           |
|    nu                   | 6.11          |
|    nu_loss              | -0.00593      |
|    policy_gradient_loss | -0.000542     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 9.26e-05      |
|    total_cost           | 2.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0.000977       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 63             |
|    time_elapsed         | 286            |
|    total_timesteps      | 129024         |
| train/                  |                |
|    approx_kl            | 0.01922524     |
|    average_cost         | 0.0009765625   |
|    clip_fraction        | 0.0352         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -7.06          |
|    cost_value_loss      | 0.00014        |
|    early_stop_epoch     | 9              |
|    entropy_loss         | -0.198         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00242       |
|    mean_cost_advantages | -0.0006348145  |
|    mean_reward_advan... | -0.00077381823 |
|    n_updates            | 620            |
|    nu                   | 6.15           |
|    nu_loss              | -0.00597       |
|    policy_gradient_loss | -0.000133      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 7.3e-06        |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 64            |
|    time_elapsed         | 291           |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 0.0004680275  |
|    average_cost         | 0.0009765625  |
|    clip_fraction        | 0.0124        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.12         |
|    cost_value_loss      | 0.000143      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.204        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000197      |
|    mean_cost_advantages | 0.00037175757 |
|    mean_reward_advan... | -0.0025809282 |
|    n_updates            | 630           |
|    nu                   | 6.19          |
|    nu_loss              | -0.00601      |
|    policy_gradient_loss | 6.08e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 5.67e-06      |
|    total_cost           | 2.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 65             |
|    time_elapsed         | 295            |
|    total_timesteps      | 133120         |
| train/                  |                |
|    approx_kl            | 0.0030484905   |
|    average_cost         | 0.0009765625   |
|    clip_fraction        | 0.0228         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -10.1          |
|    cost_value_loss      | 0.000148       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.188         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00014       |
|    mean_cost_advantages | -5.4552627e-05 |
|    mean_reward_advan... | -0.0026163463  |
|    n_updates            | 640            |
|    nu                   | 6.22           |
|    nu_loss              | -0.00604       |
|    policy_gradient_loss | -4.56e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 2.91e-06       |
|    total_cost           | 2.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 66            |
|    time_elapsed         | 300           |
|    total_timesteps      | 135168        |
| train/                  |               |
|    approx_kl            | 0.0016356949  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0201        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0987        |
|    cost_value_loss      | 6.47e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.186        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000209      |
|    mean_cost_advantages | -0.0023469399 |
|    mean_reward_advan... | -0.0031144116 |
|    n_updates            | 650           |
|    nu                   | 6.25          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 1.18e-05      |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 2.97e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 67             |
|    time_elapsed         | 304            |
|    total_timesteps      | 137216         |
| train/                  |                |
|    approx_kl            | 0.007851844    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0447         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00919       |
|    cost_value_loss      | 5.08e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.216         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00049        |
|    mean_cost_advantages | -0.00079535064 |
|    mean_reward_advan... | -0.0019522544  |
|    n_updates            | 660            |
|    nu                   | 6.28           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000304      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.71e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.00244       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 449           |
|    iterations           | 68            |
|    time_elapsed         | 309           |
|    total_timesteps      | 139264        |
| train/                  |               |
|    approx_kl            | 0.0027140793  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0203        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00629      |
|    cost_value_loss      | 5.7e-06       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.18         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000654     |
|    mean_cost_advantages | -0.0006174127 |
|    mean_reward_advan... | -0.0042015584 |
|    n_updates            | 670           |
|    nu                   | 6.3           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000282     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 3.97e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 449           |
|    iterations           | 69            |
|    time_elapsed         | 314           |
|    total_timesteps      | 141312        |
| train/                  |               |
|    approx_kl            | 0.006296791   |
|    average_cost         | 0.0024414062  |
|    clip_fraction        | 0.0218        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.25         |
|    cost_value_loss      | 0.000447      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.167        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000827     |
|    mean_cost_advantages | 0.0015726575  |
|    mean_reward_advan... | -0.0007966179 |
|    n_updates            | 680           |
|    nu                   | 6.33          |
|    nu_loss              | -0.0154       |
|    policy_gradient_loss | -0.000419     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 2.87e-05      |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 451           |
|    iterations           | 70            |
|    time_elapsed         | 317           |
|    total_timesteps      | 143360        |
| train/                  |               |
|    approx_kl            | 0.019362392   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0494        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.113         |
|    cost_value_loss      | 8.52e-06      |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.193        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00607       |
|    mean_cost_advantages | -0.0032701686 |
|    mean_reward_advan... | -0.0027516535 |
|    n_updates            | 690           |
|    nu                   | 6.35          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000347     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 2.74e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 71             |
|    time_elapsed         | 322            |
|    total_timesteps      | 145408         |
| train/                  |                |
|    approx_kl            | 0.0065997634   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0292         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00523       |
|    cost_value_loss      | 3.26e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.149         |
|    learning_rate        | 0.0003         |
|    loss                 | -5.05e-05      |
|    mean_cost_advantages | -0.00031204126 |
|    mean_reward_advan... | -0.0030325498  |
|    n_updates            | 700            |
|    nu                   | 6.36           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00028       |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.63e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 72            |
|    time_elapsed         | 327           |
|    total_timesteps      | 147456        |
| train/                  |               |
|    approx_kl            | 0.0078016063  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0709        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0165        |
|    cost_value_loss      | 2.67e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.182        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00122      |
|    mean_cost_advantages | -0.0008159865 |
|    mean_reward_advan... | -0.003359308  |
|    n_updates            | 710           |
|    nu                   | 6.38          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00103      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.82e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 73            |
|    time_elapsed         | 331           |
|    total_timesteps      | 149504        |
| train/                  |               |
|    approx_kl            | 0.00090255553 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0323        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00331       |
|    cost_value_loss      | 8.27e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.137        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00116      |
|    mean_cost_advantages | -0.0006403328 |
|    mean_reward_advan... | -0.0038464086 |
|    n_updates            | 720           |
|    nu                   | 6.4           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000216     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 0.000119      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 74             |
|    time_elapsed         | 336            |
|    total_timesteps      | 151552         |
| train/                  |                |
|    approx_kl            | 0.0033659292   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0239         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00498       |
|    cost_value_loss      | 1.8e-06        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.111         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000138      |
|    mean_cost_advantages | -0.00074394303 |
|    mean_reward_advan... | -0.0009912101  |
|    n_updates            | 730            |
|    nu                   | 6.41           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -7.76e-05      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 2.69e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 75            |
|    time_elapsed         | 340           |
|    total_timesteps      | 153600        |
| train/                  |               |
|    approx_kl            | 0.00023486378 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0138        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00298      |
|    cost_value_loss      | 3.47e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0889       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0002        |
|    mean_cost_advantages | -0.0007782951 |
|    mean_reward_advan... | -0.0014436829 |
|    n_updates            | 740           |
|    nu                   | 6.42          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -9.26e-05     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 4.17e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 76            |
|    time_elapsed         | 345           |
|    total_timesteps      | 155648        |
| train/                  |               |
|    approx_kl            | 0.0012714135  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.017         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.105         |
|    cost_value_loss      | 8.1e-06       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0845       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000468     |
|    mean_cost_advantages | -0.0010052338 |
|    mean_reward_advan... | -0.0035903968 |
|    n_updates            | 750           |
|    nu                   | 6.43          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000806     |
|    reward_explained_... | 0.98          |
|    reward_value_loss    | 0.000288      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 77            |
|    time_elapsed         | 350           |
|    total_timesteps      | 157696        |
| train/                  |               |
|    approx_kl            | 0.0016595097  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0363        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.135        |
|    cost_value_loss      | 1.34e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.094        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000352     |
|    mean_cost_advantages | -0.0011681026 |
|    mean_reward_advan... | -0.0028973005 |
|    n_updates            | 760           |
|    nu                   | 6.44          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -9.81e-05     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 4.33e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 78             |
|    time_elapsed         | 354            |
|    total_timesteps      | 159744         |
| train/                  |                |
|    approx_kl            | 0.0002781931   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00708        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.03          |
|    cost_value_loss      | 1.23e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0987        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00119       |
|    mean_cost_advantages | -0.00076957664 |
|    mean_reward_advan... | -0.0010397334  |
|    n_updates            | 770            |
|    nu                   | 6.45           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 5.24e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.55e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 79             |
|    time_elapsed         | 359            |
|    total_timesteps      | 161792         |
| train/                  |                |
|    approx_kl            | 0.0008314402   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0173         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.00199        |
|    cost_value_loss      | 1.05e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0941        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00201       |
|    mean_cost_advantages | -0.00041209089 |
|    mean_reward_advan... | -0.0016194845  |
|    n_updates            | 780            |
|    nu                   | 6.46           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -7.67e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.25e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 80             |
|    time_elapsed         | 363            |
|    total_timesteps      | 163840         |
| train/                  |                |
|    approx_kl            | 0.001992382    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00508        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.057          |
|    cost_value_loss      | 1.24e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.101         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000953       |
|    mean_cost_advantages | -0.0007649625  |
|    mean_reward_advan... | -0.00090219744 |
|    n_updates            | 790            |
|    nu                   | 6.47           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 6.89e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.08e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 81            |
|    time_elapsed         | 368           |
|    total_timesteps      | 165888        |
| train/                  |               |
|    approx_kl            | 0.00081719924 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0131        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.145         |
|    cost_value_loss      | 1.09e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0873       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000462     |
|    mean_cost_advantages | -0.0008971536 |
|    mean_reward_advan... | -0.001932648  |
|    n_updates            | 800           |
|    nu                   | 6.48          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -2.12e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 1.67e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 82            |
|    time_elapsed         | 373           |
|    total_timesteps      | 167936        |
| train/                  |               |
|    approx_kl            | 0.0026713815  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0229        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0581        |
|    cost_value_loss      | 8.98e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0956       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000534      |
|    mean_cost_advantages | -0.0006253463 |
|    mean_reward_advan... | -0.0019868617 |
|    n_updates            | 810           |
|    nu                   | 6.48          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -5.64e-05     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.23e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 83             |
|    time_elapsed         | 377            |
|    total_timesteps      | 169984         |
| train/                  |                |
|    approx_kl            | 0.0027958504   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00596        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0231        |
|    cost_value_loss      | 8.79e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0938        |
|    learning_rate        | 0.0003         |
|    loss                 | 8.51e-06       |
|    mean_cost_advantages | -0.00038556464 |
|    mean_reward_advan... | -0.0015707156  |
|    n_updates            | 820            |
|    nu                   | 6.49           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.6e-05       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.44e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 84             |
|    time_elapsed         | 382            |
|    total_timesteps      | 172032         |
| train/                  |                |
|    approx_kl            | 0.0019772956   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0118         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.155         |
|    cost_value_loss      | 8.31e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0987        |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000923      |
|    mean_cost_advantages | -0.0006243222  |
|    mean_reward_advan... | -0.00070272636 |
|    n_updates            | 830            |
|    nu                   | 6.49           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000121      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 1.5e-06        |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 85            |
|    time_elapsed         | 386           |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 0.0037625418  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0227        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.000463     |
|    cost_value_loss      | 1.25e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0846       |
|    learning_rate        | 0.0003        |
|    loss                 | 7.4e-05       |
|    mean_cost_advantages | -0.0005409877 |
|    mean_reward_advan... | -0.0014116701 |
|    n_updates            | 840           |
|    nu                   | 6.5           |
|    nu_loss              | -0            |
|    policy_gradient_loss | 8.43e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.52e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 86             |
|    time_elapsed         | 391            |
|    total_timesteps      | 176128         |
| train/                  |                |
|    approx_kl            | -0.00034804823 |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0041         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0436         |
|    cost_value_loss      | 8.91e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.075         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00137        |
|    mean_cost_advantages | -0.00047309545 |
|    mean_reward_advan... | -0.0010182359  |
|    n_updates            | 850            |
|    nu                   | 6.5            |
|    nu_loss              | -0             |
|    policy_gradient_loss | 4.62e-06       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 5.43e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 830           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 450           |
|    iterations           | 87            |
|    time_elapsed         | 395           |
|    total_timesteps      | 178176        |
| train/                  |               |
|    approx_kl            | 0.0020886522  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0152        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.2           |
|    cost_value_loss      | 1.02e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0731       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000343     |
|    mean_cost_advantages | -0.0011715314 |
|    mean_reward_advan... | -0.0011278419 |
|    n_updates            | 860           |
|    nu                   | 6.51          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -5.2e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 1.6e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 450            |
|    iterations           | 88             |
|    time_elapsed         | 400            |
|    total_timesteps      | 180224         |
| train/                  |                |
|    approx_kl            | 0.0010443549   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0161         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.142          |
|    cost_value_loss      | 1.54e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.079         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00181       |
|    mean_cost_advantages | -0.00062677986 |
|    mean_reward_advan... | -0.0019909034  |
|    n_updates            | 870            |
|    nu                   | 6.51           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000189      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 4.78e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 449            |
|    iterations           | 89             |
|    time_elapsed         | 405            |
|    total_timesteps      | 182272         |
| train/                  |                |
|    approx_kl            | 0.0016404942   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00845        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.065          |
|    cost_value_loss      | 5.62e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0711        |
|    learning_rate        | 0.0003         |
|    loss                 | -8.47e-05      |
|    mean_cost_advantages | -0.00063235266 |
|    mean_reward_advan... | 0.0001401676   |
|    n_updates            | 880            |
|    nu                   | 6.51           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000217      |
|    reward_explained_... | 0.997          |
|    reward_value_loss    | 5.54e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 449           |
|    iterations           | 90            |
|    time_elapsed         | 409           |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 0.00019947905 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0347        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.217         |
|    cost_value_loss      | 4.94e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0503       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00172      |
|    mean_cost_advantages | -0.0009391904 |
|    mean_reward_advan... | 0.005546933   |
|    n_updates            | 890           |
|    nu                   | 6.52          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000349     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 8.2e-06       |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 449            |
|    iterations           | 91             |
|    time_elapsed         | 414            |
|    total_timesteps      | 186368         |
| train/                  |                |
|    approx_kl            | -0.0001953069  |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00376        |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.107          |
|    cost_value_loss      | 4.93e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0458        |
|    learning_rate        | 0.0003         |
|    loss                 | -1.43e-06      |
|    mean_cost_advantages | -0.00042692426 |
|    mean_reward_advan... | 0.00273968     |
|    n_updates            | 900            |
|    nu                   | 6.52           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 6.7e-06        |
|    reward_explained_... | 1              |
|    reward_value_loss    | 2.87e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 449           |
|    iterations           | 92            |
|    time_elapsed         | 418           |
|    total_timesteps      | 188416        |
| train/                  |               |
|    approx_kl            | 0.0004330696  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00684       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.2           |
|    cost_value_loss      | 3.96e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0472       |
|    learning_rate        | 0.0003        |
|    loss                 | 1.2e-05       |
|    mean_cost_advantages | -0.0008075833 |
|    mean_reward_advan... | -0.0012678808 |
|    n_updates            | 910           |
|    nu                   | 6.52          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000115     |
|    reward_explained_... | 0.999         |
|    reward_value_loss    | 3.53e-05      |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 449            |
|    iterations           | 93             |
|    time_elapsed         | 423            |
|    total_timesteps      | 190464         |
| train/                  |                |
|    approx_kl            | 0.0059041632   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0219         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0138        |
|    cost_value_loss      | 2.23e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.059         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.000118       |
|    mean_cost_advantages | -0.00077518786 |
|    mean_reward_advan... | -0.003761491   |
|    n_updates            | 920            |
|    nu                   | 6.52           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000231      |
|    reward_explained_... | 0.999          |
|    reward_value_loss    | 1.89e-05       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 449            |
|    iterations           | 94             |
|    time_elapsed         | 428            |
|    total_timesteps      | 192512         |
| train/                  |                |
|    approx_kl            | 0.009612314    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0593         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0629         |
|    cost_value_loss      | 8.14e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0809        |
|    learning_rate        | 0.0003         |
|    loss                 | 8.53e-06       |
|    mean_cost_advantages | -0.00048366882 |
|    mean_reward_advan... | 0.00036942598  |
|    n_updates            | 930            |
|    nu                   | 6.52           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000493      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 7.28e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 835            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 448            |
|    iterations           | 95             |
|    time_elapsed         | 434            |
|    total_timesteps      | 194560         |
| train/                  |                |
|    approx_kl            | 0.0031236324   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0371         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0384        |
|    cost_value_loss      | 9.61e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.11          |
|    learning_rate        | 0.0003         |
|    loss                 | -6.21e-05      |
|    mean_cost_advantages | -0.00040109723 |
|    mean_reward_advan... | 0.00074856146  |
|    n_updates            | 940            |
|    nu                   | 6.53           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000292       |
|    reward_explained_... | 1              |
|    reward_value_loss    | 4.81e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 9.99e+03       |
|    mean_ep_length       | 12             |
|    mean_reward          | 9.99e+03       |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 830            |
|    ep_len_mean          | 12             |
|    ep_rew_mean          | 9.99e+03       |
| time/                   |                |
|    fps                  | 447            |
|    iterations           | 96             |
|    time_elapsed         | 439            |
|    total_timesteps      | 196608         |
| train/                  |                |
|    approx_kl            | 0.0030154926   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0212         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0472        |
|    cost_value_loss      | 6.51e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.108         |
|    learning_rate        | 0.0003         |
|    loss                 | -0.000592      |
|    mean_cost_advantages | -0.00064540183 |
|    mean_reward_advan... | -0.0015429856  |
|    n_updates            | 950            |
|    nu                   | 6.53           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -3.12e-05      |
|    reward_explained_... | 1              |
|    reward_value_loss    | 2.87e-06       |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 446           |
|    iterations           | 97            |
|    time_elapsed         | 445           |
|    total_timesteps      | 198656        |
| train/                  |               |
|    approx_kl            | 0.0045660525  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0231        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.135        |
|    cost_value_loss      | 5.24e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.105        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000243      |
|    mean_cost_advantages | -0.0002716101 |
|    mean_reward_advan... | -0.0016727879 |
|    n_updates            | 960           |
|    nu                   | 6.53          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -6.4e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 6.49e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 9.99e+03      |
|    mean_ep_length       | 12            |
|    mean_reward          | 9.99e+03      |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 835           |
|    ep_len_mean          | 12            |
|    ep_rew_mean          | 9.99e+03      |
| time/                   |               |
|    fps                  | 445           |
|    iterations           | 98            |
|    time_elapsed         | 450           |
|    total_timesteps      | 200704        |
| train/                  |               |
|    approx_kl            | 0.0020877023  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0225        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.133         |
|    cost_value_loss      | 7.35e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.121        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00213       |
|    mean_cost_advantages | -0.0006674015 |
|    mean_reward_advan... | -0.001490933  |
|    n_updates            | 970           |
|    nu                   | 6.53          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.17e-05      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 5.83e-06      |
|    total_cost           | 0.0           |
-------------------------------------------
Mean reward: 9994.500000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 07.77 minutes[0m
