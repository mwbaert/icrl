{"eval/mean_reward": -573.4833333333333, "eval/mean_ep_length": 122.2, "eval/best_mean_reward": -573.4833333333333, "rollout/adjusted_reward": -0.6375879049301147, "eval/true_cost": 0.101171875, "time/iterations": 4, "time/fps": 1128, "time/time_elapsed": 36, "time/total_timesteps": 40960, "infos/cost": 0.0, "rollout/ep_rew_mean": -59.020000010000004, "rollout/ep_len_mean": 104.37, "_step": 40960, "_runtime": 43, "_timestamp": 1657194720, "train/learning_rate": 0.0003, "train/entropy_loss": -1.3152887837961316, "train/policy_gradient_loss": -0.011270062708197961, "train/reward_value_loss": 46.35654840916395, "train/cost_value_loss": 2.86513218767941, "train/approx_kl": 0.015544019639492035, "train/clip_fraction": 0.23240966796875, "train/loss": 30.19097900390625, "train/mean_reward_advantages": -4.498952388763428, "train/mean_cost_advantages": 0.11994247138500214, "train/reward_explained_variance": -0.5679143667221069, "train/cost_explained_variance": 0.33950597047805786, "train/nu": 1.1989365816116333, "train/nu_loss": -0.09266573935747147, "train/average_cost": 0.08193359524011612, "train/total_cost": 839.0, "train/early_stop_epoch": 7, "train/n_updates": 30, "train/clip_range": 0.2}