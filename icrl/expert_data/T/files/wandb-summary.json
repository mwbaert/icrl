{"eval/mean_reward": 1.0, "eval/mean_ep_length": 15.2, "eval/best_mean_reward": 1.0, "rollout/adjusted_reward": 0.03259480744600296, "eval/true_cost": 0.008984375, "time/iterations": 40, "time/fps": 1011, "time/time_elapsed": 405, "time/total_timesteps": 409600, "infos/cost": 0.0, "rollout/ep_rew_mean": 1.0, "rollout/ep_len_mean": 14.86, "_step": 409600, "_runtime": 441, "_timestamp": 1657871125, "train/learning_rate": 5e-05, "train/entropy_loss": -0.12409561150241644, "train/policy_gradient_loss": -4.329318382131409e-05, "train/reward_value_loss": 4.135847951811655e-06, "train/cost_value_loss": 0.03403580843656528, "train/approx_kl": 0.0008829906582832336, "train/clip_fraction": 0.024736328125, "train/loss": 0.014174756594002247, "train/mean_reward_advantages": -0.00044744013575837016, "train/mean_cost_advantages": -0.0038463003002107143, "train/reward_explained_variance": 0.9976678395178169, "train/cost_explained_variance": -0.5847245454788208, "train/nu": 3.8611865043640137, "train/nu_loss": -0.020861782133579254, "train/average_cost": 0.0054687499068677425, "train/total_cost": 56.0, "train/early_stop_epoch": 10, "train/n_updates": 390, "train/clip_range": 0.2}