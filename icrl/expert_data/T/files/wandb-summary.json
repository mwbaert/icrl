{"eval/mean_reward": -8.65, "eval/mean_ep_length": 15.6, "eval/best_mean_reward": -8.65, "rollout/adjusted_reward": -0.5696163177490234, "eval/true_cost": 0.0041015625, "time/iterations": 20, "time/fps": 989, "time/time_elapsed": 207, "time/total_timesteps": 204800, "infos/cost": 0.0, "rollout/ep_rew_mean": -9.27333333, "rollout/ep_len_mean": 16.5, "_step": 204800, "_runtime": 228, "_timestamp": 1657802807, "train/learning_rate": 0.0003, "train/entropy_loss": -0.4371743669733405, "train/policy_gradient_loss": -0.008505799511232159, "train/reward_value_loss": 0.48000865122303366, "train/cost_value_loss": 0.1689727194859491, "train/approx_kl": 0.01958555355668068, "train/clip_fraction": 0.14095052083333334, "train/loss": 0.22004131972789764, "train/mean_reward_advantages": 0.17286117374897003, "train/mean_cost_advantages": -0.09819922596216202, "train/reward_explained_variance": 0.95320975035429, "train/cost_explained_variance": -0.538179874420166, "train/nu": 1.8450160026550293, "train/nu_loss": -0.018612369894981384, "train/average_cost": 0.01035156287252903, "train/total_cost": 106.0, "train/early_stop_epoch": 2, "train/n_updates": 190, "train/clip_range": 0.2}