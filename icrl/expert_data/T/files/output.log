[32;1mConfigured folder ./cpg/wandb/run-20220714_144259-3x6nf6cu/files for saving[0m
[32;1mName: T-v0_CT-v0_dnc_True_dno_True_dnr_True_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
Using cpu device
/home/mwbaert/Documents/research/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
-----------------------------------
| eval/               |           |
|    best_mean_reward | -4.65e+03 |
|    mean_ep_length   | 384       |
|    mean_reward      | -4.65e+03 |
|    true_cost        | 0.0756    |
| infos/              |           |
|    cost             | 0.17      |
| rollout/            |           |
|    adjusted_reward  | -1.05     |
|    ep_len_mean      | 311       |
|    ep_rew_mean      | -303      |
| time/               |           |
|    fps              | 1420      |
|    iterations       | 1         |
|    time_elapsed     | 7         |
|    total_timesteps  | 10240     |
-----------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -1.63e+03  |
|    mean_ep_length       | 211        |
|    mean_reward          | -1.63e+03  |
|    true_cost            | 0.0488     |
| infos/                  |            |
|    cost                 | 0.09       |
| rollout/                |            |
|    adjusted_reward      | -1.12      |
|    ep_len_mean          | 369        |
|    ep_rew_mean          | -373       |
| time/                   |            |
|    fps                  | 1091       |
|    iterations           | 2          |
|    time_elapsed         | 18         |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.01693741 |
|    average_cost         | 0.07626953 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    cost_explained_va... | -59        |
|    cost_value_loss      | 4.79       |
|    early_stop_epoch     | 5          |
|    entropy_loss         | -1.38      |
|    learning_rate        | 0.0003     |
|    loss                 | 11.6       |
|    mean_cost_advantages | 1.2008955  |
|    mean_reward_advan... | -15.582329 |
|    n_updates            | 10         |
|    nu                   | 1.06       |
|    nu_loss              | -0.0763    |
|    policy_gradient_loss | -0.00651   |
|    reward_explained_... | -411       |
|    reward_value_loss    | 63.3       |
|    total_cost           | 781.0      |
----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.48e+03   |
|    mean_ep_length       | 449         |
|    mean_reward          | -1.48e+03   |
|    true_cost            | 0.012       |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -1.29       |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | -437        |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 3           |
|    time_elapsed         | 33          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.016393224 |
|    average_cost         | 0.047851562 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.847      |
|    cost_value_loss      | 3.72        |
|    early_stop_epoch     | 9           |
|    entropy_loss         | -1.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    mean_cost_advantages | 0.5070957   |
|    mean_reward_advan... | -14.065089  |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.0509     |
|    policy_gradient_loss | -0.00471    |
|    reward_explained_... | -1.64       |
|    reward_value_loss    | 62.6        |
|    total_cost           | 490.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -1.17e+03   |
|    mean_ep_length       | 500         |
|    mean_reward          | -1.17e+03   |
|    true_cost            | 0.0153      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -1.09       |
|    ep_len_mean          | 406         |
|    ep_rew_mean          | -447        |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 4           |
|    time_elapsed         | 46          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.008995003 |
|    average_cost         | 0.011328125 |
|    clip_fraction        | 0.0649      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.341       |
|    cost_value_loss      | 1.08        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 20.3        |
|    mean_cost_advantages | -0.07604708 |
|    mean_reward_advan... | -14.216626  |
|    n_updates            | 30          |
|    nu                   | 1.19        |
|    nu_loss              | -0.0128     |
|    policy_gradient_loss | -0.00347    |
|    reward_explained_... | -0.549      |
|    reward_value_loss    | 87.7        |
|    total_cost           | 116.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -623          |
|    mean_ep_length       | 465           |
|    mean_reward          | -623          |
|    true_cost            | 0.00273       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -1.05         |
|    ep_len_mean          | 432           |
|    ep_rew_mean          | -476          |
| time/                   |               |
|    fps                  | 871           |
|    iterations           | 5             |
|    time_elapsed         | 58            |
|    total_timesteps      | 51200         |
| train/                  |               |
|    approx_kl            | 0.012173331   |
|    average_cost         | 0.015136719   |
|    clip_fraction        | 0.0986        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.159         |
|    cost_value_loss      | 1.15          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -1.31         |
|    learning_rate        | 0.0003        |
|    loss                 | 28.7          |
|    mean_cost_advantages | -0.0016140759 |
|    mean_reward_advan... | -8.64918      |
|    n_updates            | 40            |
|    nu                   | 1.24          |
|    nu_loss              | -0.0179       |
|    policy_gradient_loss | -0.00462      |
|    reward_explained_... | -0.0662       |
|    reward_value_loss    | 83.9          |
|    total_cost           | 155.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -623         |
|    mean_ep_length       | 251          |
|    mean_reward          | -631         |
|    true_cost            | 0.00527      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.895       |
|    ep_len_mean          | 451          |
|    ep_rew_mean          | -486         |
| time/                   |              |
|    fps                  | 863          |
|    iterations           | 6            |
|    time_elapsed         | 71           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.01121325   |
|    average_cost         | 0.0026367188 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.457        |
|    cost_value_loss      | 0.169        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.3         |
|    learning_rate        | 0.0003       |
|    loss                 | 19.8         |
|    mean_cost_advantages | -0.14119251  |
|    mean_reward_advan... | -6.8624907   |
|    n_updates            | 50           |
|    nu                   | 1.29         |
|    nu_loss              | -0.00327     |
|    policy_gradient_loss | -0.00417     |
|    reward_explained_... | 0.086        |
|    reward_value_loss    | 78.8         |
|    total_cost           | 27.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -269         |
|    mean_ep_length       | 283          |
|    mean_reward          | -269         |
|    true_cost            | 0.00352      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.839       |
|    ep_len_mean          | 440          |
|    ep_rew_mean          | -438         |
| time/                   |              |
|    fps                  | 871          |
|    iterations           | 7            |
|    time_elapsed         | 82           |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.012481359  |
|    average_cost         | 0.0053710938 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.6         |
|    cost_value_loss      | 0.355        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.28        |
|    learning_rate        | 0.0003       |
|    loss                 | 22.1         |
|    mean_cost_advantages | -0.044589363 |
|    mean_reward_advan... | -3.6213355   |
|    n_updates            | 60           |
|    nu                   | 1.33         |
|    nu_loss              | -0.00691     |
|    policy_gradient_loss | -0.00461     |
|    reward_explained_... | 0.0983       |
|    reward_value_loss    | 69.6         |
|    total_cost           | 55.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -269        |
|    mean_ep_length       | 249         |
|    mean_reward          | -328        |
|    true_cost            | 0.00928     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.758      |
|    ep_len_mean          | 330         |
|    ep_rew_mean          | -283        |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 8           |
|    time_elapsed         | 94          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.009767188 |
|    average_cost         | 0.003515625 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.215       |
|    cost_value_loss      | 0.172       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.27       |
|    learning_rate        | 0.0003      |
|    loss                 | 57          |
|    mean_cost_advantages | -0.08070983 |
|    mean_reward_advan... | -1.6362174  |
|    n_updates            | 70          |
|    nu                   | 1.37        |
|    nu_loss              | -0.00468    |
|    policy_gradient_loss | -0.00603    |
|    reward_explained_... | 0.125       |
|    reward_value_loss    | 76.8        |
|    total_cost           | 36.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -60.6        |
|    mean_ep_length       | 82.6         |
|    mean_reward          | -60.6        |
|    true_cost            | 0.00566      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.754       |
|    ep_len_mean          | 121          |
|    ep_rew_mean          | -89.9        |
| time/                   |              |
|    fps                  | 873          |
|    iterations           | 9            |
|    time_elapsed         | 105          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.011856141  |
|    average_cost         | 0.008984375  |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.33        |
|    cost_value_loss      | 0.392        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.23        |
|    learning_rate        | 0.0003       |
|    loss                 | 62.2         |
|    mean_cost_advantages | -0.006220527 |
|    mean_reward_advan... | 2.5251122    |
|    n_updates            | 80           |
|    nu                   | 1.41         |
|    nu_loss              | -0.0123      |
|    policy_gradient_loss | -0.00844     |
|    reward_explained_... | 0.374        |
|    reward_value_loss    | 107          |
|    total_cost           | 92.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -54.8        |
|    mean_ep_length       | 75.6         |
|    mean_reward          | -54.8        |
|    true_cost            | 0.00723      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.773       |
|    ep_len_mean          | 68.7         |
|    ep_rew_mean          | -52.8        |
| time/                   |              |
|    fps                  | 878          |
|    iterations           | 10           |
|    time_elapsed         | 116          |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0132795675 |
|    average_cost         | 0.005761719  |
|    clip_fraction        | 0.191        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.115       |
|    cost_value_loss      | 0.216        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.18        |
|    learning_rate        | 0.0003       |
|    loss                 | 66           |
|    mean_cost_advantages | -0.09924738  |
|    mean_reward_advan... | 6.694299     |
|    n_updates            | 90           |
|    nu                   | 1.45         |
|    nu_loss              | -0.00814     |
|    policy_gradient_loss | -0.0118      |
|    reward_explained_... | 0.636        |
|    reward_value_loss    | 117          |
|    total_cost           | 59.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -43.8        |
|    mean_ep_length       | 61.6         |
|    mean_reward          | -43.8        |
|    true_cost            | 0.00645      |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | -0.771       |
|    ep_len_mean          | 48           |
|    ep_rew_mean          | -37.4        |
| time/                   |              |
|    fps                  | 878          |
|    iterations           | 11           |
|    time_elapsed         | 128          |
|    total_timesteps      | 112640       |
| train/                  |              |
|    approx_kl            | 0.01248907   |
|    average_cost         | 0.0072265626 |
|    clip_fraction        | 0.23         |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.237       |
|    cost_value_loss      | 0.203        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.1         |
|    learning_rate        | 0.0003       |
|    loss                 | 50.3         |
|    mean_cost_advantages | -0.10864131  |
|    mean_reward_advan... | 10.807657    |
|    n_updates            | 100          |
|    nu                   | 1.49         |
|    nu_loss              | -0.0105      |
|    policy_gradient_loss | -0.0117      |
|    reward_explained_... | 0.78         |
|    reward_value_loss    | 108          |
|    total_cost           | 74.0         |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -29.1        |
|    mean_ep_length       | 39           |
|    mean_reward          | -29.1        |
|    true_cost            | 0.0105       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.776       |
|    ep_len_mean          | 36.8         |
|    ep_rew_mean          | -27.5        |
| time/                   |              |
|    fps                  | 870          |
|    iterations           | 12           |
|    time_elapsed         | 141          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.015422888  |
|    average_cost         | 0.0063476562 |
|    clip_fraction        | 0.196        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.469       |
|    cost_value_loss      | 0.145        |
|    early_stop_epoch     | 8            |
|    entropy_loss         | -1.02        |
|    learning_rate        | 0.0003       |
|    loss                 | 32.4         |
|    mean_cost_advantages | -0.093880914 |
|    mean_reward_advan... | 11.168596    |
|    n_updates            | 110          |
|    nu                   | 1.53         |
|    nu_loss              | -0.00946     |
|    policy_gradient_loss | -0.00963     |
|    reward_explained_... | 0.826        |
|    reward_value_loss    | 70.4         |
|    total_cost           | 65.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -17.2       |
|    mean_ep_length       | 28.8        |
|    mean_reward          | -17.2       |
|    true_cost            | 0.00488     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.728      |
|    ep_len_mean          | 32.2        |
|    ep_rew_mean          | -23.3       |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 13          |
|    time_elapsed         | 152         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.012394836 |
|    average_cost         | 0.008886719 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.765      |
|    cost_value_loss      | 0.147       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.925      |
|    learning_rate        | 0.0003      |
|    loss                 | 23.6        |
|    mean_cost_advantages | -0.05213554 |
|    mean_reward_advan... | 8.146252    |
|    n_updates            | 120         |
|    nu                   | 1.57        |
|    nu_loss              | -0.0136     |
|    policy_gradient_loss | -0.00788    |
|    reward_explained_... | 0.821       |
|    reward_value_loss    | 41.2        |
|    total_cost           | 91.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -16.9       |
|    mean_ep_length       | 25          |
|    mean_reward          | -16.9       |
|    true_cost            | 0.00586     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.717      |
|    ep_len_mean          | 28.5        |
|    ep_rew_mean          | -19.9       |
| time/                   |             |
|    fps                  | 893         |
|    iterations           | 14          |
|    time_elapsed         | 160         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.015392393 |
|    average_cost         | 0.003515625 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.6        |
|    cost_value_loss      | 0.0722      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.903      |
|    learning_rate        | 0.0003      |
|    loss                 | 20.6        |
|    mean_cost_advantages | -0.05706335 |
|    mean_reward_advan... | 4.394072    |
|    n_updates            | 130         |
|    nu                   | 1.6         |
|    nu_loss              | -0.00551    |
|    policy_gradient_loss | -0.0074     |
|    reward_explained_... | 0.822       |
|    reward_value_loss    | 22          |
|    total_cost           | 36.0        |
-----------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -14.9        |
|    mean_ep_length       | 22           |
|    mean_reward          | -14.9        |
|    true_cost            | 0.0083       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.672       |
|    ep_len_mean          | 24.8         |
|    ep_rew_mean          | -16.4        |
| time/                   |              |
|    fps                  | 902          |
|    iterations           | 15           |
|    time_elapsed         | 170          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.015264714  |
|    average_cost         | 0.004003906  |
|    clip_fraction        | 0.164        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.21        |
|    cost_value_loss      | 0.0895       |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.823       |
|    learning_rate        | 0.0003       |
|    loss                 | 5.2          |
|    mean_cost_advantages | -0.027990151 |
|    mean_reward_advan... | 2.50772      |
|    n_updates            | 140          |
|    nu                   | 1.64         |
|    nu_loss              | -0.00643     |
|    policy_gradient_loss | -0.00901     |
|    reward_explained_... | 0.842        |
|    reward_value_loss    | 10.4         |
|    total_cost           | 41.0         |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -11.2       |
|    mean_ep_length       | 19.6        |
|    mean_reward          | -11.2       |
|    true_cost            | 0.0114      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.64       |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | -13.4       |
| time/                   |             |
|    fps                  | 905         |
|    iterations           | 16          |
|    time_elapsed         | 180         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.016462147 |
|    average_cost         | 0.006152344 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -3.1        |
|    cost_value_loss      | 0.0963      |
|    early_stop_epoch     | 8           |
|    entropy_loss         | -0.759      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.4         |
|    mean_cost_advantages | 0.004447484 |
|    mean_reward_advan... | 1.4056093   |
|    n_updates            | 150         |
|    nu                   | 1.68        |
|    nu_loss              | -0.0101     |
|    policy_gradient_loss | -0.0107     |
|    reward_explained_... | 0.879       |
|    reward_value_loss    | 4.05        |
|    total_cost           | 63.0        |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -11.2        |
|    mean_ep_length       | 17.4         |
|    mean_reward          | -129         |
|    true_cost            | 0.0116       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.619       |
|    ep_len_mean          | 19           |
|    ep_rew_mean          | -11.2        |
| time/                   |              |
|    fps                  | 918          |
|    iterations           | 17           |
|    time_elapsed         | 189          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.015310066  |
|    average_cost         | 0.00859375   |
|    clip_fraction        | 0.2          |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.13        |
|    cost_value_loss      | 0.142        |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.685       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.08         |
|    mean_cost_advantages | 0.0013935417 |
|    mean_reward_advan... | 0.76927674   |
|    n_updates            | 160          |
|    nu                   | 1.71         |
|    nu_loss              | -0.0144      |
|    policy_gradient_loss | -0.0119      |
|    reward_explained_... | 0.9          |
|    reward_value_loss    | 2.11         |
|    total_cost           | 88.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -11.2        |
|    mean_ep_length       | 18.4         |
|    mean_reward          | -49.7        |
|    true_cost            | 0.0185       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.608       |
|    ep_len_mean          | 18           |
|    ep_rew_mean          | -10.3        |
| time/                   |              |
|    fps                  | 948          |
|    iterations           | 18           |
|    time_elapsed         | 194          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.015060884  |
|    average_cost         | 0.011035156  |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.48        |
|    cost_value_loss      | 0.167        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.61        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.441        |
|    mean_cost_advantages | -0.019708773 |
|    mean_reward_advan... | 0.41145977   |
|    n_updates            | 170          |
|    nu                   | 1.75         |
|    nu_loss              | -0.0189      |
|    policy_gradient_loss | -0.00976     |
|    reward_explained_... | 0.927        |
|    reward_value_loss    | 1.16         |
|    total_cost           | 113.0        |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -10.7       |
|    mean_ep_length       | 18.8        |
|    mean_reward          | -10.7       |
|    true_cost            | 0.00928     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.588      |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | -9.8        |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 19          |
|    time_elapsed         | 200         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.015597534 |
|    average_cost         | 0.019628907 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.42       |
|    cost_value_loss      | 0.295       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.311       |
|    mean_cost_advantages | 0.028583456 |
|    mean_reward_advan... | 0.26929787  |
|    n_updates            | 180         |
|    nu                   | 1.8         |
|    nu_loss              | -0.0344     |
|    policy_gradient_loss | -0.00861    |
|    reward_explained_... | 0.94        |
|    reward_value_loss    | 0.664       |
|    total_cost           | 201.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.65        |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -8.65        |
|    true_cost            | 0.0041       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.57        |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | -9.27        |
| time/                   |              |
|    fps                  | 989          |
|    iterations           | 20           |
|    time_elapsed         | 207          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.019585554  |
|    average_cost         | 0.010351563  |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.538       |
|    cost_value_loss      | 0.169        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.437       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.22         |
|    mean_cost_advantages | -0.098199226 |
|    mean_reward_advan... | 0.17286117   |
|    n_updates            | 190          |
|    nu                   | 1.85         |
|    nu_loss              | -0.0186      |
|    policy_gradient_loss | -0.00851     |
|    reward_explained_... | 0.953        |
|    reward_value_loss    | 0.48         |
|    total_cost           | 106.0        |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/wrappers/monitoring/video_recorder.py:57: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  logger.deprecation(
Mean reward: -9.416667 +/- 0.964653./home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/util.py:37: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping, Sequence
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. [0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:305: UserWarning: [33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps[0m
  logger.warn(

[32;1mTime taken: 03.80 minutes[0m
