[32;1mConfigured folder ./cpg/wandb/run-20220715_093804-1ewrsauu/files for saving[0m
[32;1mName: T-v0_CT-v0_dnc_True_dno_True_dnr_True_lr_5e-05_s_20_sid_-1[0m
Wrapping eval env in a VecNormalize.
Using cpu device
/home/mwbaert/Documents/research/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
----------------------------------
| eval/               |          |
|    best_mean_reward | -999     |
|    mean_ep_length   | 288      |
|    mean_reward      | -999     |
|    true_cost        | 0.0402   |
| infos/              |          |
|    cost             | 0.09     |
| rollout/            |          |
|    adjusted_reward  | -0.037   |
|    ep_len_mean      | 349      |
|    ep_rew_mean      | 0.64     |
| time/               |          |
|    fps              | 2083     |
|    iterations       | 1        |
|    time_elapsed     | 4        |
|    total_timesteps  | 10240    |
----------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -779        |
|    mean_ep_length       | 452         |
|    mean_reward          | -779        |
|    true_cost            | 0.0196      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.0188     |
|    ep_len_mean          | 404         |
|    ep_rew_mean          | 0.574       |
| time/                   |             |
|    fps                  | 1326        |
|    iterations           | 2           |
|    time_elapsed         | 15          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.009802183 |
|    average_cost         | 0.03857422  |
|    clip_fraction        | 0.023       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -15.7       |
|    cost_value_loss      | 1.5         |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.38       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.561       |
|    mean_cost_advantages | 0.60751575  |
|    mean_reward_advan... | 0.07766205  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.0386     |
|    policy_gradient_loss | -0.00353    |
|    reward_explained_... | 0.0104      |
|    reward_value_loss    | 0.0348      |
|    total_cost           | 395.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -379        |
|    mean_ep_length       | 386         |
|    mean_reward          | -379        |
|    true_cost            | 0.00801     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.00748    |
|    ep_len_mean          | 430         |
|    ep_rew_mean          | 0.529       |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 3           |
|    time_elapsed         | 25          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.009576447 |
|    average_cost         | 0.018652344 |
|    clip_fraction        | 0.0424      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.39       |
|    cost_value_loss      | 0.868       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.37       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.269       |
|    mean_cost_advantages | 0.20335153  |
|    mean_reward_advan... | 0.056872584 |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.0199     |
|    policy_gradient_loss | -0.00377    |
|    reward_explained_... | -1.53       |
|    reward_value_loss    | 0.0111      |
|    total_cost           | 191.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -219         |
|    mean_ep_length       | 397          |
|    mean_reward          | -219         |
|    true_cost            | 0.00986      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.0101      |
|    ep_len_mean          | 434          |
|    ep_rew_mean          | 0.516        |
| time/                   |              |
|    fps                  | 1116         |
|    iterations           | 4            |
|    time_elapsed         | 36           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0052164877 |
|    average_cost         | 0.007421875  |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.122        |
|    cost_value_loss      | 0.34         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.36        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.203        |
|    mean_cost_advantages | 0.023794468  |
|    mean_reward_advan... | 0.046044134  |
|    n_updates            | 30           |
|    nu                   | 1.18         |
|    nu_loss              | -0.00836     |
|    policy_gradient_loss | -0.0016      |
|    reward_explained_... | -1.75        |
|    reward_value_loss    | 0.00759      |
|    total_cost           | 76.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -119        |
|    mean_ep_length       | 451         |
|    mean_reward          | -119        |
|    true_cost            | 0.00742     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.00702    |
|    ep_len_mean          | 424         |
|    ep_rew_mean          | 0.51        |
| time/                   |             |
|    fps                  | 1065        |
|    iterations           | 5           |
|    time_elapsed         | 48          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.009439692 |
|    average_cost         | 0.009472656 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.362      |
|    cost_value_loss      | 0.432       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.34       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.471       |
|    mean_cost_advantages | 0.045470007 |
|    mean_reward_advan... | 0.03718125  |
|    n_updates            | 40          |
|    nu                   | 1.24        |
|    nu_loss              | -0.0112     |
|    policy_gradient_loss | -0.00258    |
|    reward_explained_... | -0.0143     |
|    reward_value_loss    | 0.00867     |
|    total_cost           | 97.0        |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -119         |
|    mean_ep_length       | 272          |
|    mean_reward          | -439         |
|    true_cost            | 0.0126       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.0125      |
|    ep_len_mean          | 361          |
|    ep_rew_mean          | 0.62         |
| time/                   |              |
|    fps                  | 1039         |
|    iterations           | 6            |
|    time_elapsed         | 59           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0064238706 |
|    average_cost         | 0.0069335937 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.249       |
|    cost_value_loss      | 0.31         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -1.33        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.147        |
|    mean_cost_advantages | 0.008422768  |
|    mean_reward_advan... | 0.038808864  |
|    n_updates            | 50           |
|    nu                   | 1.29         |
|    nu_loss              | -0.00859     |
|    policy_gradient_loss | -0.0025      |
|    reward_explained_... | -0.0977      |
|    reward_value_loss    | 0.00921      |
|    total_cost           | 71.0         |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -119        |
|    mean_ep_length       | 192         |
|    mean_reward          | -259        |
|    true_cost            | 0.0187      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.0186     |
|    ep_len_mean          | 217         |
|    ep_rew_mean          | 0.9         |
| time/                   |             |
|    fps                  | 1034        |
|    iterations           | 7           |
|    time_elapsed         | 69          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.00757106  |
|    average_cost         | 0.011914062 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.453      |
|    cost_value_loss      | 0.395       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.3        |
|    learning_rate        | 5e-05       |
|    loss                 | 0.124       |
|    mean_cost_advantages | 0.019947216 |
|    mean_reward_advan... | 0.04271786  |
|    n_updates            | 60          |
|    nu                   | 1.35        |
|    nu_loss              | -0.0154     |
|    policy_gradient_loss | -0.00334    |
|    reward_explained_... | 0.197       |
|    reward_value_loss    | 0.0168      |
|    total_cost           | 122.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -119        |
|    mean_ep_length       | 102         |
|    mean_reward          | -279        |
|    true_cost            | 0.0262      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.0275     |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 1034        |
|    iterations           | 8           |
|    time_elapsed         | 79          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.010484113 |
|    average_cost         | 0.018164063 |
|    clip_fraction        | 0.0658      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.03       |
|    cost_value_loss      | 0.489       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.27       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0545      |
|    mean_cost_advantages | 0.0516056   |
|    mean_reward_advan... | 0.08068697  |
|    n_updates            | 70          |
|    nu                   | 1.41        |
|    nu_loss              | -0.0245     |
|    policy_gradient_loss | -0.00455    |
|    reward_explained_... | 0.0183      |
|    reward_value_loss    | 0.0228      |
|    total_cost           | 186.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -79         |
|    mean_ep_length       | 86.2        |
|    mean_reward          | -79         |
|    true_cost            | 0.0314      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.0317     |
|    ep_len_mean          | 81.8        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 1031        |
|    iterations           | 9           |
|    time_elapsed         | 89          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.011034707 |
|    average_cost         | 0.025878906 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.884      |
|    cost_value_loss      | 0.637       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.22       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.416       |
|    mean_cost_advantages | 0.06295715  |
|    mean_reward_advan... | 0.0929414   |
|    n_updates            | 80          |
|    nu                   | 1.48        |
|    nu_loss              | -0.0365     |
|    policy_gradient_loss | -0.00616    |
|    reward_explained_... | 0.374       |
|    reward_value_loss    | 0.0245      |
|    total_cost           | 265.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -79           |
|    mean_ep_length       | 83.8          |
|    mean_reward          | -259          |
|    true_cost            | 0.0334        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | -0.0335       |
|    ep_len_mean          | 55.1          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1023          |
|    iterations           | 10            |
|    time_elapsed         | 100           |
|    total_timesteps      | 102400        |
| train/                  |               |
|    approx_kl            | 0.008254193   |
|    average_cost         | 0.030371094   |
|    clip_fraction        | 0.0965        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.228        |
|    cost_value_loss      | 0.523         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -1.17         |
|    learning_rate        | 5e-05         |
|    loss                 | 0.33          |
|    mean_cost_advantages | -0.0071878275 |
|    mean_reward_advan... | 0.103112124   |
|    n_updates            | 90            |
|    nu                   | 1.55          |
|    nu_loss              | -0.0449       |
|    policy_gradient_loss | -0.00575      |
|    reward_explained_... | 0.493         |
|    reward_value_loss    | 0.0248        |
|    total_cost           | 311.0         |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -79         |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -239        |
|    true_cost            | 0.0529      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.0576     |
|    ep_len_mean          | 32.8        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 1020        |
|    iterations           | 11          |
|    time_elapsed         | 110         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.013608369 |
|    average_cost         | 0.033203125 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0513      |
|    cost_value_loss      | 0.44        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -1.1        |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0884      |
|    mean_cost_advantages | -0.08524182 |
|    mean_reward_advan... | 0.10708076  |
|    n_updates            | 100         |
|    nu                   | 1.62        |
|    nu_loss              | -0.0514     |
|    policy_gradient_loss | -0.00814    |
|    reward_explained_... | 0.572       |
|    reward_value_loss    | 0.0214      |
|    total_cost           | 340.0       |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -79          |
|    mean_ep_length       | 31.6         |
|    mean_reward          | -199         |
|    true_cost            | 0.0579       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.0622      |
|    ep_len_mean          | 26.2         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1039         |
|    iterations           | 12           |
|    time_elapsed         | 118          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.015984196  |
|    average_cost         | 0.052734375  |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0977       |
|    cost_value_loss      | 0.484        |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -1.03        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.175        |
|    mean_cost_advantages | -0.011020328 |
|    mean_reward_advan... | 0.11122066   |
|    n_updates            | 110          |
|    nu                   | 1.7          |
|    nu_loss              | -0.0857      |
|    policy_gradient_loss | -0.00825     |
|    reward_explained_... | 0.67         |
|    reward_value_loss    | 0.0123       |
|    total_cost           | 540.0        |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -79         |
|    mean_ep_length       | 19.4        |
|    mean_reward          | -179        |
|    true_cost            | 0.0518      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.0512     |
|    ep_len_mean          | 26.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 1048        |
|    iterations           | 13          |
|    time_elapsed         | 127         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.015488347 |
|    average_cost         | 0.057421874 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.376       |
|    cost_value_loss      | 0.38        |
|    early_stop_epoch     | 7           |
|    entropy_loss         | -0.935      |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0917      |
|    mean_cost_advantages | -0.10351697 |
|    mean_reward_advan... | 0.06991537  |
|    n_updates            | 120         |
|    nu                   | 1.78        |
|    nu_loss              | -0.0978     |
|    policy_gradient_loss | -0.00946    |
|    reward_explained_... | 0.678       |
|    reward_value_loss    | 0.00633     |
|    total_cost           | 588.0       |
-----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -79         |
|    mean_ep_length       | 13.4        |
|    mean_reward          | -199        |
|    true_cost            | 0.0459      |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | -0.0403     |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 1054        |
|    iterations           | 14          |
|    time_elapsed         | 135         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.015931452 |
|    average_cost         | 0.050878905 |
|    clip_fraction        | 0.0961      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.401       |
|    cost_value_loss      | 0.283       |
|    early_stop_epoch     | 9           |
|    entropy_loss         | -0.855      |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0855      |
|    mean_cost_advantages | -0.10000467 |
|    mean_reward_advan... | 0.04012782  |
|    n_updates            | 130         |
|    nu                   | 1.87        |
|    nu_loss              | -0.0908     |
|    policy_gradient_loss | -0.00749    |
|    reward_explained_... | 0.728       |
|    reward_value_loss    | 0.00338     |
|    total_cost           | 521.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -79         |
|    mean_ep_length       | 19.6        |
|    mean_reward          | -119        |
|    true_cost            | 0.0424      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.0323     |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 1053        |
|    iterations           | 15          |
|    time_elapsed         | 145         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.011264873 |
|    average_cost         | 0.045703124 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.278       |
|    cost_value_loss      | 0.246       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.778      |
|    learning_rate        | 5e-05       |
|    loss                 | 0.14        |
|    mean_cost_advantages | -0.09396973 |
|    mean_reward_advan... | 0.02413335  |
|    n_updates            | 140         |
|    nu                   | 1.96        |
|    nu_loss              | -0.0854     |
|    policy_gradient_loss | -0.00709    |
|    reward_explained_... | 0.765       |
|    reward_value_loss    | 0.00189     |
|    total_cost           | 468.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -79         |
|    mean_ep_length       | 16          |
|    mean_reward          | -119        |
|    true_cost            | 0.0409      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.0295     |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 1052        |
|    iterations           | 16          |
|    time_elapsed         | 155         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.008717729 |
|    average_cost         | 0.04140625  |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0911      |
|    cost_value_loss      | 0.235       |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.719      |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0732      |
|    mean_cost_advantages | -0.05945524 |
|    mean_reward_advan... | 0.01740871  |
|    n_updates            | 150         |
|    nu                   | 2.05        |
|    nu_loss              | -0.081      |
|    policy_gradient_loss | -0.0061     |
|    reward_explained_... | 0.801       |
|    reward_value_loss    | 0.00109     |
|    total_cost           | 424.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -39          |
|    mean_ep_length       | 18.8         |
|    mean_reward          | -39          |
|    true_cost            | 0.0328       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.0152      |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1052         |
|    iterations           | 17           |
|    time_elapsed         | 165          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.009896046  |
|    average_cost         | 0.040136717  |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.228       |
|    cost_value_loss      | 0.221        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.658       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.106        |
|    mean_cost_advantages | -0.012327326 |
|    mean_reward_advan... | 0.0091803605 |
|    n_updates            | 160          |
|    nu                   | 2.14         |
|    nu_loss              | -0.0822      |
|    policy_gradient_loss | -0.00692     |
|    reward_explained_... | 0.841        |
|    reward_value_loss    | 0.000608     |
|    total_cost           | 411.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 1            |
|    true_cost            | 0.0412       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | -0.0326      |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1045         |
|    iterations           | 18           |
|    time_elapsed         | 176          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0079925675 |
|    average_cost         | 0.032714844  |
|    clip_fraction        | 0.0863       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.27        |
|    cost_value_loss      | 0.19         |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.601       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.119        |
|    mean_cost_advantages | -0.03583421  |
|    mean_reward_advan... | 0.006496639  |
|    n_updates            | 170          |
|    nu                   | 2.23         |
|    nu_loss              | -0.07        |
|    policy_gradient_loss | -0.00718     |
|    reward_explained_... | 0.881        |
|    reward_value_loss    | 0.000395     |
|    total_cost           | 335.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -39          |
|    true_cost            | 0.0315       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.012       |
|    ep_len_mean          | 16.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1040         |
|    iterations           | 19           |
|    time_elapsed         | 186          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.011074683  |
|    average_cost         | 0.041015625  |
|    clip_fraction        | 0.0679       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.519       |
|    cost_value_loss      | 0.204        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.543       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.105        |
|    mean_cost_advantages | 0.022159606  |
|    mean_reward_advan... | 0.0050441623 |
|    n_updates            | 180          |
|    nu                   | 2.33         |
|    nu_loss              | -0.0915      |
|    policy_gradient_loss | -0.00653     |
|    reward_explained_... | 0.91         |
|    reward_value_loss    | 0.000243     |
|    total_cost           | 420.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15           |
|    mean_reward          | -79          |
|    true_cost            | 0.0282       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.00572     |
|    ep_len_mean          | 16.3         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1041         |
|    iterations           | 20           |
|    time_elapsed         | 196          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0083714435 |
|    average_cost         | 0.031054687  |
|    clip_fraction        | 0.0627       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.202       |
|    cost_value_loss      | 0.163        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.495       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0777       |
|    mean_cost_advantages | -0.03778791  |
|    mean_reward_advan... | 0.0021381525 |
|    n_updates            | 190          |
|    nu                   | 2.42         |
|    nu_loss              | -0.0723      |
|    policy_gradient_loss | -0.00534     |
|    reward_explained_... | 0.936        |
|    reward_value_loss    | 0.000147     |
|    total_cost           | 318.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -79          |
|    true_cost            | 0.0223       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | 0.00696      |
|    ep_len_mean          | 15.9         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1037         |
|    iterations           | 21           |
|    time_elapsed         | 207          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.0059233583 |
|    average_cost         | 0.027929688  |
|    clip_fraction        | 0.0528       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.403       |
|    cost_value_loss      | 0.145        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.451       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0836       |
|    mean_cost_advantages | -0.014740348 |
|    mean_reward_advan... | 0.002355933  |
|    n_updates            | 200          |
|    nu                   | 2.52         |
|    nu_loss              | -0.0676      |
|    policy_gradient_loss | -0.00427     |
|    reward_explained_... | 0.956        |
|    reward_value_loss    | 9.5e-05      |
|    total_cost           | 286.0        |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1              |
|    mean_ep_length       | 16.2           |
|    mean_reward          | -39            |
|    true_cost            | 0.0271         |
| infos/                  |                |
|    cost                 | 0.04           |
| rollout/                |                |
|    adjusted_reward      | -0.0054        |
|    ep_len_mean          | 15.5           |
|    ep_rew_mean          | 1              |
| time/                   |                |
|    fps                  | 1037           |
|    iterations           | 22             |
|    time_elapsed         | 217            |
|    total_timesteps      | 225280         |
| train/                  |                |
|    approx_kl            | 0.0052252      |
|    average_cost         | 0.022265624    |
|    clip_fraction        | 0.0475         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.538         |
|    cost_value_loss      | 0.128          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.412         |
|    learning_rate        | 5e-05          |
|    loss                 | 0.0436         |
|    mean_cost_advantages | -0.02552928    |
|    mean_reward_advan... | -0.00014417856 |
|    n_updates            | 210            |
|    nu                   | 2.61           |
|    nu_loss              | -0.056         |
|    policy_gradient_loss | -0.00423       |
|    reward_explained_... | 0.966          |
|    reward_value_loss    | 6.78e-05       |
|    total_cost           | 228.0          |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1              |
|    mean_ep_length       | 15.8           |
|    mean_reward          | 1              |
|    true_cost            | 0.0221         |
| infos/                  |                |
|    cost                 | 0.04           |
| rollout/                |                |
|    adjusted_reward      | 0.00568        |
|    ep_len_mean          | 15.2           |
|    ep_rew_mean          | 1              |
| time/                   |                |
|    fps                  | 1036           |
|    iterations           | 23             |
|    time_elapsed         | 227            |
|    total_timesteps      | 235520         |
| train/                  |                |
|    approx_kl            | 0.0036455854   |
|    average_cost         | 0.026953125    |
|    clip_fraction        | 0.0338         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.918         |
|    cost_value_loss      | 0.142          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.374         |
|    learning_rate        | 5e-05          |
|    loss                 | 0.0545         |
|    mean_cost_advantages | 0.013447573    |
|    mean_reward_advan... | -0.00035726378 |
|    n_updates            | 220            |
|    nu                   | 2.7            |
|    nu_loss              | -0.0703        |
|    policy_gradient_loss | -0.00279       |
|    reward_explained_... | 0.972          |
|    reward_value_loss    | 5.28e-05       |
|    total_cost           | 276.0          |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15.6          |
|    mean_reward          | 1             |
|    true_cost            | 0.0171        |
| infos/                  |               |
|    cost                 | 0.01          |
| rollout/                |               |
|    adjusted_reward      | 0.0178        |
|    ep_len_mean          | 15.2          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1036          |
|    iterations           | 24            |
|    time_elapsed         | 237           |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.006344753   |
|    average_cost         | 0.022070313   |
|    clip_fraction        | 0.0409        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.447        |
|    cost_value_loss      | 0.12          |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.335        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.0781        |
|    mean_cost_advantages | -0.032588914  |
|    mean_reward_advan... | 0.00051346986 |
|    n_updates            | 230           |
|    nu                   | 2.8           |
|    nu_loss              | -0.0597       |
|    policy_gradient_loss | -0.00275      |
|    reward_explained_... | 0.977         |
|    reward_value_loss    | 4.1e-05       |
|    total_cost           | 226.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -39          |
|    true_cost            | 0.0179       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | 0.0146       |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1036         |
|    iterations           | 25           |
|    time_elapsed         | 247          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.0041110376 |
|    average_cost         | 0.017089844  |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.753       |
|    cost_value_loss      | 0.101        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.307       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0687       |
|    mean_cost_advantages | -0.014831021 |
|    mean_reward_advan... | 0.0011203093 |
|    n_updates            | 240          |
|    nu                   | 2.88         |
|    nu_loss              | -0.0478      |
|    policy_gradient_loss | -0.00248     |
|    reward_explained_... | 0.983        |
|    reward_value_loss    | 2.98e-05     |
|    total_cost           | 175.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.0107        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 0.0343        |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1036          |
|    iterations           | 26            |
|    time_elapsed         | 256           |
|    total_timesteps      | 266240        |
| train/                  |               |
|    approx_kl            | 0.005998852   |
|    average_cost         | 0.017871093   |
|    clip_fraction        | 0.03          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.13         |
|    cost_value_loss      | 0.103         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.27         |
|    learning_rate        | 5e-05         |
|    loss                 | 0.0591        |
|    mean_cost_advantages | -0.0021897599 |
|    mean_reward_advan... | 0.0013563714  |
|    n_updates            | 250           |
|    nu                   | 2.97          |
|    nu_loss              | -0.0516       |
|    policy_gradient_loss | -0.00217      |
|    reward_explained_... | 0.985         |
|    reward_value_loss    | 2.62e-05      |
|    total_cost           | 183.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -39          |
|    true_cost            | 0.0082       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0411       |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1036         |
|    iterations           | 27           |
|    time_elapsed         | 266          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.0019972574 |
|    average_cost         | 0.0107421875 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.792       |
|    cost_value_loss      | 0.0692       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.238       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.037        |
|    mean_cost_advantages | -0.024755053 |
|    mean_reward_advan... | 0.0005011263 |
|    n_updates            | 260          |
|    nu                   | 3.06         |
|    nu_loss              | -0.0319      |
|    policy_gradient_loss | -0.00183     |
|    reward_explained_... | 0.989        |
|    reward_value_loss    | 1.92e-05     |
|    total_cost           | 110.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1            |
|    mean_ep_length       | 15           |
|    mean_reward          | 1            |
|    true_cost            | 0.0104       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | 0.0344       |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 1035         |
|    iterations           | 28           |
|    time_elapsed         | 276          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.0022544006 |
|    average_cost         | 0.008203125  |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.22        |
|    cost_value_loss      | 0.0539       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.215       |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0186       |
|    mean_cost_advantages | -0.013160368 |
|    mean_reward_advan... | 0.001228051  |
|    n_updates            | 270          |
|    nu                   | 3.14         |
|    nu_loss              | -0.0251      |
|    policy_gradient_loss | -0.00128     |
|    reward_explained_... | 0.992        |
|    reward_value_loss    | 1.39e-05     |
|    total_cost           | 84.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.6          |
|    mean_reward          | -39           |
|    true_cost            | 0.00586       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0477        |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1035          |
|    iterations           | 29            |
|    time_elapsed         | 286           |
|    total_timesteps      | 296960        |
| train/                  |               |
|    approx_kl            | 0.003834209   |
|    average_cost         | 0.010351563   |
|    clip_fraction        | 0.0264        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.64         |
|    cost_value_loss      | 0.0651        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.195        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.0221        |
|    mean_cost_advantages | 0.00044652802 |
|    mean_reward_advan... | -0.0011184644 |
|    n_updates            | 280           |
|    nu                   | 3.21          |
|    nu_loss              | -0.0325       |
|    policy_gradient_loss | -0.0011       |
|    reward_explained_... | 0.992         |
|    reward_value_loss    | 1.22e-05      |
|    total_cost           | 106.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.00684       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0443        |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1035          |
|    iterations           | 30            |
|    time_elapsed         | 296           |
|    total_timesteps      | 307200        |
| train/                  |               |
|    approx_kl            | 0.0010594146  |
|    average_cost         | 0.005859375   |
|    clip_fraction        | 0.0126        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1            |
|    cost_value_loss      | 0.0415        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.175        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.00176       |
|    mean_cost_advantages | -0.013921079  |
|    mean_reward_advan... | 0.00010678694 |
|    n_updates            | 290           |
|    nu                   | 3.29          |
|    nu_loss              | -0.0188       |
|    policy_gradient_loss | -0.00106      |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 8.7e-06       |
|    total_cost           | 60.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.0119        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0272        |
|    ep_len_mean          | 14.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1035          |
|    iterations           | 31            |
|    time_elapsed         | 306           |
|    total_timesteps      | 317440        |
| train/                  |               |
|    approx_kl            | 0.0015565256  |
|    average_cost         | 0.0068359375  |
|    clip_fraction        | 0.0128        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.6          |
|    cost_value_loss      | 0.0445        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.169        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.00418       |
|    mean_cost_advantages | -0.0019905188 |
|    mean_reward_advan... | 0.0004843417  |
|    n_updates            | 300           |
|    nu                   | 3.35          |
|    nu_loss              | -0.0225       |
|    policy_gradient_loss | -0.000803     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 7.59e-06      |
|    total_cost           | 70.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 14.6          |
|    mean_reward          | -39           |
|    true_cost            | 0.00605       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0463        |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1026          |
|    iterations           | 32            |
|    time_elapsed         | 319           |
|    total_timesteps      | 327680        |
| train/                  |               |
|    approx_kl            | 0.0025845927  |
|    average_cost         | 0.011914062   |
|    clip_fraction        | 0.0211        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.47         |
|    cost_value_loss      | 0.0684        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.159        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.0356        |
|    mean_cost_advantages | 0.010461534   |
|    mean_reward_advan... | 0.00071026507 |
|    n_updates            | 310           |
|    nu                   | 3.42          |
|    nu_loss              | -0.04         |
|    policy_gradient_loss | -0.000605     |
|    reward_explained_... | 0.994         |
|    reward_value_loss    | 9.84e-06      |
|    total_cost           | 122.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.00801       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0392        |
|    ep_len_mean          | 14.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1021          |
|    iterations           | 33            |
|    time_elapsed         | 330           |
|    total_timesteps      | 337920        |
| train/                  |               |
|    approx_kl            | 0.00060600834 |
|    average_cost         | 0.0060546873  |
|    clip_fraction        | 0.00794       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.707        |
|    cost_value_loss      | 0.0383        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.148        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.057         |
|    mean_cost_advantages | -0.01418581   |
|    mean_reward_advan... | 0.0004376311  |
|    n_updates            | 320           |
|    nu                   | 3.49          |
|    nu_loss              | -0.0207       |
|    policy_gradient_loss | -0.000305     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.58e-06      |
|    total_cost           | 62.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.0131        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | 0.0209        |
|    ep_len_mean          | 14.8          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1020          |
|    iterations           | 34            |
|    time_elapsed         | 341           |
|    total_timesteps      | 348160        |
| train/                  |               |
|    approx_kl            | 0.0004328647  |
|    average_cost         | 0.008007812   |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.954        |
|    cost_value_loss      | 0.0497        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.143        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.00881       |
|    mean_cost_advantages | 0.010967443   |
|    mean_reward_advan... | 0.00014489642 |
|    n_updates            | 330           |
|    nu                   | 3.55          |
|    nu_loss              | -0.0279       |
|    policy_gradient_loss | -0.000237     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 6.11e-06      |
|    total_cost           | 82.0          |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.000977      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0632        |
|    ep_len_mean          | 15.1          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1030          |
|    iterations           | 35            |
|    time_elapsed         | 347           |
|    total_timesteps      | 358400        |
| train/                  |               |
|    approx_kl            | 0.017400315   |
|    average_cost         | 0.013085937   |
|    clip_fraction        | 0.0428        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.823        |
|    cost_value_loss      | 0.075         |
|    early_stop_epoch     | 2             |
|    entropy_loss         | -0.146        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.0365        |
|    mean_cost_advantages | 0.014577774   |
|    mean_reward_advan... | 2.4749315e-07 |
|    n_updates            | 340           |
|    nu                   | 3.61          |
|    nu_loss              | -0.0464       |
|    policy_gradient_loss | -0.000501     |
|    reward_explained_... | 0.995         |
|    reward_value_loss    | 8.55e-06      |
|    total_cost           | 134.0         |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1              |
|    mean_ep_length       | 15             |
|    mean_reward          | 1              |
|    true_cost            | 0.00195        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 0.0594         |
|    ep_len_mean          | 15             |
|    ep_rew_mean          | 1              |
| time/                   |                |
|    fps                  | 1025           |
|    iterations           | 36             |
|    time_elapsed         | 359            |
|    total_timesteps      | 368640         |
| train/                  |                |
|    approx_kl            | 0.0019947884   |
|    average_cost         | 0.0009765625   |
|    clip_fraction        | 0.0172         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0869        |
|    cost_value_loss      | 0.00867        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.127         |
|    learning_rate        | 5e-05          |
|    loss                 | 0.00235        |
|    mean_cost_advantages | -0.03624385    |
|    mean_reward_advan... | -0.00072770414 |
|    n_updates            | 350            |
|    nu                   | 3.66           |
|    nu_loss              | -0.00352       |
|    policy_gradient_loss | -0.000479      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 3.81e-06       |
|    total_cost           | 10.0           |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.00508       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0481        |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1019          |
|    iterations           | 37            |
|    time_elapsed         | 371           |
|    total_timesteps      | 378880        |
| train/                  |               |
|    approx_kl            | 0.00065788283 |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0119        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.25         |
|    cost_value_loss      | 0.0138        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.126        |
|    learning_rate        | 5e-05         |
|    loss                 | -0.000915     |
|    mean_cost_advantages | 0.0021405723  |
|    mean_reward_advan... | 1.5884585e-05 |
|    n_updates            | 360           |
|    nu                   | 3.72          |
|    nu_loss              | -0.00716      |
|    policy_gradient_loss | -0.000208     |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 3.03e-06      |
|    total_cost           | 20.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.00723       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0398        |
|    ep_len_mean          | 14.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1015          |
|    iterations           | 38            |
|    time_elapsed         | 383           |
|    total_timesteps      | 389120        |
| train/                  |               |
|    approx_kl            | 0.0018977029  |
|    average_cost         | 0.005078125   |
|    clip_fraction        | 0.0131        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.45         |
|    cost_value_loss      | 0.0327        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.128        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.00281       |
|    mean_cost_advantages | 0.015793387   |
|    mean_reward_advan... | 0.00041416558 |
|    n_updates            | 370           |
|    nu                   | 3.77          |
|    nu_loss              | -0.0189       |
|    policy_gradient_loss | -0.00017      |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.01e-06      |
|    total_cost           | 52.0          |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1             |
|    mean_ep_length       | 15            |
|    mean_reward          | 1             |
|    true_cost            | 0.00547       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | 0.0461        |
|    ep_len_mean          | 14.9          |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 1013          |
|    iterations           | 39            |
|    time_elapsed         | 393           |
|    total_timesteps      | 399360        |
| train/                  |               |
|    approx_kl            | 0.0006000171  |
|    average_cost         | 0.0072265626  |
|    clip_fraction        | 0.00902       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.09         |
|    cost_value_loss      | 0.0425        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.133        |
|    learning_rate        | 5e-05         |
|    loss                 | 0.00255       |
|    mean_cost_advantages | 0.01362358    |
|    mean_reward_advan... | 0.00043316893 |
|    n_updates            | 380           |
|    nu                   | 3.81          |
|    nu_loss              | -0.0272       |
|    policy_gradient_loss | -2.06e-05     |
|    reward_explained_... | 0.997         |
|    reward_value_loss    | 5.16e-06      |
|    total_cost           | 74.0          |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1              |
|    mean_ep_length       | 15.2           |
|    mean_reward          | 1              |
|    true_cost            | 0.00898        |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | 0.0326         |
|    ep_len_mean          | 14.9           |
|    ep_rew_mean          | 1              |
| time/                   |                |
|    fps                  | 1011           |
|    iterations           | 40             |
|    time_elapsed         | 405            |
|    total_timesteps      | 409600         |
| train/                  |                |
|    approx_kl            | 0.00088299066  |
|    average_cost         | 0.00546875     |
|    clip_fraction        | 0.0247         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.585         |
|    cost_value_loss      | 0.034          |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.124         |
|    learning_rate        | 5e-05          |
|    loss                 | 0.0142         |
|    mean_cost_advantages | -0.0038463003  |
|    mean_reward_advan... | -0.00044744014 |
|    n_updates            | 390            |
|    nu                   | 3.86           |
|    nu_loss              | -0.0209        |
|    policy_gradient_loss | -4.33e-05      |
|    reward_explained_... | 0.998          |
|    reward_value_loss    | 4.14e-06       |
|    total_cost           | 56.0           |
--------------------------------------------
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/wrappers/monitoring/video_recorder.py:57: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  logger.deprecation(
Mean reward: 1.000000 +/- 0.000000.
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/util.py:37: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping, Sequence
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. [0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.[0m
  logger.warn(
/home/mwbaert/anaconda3/envs/icrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:305: UserWarning: [33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps[0m
  logger.warn(
[32;1mTime taken: 07.35 minutes[0m
