{"eval/mean_reward": 60.0, "eval/mean_ep_length": 200.0, "eval/best_mean_reward": 60.0, "rollout/adjusted_reward": 0.3000223636627197, "eval/true_cost": 0.0, "time/iterations": 49, "time/fps": 634, "time/time_elapsed": 158, "time/total_timesteps": 100352, "infos/traversals_so_far": 2.06, "infos/cost": 0.0, "rollout/ep_rew_mean": 60.0, "rollout/ep_len_mean": 200.0, "_step": 100352, "_runtime": 285, "_timestamp": 1646992397, "train/learning_rate": 0.0003, "train/entropy_loss": -0.00040355593318963656, "train/policy_gradient_loss": 6.202026110121463e-11, "train/reward_value_loss": 0.24435650841332973, "train/cost_value_loss": 9.45679993424875e-08, "train/approx_kl": -8.381903171539307e-08, "train/clip_fraction": 0.0, "train/loss": 0.14973612129688263, "train/mean_reward_advantages": 0.017902251332998276, "train/mean_cost_advantages": -0.0002599824219942093, "train/reward_explained_variance": -8.578607559204102, "train/cost_explained_variance": -0.8492413759231567, "train/nu": 2.514631986618042, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 480, "train/clip_range": 0.2}