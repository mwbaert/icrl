{"eval/mean_reward": -6.85, "eval/mean_ep_length": 18.4, "eval/best_mean_reward": -6.233333333333334, "rollout/adjusted_reward": -0.42898765206336975, "eval/true_cost": 0.0, "time/iterations": 98, "time/fps": 916, "time/time_elapsed": 1095, "time/total_timesteps": 1003520, "infos/cost": 0.0, "rollout/ep_rew_mean": -7.115833449999999, "rollout/ep_len_mean": 16.53, "_timestamp": 1656578388, "_runtime": 1103, "_step": 1003520, "train/learning_rate": 0.0003, "train/entropy_loss": -0.04640190463978797, "train/policy_gradient_loss": -0.00010065344438014018, "train/reward_value_loss": 0.2846977105829865, "train/cost_value_loss": 0.00042645418120095256, "train/approx_kl": 0.04461027309298515, "train/clip_fraction": 0.051171875, "train/loss": 0.20994287729263306, "train/mean_reward_advantages": 0.05020473524928093, "train/mean_cost_advantages": -0.0002548116899561137, "train/reward_explained_variance": 0.9400279484689236, "train/cost_explained_variance": -5336.7431640625, "train/nu": 2.271182060241699, "train/nu_loss": -0.00022178402286954224, "train/average_cost": 9.765625145519152e-05, "train/total_cost": 1.0, "train/early_stop_epoch": 0, "train/n_updates": 970, "train/clip_range": 0.2, "_wandb": {"runtime": 1105}}