[32mConfigured folder /tmp/wandb/run-20220630_082125-3fysmh30/files for saving
[32mName: JTL-v0_CJTL-v0_dnc_True_dno_True_dnr_True_goal_2_ws_True_s_20_sid_-1
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
Wrapping eval env in a VecNormalize.
Using cpu device
----------------------------------
| eval/               |          |
|    best_mean_reward | -178     |
|    mean_ep_length   | 196      |
|    mean_reward      | -178     |
|    true_cost        | 0.516    |
| infos/              |          |
|    cost             | 0.7      |
| rollout/            |          |
|    adjusted_reward  | -1.35    |
|    ep_len_mean      | 164      |
|    ep_rew_mean      | -136     |
| time/               |          |
|    fps              | 1499     |
|    iterations       | 1        |
|    time_elapsed     | 6        |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -116        |
|    mean_ep_length       | 172         |
|    mean_reward          | -116        |
|    true_cost            | 0.313       |
| infos/                  |             |
|    cost                 | 0.12        |
| rollout/                |             |
|    adjusted_reward      | -1.06       |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | -114        |
| time/                   |             |
|    fps                  | 1066        |
|    iterations           | 2           |
|    time_elapsed         | 19          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.027676467 |
|    average_cost         | 0.51572263  |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -260        |
|    cost_value_loss      | 26.9        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 13.9        |
|    mean_cost_advantages | 7.9654617   |
|    mean_reward_advan... | -12.455735  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.516      |
|    policy_gradient_loss | -0.0262     |
|    reward_explained_... | -399        |
|    reward_value_loss    | 55.9        |
|    total_cost           | 5281.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -116        |
|    mean_ep_length       | 176         |
|    mean_reward          | -132        |
|    true_cost            | 0.263       |
| infos/                  |             |
|    cost                 | 0.48        |
| rollout/                |             |
|    adjusted_reward      | -1.01       |
|    ep_len_mean          | 142         |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 1120        |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.017794944 |
|    average_cost         | 0.31347656  |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.15       |
|    cost_value_loss      | 23.7        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 29          |
|    mean_cost_advantages | 3.0021703   |
|    mean_reward_advan... | -7.586212   |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.334      |
|    policy_gradient_loss | -0.0193     |
|    reward_explained_... | -3.37       |
|    reward_value_loss    | 51.5        |
|    total_cost           | 3210.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 96.8        |
|    mean_reward          | -64.5       |
|    true_cost            | 0.198       |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | -0.913      |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | -105        |
| time/                   |             |
|    fps                  | 1158        |
|    iterations           | 4           |
|    time_elapsed         | 35          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.015689388 |
|    average_cost         | 0.26337892  |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.18       |
|    cost_value_loss      | 22.1        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.28       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.2        |
|    mean_cost_advantages | 1.536607    |
|    mean_reward_advan... | -5.6351767  |
|    n_updates            | 30          |
|    nu                   | 1.19        |
|    nu_loss              | -0.297      |
|    policy_gradient_loss | -0.015      |
|    reward_explained_... | -2.28       |
|    reward_value_loss    | 51.9        |
|    total_cost           | 2697.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 162         |
|    mean_reward          | -105        |
|    true_cost            | 0.0945      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.844      |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | -107        |
| time/                   |             |
|    fps                  | 1155        |
|    iterations           | 5           |
|    time_elapsed         | 44          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.021751108 |
|    average_cost         | 0.19804688  |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.3        |
|    cost_value_loss      | 20.3        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 43          |
|    mean_cost_advantages | 0.39166063  |
|    mean_reward_advan... | -3.8205962  |
|    n_updates            | 40          |
|    nu                   | 1.26        |
|    nu_loss              | -0.236      |
|    policy_gradient_loss | -0.0168     |
|    reward_explained_... | -1.04       |
|    reward_value_loss    | 53.8        |
|    total_cost           | 2028.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 165         |
|    mean_reward          | -110        |
|    true_cost            | 0.0687      |
| infos/                  |             |
|    cost                 | 0.06        |
| rollout/                |             |
|    adjusted_reward      | -0.811      |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | -123        |
| time/                   |             |
|    fps                  | 1157        |
|    iterations           | 6           |
|    time_elapsed         | 53          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.019117853 |
|    average_cost         | 0.09453125  |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.41       |
|    cost_value_loss      | 10.1        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.1        |
|    learning_rate        | 0.0003      |
|    loss                 | 42.4        |
|    mean_cost_advantages | -1.2796422  |
|    mean_reward_advan... | -3.7289498  |
|    n_updates            | 50          |
|    nu                   | 1.32        |
|    nu_loss              | -0.119      |
|    policy_gradient_loss | -0.00519    |
|    reward_explained_... | -1.66       |
|    reward_value_loss    | 57.3        |
|    total_cost           | 968.0       |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -64.5      |
|    mean_ep_length       | 200        |
|    mean_reward          | -167       |
|    true_cost            | 0.0587     |
| infos/                  |            |
|    cost                 | 0.15       |
| rollout/                |            |
|    adjusted_reward      | -0.788     |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | -125       |
| time/                   |            |
|    fps                  | 1142       |
|    iterations           | 7          |
|    time_elapsed         | 62         |
|    total_timesteps      | 71680      |
| train/                  |            |
|    approx_kl            | 0.01818358 |
|    average_cost         | 0.06865235 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    cost_explained_va... | -1.23      |
|    cost_value_loss      | 7.19       |
|    early_stop_epoch     | 3          |
|    entropy_loss         | -1.01      |
|    learning_rate        | 0.0003     |
|    loss                 | 39         |
|    mean_cost_advantages | -1.2127723 |
|    mean_reward_advan... | -2.920778  |
|    n_updates            | 60         |
|    nu                   | 1.38       |
|    nu_loss              | -0.0906    |
|    policy_gradient_loss | -0.00694   |
|    reward_explained_... | -1.21      |
|    reward_value_loss    | 60.6       |
|    total_cost           | 703.0      |
----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 183         |
|    mean_reward          | -133        |
|    true_cost            | 0.0409      |
| infos/                  |             |
|    cost                 | 0.08        |
| rollout/                |             |
|    adjusted_reward      | -0.756      |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | -125        |
| time/                   |             |
|    fps                  | 1113        |
|    iterations           | 8           |
|    time_elapsed         | 73          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.016614513 |
|    average_cost         | 0.058691405 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.31       |
|    cost_value_loss      | 6.01        |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -0.95       |
|    learning_rate        | 0.0003      |
|    loss                 | 35.2        |
|    mean_cost_advantages | -1.0981417  |
|    mean_reward_advan... | -1.8406608  |
|    n_updates            | 70          |
|    nu                   | 1.43        |
|    nu_loss              | -0.0808     |
|    policy_gradient_loss | -0.0027     |
|    reward_explained_... | -0.415      |
|    reward_value_loss    | 62.8        |
|    total_cost           | 601.0       |
-----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 166         |
|    mean_reward          | -97.4       |
|    true_cost            | 0.0416      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.761      |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | -124        |
| time/                   |             |
|    fps                  | 1055        |
|    iterations           | 9           |
|    time_elapsed         | 87          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.022605682 |
|    average_cost         | 0.04091797  |
|    clip_fraction        | 0.073       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.541      |
|    cost_value_loss      | 3.88        |
|    early_stop_epoch     | 9           |
|    entropy_loss         | -0.922      |
|    learning_rate        | 0.0003      |
|    loss                 | 17.1        |
|    mean_cost_advantages | -0.9841167  |
|    mean_reward_advan... | -1.52025    |
|    n_updates            | 80          |
|    nu                   | 1.48        |
|    nu_loss              | -0.0586     |
|    policy_gradient_loss | -0.00795    |
|    reward_explained_... | -0.0365     |
|    reward_value_loss    | 61.6        |
|    total_cost           | 419.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 191         |
|    mean_reward          | -124        |
|    true_cost            | 0.0451      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.723      |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | -119        |
| time/                   |             |
|    fps                  | 1049        |
|    iterations           | 10          |
|    time_elapsed         | 97          |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.03232231  |
|    average_cost         | 0.04160156  |
|    clip_fraction        | 0.0782      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.536      |
|    cost_value_loss      | 5.08        |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.857      |
|    learning_rate        | 0.0003      |
|    loss                 | 32.8        |
|    mean_cost_advantages | -0.7991414  |
|    mean_reward_advan... | -0.73102677 |
|    n_updates            | 90          |
|    nu                   | 1.53        |
|    nu_loss              | -0.0617     |
|    policy_gradient_loss | -0.0139     |
|    reward_explained_... | -0.094      |
|    reward_value_loss    | 68.1        |
|    total_cost           | 426.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 172         |
|    mean_reward          | -133        |
|    true_cost            | 0.0275      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.684      |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | -115        |
| time/                   |             |
|    fps                  | 1060        |
|    iterations           | 11          |
|    time_elapsed         | 106         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.020971786 |
|    average_cost         | 0.04511719  |
|    clip_fraction        | 0.09        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.27       |
|    cost_value_loss      | 5.4         |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.849      |
|    learning_rate        | 0.0003      |
|    loss                 | 20.4        |
|    mean_cost_advantages | -0.55596864 |
|    mean_reward_advan... | 0.08551643  |
|    n_updates            | 100         |
|    nu                   | 1.58        |
|    nu_loss              | -0.0692     |
|    policy_gradient_loss | -0.00593    |
|    reward_explained_... | 0.508       |
|    reward_value_loss    | 63.3        |
|    total_cost           | 462.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 182         |
|    mean_reward          | -105        |
|    true_cost            | 0.0161      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.643      |
|    ep_len_mean          | 175         |
|    ep_rew_mean          | -109        |
| time/                   |             |
|    fps                  | 1055        |
|    iterations           | 12          |
|    time_elapsed         | 116         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.016308665 |
|    average_cost         | 0.027539063 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0239     |
|    cost_value_loss      | 3.3         |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.838      |
|    learning_rate        | 0.0003      |
|    loss                 | 23.1        |
|    mean_cost_advantages | -0.6254406  |
|    mean_reward_advan... | -0.11158489 |
|    n_updates            | 110         |
|    nu                   | 1.63        |
|    nu_loss              | -0.0435     |
|    policy_gradient_loss | -0.0035     |
|    reward_explained_... | 0.444       |
|    reward_value_loss    | 61.9        |
|    total_cost           | 282.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -64.5       |
|    mean_ep_length       | 151         |
|    mean_reward          | -88.9       |
|    true_cost            | 0.0228      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.615      |
|    ep_len_mean          | 155         |
|    ep_rew_mean          | -90.2       |
| time/                   |             |
|    fps                  | 1023        |
|    iterations           | 13          |
|    time_elapsed         | 130         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.013208432 |
|    average_cost         | 0.016113281 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.268       |
|    cost_value_loss      | 1.07        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.799      |
|    learning_rate        | 0.0003      |
|    loss                 | 27.8        |
|    mean_cost_advantages | -0.5858081  |
|    mean_reward_advan... | -0.08224276 |
|    n_updates            | 120         |
|    nu                   | 1.67        |
|    nu_loss              | -0.0262     |
|    policy_gradient_loss | -0.00687    |
|    reward_explained_... | 0.44        |
|    reward_value_loss    | 58.1        |
|    total_cost           | 165.0       |
-----------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -63         |
|    mean_ep_length       | 115         |
|    mean_reward          | -63         |
|    true_cost            | 0.0164      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.586      |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 994         |
|    iterations           | 14          |
|    time_elapsed         | 144         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.016904918 |
|    average_cost         | 0.022753906 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.14       |
|    cost_value_loss      | 1.56        |
|    early_stop_epoch     | 8           |
|    entropy_loss         | -0.79       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.9        |
|    mean_cost_advantages | -0.3503374  |
|    mean_reward_advan... | 0.72700083  |
|    n_updates            | 130         |
|    nu                   | 1.71        |
|    nu_loss              | -0.0379     |
|    policy_gradient_loss | -0.0112     |
|    reward_explained_... | 0.469       |
|    reward_value_loss    | 51.3        |
|    total_cost           | 233.0       |
-----------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -32.1       |
|    mean_ep_length       | 70          |
|    mean_reward          | -32.1       |
|    true_cost            | 0.0359      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.54       |
|    ep_len_mean          | 64.5        |
|    ep_rew_mean          | -30.8       |
| time/                   |             |
|    fps                  | 973         |
|    iterations           | 15          |
|    time_elapsed         | 157         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.011161821 |
|    average_cost         | 0.01640625  |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.288      |
|    cost_value_loss      | 0.68        |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.805      |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    mean_cost_advantages | -0.4631791  |
|    mean_reward_advan... | 2.220986    |
|    n_updates            | 140         |
|    nu                   | 1.74        |
|    nu_loss              | -0.028      |
|    policy_gradient_loss | -0.00832    |
|    reward_explained_... | 0.545       |
|    reward_value_loss    | 47.1        |
|    total_cost           | 168.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -21.3      |
|    mean_ep_length       | 42.8       |
|    mean_reward          | -21.3      |
|    true_cost            | 0.0191     |
| infos/                  |            |
|    cost                 | 0          |
| rollout/                |            |
|    adjusted_reward      | -0.509     |
|    ep_len_mean          | 43.9       |
|    ep_rew_mean          | -20.2      |
| time/                   |            |
|    fps                  | 990        |
|    iterations           | 16         |
|    time_elapsed         | 165        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.03847749 |
|    average_cost         | 0.0359375  |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -7.13      |
|    cost_value_loss      | 2.79       |
|    early_stop_epoch     | 1          |
|    entropy_loss         | -0.856     |
|    learning_rate        | 0.0003     |
|    loss                 | 15         |
|    mean_cost_advantages | -0.1395843 |
|    mean_reward_advan... | 4.506968   |
|    n_updates            | 150        |
|    nu                   | 1.78       |
|    nu_loss              | -0.0627    |
|    policy_gradient_loss | -0.0195    |
|    reward_explained_... | 0.733      |
|    reward_value_loss    | 39         |
|    total_cost           | 368.0      |
----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -13.6       |
|    mean_ep_length       | 34.6        |
|    mean_reward          | -13.6       |
|    true_cost            | 0.0163      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.493      |
|    ep_len_mean          | 32.8        |
|    ep_rew_mean          | -15.3       |
| time/                   |             |
|    fps                  | 997         |
|    iterations           | 17          |
|    time_elapsed         | 174         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.017379016 |
|    average_cost         | 0.019140625 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0766      |
|    cost_value_loss      | 1.11        |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -0.825      |
|    learning_rate        | 0.0003      |
|    loss                 | 13.6        |
|    mean_cost_advantages | -0.38609964 |
|    mean_reward_advan... | 3.9657578   |
|    n_updates            | 160         |
|    nu                   | 1.82        |
|    nu_loss              | -0.0341     |
|    policy_gradient_loss | -0.0136     |
|    reward_explained_... | 0.739       |
|    reward_value_loss    | 26.7        |
|    total_cost           | 196.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -13.6       |
|    mean_ep_length       | 48.4        |
|    mean_reward          | -25.5       |
|    true_cost            | 0.00967     |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.481      |
|    ep_len_mean          | 28.9        |
|    ep_rew_mean          | -13.3       |
| time/                   |             |
|    fps                  | 1007        |
|    iterations           | 18          |
|    time_elapsed         | 182         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.022236275 |
|    average_cost         | 0.016308594 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.23       |
|    cost_value_loss      | 0.532       |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.743      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.77        |
|    mean_cost_advantages | -0.30554706 |
|    mean_reward_advan... | 3.6468644   |
|    n_updates            | 170         |
|    nu                   | 1.85        |
|    nu_loss              | -0.0296     |
|    policy_gradient_loss | -0.00916    |
|    reward_explained_... | 0.787       |
|    reward_value_loss    | 9.87        |
|    total_cost           | 167.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -13.6       |
|    mean_ep_length       | 26.4        |
|    mean_reward          | -13.8       |
|    true_cost            | 0.0085      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.475      |
|    ep_len_mean          | 24.5        |
|    ep_rew_mean          | -11.6       |
| time/                   |             |
|    fps                  | 1024        |
|    iterations           | 19          |
|    time_elapsed         | 189         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.015260935 |
|    average_cost         | 0.009667968 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.75       |
|    cost_value_loss      | 0.264       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.68       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.52        |
|    mean_cost_advantages | -0.18210968 |
|    mean_reward_advan... | 1.9557155   |
|    n_updates            | 180         |
|    nu                   | 1.88        |
|    nu_loss              | -0.0179     |
|    policy_gradient_loss | -0.00588    |
|    reward_explained_... | 0.783       |
|    reward_value_loss    | 6.4         |
|    total_cost           | 99.0        |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -9.25       |
|    mean_ep_length       | 20.8        |
|    mean_reward          | -9.25       |
|    true_cost            | 0.00303     |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.465      |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | -9.75       |
| time/                   |             |
|    fps                  | 1031        |
|    iterations           | 20          |
|    time_elapsed         | 198         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.019004242 |
|    average_cost         | 0.008496094 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.489      |
|    cost_value_loss      | 0.0971      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.557      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.02        |
|    mean_cost_advantages | -0.07710352 |
|    mean_reward_advan... | 0.9176428   |
|    n_updates            | 190         |
|    nu                   | 1.91        |
|    nu_loss              | -0.016      |
|    policy_gradient_loss | -0.00977    |
|    reward_explained_... | 0.841       |
|    reward_value_loss    | 2.41        |
|    total_cost           | 87.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -8.65        |
|    mean_ep_length       | 19           |
|    mean_reward          | -8.65        |
|    true_cost            | 0.00264      |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.455       |
|    ep_len_mean          | 19.3         |
|    ep_rew_mean          | -8.5         |
| time/                   |              |
|    fps                  | 1038         |
|    iterations           | 21           |
|    time_elapsed         | 207          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.018954942  |
|    average_cost         | 0.0030273437 |
|    clip_fraction        | 0.158        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.921       |
|    cost_value_loss      | 0.0398       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.436       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.859        |
|    mean_cost_advantages | -0.034194075 |
|    mean_reward_advan... | 0.7184589    |
|    n_updates            | 200          |
|    nu                   | 1.93         |
|    nu_loss              | -0.00578     |
|    policy_gradient_loss | -0.00744     |
|    reward_explained_... | 0.878        |
|    reward_value_loss    | 1.36         |
|    total_cost           | 31.0         |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -7.72        |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -7.72        |
|    true_cost            | 0.00156      |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | -0.449       |
|    ep_len_mean          | 17.8         |
|    ep_rew_mean          | -7.93        |
| time/                   |              |
|    fps                  | 1045         |
|    iterations           | 22           |
|    time_elapsed         | 215          |
|    total_timesteps      | 225280       |
| train/                  |              |
|    approx_kl            | 0.019434584  |
|    average_cost         | 0.0026367188 |
|    clip_fraction        | 0.0973       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.49        |
|    cost_value_loss      | 0.0396       |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.32        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.245        |
|    mean_cost_advantages | 0.0024905787 |
|    mean_reward_advan... | 0.46313494   |
|    n_updates            | 210          |
|    nu                   | 1.96         |
|    nu_loss              | -0.0051      |
|    policy_gradient_loss | -0.00596     |
|    reward_explained_... | 0.901        |
|    reward_value_loss    | 0.784        |
|    total_cost           | 27.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -7           |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -7           |
|    true_cost            | 0.000684     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.447       |
|    ep_len_mean          | 16.9         |
|    ep_rew_mean          | -7.39        |
| time/                   |              |
|    fps                  | 1027         |
|    iterations           | 23           |
|    time_elapsed         | 229          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.008063232  |
|    average_cost         | 0.0015625    |
|    clip_fraction        | 0.0662       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -5.53        |
|    cost_value_loss      | 0.019        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.25        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.178        |
|    mean_cost_advantages | 0.0037249061 |
|    mean_reward_advan... | 0.18166699   |
|    n_updates            | 220          |
|    nu                   | 1.98         |
|    nu_loss              | -0.00306     |
|    policy_gradient_loss | -0.00332     |
|    reward_explained_... | 0.928        |
|    reward_value_loss    | 0.427        |
|    total_cost           | 16.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -7            |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -7.27         |
|    true_cost            | 0.00127       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 16.5          |
|    ep_rew_mean          | -7.48         |
| time/                   |               |
|    fps                  | 1010          |
|    iterations           | 24            |
|    time_elapsed         | 243           |
|    total_timesteps      | 245760        |
| train/                  |               |
|    approx_kl            | 0.006789612   |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.0665        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.04         |
|    cost_value_loss      | 0.00942       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.209        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.144         |
|    mean_cost_advantages | -0.02322868   |
|    mean_reward_advan... | 0.07593754    |
|    n_updates            | 230           |
|    nu                   | 2             |
|    nu_loss              | -0.00135      |
|    policy_gradient_loss | -0.00215      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.382         |
|    total_cost           | 7.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.98        |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -6.98        |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.454       |
|    ep_len_mean          | 16.3         |
|    ep_rew_mean          | -7.42        |
| time/                   |              |
|    fps                  | 998          |
|    iterations           | 25           |
|    time_elapsed         | 256          |
|    total_timesteps      | 256000       |
| train/                  |              |
|    approx_kl            | 0.007395225  |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0.0607       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -12.8        |
|    cost_value_loss      | 0.0416       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.167       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0916       |
|    mean_cost_advantages | 0.003739427  |
|    mean_reward_advan... | 0.101668395  |
|    n_updates            | 240          |
|    nu                   | 2.02         |
|    nu_loss              | -0.00254     |
|    policy_gradient_loss | -0.00193     |
|    reward_explained_... | 0.94         |
|    reward_value_loss    | 0.327        |
|    total_cost           | 13.0         |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.98         |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -7.8          |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.456        |
|    ep_len_mean          | 16.1          |
|    ep_rew_mean          | -7.36         |
| time/                   |               |
|    fps                  | 986           |
|    iterations           | 26            |
|    time_elapsed         | 269           |
|    total_timesteps      | 266240        |
| train/                  |               |
|    approx_kl            | 0.0052863373  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0452        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.706        |
|    cost_value_loss      | 0.00897       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.127        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.204         |
|    mean_cost_advantages | 0.016795507   |
|    mean_reward_advan... | 0.025601393   |
|    n_updates            | 250           |
|    nu                   | 2.04          |
|    nu_loss              | -0.000987     |
|    policy_gradient_loss | -0.00088      |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 0.299         |
|    total_cost           | 5.0           |
-------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.98        |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -7.77        |
|    true_cost            | 0.00117      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.461       |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | -7.31        |
| time/                   |              |
|    fps                  | 985          |
|    iterations           | 27           |
|    time_elapsed         | 280          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.015363051  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -68.3        |
|    cost_value_loss      | 0.000543     |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.0637      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.171        |
|    mean_cost_advantages | 0.0031207544 |
|    mean_reward_advan... | -0.026954005 |
|    n_updates            | 260          |
|    nu                   | 2.05         |
|    nu_loss              | -0.000199    |
|    policy_gradient_loss | -0.00032     |
|    reward_explained_... | 0.951        |
|    reward_value_loss    | 0.248        |
|    total_cost           | 1.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.98        |
|    mean_ep_length       | 16           |
|    mean_reward          | -7.45        |
|    true_cost            | 0.00293      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.463       |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | -7.31        |
| time/                   |              |
|    fps                  | 975          |
|    iterations           | 28           |
|    time_elapsed         | 293          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.008429742  |
|    average_cost         | 0.001171875  |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -6.76        |
|    cost_value_loss      | 0.0186       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0474      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.13         |
|    mean_cost_advantages | 0.0067678792 |
|    mean_reward_advan... | 0.038330212  |
|    n_updates            | 270          |
|    nu                   | 2.07         |
|    nu_loss              | -0.00241     |
|    policy_gradient_loss | -0.000408    |
|    reward_explained_... | 0.934        |
|    reward_value_loss    | 0.346        |
|    total_cost           | 12.0         |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.98          |
|    mean_ep_length       | 17             |
|    mean_reward          | -8             |
|    true_cost            | 0.000781       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.457         |
|    ep_len_mean          | 15.7           |
|    ep_rew_mean          | -7.14          |
| time/                   |                |
|    fps                  | 966            |
|    iterations           | 29             |
|    time_elapsed         | 307            |
|    total_timesteps      | 296960         |
| train/                  |                |
|    approx_kl            | 0.0031467876   |
|    average_cost         | 0.0029296875   |
|    clip_fraction        | 0.0104         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -309           |
|    cost_value_loss      | 0.0135         |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0455        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.287          |
|    mean_cost_advantages | 0.011849966    |
|    mean_reward_advan... | -0.00085266156 |
|    n_updates            | 280            |
|    nu                   | 2.08           |
|    nu_loss              | -0.00606       |
|    policy_gradient_loss | 8.68e-05       |
|    reward_explained_... | 0.94           |
|    reward_value_loss    | 0.311          |
|    total_cost           | 30.0           |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.98        |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -7.32        |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.459       |
|    ep_len_mean          | 15.9         |
|    ep_rew_mean          | -7.29        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 30           |
|    time_elapsed         | 320          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.008318878  |
|    average_cost         | 0.00078125   |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -3           |
|    cost_value_loss      | 0.00376      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0452      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.11         |
|    mean_cost_advantages | -0.009991771 |
|    mean_reward_advan... | 0.069187716  |
|    n_updates            | 290          |
|    nu                   | 2.09         |
|    nu_loss              | -0.00163     |
|    policy_gradient_loss | 0.000119     |
|    reward_explained_... | 0.952        |
|    reward_value_loss    | 0.258        |
|    total_cost           | 8.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.98         |
|    mean_ep_length       | 17            |
|    mean_reward          | -7.85         |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.457        |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | -7.11         |
| time/                   |               |
|    fps                  | 949           |
|    iterations           | 31            |
|    time_elapsed         | 334           |
|    total_timesteps      | 317440        |
| train/                  |               |
|    approx_kl            | 0.0012008019  |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.00253       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -12.5         |
|    cost_value_loss      | 0.0014        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.011        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0778        |
|    mean_cost_advantages | -0.007649237  |
|    mean_reward_advan... | 0.04025478    |
|    n_updates            | 300           |
|    nu                   | 2.11          |
|    nu_loss              | -0.000614     |
|    policy_gradient_loss | -2.24e-05     |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 0.298         |
|    total_cost           | 3.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.98        |
|    mean_ep_length       | 16           |
|    mean_reward          | -7.2         |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.455       |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | -7.11        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 32           |
|    time_elapsed         | 342          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.035996243  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0384       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -46.3        |
|    cost_value_loss      | 0.00105      |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0449      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.115        |
|    mean_cost_advantages | -0.000429497 |
|    mean_reward_advan... | 0.03524766   |
|    n_updates            | 310          |
|    nu                   | 2.12         |
|    nu_loss              | -0.000411    |
|    policy_gradient_loss | -0.000112    |
|    reward_explained_... | 0.949        |
|    reward_value_loss    | 0.264        |
|    total_cost           | 2.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.98        |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -7.87        |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.459       |
|    ep_len_mean          | 16.3         |
|    ep_rew_mean          | -7.54        |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 33           |
|    time_elapsed         | 349          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.022979915  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0581       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.248       |
|    cost_value_loss      | 6.74e-06     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0494      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0962       |
|    mean_cost_advantages | 0.0025094536 |
|    mean_reward_advan... | 0.036750842  |
|    n_updates            | 320          |
|    nu                   | 2.13         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.00013      |
|    reward_explained_... | 0.945        |
|    reward_value_loss    | 0.286        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.98         |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -7.6          |
|    true_cost            | 0.000195      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.458        |
|    ep_len_mean          | 16.4          |
|    ep_rew_mean          | -7.6          |
| time/                   |               |
|    fps                  | 960           |
|    iterations           | 34            |
|    time_elapsed         | 362           |
|    total_timesteps      | 348160        |
| train/                  |               |
|    approx_kl            | 0.016419442   |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.00961       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -2.74         |
|    cost_value_loss      | 0.0058        |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.0322       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.112         |
|    mean_cost_advantages | 0.0042790347  |
|    mean_reward_advan... | -0.007679902  |
|    n_updates            | 330           |
|    nu                   | 2.13          |
|    nu_loss              | -0.000623     |
|    policy_gradient_loss | -0.00018      |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 0.302         |
|    total_cost           | 3.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.43         |
|    mean_ep_length       | 14.6          |
|    mean_reward          | -6.43         |
|    true_cost            | 0.00215       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.458        |
|    ep_len_mean          | 16.1          |
|    ep_rew_mean          | -7.27         |
| time/                   |               |
|    fps                  | 953           |
|    iterations           | 35            |
|    time_elapsed         | 376           |
|    total_timesteps      | 358400        |
| train/                  |               |
|    approx_kl            | 0.009558567   |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0111        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -366          |
|    cost_value_loss      | 0.000946      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0325       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.16          |
|    mean_cost_advantages | 0.00038563146 |
|    mean_reward_advan... | 0.01686901    |
|    n_updates            | 340           |
|    nu                   | 2.14          |
|    nu_loss              | -0.000417     |
|    policy_gradient_loss | -0.000585     |
|    reward_explained_... | 0.93          |
|    reward_value_loss    | 0.342         |
|    total_cost           | 2.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.43         |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -7.05         |
|    true_cost            | 0.0185        |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.495        |
|    ep_len_mean          | 17.7          |
|    ep_rew_mean          | -7.87         |
| time/                   |               |
|    fps                  | 946           |
|    iterations           | 36            |
|    time_elapsed         | 389           |
|    total_timesteps      | 368640        |
| train/                  |               |
|    approx_kl            | 0.010120473   |
|    average_cost         | 0.0021484375  |
|    clip_fraction        | 0.0189        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.9          |
|    cost_value_loss      | 0.0298        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.041        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.146         |
|    mean_cost_advantages | 0.0053204447  |
|    mean_reward_advan... | -0.0017010283 |
|    n_updates            | 350           |
|    nu                   | 2.15          |
|    nu_loss              | -0.0046       |
|    policy_gradient_loss | 0.0133        |
|    reward_explained_... | 0.934         |
|    reward_value_loss    | 0.32          |
|    total_cost           | 22.0          |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -6.43      |
|    mean_ep_length       | 15.8       |
|    mean_reward          | -7.32      |
|    true_cost            | 0.000391   |
| infos/                  |            |
|    cost                 | 0          |
| rollout/                |            |
|    adjusted_reward      | -0.458     |
|    ep_len_mean          | 16.1       |
|    ep_rew_mean          | -7.39      |
| time/                   |            |
|    fps                  | 955        |
|    iterations           | 37         |
|    time_elapsed         | 396        |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.03377711 |
|    average_cost         | 0.01845703 |
|    clip_fraction        | 0.0174     |
|    clip_range           | 0.2        |
|    cost_explained_va... | -109       |
|    cost_value_loss      | 1.17       |
|    early_stop_epoch     | 0          |
|    entropy_loss         | -0.0476    |
|    learning_rate        | 0.0003     |
|    loss                 | 2.4        |
|    mean_cost_advantages | 0.28090495 |
|    mean_reward_advan... | -0.0918821 |
|    n_updates            | 360        |
|    nu                   | 2.16       |
|    nu_loss              | -0.0397    |
|    policy_gradient_loss | -0.0248    |
|    reward_explained_... | 0.844      |
|    reward_value_loss    | 0.437      |
|    total_cost           | 189.0      |
----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.43        |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -6.98        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.459       |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | -7.2         |
| time/                   |              |
|    fps                  | 949          |
|    iterations           | 38           |
|    time_elapsed         | 410          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.0020582625 |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.00738      |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.218        |
|    cost_value_loss      | 0.0249       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0326      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0886       |
|    mean_cost_advantages | 0.002725543  |
|    mean_reward_advan... | -0.021101821 |
|    n_updates            | 370          |
|    nu                   | 2.17         |
|    nu_loss              | -0.000843    |
|    policy_gradient_loss | -0.0003      |
|    reward_explained_... | 0.942        |
|    reward_value_loss    | 0.289        |
|    total_cost           | 4.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.43        |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -7.1         |
|    true_cost            | 0.000195     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.459       |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | -7.38        |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 39           |
|    time_elapsed         | 425          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.006135622  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.265       |
|    cost_value_loss      | 1.59e-05     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0335      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.132        |
|    mean_cost_advantages | 0.005033484  |
|    mean_reward_advan... | -0.012234464 |
|    n_updates            | 380          |
|    nu                   | 2.17         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000443    |
|    reward_explained_... | 0.936        |
|    reward_value_loss    | 0.317        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.43        |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -7.15        |
|    true_cost            | 0.000488     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.456       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -7.08        |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 40           |
|    time_elapsed         | 436          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.021150008  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -110         |
|    cost_value_loss      | 0.000967     |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.0414      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.139        |
|    mean_cost_advantages | 0.003822927  |
|    mean_reward_advan... | 0.07949817   |
|    n_updates            | 390          |
|    nu                   | 2.18         |
|    nu_loss              | -0.000425    |
|    policy_gradient_loss | -0.000257    |
|    reward_explained_... | 0.94         |
|    reward_value_loss    | 0.313        |
|    total_cost           | 2.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.43         |
|    mean_ep_length       | 16.8          |
|    mean_reward          | -7.73         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.456        |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | -7.12         |
| time/                   |               |
|    fps                  | 933           |
|    iterations           | 41            |
|    time_elapsed         | 449           |
|    total_timesteps      | 419840        |
| train/                  |               |
|    approx_kl            | 0.0048630023  |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0315        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0354        |
|    cost_value_loss      | 0.0105        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.082        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.23          |
|    mean_cost_advantages | 0.0014026483  |
|    mean_reward_advan... | 0.091827884   |
|    n_updates            | 400           |
|    nu                   | 2.19          |
|    nu_loss              | -0.00106      |
|    policy_gradient_loss | -0.000586     |
|    reward_explained_... | 0.948         |
|    reward_value_loss    | 0.27          |
|    total_cost           | 5.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.43        |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -6.57        |
|    true_cost            | 0.000293     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.456       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -7.05        |
| time/                   |              |
|    fps                  | 928          |
|    iterations           | 42           |
|    time_elapsed         | 463          |
|    total_timesteps      | 430080       |
| train/                  |              |
|    approx_kl            | 0.0043007764 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0526       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.197       |
|    cost_value_loss      | 4.68e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.113       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.171        |
|    mean_cost_advantages | 0.001742325  |
|    mean_reward_advan... | 0.016542612  |
|    n_updates            | 410          |
|    nu                   | 2.19         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00065     |
|    reward_explained_... | 0.949        |
|    reward_value_loss    | 0.251        |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.43         |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -7.4          |
|    true_cost            | 0.00156       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.46         |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | -7.31         |
| time/                   |               |
|    fps                  | 923           |
|    iterations           | 43            |
|    time_elapsed         | 476           |
|    total_timesteps      | 440320        |
| train/                  |               |
|    approx_kl            | 0.004230666   |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0641        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.306        |
|    cost_value_loss      | 0.00769       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.116        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.156         |
|    mean_cost_advantages | 0.0030923267  |
|    mean_reward_advan... | 0.027093764   |
|    n_updates            | 420           |
|    nu                   | 2.2           |
|    nu_loss              | -0.000642     |
|    policy_gradient_loss | -0.000861     |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 0.272         |
|    total_cost           | 3.0           |
-------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -6.43       |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -7.42       |
|    true_cost            | 9.77e-05    |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.457      |
|    ep_len_mean          | 15.9        |
|    ep_rew_mean          | -7.27       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 44          |
|    time_elapsed         | 490         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.014585266 |
|    average_cost         | 0.0015625   |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -14.1       |
|    cost_value_loss      | 0.0252      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.118      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.076       |
|    mean_cost_advantages | 0.010144119 |
|    mean_reward_advan... | 0.021635637 |
|    n_updates            | 430         |
|    nu                   | 2.2         |
|    nu_loss              | -0.00343    |
|    policy_gradient_loss | -0.00318    |
|    reward_explained_... | 0.944       |
|    reward_value_loss    | 0.267       |
|    total_cost           | 16.0        |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 14.4          |
|    mean_reward          | -6.23         |
|    true_cost            | 0.000488      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.455        |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | -7.14         |
| time/                   |               |
|    fps                  | 924           |
|    iterations           | 45            |
|    time_elapsed         | 498           |
|    total_timesteps      | 460800        |
| train/                  |               |
|    approx_kl            | 0.016616344   |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.0686        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -11.2         |
|    cost_value_loss      | 0.000592      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0975       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.18          |
|    mean_cost_advantages | -0.0018887256 |
|    mean_reward_advan... | 0.023997182   |
|    n_updates            | 440           |
|    nu                   | 2.21          |
|    nu_loss              | -0.000215     |
|    policy_gradient_loss | -0.000591     |
|    reward_explained_... | 0.938         |
|    reward_value_loss    | 0.31          |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -7.13         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | -7.09         |
| time/                   |               |
|    fps                  | 927           |
|    iterations           | 46            |
|    time_elapsed         | 507           |
|    total_timesteps      | 471040        |
| train/                  |               |
|    approx_kl            | 0.01699416    |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.0301        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0423        |
|    cost_value_loss      | 0.00653       |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.0757       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.136         |
|    mean_cost_advantages | 0.0012587064  |
|    mean_reward_advan... | 0.03043591    |
|    n_updates            | 450           |
|    nu                   | 2.21          |
|    nu_loss              | -0.00108      |
|    policy_gradient_loss | -0.00107      |
|    reward_explained_... | 0.957         |
|    reward_value_loss    | 0.214         |
|    total_cost           | 5.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -6.82         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.453        |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | -7.09         |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 47            |
|    time_elapsed         | 521           |
|    total_timesteps      | 481280        |
| train/                  |               |
|    approx_kl            | 0.0061277105  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0119        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.357         |
|    cost_value_loss      | 1.06e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0389       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0698        |
|    mean_cost_advantages | -0.0044887057 |
|    mean_reward_advan... | -0.0016888336 |
|    n_updates            | 460           |
|    nu                   | 2.21          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000234      |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 0.199         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -7.13         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.454        |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | -7.09         |
| time/                   |               |
|    fps                  | 918           |
|    iterations           | 48            |
|    time_elapsed         | 535           |
|    total_timesteps      | 491520        |
| train/                  |               |
|    approx_kl            | 0.0024796748  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00803       |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.247         |
|    cost_value_loss      | 7.64e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0357       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.051         |
|    mean_cost_advantages | -0.0010579498 |
|    mean_reward_advan... | -0.017372612  |
|    n_updates            | 470           |
|    nu                   | 2.22          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -4.23e-05     |
|    reward_explained_... | 0.96          |
|    reward_value_loss    | 0.184         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 16            |
|    mean_reward          | -7.25         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | -7.07         |
| time/                   |               |
|    fps                  | 912           |
|    iterations           | 49            |
|    time_elapsed         | 549           |
|    total_timesteps      | 501760        |
| train/                  |               |
|    approx_kl            | 0.0017175104  |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.011         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -909          |
|    cost_value_loss      | 0.00051       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.038        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.116         |
|    mean_cost_advantages | 0.00032292446 |
|    mean_reward_advan... | -0.08215015   |
|    n_updates            | 480           |
|    nu                   | 2.22          |
|    nu_loss              | -0.000217     |
|    policy_gradient_loss | 0.000403      |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 0.202         |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15            |
|    mean_reward          | -6.75         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | -6.99         |
| time/                   |               |
|    fps                  | 908           |
|    iterations           | 50            |
|    time_elapsed         | 563           |
|    total_timesteps      | 512000        |
| train/                  |               |
|    approx_kl            | 0.00069970323 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0131        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0831        |
|    cost_value_loss      | 6.84e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0397       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0582        |
|    mean_cost_advantages | -0.0005074686 |
|    mean_reward_advan... | -0.02334919   |
|    n_updates            | 490           |
|    nu                   | 2.22          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000352      |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 0.226         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 16            |
|    mean_reward          | -7.33         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.451        |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | -6.95         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 51            |
|    time_elapsed         | 571           |
|    total_timesteps      | 522240        |
| train/                  |               |
|    approx_kl            | 0.046697572   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0643        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0275       |
|    cost_value_loss      | 2.89e-07      |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0713       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.189         |
|    mean_cost_advantages | 2.0045622e-05 |
|    mean_reward_advan... | 0.021105561   |
|    n_updates            | 500           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000313      |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 0.207         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15            |
|    mean_reward          | -6.83         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.451        |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | -6.93         |
| time/                   |               |
|    fps                  | 920           |
|    iterations           | 52            |
|    time_elapsed         | 578           |
|    total_timesteps      | 532480        |
| train/                  |               |
|    approx_kl            | 0.023564722   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0627        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.341        |
|    cost_value_loss      | 3.03e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0799       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0837        |
|    mean_cost_advantages | 0.00058545667 |
|    mean_reward_advan... | 0.01263957    |
|    n_updates            | 510           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000703      |
|    reward_explained_... | 0.962         |
|    reward_value_loss    | 0.175         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -7.1          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | -7.07         |
| time/                   |               |
|    fps                  | 915           |
|    iterations           | 53            |
|    time_elapsed         | 592           |
|    total_timesteps      | 542720        |
| train/                  |               |
|    approx_kl            | 0.0059986836  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0257        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0379        |
|    cost_value_loss      | 5.18e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0905       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0535        |
|    mean_cost_advantages | -0.0003994635 |
|    mean_reward_advan... | 0.029595543   |
|    n_updates            | 520           |
|    nu                   | 2.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000364      |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 0.217         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 16.6           |
|    mean_reward          | -7.6           |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.453         |
|    ep_len_mean          | 15.6           |
|    ep_rew_mean          | -7.07          |
| time/                   |                |
|    fps                  | 912            |
|    iterations           | 54             |
|    time_elapsed         | 605            |
|    total_timesteps      | 552960         |
| train/                  |                |
|    approx_kl            | 0.004029345    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.022          |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.069          |
|    cost_value_loss      | 1.6e-07        |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0733        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.105          |
|    mean_cost_advantages | -0.00021750995 |
|    mean_reward_advan... | 0.002904116    |
|    n_updates            | 530            |
|    nu                   | 2.23           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000228       |
|    reward_explained_... | 0.958          |
|    reward_value_loss    | 0.192          |
|    total_cost           | 0.0            |
--------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -7.05        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.451       |
|    ep_len_mean          | 15.4         |
|    ep_rew_mean          | -6.94        |
| time/                   |              |
|    fps                  | 909          |
|    iterations           | 55           |
|    time_elapsed         | 619          |
|    total_timesteps      | 563200       |
| train/                  |              |
|    approx_kl            | 0.004998884  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.00925      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.43        |
|    cost_value_loss      | 6.26e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0481      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0652       |
|    mean_cost_advantages | 0.0005334785 |
|    mean_reward_advan... | 0.021100296  |
|    n_updates            | 540          |
|    nu                   | 2.23         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.000132     |
|    reward_explained_... | 0.956        |
|    reward_value_loss    | 0.203        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -6.63         |
|    true_cost            | 0.00361       |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.46         |
|    ep_len_mean          | 16.1          |
|    ep_rew_mean          | -7.34         |
| time/                   |               |
|    fps                  | 907           |
|    iterations           | 56            |
|    time_elapsed         | 632           |
|    total_timesteps      | 573440        |
| train/                  |               |
|    approx_kl            | 0.034082055   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0085        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.29         |
|    cost_value_loss      | 8.94e-07      |
|    early_stop_epoch     | 8             |
|    entropy_loss         | -0.0269       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0669        |
|    mean_cost_advantages | -0.0022017322 |
|    mean_reward_advan... | 0.05172991    |
|    n_updates            | 550           |
|    nu                   | 2.24          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000198      |
|    reward_explained_... | 0.962         |
|    reward_value_loss    | 0.179         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -7.35        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.436       |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | -7.11        |
| time/                   |              |
|    fps                  | 910          |
|    iterations           | 57           |
|    time_elapsed         | 640          |
|    total_timesteps      | 583680       |
| train/                  |              |
|    approx_kl            | 0.01837967   |
|    average_cost         | 0.0036132813 |
|    clip_fraction        | 0.00833      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -17.6        |
|    cost_value_loss      | 0.226        |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.0346      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.505        |
|    mean_cost_advantages | 0.041843038  |
|    mean_reward_advan... | -0.09022911  |
|    n_updates            | 560          |
|    nu                   | 2.24         |
|    nu_loss              | -0.00808     |
|    policy_gradient_loss | -0.00718     |
|    reward_explained_... | 0.943        |
|    reward_value_loss    | 0.257        |
|    total_cost           | 37.0         |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 16            |
|    mean_reward          | -7.42         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.45         |
|    ep_len_mean          | 15.5          |
|    ep_rew_mean          | -6.98         |
| time/                   |               |
|    fps                  | 916           |
|    iterations           | 58            |
|    time_elapsed         | 647           |
|    total_timesteps      | 593920        |
| train/                  |               |
|    approx_kl            | 0.021861676   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0165        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0379        |
|    cost_value_loss      | 7.06e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0231       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0389        |
|    mean_cost_advantages | -0.0010233509 |
|    mean_reward_advan... | -0.14079377   |
|    n_updates            | 570           |
|    nu                   | 2.24          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00133      |
|    reward_explained_... | 0.949         |
|    reward_value_loss    | 0.223         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.07
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15            |
|    mean_reward          | -6.5          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.45         |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | -6.95         |
| time/                   |               |
|    fps                  | 915           |
|    iterations           | 59            |
|    time_elapsed         | 659           |
|    total_timesteps      | 604160        |
| train/                  |               |
|    approx_kl            | 0.06605492    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0249        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0128        |
|    cost_value_loss      | 1.54e-05      |
|    early_stop_epoch     | 6             |
|    entropy_loss         | -0.0329       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0701        |
|    mean_cost_advantages | -0.0005760666 |
|    mean_reward_advan... | 0.054384124   |
|    n_updates            | 580           |
|    nu                   | 2.24          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000374      |
|    reward_explained_... | 0.965         |
|    reward_value_loss    | 0.162         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -6.55         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.453        |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | -7.18         |
| time/                   |               |
|    fps                  | 912           |
|    iterations           | 60            |
|    time_elapsed         | 673           |
|    total_timesteps      | 614400        |
| train/                  |               |
|    approx_kl            | 0.0041017197  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0443        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0498       |
|    cost_value_loss      | 3.39e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0883       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0717        |
|    mean_cost_advantages | 7.036897e-05  |
|    mean_reward_advan... | -0.0033853776 |
|    n_updates            | 590           |
|    nu                   | 2.24          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000198      |
|    reward_explained_... | 0.958         |
|    reward_value_loss    | 0.185         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15            |
|    mean_reward          | -6.75         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | -6.97         |
| time/                   |               |
|    fps                  | 909           |
|    iterations           | 61            |
|    time_elapsed         | 687           |
|    total_timesteps      | 624640        |
| train/                  |               |
|    approx_kl            | 0.003659048   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0446        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0216       |
|    cost_value_loss      | 1.64e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0844       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.133         |
|    mean_cost_advantages | 0.00039195307 |
|    mean_reward_advan... | 0.0041221296  |
|    n_updates            | 600           |
|    nu                   | 2.24          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000363      |
|    reward_explained_... | 0.957         |
|    reward_value_loss    | 0.197         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -6.47        |
|    true_cost            | 0.00742      |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.466       |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | -7.09        |
| time/                   |              |
|    fps                  | 908          |
|    iterations           | 62           |
|    time_elapsed         | 699          |
|    total_timesteps      | 634880       |
| train/                  |              |
|    approx_kl            | 0.016400587  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0131      |
|    cost_value_loss      | 8.67e-07     |
|    early_stop_epoch     | 7            |
|    entropy_loss         | -0.054       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.112        |
|    mean_cost_advantages | 4.745872e-05 |
|    mean_reward_advan... | 0.020340085  |
|    n_updates            | 610          |
|    nu                   | 2.25         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000382    |
|    reward_explained_... | 0.946        |
|    reward_value_loss    | 0.241        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -6.23       |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -6.95       |
|    true_cost            | 0.000293    |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.452      |
|    ep_len_mean          | 15.7        |
|    ep_rew_mean          | -7.1        |
| time/                   |             |
|    fps                  | 913         |
|    iterations           | 63          |
|    time_elapsed         | 706         |
|    total_timesteps      | 645120      |
| train/                  |             |
|    approx_kl            | 0.015117723 |
|    average_cost         | 0.007421875 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.72e+04   |
|    cost_value_loss      | 0.0344      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.046      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0546      |
|    mean_cost_advantages | 0.044725634 |
|    mean_reward_advan... | 0.048929244 |
|    n_updates            | 620         |
|    nu                   | 2.25        |
|    nu_loss              | -0.0167     |
|    policy_gradient_loss | -0.00412    |
|    reward_explained_... | 0.962       |
|    reward_value_loss    | 0.177       |
|    total_cost           | 76.0        |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 14.6          |
|    mean_reward          | -6.43         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.451        |
|    ep_len_mean          | 15.3          |
|    ep_rew_mean          | -6.9          |
| time/                   |               |
|    fps                  | 910           |
|    iterations           | 64            |
|    time_elapsed         | 719           |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.008438188   |
|    average_cost         | 0.00029296876 |
|    clip_fraction        | 0.0127        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.118        |
|    cost_value_loss      | 0.00171       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0377       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.111         |
|    mean_cost_advantages | -0.028260723  |
|    mean_reward_advan... | 0.02157164    |
|    n_updates            | 630           |
|    nu                   | 2.25          |
|    nu_loss              | -0.000658     |
|    policy_gradient_loss | -4.96e-05     |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 0.205         |
|    total_cost           | 3.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -6.9         |
|    true_cost            | 0.00244      |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.456       |
|    ep_len_mean          | 15.4         |
|    ep_rew_mean          | -6.94        |
| time/                   |              |
|    fps                  | 906          |
|    iterations           | 65           |
|    time_elapsed         | 734          |
|    total_timesteps      | 665600       |
| train/                  |              |
|    approx_kl            | 0.002153194  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.00867      |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.49        |
|    cost_value_loss      | 0.000506     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.03        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0452       |
|    mean_cost_advantages | -0.009330189 |
|    mean_reward_advan... | 0.056433458  |
|    n_updates            | 640          |
|    nu                   | 2.25         |
|    nu_loss              | -0.00022     |
|    policy_gradient_loss | 0.00011      |
|    reward_explained_... | 0.965        |
|    reward_value_loss    | 0.159        |
|    total_cost           | 1.0          |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.24
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 15           |
|    mean_reward          | -6.75        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.452       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -7.04        |
| time/                   |              |
|    fps                  | 908          |
|    iterations           | 66           |
|    time_elapsed         | 743          |
|    total_timesteps      | 675840       |
| train/                  |              |
|    approx_kl            | 0.24123792   |
|    average_cost         | 0.0024414062 |
|    clip_fraction        | 0.0497       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.158       |
|    cost_value_loss      | 0.0693       |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.05        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.113        |
|    mean_cost_advantages | 0.015014149  |
|    mean_reward_advan... | 0.038687766  |
|    n_updates            | 650          |
|    nu                   | 2.25         |
|    nu_loss              | -0.0055      |
|    policy_gradient_loss | -0.0018      |
|    reward_explained_... | 0.96         |
|    reward_value_loss    | 0.183        |
|    total_cost           | 25.0         |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -7.22        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.451       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -7.04        |
| time/                   |              |
|    fps                  | 906          |
|    iterations           | 67           |
|    time_elapsed         | 757          |
|    total_timesteps      | 686080       |
| train/                  |              |
|    approx_kl            | 0.0057254597 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0487       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.27        |
|    cost_value_loss      | 4.11e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0961      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.117        |
|    mean_cost_advantages | -0.011821164 |
|    mean_reward_advan... | -0.021112707 |
|    n_updates            | 660          |
|    nu                   | 2.25         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -6.07e-05    |
|    reward_explained_... | 0.955        |
|    reward_value_loss    | 0.209        |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -7.18        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.451       |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | -7.06        |
| time/                   |              |
|    fps                  | 903          |
|    iterations           | 68           |
|    time_elapsed         | 770          |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.005336373  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.497        |
|    cost_value_loss      | 6.61e-07     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0925      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.094        |
|    mean_cost_advantages | -0.003047862 |
|    mean_reward_advan... | 0.053502757  |
|    n_updates            | 670          |
|    nu                   | 2.26         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000165    |
|    reward_explained_... | 0.962        |
|    reward_value_loss    | 0.178        |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 15.4           |
|    mean_reward          | -7.07          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.453         |
|    ep_len_mean          | 15.6           |
|    ep_rew_mean          | -7.09          |
| time/                   |                |
|    fps                  | 901            |
|    iterations           | 69             |
|    time_elapsed         | 784            |
|    total_timesteps      | 706560         |
| train/                  |                |
|    approx_kl            | 0.008937059    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0443         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.247          |
|    cost_value_loss      | 5.38e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0987        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0965         |
|    mean_cost_advantages | -0.00090121885 |
|    mean_reward_advan... | 0.0034338161   |
|    n_updates            | 680            |
|    nu                   | 2.26           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000103      |
|    reward_explained_... | 0.957          |
|    reward_value_loss    | 0.194          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 14.8           |
|    mean_reward          | -6.72          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.449         |
|    ep_len_mean          | 15.2           |
|    ep_rew_mean          | -6.82          |
| time/                   |                |
|    fps                  | 895            |
|    iterations           | 70             |
|    time_elapsed         | 800            |
|    total_timesteps      | 716800         |
| train/                  |                |
|    approx_kl            | 0.016583968    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0429         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.108          |
|    cost_value_loss      | 4.59e-07       |
|    early_stop_epoch     | 1              |
|    entropy_loss         | -0.0947        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0908         |
|    mean_cost_advantages | -0.00029689184 |
|    mean_reward_advan... | 0.0028473362   |
|    n_updates            | 690            |
|    nu                   | 2.26           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.0019         |
|    reward_explained_... | 0.951          |
|    reward_value_loss    | 0.23           |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -7.1          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | -7.14         |
| time/                   |               |
|    fps                  | 893           |
|    iterations           | 71            |
|    time_elapsed         | 813           |
|    total_timesteps      | 727040        |
| train/                  |               |
|    approx_kl            | 0.0041951807  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0361        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.139         |
|    cost_value_loss      | 6.37e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0908       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.119         |
|    mean_cost_advantages | -0.0003397864 |
|    mean_reward_advan... | 0.0035140861  |
|    n_updates            | 700           |
|    nu                   | 2.26          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000104      |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 0.193         |
|    total_cost           | 0.0           |
-------------------------------------------
---------------------------------------------
| eval/                   |                 |
|    best_mean_reward     | -6.23           |
|    mean_ep_length       | 15.4            |
|    mean_reward          | -7.07           |
|    true_cost            | 0               |
| infos/                  |                 |
|    cost                 | 0               |
| rollout/                |                 |
|    adjusted_reward      | -0.453          |
|    ep_len_mean          | 16.2            |
|    ep_rew_mean          | -7.43           |
| time/                   |                 |
|    fps                  | 891             |
|    iterations           | 72              |
|    time_elapsed         | 827             |
|    total_timesteps      | 737280          |
| train/                  |                 |
|    approx_kl            | 0.009775873     |
|    average_cost         | 0.0             |
|    clip_fraction        | 0.0369          |
|    clip_range           | 0.2             |
|    cost_explained_va... | 0.0453          |
|    cost_value_loss      | 2.55e-06        |
|    early_stop_epoch     | 10              |
|    entropy_loss         | -0.0947         |
|    learning_rate        | 0.0003          |
|    loss                 | 0.212           |
|    mean_cost_advantages | -0.000102252074 |
|    mean_reward_advan... | -0.024949873    |
|    n_updates            | 710             |
|    nu                   | 2.26            |
|    nu_loss              | -0              |
|    policy_gradient_loss | -0.000107       |
|    reward_explained_... | 0.953           |
|    reward_value_loss    | 0.207           |
|    total_cost           | 0.0             |
---------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -8.12        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.451       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -7.05        |
| time/                   |              |
|    fps                  | 895          |
|    iterations           | 73           |
|    time_elapsed         | 834          |
|    total_timesteps      | 747520       |
| train/                  |              |
|    approx_kl            | 0.019720377  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.103       |
|    cost_value_loss      | 8.29e-08     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0971      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0571       |
|    mean_cost_advantages | 6.881632e-05 |
|    mean_reward_advan... | -0.020109637 |
|    n_updates            | 720          |
|    nu                   | 2.26         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.000981     |
|    reward_explained_... | 0.949        |
|    reward_value_loss    | 0.229        |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -7.48         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.5          |
|    ep_rew_mean          | -6.99         |
| time/                   |               |
|    fps                  | 893           |
|    iterations           | 74            |
|    time_elapsed         | 847           |
|    total_timesteps      | 757760        |
| train/                  |               |
|    approx_kl            | 0.0046226364  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0343        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0559       |
|    cost_value_loss      | 1.11e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0852       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0714        |
|    mean_cost_advantages | 2.0973712e-05 |
|    mean_reward_advan... | 0.036594052   |
|    n_updates            | 730           |
|    nu                   | 2.26          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000135     |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 0.202         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 16            |
|    mean_reward          | -7.33         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | -7.01         |
| time/                   |               |
|    fps                  | 891           |
|    iterations           | 75            |
|    time_elapsed         | 861           |
|    total_timesteps      | 768000        |
| train/                  |               |
|    approx_kl            | 0.002090788   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0318        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0275       |
|    cost_value_loss      | 4.27e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0811       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0443        |
|    mean_cost_advantages | -1.885624e-05 |
|    mean_reward_advan... | -0.0060690995 |
|    n_updates            | 740           |
|    nu                   | 2.26          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000213      |
|    reward_explained_... | 0.956         |
|    reward_value_loss    | 0.197         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 16            |
|    mean_reward          | -7.25         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.45         |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | -6.89         |
| time/                   |               |
|    fps                  | 889           |
|    iterations           | 76            |
|    time_elapsed         | 874           |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 0.023336466   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0375        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0687       |
|    cost_value_loss      | 2.01e-06      |
|    early_stop_epoch     | 9             |
|    entropy_loss         | -0.0808       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.132         |
|    mean_cost_advantages | 3.2324955e-05 |
|    mean_reward_advan... | 0.00024192556 |
|    n_updates            | 750           |
|    nu                   | 2.26          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000209      |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 0.223         |
|    total_cost           | 0.0           |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -6.87        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.451       |
|    ep_len_mean          | 15.4         |
|    ep_rew_mean          | -6.98        |
| time/                   |              |
|    fps                  | 887          |
|    iterations           | 77           |
|    time_elapsed         | 888          |
|    total_timesteps      | 788480       |
| train/                  |              |
|    approx_kl            | 0.007137184  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -36          |
|    cost_value_loss      | 1.76e-06     |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0624      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.214        |
|    mean_cost_advantages | 0.005014435  |
|    mean_reward_advan... | -0.021047363 |
|    n_updates            | 760          |
|    nu                   | 2.27         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -3.6e-05     |
|    reward_explained_... | 0.951        |
|    reward_value_loss    | 0.222        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15            |
|    mean_reward          | -6.67         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.451        |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | -7.16         |
| time/                   |               |
|    fps                  | 887           |
|    iterations           | 78            |
|    time_elapsed         | 899           |
|    total_timesteps      | 798720        |
| train/                  |               |
|    approx_kl            | 0.01727644    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0384        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.155        |
|    cost_value_loss      | 9.44e-07      |
|    early_stop_epoch     | 6             |
|    entropy_loss         | -0.0732       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0529        |
|    mean_cost_advantages | -0.0003080017 |
|    mean_reward_advan... | 0.019456545   |
|    n_updates            | 770           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000474      |
|    reward_explained_... | 0.964         |
|    reward_value_loss    | 0.165         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 16.2           |
|    mean_reward          | -7.37          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.451         |
|    ep_len_mean          | 15.6           |
|    ep_rew_mean          | -7.06          |
| time/                   |                |
|    fps                  | 885            |
|    iterations           | 79             |
|    time_elapsed         | 913            |
|    total_timesteps      | 808960         |
| train/                  |                |
|    approx_kl            | 0.002791856    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.021          |
|    clip_range           | 0.2            |
|    cost_explained_va... | -1.05          |
|    cost_value_loss      | 6.85e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0671        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.144          |
|    mean_cost_advantages | -0.00059388124 |
|    mean_reward_advan... | -0.032952685   |
|    n_updates            | 780            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 3.57e-05       |
|    reward_explained_... | 0.957          |
|    reward_value_loss    | 0.19           |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 14.8           |
|    mean_reward          | -6.47          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.451         |
|    ep_len_mean          | 15.8           |
|    ep_rew_mean          | -7.15          |
| time/                   |                |
|    fps                  | 890            |
|    iterations           | 80             |
|    time_elapsed         | 920            |
|    total_timesteps      | 819200         |
| train/                  |                |
|    approx_kl            | 0.016137745    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0448         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.223         |
|    cost_value_loss      | 2.69e-06       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.0506        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0599         |
|    mean_cost_advantages | -0.00026693568 |
|    mean_reward_advan... | -0.015431024   |
|    n_updates            | 790            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000169       |
|    reward_explained_... | 0.958          |
|    reward_value_loss    | 0.183          |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15            |
|    mean_reward          | -6.83         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.5          |
|    ep_rew_mean          | -7            |
| time/                   |               |
|    fps                  | 887           |
|    iterations           | 81            |
|    time_elapsed         | 934           |
|    total_timesteps      | 829440        |
| train/                  |               |
|    approx_kl            | 0.0022645     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0183        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.115        |
|    cost_value_loss      | 6.34e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0534       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.101         |
|    mean_cost_advantages | 0.00011892863 |
|    mean_reward_advan... | -0.0003504975 |
|    n_updates            | 800           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 7.65e-05      |
|    reward_explained_... | 0.959         |
|    reward_value_loss    | 0.185         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -6.55         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.452        |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | -7.15         |
| time/                   |               |
|    fps                  | 886           |
|    iterations           | 82            |
|    time_elapsed         | 946           |
|    total_timesteps      | 839680        |
| train/                  |               |
|    approx_kl            | 0.02277925    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0347        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.00175       |
|    cost_value_loss      | 1.91e-06      |
|    early_stop_epoch     | 7             |
|    entropy_loss         | -0.0722       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0996        |
|    mean_cost_advantages | 0.00013093371 |
|    mean_reward_advan... | 0.005788183   |
|    n_updates            | 810           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000102      |
|    reward_explained_... | 0.955         |
|    reward_value_loss    | 0.199         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -6.98        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.452       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -7.06        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 83           |
|    time_elapsed         | 954          |
|    total_timesteps      | 849920       |
| train/                  |              |
|    approx_kl            | 0.015322512  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0173      |
|    cost_value_loss      | 1.32e-08     |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.0486      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0713       |
|    mean_cost_advantages | 1.948496e-05 |
|    mean_reward_advan... | -0.05608409  |
|    n_updates            | 820          |
|    nu                   | 2.27         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.000385     |
|    reward_explained_... | 0.953        |
|    reward_value_loss    | 0.21         |
|    total_cost           | 0.0          |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15            |
|    mean_reward          | -6.75         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.451        |
|    ep_len_mean          | 15.5          |
|    ep_rew_mean          | -7            |
| time/                   |               |
|    fps                  | 888           |
|    iterations           | 84            |
|    time_elapsed         | 968           |
|    total_timesteps      | 860160        |
| train/                  |               |
|    approx_kl            | 0.0013342907  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00532       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0332       |
|    cost_value_loss      | 1.66e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0304       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.045         |
|    mean_cost_advantages | 5.5475975e-05 |
|    mean_reward_advan... | -0.021059703  |
|    n_updates            | 830           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000216      |
|    reward_explained_... | 0.958         |
|    reward_value_loss    | 0.195         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 32.4          |
|    mean_reward          | -9.5          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.308        |
|    ep_len_mean          | 27.8          |
|    ep_rew_mean          | -8.74         |
| time/                   |               |
|    fps                  | 892           |
|    iterations           | 85            |
|    time_elapsed         | 975           |
|    total_timesteps      | 870400        |
| train/                  |               |
|    approx_kl            | 0.022402698   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0398        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0378       |
|    cost_value_loss      | 1.34e-08      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0494       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.188         |
|    mean_cost_advantages | -2.636451e-05 |
|    mean_reward_advan... | -0.034122318  |
|    n_updates            | 840           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 0.000229      |
|    reward_explained_... | 0.958         |
|    reward_value_loss    | 0.193         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -6.85         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.443        |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | -7.04         |
| time/                   |               |
|    fps                  | 896           |
|    iterations           | 86            |
|    time_elapsed         | 982           |
|    total_timesteps      | 880640        |
| train/                  |               |
|    approx_kl            | 0.062171996   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.151         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0611        |
|    cost_value_loss      | 3.25e-07      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0987       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.306         |
|    mean_cost_advantages | 0.00010573503 |
|    mean_reward_advan... | -0.7863683    |
|    n_updates            | 850           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00757      |
|    reward_explained_... | 0.826         |
|    reward_value_loss    | 0.584         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 15.2           |
|    mean_reward          | -6.78          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.452         |
|    ep_len_mean          | 15.7           |
|    ep_rew_mean          | -7.08          |
| time/                   |                |
|    fps                  | 899            |
|    iterations           | 87             |
|    time_elapsed         | 990            |
|    total_timesteps      | 890880         |
| train/                  |                |
|    approx_kl            | 0.029427877    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0189         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0585         |
|    cost_value_loss      | 3.98e-08       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.0289        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.139          |
|    mean_cost_advantages | -0.00027717603 |
|    mean_reward_advan... | 0.34126613     |
|    n_updates            | 860            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000648      |
|    reward_explained_... | 0.934          |
|    reward_value_loss    | 0.24           |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 15.4           |
|    mean_reward          | -6.98          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.452         |
|    ep_len_mean          | 15.8           |
|    ep_rew_mean          | -7.18          |
| time/                   |                |
|    fps                  | 897            |
|    iterations           | 88             |
|    time_elapsed         | 1003           |
|    total_timesteps      | 901120         |
| train/                  |                |
|    approx_kl            | 0.007210686    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0152         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0425        |
|    cost_value_loss      | 5.77e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0428        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.121          |
|    mean_cost_advantages | -0.00012701743 |
|    mean_reward_advan... | 0.13430436     |
|    n_updates            | 870            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000422       |
|    reward_explained_... | 0.961          |
|    reward_value_loss    | 0.185          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 21.8           |
|    mean_reward          | -7.78          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.328         |
|    ep_len_mean          | 25.3           |
|    ep_rew_mean          | -8.48          |
| time/                   |                |
|    fps                  | 901            |
|    iterations           | 89             |
|    time_elapsed         | 1010           |
|    total_timesteps      | 911360         |
| train/                  |                |
|    approx_kl            | 0.02003754     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0321         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0366        |
|    cost_value_loss      | 1.94e-07       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.0477        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.146          |
|    mean_cost_advantages | -2.6750125e-05 |
|    mean_reward_advan... | 0.055797886    |
|    n_updates            | 880            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 5.37e-05       |
|    reward_explained_... | 0.964          |
|    reward_value_loss    | 0.172          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -6.52        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.433       |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | -7.12        |
| time/                   |              |
|    fps                  | 905          |
|    iterations           | 90           |
|    time_elapsed         | 1017         |
|    total_timesteps      | 921600       |
| train/                  |              |
|    approx_kl            | 0.048765283  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.152        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.104        |
|    cost_value_loss      | 2.42e-07     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.1         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.289        |
|    mean_cost_advantages | 7.726672e-06 |
|    mean_reward_advan... | -0.6716436   |
|    n_updates            | 890          |
|    nu                   | 2.27         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0079      |
|    reward_explained_... | 0.833        |
|    reward_value_loss    | 0.59         |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 15             |
|    mean_reward          | -6.58          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.451         |
|    ep_len_mean          | 15.5           |
|    ep_rew_mean          | -6.98          |
| time/                   |                |
|    fps                  | 907            |
|    iterations           | 91             |
|    time_elapsed         | 1026           |
|    total_timesteps      | 931840         |
| train/                  |                |
|    approx_kl            | 0.06087611     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0723         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0987         |
|    cost_value_loss      | 6.76e-07       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.0259        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.143          |
|    mean_cost_advantages | -0.00029330832 |
|    mean_reward_advan... | 0.21487963     |
|    n_updates            | 900            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00437       |
|    reward_explained_... | 0.922          |
|    reward_value_loss    | 0.318          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.23        |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -7.48        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.453       |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | -7.09        |
| time/                   |              |
|    fps                  | 910          |
|    iterations           | 92           |
|    time_elapsed         | 1034         |
|    total_timesteps      | 942080       |
| train/                  |              |
|    approx_kl            | 0.01597913   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -15.1        |
|    cost_value_loss      | 1.63e-06     |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.041       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0984       |
|    mean_cost_advantages | 0.0016682863 |
|    mean_reward_advan... | 0.1845102    |
|    n_updates            | 910          |
|    nu                   | 2.27         |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.000537     |
|    reward_explained_... | 0.958        |
|    reward_value_loss    | 0.201        |
|    total_cost           | 0.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 15             |
|    mean_reward          | -6.75          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.453         |
|    ep_len_mean          | 15.4           |
|    ep_rew_mean          | -6.93          |
| time/                   |                |
|    fps                  | 908            |
|    iterations           | 93             |
|    time_elapsed         | 1048           |
|    total_timesteps      | 952320         |
| train/                  |                |
|    approx_kl            | 0.0034903474   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0294         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -2.53          |
|    cost_value_loss      | 8.47e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0712        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0778         |
|    mean_cost_advantages | -0.00073744694 |
|    mean_reward_advan... | 0.034536373    |
|    n_updates            | 920            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.00014        |
|    reward_explained_... | 0.957          |
|    reward_value_loss    | 0.202          |
|    total_cost           | 0.0            |
--------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 15.2           |
|    mean_reward          | -6.95          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.451         |
|    ep_len_mean          | 15.6           |
|    ep_rew_mean          | -7.12          |
| time/                   |                |
|    fps                  | 906            |
|    iterations           | 94             |
|    time_elapsed         | 1062           |
|    total_timesteps      | 962560         |
| train/                  |                |
|    approx_kl            | 0.0014024929   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0123         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.115          |
|    cost_value_loss      | 1.82e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0548        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.101          |
|    mean_cost_advantages | -8.7893175e-05 |
|    mean_reward_advan... | 0.013327876    |
|    n_updates            | 930            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 4.46e-05       |
|    reward_explained_... | 0.955          |
|    reward_value_loss    | 0.212          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -6.23          |
|    mean_ep_length       | 17.8           |
|    mean_reward          | -6.83          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.372         |
|    ep_len_mean          | 21.5           |
|    ep_rew_mean          | -7.84          |
| time/                   |                |
|    fps                  | 909            |
|    iterations           | 95             |
|    time_elapsed         | 1069           |
|    total_timesteps      | 972800         |
| train/                  |                |
|    approx_kl            | 0.03362542     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0443         |
|    clip_range           | 0.2            |
|    cost_explained_va... | 0.0442         |
|    cost_value_loss      | 6.24e-09       |
|    early_stop_epoch     | 0              |
|    entropy_loss         | -0.0599        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0621         |
|    mean_cost_advantages | -4.8127262e-05 |
|    mean_reward_advan... | -0.004360016   |
|    n_updates            | 940            |
|    nu                   | 2.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 0.000119       |
|    reward_explained_... | 0.952          |
|    reward_value_loss    | 0.22           |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 17            |
|    mean_reward          | -8            |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.448        |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | -7.04         |
| time/                   |               |
|    fps                  | 913           |
|    iterations           | 96            |
|    time_elapsed         | 1076          |
|    total_timesteps      | 983040        |
| train/                  |               |
|    approx_kl            | 0.0867585     |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.15          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.36e+03     |
|    cost_value_loss      | 0.0005        |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0664       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.171         |
|    mean_cost_advantages | 0.00056811736 |
|    mean_reward_advan... | -0.440499     |
|    n_updates            | 950           |
|    nu                   | 2.27          |
|    nu_loss              | -0.000222     |
|    policy_gradient_loss | -0.00635      |
|    reward_explained_... | 0.899         |
|    reward_value_loss    | 0.405         |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -7.35         |
|    true_cost            | 9.77e-05      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.459        |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | -7.24         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 97            |
|    time_elapsed         | 1085          |
|    total_timesteps      | 993280        |
| train/                  |               |
|    approx_kl            | 0.030069089   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0146        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -4.38         |
|    cost_value_loss      | 2.47e-06      |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.0287       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0599        |
|    mean_cost_advantages | -0.0024377175 |
|    mean_reward_advan... | 0.23260143    |
|    n_updates            | 960           |
|    nu                   | 2.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00114      |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 0.192         |
|    total_cost           | 0.0           |
-------------------------------------------
/home/jovyan/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
Early stopping at step 0 due to reaching max kl: 0.04
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.23         |
|    mean_ep_length       | 18.4          |
|    mean_reward          | -6.85         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.429        |
|    ep_len_mean          | 16.5          |
|    ep_rew_mean          | -7.12         |
| time/                   |               |
|    fps                  | 916           |
|    iterations           | 98            |
|    time_elapsed         | 1095          |
|    total_timesteps      | 1003520       |
| train/                  |               |
|    approx_kl            | 0.044610273   |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.0512        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -5.34e+03     |
|    cost_value_loss      | 0.000426      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0464       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.21          |
|    mean_cost_advantages | -0.0002548117 |
|    mean_reward_advan... | 0.050204735   |
|    n_updates            | 970           |
|    nu                   | 2.27          |
|    nu_loss              | -0.000222     |
|    policy_gradient_loss | -0.000101     |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 0.285         |
|    total_cost           | 1.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
[32mTime taken: 18.41 minutes