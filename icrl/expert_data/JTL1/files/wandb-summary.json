{"eval/mean_reward": -5.2333333333333325, "eval/mean_ep_length": 11.8, "eval/best_mean_reward": -4.583333333333333, "rollout/adjusted_reward": -0.459228515625, "eval/true_cost": 0.0, "time/iterations": 98, "time/fps": 304, "time/time_elapsed": 3290, "time/total_timesteps": 1003520, "infos/cost": 0.0, "rollout/ep_rew_mean": -5.736666599999999, "rollout/ep_len_mean": 12.46, "_timestamp": 1656581748, "_runtime": 3301, "_step": 1003520, "train/learning_rate": 0.0003, "train/entropy_loss": -0.022225493586538504, "train/policy_gradient_loss": 2.3744032426420563e-05, "train/reward_value_loss": 0.18599637598381377, "train/cost_value_loss": 1.6186824119301946e-07, "train/approx_kl": 0.004301259759813547, "train/clip_fraction": 0.01064453125, "train/loss": 0.08529682457447052, "train/mean_reward_advantages": 0.010348756797611713, "train/mean_cost_advantages": 0.000841560133267194, "train/reward_explained_variance": 0.9476533271372318, "train/cost_explained_variance": 0.13465416431427002, "train/nu": 5.639554977416992, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 970, "train/clip_range": 0.2, "_wandb": {"runtime": 3342}}