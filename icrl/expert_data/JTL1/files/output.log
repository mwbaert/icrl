[32mConfigured folder /tmp/wandb/run-20220630_084047-bu8d940c/files for saving
[32mName: JTL-v0_CJTL-v0_dnc_True_dno_True_dnr_True_goal_1_ws_True_s_20_sid_-1
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/lib/python3/dist-packages/pkg_resources/_vendor/pyparsing.py:943: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  collections.MutableMapping.register(ParseResults)
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:216: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator. 
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:228: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:233: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
Wrapping eval env in a VecNormalize.
Using cpu device
----------------------------------
| eval/               |          |
|    best_mean_reward | -128     |
|    mean_ep_length   | 154      |
|    mean_reward      | -128     |
|    true_cost        | 0.505    |
| infos/              |          |
|    cost             | 0.49     |
| rollout/            |          |
|    adjusted_reward  | -1.37    |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | -151     |
| time/               |          |
|    fps              | 862      |
|    iterations       | 1        |
|    time_elapsed     | 11       |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -101        |
|    mean_ep_length       | 137         |
|    mean_reward          | -101        |
|    true_cost            | 0.295       |
| infos/                  |             |
|    cost                 | 0.53        |
| rollout/                |             |
|    adjusted_reward      | -1.06       |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | -144        |
| time/                   |             |
|    fps                  | 680         |
|    iterations           | 2           |
|    time_elapsed         | 30          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.027813578 |
|    average_cost         | 0.50517577  |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -271        |
|    cost_value_loss      | 26.9        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 16          |
|    mean_cost_advantages | 7.8759055   |
|    mean_reward_advan... | -13.075668  |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.505      |
|    policy_gradient_loss | -0.0275     |
|    reward_explained_... | -377        |
|    reward_value_loss    | 61.6        |
|    total_cost           | 5173.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -101        |
|    mean_ep_length       | 187         |
|    mean_reward          | -125        |
|    true_cost            | 0.141       |
| infos/                  |             |
|    cost                 | 0.33        |
| rollout/                |             |
|    adjusted_reward      | -0.862      |
|    ep_len_mean          | 167         |
|    ep_rew_mean          | -121        |
| time/                   |             |
|    fps                  | 590         |
|    iterations           | 3           |
|    time_elapsed         | 52          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.030004699 |
|    average_cost         | 0.29501954  |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -4.55       |
|    cost_value_loss      | 22.8        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.3        |
|    learning_rate        | 0.0003      |
|    loss                 | 19.4        |
|    mean_cost_advantages | 3.0212564   |
|    mean_reward_advan... | -8.16594    |
|    n_updates            | 20          |
|    nu                   | 1.13        |
|    nu_loss              | -0.314      |
|    policy_gradient_loss | -0.0267     |
|    reward_explained_... | -2.27       |
|    reward_value_loss    | 44.7        |
|    total_cost           | 3021.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -97         |
|    mean_ep_length       | 155         |
|    mean_reward          | -97         |
|    true_cost            | 0.121       |
| infos/                  |             |
|    cost                 | 0.24        |
| rollout/                |             |
|    adjusted_reward      | -0.778      |
|    ep_len_mean          | 142         |
|    ep_rew_mean          | -91.2       |
| time/                   |             |
|    fps                  | 593         |
|    iterations           | 4           |
|    time_elapsed         | 69          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.027023163 |
|    average_cost         | 0.14111328  |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.14       |
|    cost_value_loss      | 13.4        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.19       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.6        |
|    mean_cost_advantages | 0.04111496  |
|    mean_reward_advan... | -5.2795997  |
|    n_updates            | 30          |
|    nu                   | 1.19        |
|    nu_loss              | -0.159      |
|    policy_gradient_loss | -0.0125     |
|    reward_explained_... | -1.25       |
|    reward_value_loss    | 40.9        |
|    total_cost           | 1445.0      |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -97         |
|    mean_ep_length       | 186         |
|    mean_reward          | -122        |
|    true_cost            | 0.0741      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.713      |
|    ep_len_mean          | 136         |
|    ep_rew_mean          | -86         |
| time/                   |             |
|    fps                  | 561         |
|    iterations           | 5           |
|    time_elapsed         | 91          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.017798215 |
|    average_cost         | 0.121191405 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.59       |
|    cost_value_loss      | 12.4        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -1.1        |
|    learning_rate        | 0.0003      |
|    loss                 | 24.8        |
|    mean_cost_advantages | -0.3501096  |
|    mean_reward_advan... | -2.52214    |
|    n_updates            | 40          |
|    nu                   | 1.25        |
|    nu_loss              | -0.144      |
|    policy_gradient_loss | -0.0153     |
|    reward_explained_... | 0.0116      |
|    reward_value_loss    | 37.9        |
|    total_cost           | 1241.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -53.4       |
|    mean_ep_length       | 92.6        |
|    mean_reward          | -53.4       |
|    true_cost            | 0.114       |
| infos/                  |             |
|    cost                 | 0.1         |
| rollout/                |             |
|    adjusted_reward      | -0.72       |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 6           |
|    time_elapsed         | 110         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.016834486 |
|    average_cost         | 0.074121095 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.692      |
|    cost_value_loss      | 8.15        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -1.03       |
|    learning_rate        | 0.0003      |
|    loss                 | 25.3        |
|    mean_cost_advantages | -0.8947586  |
|    mean_reward_advan... | -1.6681379  |
|    n_updates            | 50          |
|    nu                   | 1.3         |
|    nu_loss              | -0.0924     |
|    policy_gradient_loss | -0.013      |
|    reward_explained_... | 0.0779      |
|    reward_value_loss    | 42.5        |
|    total_cost           | 759.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -53.4       |
|    mean_ep_length       | 142         |
|    mean_reward          | -93.5       |
|    true_cost            | 0.0689      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.669      |
|    ep_len_mean          | 80.9        |
|    ep_rew_mean          | -46.7       |
| time/                   |             |
|    fps                  | 567         |
|    iterations           | 7           |
|    time_elapsed         | 126         |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.015641537 |
|    average_cost         | 0.11416016  |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.958      |
|    cost_value_loss      | 15.2        |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.992      |
|    learning_rate        | 0.0003      |
|    loss                 | 27.3        |
|    mean_cost_advantages | -0.18788941 |
|    mean_reward_advan... | 0.5103726   |
|    n_updates            | 60          |
|    nu                   | 1.36        |
|    nu_loss              | -0.149      |
|    policy_gradient_loss | -0.00965    |
|    reward_explained_... | 0.604       |
|    reward_value_loss    | 43.1        |
|    total_cost           | 1169.0      |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -40.6       |
|    mean_ep_length       | 76.8        |
|    mean_reward          | -40.6       |
|    true_cost            | 0.0772      |
| infos/                  |             |
|    cost                 | 0.12        |
| rollout/                |             |
|    adjusted_reward      | -0.676      |
|    ep_len_mean          | 66.6        |
|    ep_rew_mean          | -36.9       |
| time/                   |             |
|    fps                  | 540         |
|    iterations           | 8           |
|    time_elapsed         | 151         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.017082263 |
|    average_cost         | 0.06894531  |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.066      |
|    cost_value_loss      | 7.43        |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.935      |
|    learning_rate        | 0.0003      |
|    loss                 | 25.6        |
|    mean_cost_advantages | -0.9597791  |
|    mean_reward_advan... | 1.6853082   |
|    n_updates            | 70          |
|    nu                   | 1.41        |
|    nu_loss              | -0.0935     |
|    policy_gradient_loss | -0.00591    |
|    reward_explained_... | 0.556       |
|    reward_value_loss    | 42.3        |
|    total_cost           | 706.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -19.6       |
|    mean_ep_length       | 38.2        |
|    mean_reward          | -19.6       |
|    true_cost            | 0.0603      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.641      |
|    ep_len_mean          | 55.9        |
|    ep_rew_mean          | -31.1       |
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 9           |
|    time_elapsed         | 168         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.023584658 |
|    average_cost         | 0.07724609  |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.912      |
|    cost_value_loss      | 7.49        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.843      |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    mean_cost_advantages | -0.5978317  |
|    mean_reward_advan... | 1.9667103   |
|    n_updates            | 80          |
|    nu                   | 1.46        |
|    nu_loss              | -0.109      |
|    policy_gradient_loss | -0.0161     |
|    reward_explained_... | 0.653       |
|    reward_value_loss    | 37.6        |
|    total_cost           | 791.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -19.6       |
|    mean_ep_length       | 65.2        |
|    mean_reward          | -40.1       |
|    true_cost            | 0.0439      |
| infos/                  |             |
|    cost                 | 0           |
| rollout/                |             |
|    adjusted_reward      | -0.603      |
|    ep_len_mean          | 37.3        |
|    ep_rew_mean          | -19.3       |
| time/                   |             |
|    fps                  | 538         |
|    iterations           | 10          |
|    time_elapsed         | 190         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.015231552 |
|    average_cost         | 0.060253907 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0612     |
|    cost_value_loss      | 4.8         |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.723      |
|    learning_rate        | 0.0003      |
|    loss                 | 19.5        |
|    mean_cost_advantages | -0.7936441  |
|    mean_reward_advan... | 2.3441682   |
|    n_updates            | 90          |
|    nu                   | 1.51        |
|    nu_loss              | -0.0882     |
|    policy_gradient_loss | -0.0079     |
|    reward_explained_... | 0.646       |
|    reward_value_loss    | 37.4        |
|    total_cost           | 617.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -16.8       |
|    mean_ep_length       | 31.6        |
|    mean_reward          | -16.8       |
|    true_cost            | 0.048       |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.618      |
|    ep_len_mean          | 33.6        |
|    ep_rew_mean          | -17.8       |
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 11          |
|    time_elapsed         | 205         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.016402056 |
|    average_cost         | 0.043945312 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.211       |
|    cost_value_loss      | 1.7         |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.684      |
|    learning_rate        | 0.0003      |
|    loss                 | 17.3        |
|    mean_cost_advantages | -0.8619936  |
|    mean_reward_advan... | 2.3957524   |
|    n_updates            | 100         |
|    nu                   | 1.56        |
|    nu_loss              | -0.0666     |
|    policy_gradient_loss | -0.00752    |
|    reward_explained_... | 0.746       |
|    reward_value_loss    | 24.3        |
|    total_cost           | 450.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -16.8       |
|    mean_ep_length       | 66.8        |
|    mean_reward          | -35.3       |
|    true_cost            | 0.0339      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.552      |
|    ep_len_mean          | 33.7        |
|    ep_rew_mean          | -17.2       |
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 12          |
|    time_elapsed         | 221         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.026467618 |
|    average_cost         | 0.048046876 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0667     |
|    cost_value_loss      | 1.26        |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.611      |
|    learning_rate        | 0.0003      |
|    loss                 | 37.3        |
|    mean_cost_advantages | -0.40863428 |
|    mean_reward_advan... | 1.1711282   |
|    n_updates            | 110         |
|    nu                   | 1.61        |
|    nu_loss              | -0.0752     |
|    policy_gradient_loss | -0.0112     |
|    reward_explained_... | 0.743       |
|    reward_value_loss    | 26.1        |
|    total_cost           | 492.0       |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -13.9       |
|    mean_ep_length       | 30          |
|    mean_reward          | -13.9       |
|    true_cost            | 0.0341      |
| infos/                  |             |
|    cost                 | 0.01        |
| rollout/                |             |
|    adjusted_reward      | -0.544      |
|    ep_len_mean          | 31.8        |
|    ep_rew_mean          | -15.4       |
| time/                   |             |
|    fps                  | 513         |
|    iterations           | 13          |
|    time_elapsed         | 259         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.019419994 |
|    average_cost         | 0.03388672  |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.407       |
|    cost_value_loss      | 0.417       |
|    early_stop_epoch     | 6           |
|    entropy_loss         | -0.571      |
|    learning_rate        | 0.0003      |
|    loss                 | 5.62        |
|    mean_cost_advantages | -0.3182558  |
|    mean_reward_advan... | 0.35032535  |
|    n_updates            | 120         |
|    nu                   | 1.66        |
|    nu_loss              | -0.0547     |
|    policy_gradient_loss | -0.00466    |
|    reward_explained_... | 0.834       |
|    reward_value_loss    | 14.2        |
|    total_cost           | 347.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -12.7       |
|    mean_ep_length       | 28.2        |
|    mean_reward          | -12.7       |
|    true_cost            | 0.0315      |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.554      |
|    ep_len_mean          | 28.1        |
|    ep_rew_mean          | -14.1       |
| time/                   |             |
|    fps                  | 517         |
|    iterations           | 14          |
|    time_elapsed         | 277         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.017415557 |
|    average_cost         | 0.034082033 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.303       |
|    cost_value_loss      | 0.272       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.519      |
|    learning_rate        | 0.0003      |
|    loss                 | 4.03        |
|    mean_cost_advantages | -0.16932991 |
|    mean_reward_advan... | 0.6689205   |
|    n_updates            | 130         |
|    nu                   | 1.7         |
|    nu_loss              | -0.0566     |
|    policy_gradient_loss | -0.00415    |
|    reward_explained_... | 0.854       |
|    reward_value_loss    | 3.96        |
|    total_cost           | 349.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -12.7        |
|    mean_ep_length       | 26.4         |
|    mean_reward          | -13.6        |
|    true_cost            | 0.034        |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.583       |
|    ep_len_mean          | 25.2         |
|    ep_rew_mean          | -13.1        |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 15           |
|    time_elapsed         | 290          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.024140853  |
|    average_cost         | 0.031542968  |
|    clip_fraction        | 0.191        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.211        |
|    cost_value_loss      | 0.194        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.435       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.502        |
|    mean_cost_advantages | -0.090179965 |
|    mean_reward_advan... | 0.2822081    |
|    n_updates            | 140          |
|    nu                   | 1.75         |
|    nu_loss              | -0.0538      |
|    policy_gradient_loss | -0.00618     |
|    reward_explained_... | 0.935        |
|    reward_value_loss    | 1.01         |
|    total_cost           | 323.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -12.2       |
|    mean_ep_length       | 22.6        |
|    mean_reward          | -12.2       |
|    true_cost            | 0.0349      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.597      |
|    ep_len_mean          | 23.2        |
|    ep_rew_mean          | -12.3       |
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 16          |
|    time_elapsed         | 308         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.022215499 |
|    average_cost         | 0.033984374 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0705     |
|    cost_value_loss      | 0.184       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.349      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.638       |
|    mean_cost_advantages | -0.06555138 |
|    mean_reward_advan... | 0.45536232  |
|    n_updates            | 150         |
|    nu                   | 1.79        |
|    nu_loss              | -0.0594     |
|    policy_gradient_loss | -0.00365    |
|    reward_explained_... | 0.945       |
|    reward_value_loss    | 0.838       |
|    total_cost           | 348.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -11.7         |
|    mean_ep_length       | 21.8          |
|    mean_reward          | -11.7         |
|    true_cost            | 0.0369        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | -0.6          |
|    ep_len_mean          | 22.4          |
|    ep_rew_mean          | -11.9         |
| time/                   |               |
|    fps                  | 515           |
|    iterations           | 17            |
|    time_elapsed         | 337           |
|    total_timesteps      | 174080        |
| train/                  |               |
|    approx_kl            | 0.017487157   |
|    average_cost         | 0.034863282   |
|    clip_fraction        | 0.11          |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0647       |
|    cost_value_loss      | 0.13          |
|    early_stop_epoch     | 4             |
|    entropy_loss         | -0.253        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.146         |
|    mean_cost_advantages | -0.0019961358 |
|    mean_reward_advan... | 0.42619166    |
|    n_updates            | 160           |
|    nu                   | 1.83          |
|    nu_loss              | -0.0624       |
|    policy_gradient_loss | -0.00407      |
|    reward_explained_... | 0.97          |
|    reward_value_loss    | 0.361         |
|    total_cost           | 357.0         |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -11.7         |
|    mean_ep_length       | 22.4          |
|    mean_reward          | -11.9         |
|    true_cost            | 0.0361        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | -0.596        |
|    ep_len_mean          | 22.1          |
|    ep_rew_mean          | -11.7         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 18            |
|    time_elapsed         | 383           |
|    total_timesteps      | 184320        |
| train/                  |               |
|    approx_kl            | 0.012664074   |
|    average_cost         | 0.03691406    |
|    clip_fraction        | 0.0975        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0595       |
|    cost_value_loss      | 0.136         |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.203        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.259         |
|    mean_cost_advantages | 0.00092505215 |
|    mean_reward_advan... | 0.25983307    |
|    n_updates            | 170           |
|    nu                   | 1.87          |
|    nu_loss              | -0.0676       |
|    policy_gradient_loss | -0.00363      |
|    reward_explained_... | 0.98          |
|    reward_value_loss    | 0.234         |
|    total_cost           | 378.0         |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -11.7        |
|    mean_ep_length       | 22.2         |
|    mean_reward          | -11.7        |
|    true_cost            | 0.0393       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | -0.595       |
|    ep_len_mean          | 21.7         |
|    ep_rew_mean          | -11.2        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 19           |
|    time_elapsed         | 418          |
|    total_timesteps      | 194560       |
| train/                  |              |
|    approx_kl            | 0.015327779  |
|    average_cost         | 0.036132812  |
|    clip_fraction        | 0.0967       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0476       |
|    cost_value_loss      | 0.12         |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.204       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.147        |
|    mean_cost_advantages | -0.004465183 |
|    mean_reward_advan... | 0.1411943    |
|    n_updates            | 180          |
|    nu                   | 1.91         |
|    nu_loss              | -0.0676      |
|    policy_gradient_loss | -0.00382     |
|    reward_explained_... | 0.985        |
|    reward_value_loss    | 0.18         |
|    total_cost           | 370.0        |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -11.1       |
|    mean_ep_length       | 21.2        |
|    mean_reward          | -11.1       |
|    true_cost            | 0.0411      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.601      |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | -11         |
| time/                   |             |
|    fps                  | 461         |
|    iterations           | 20          |
|    time_elapsed         | 443         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.018251788 |
|    average_cost         | 0.039257813 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.199      |
|    cost_value_loss      | 0.149       |
|    early_stop_epoch     | 3           |
|    entropy_loss         | -0.207      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0824      |
|    mean_cost_advantages | 0.029461632 |
|    mean_reward_advan... | 0.16982639  |
|    n_updates            | 190         |
|    nu                   | 1.95        |
|    nu_loss              | -0.075      |
|    policy_gradient_loss | -0.00526    |
|    reward_explained_... | 0.988       |
|    reward_value_loss    | 0.137       |
|    total_cost           | 402.0       |
-----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -10.6       |
|    mean_ep_length       | 20.2        |
|    mean_reward          | -10.6       |
|    true_cost            | 0.0448      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.607      |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | -10.7       |
| time/                   |             |
|    fps                  | 430         |
|    iterations           | 21          |
|    time_elapsed         | 499         |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.015403025 |
|    average_cost         | 0.04111328  |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.226      |
|    cost_value_loss      | 0.147       |
|    early_stop_epoch     | 9           |
|    entropy_loss         | -0.219      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0843      |
|    mean_cost_advantages | 0.011751796 |
|    mean_reward_advan... | 0.13879295  |
|    n_updates            | 200         |
|    nu                   | 1.99        |
|    nu_loss              | -0.0802     |
|    policy_gradient_loss | -0.00507    |
|    reward_explained_... | 0.987       |
|    reward_value_loss    | 0.14        |
|    total_cost           | 421.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -10.6       |
|    mean_ep_length       | 21.8        |
|    mean_reward          | -11.2       |
|    true_cost            | 0.0444      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.607      |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | -10.5       |
| time/                   |             |
|    fps                  | 436         |
|    iterations           | 22          |
|    time_elapsed         | 515         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.015632171 |
|    average_cost         | 0.04482422  |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0409      |
|    cost_value_loss      | 0.143       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.234      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0943      |
|    mean_cost_advantages | 0.048325576 |
|    mean_reward_advan... | 0.1304689   |
|    n_updates            | 210         |
|    nu                   | 2.03        |
|    nu_loss              | -0.0893     |
|    policy_gradient_loss | -0.00474    |
|    reward_explained_... | 0.991       |
|    reward_value_loss    | 0.0959      |
|    total_cost           | 459.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -9.23       |
|    mean_ep_length       | 18          |
|    mean_reward          | -9.23       |
|    true_cost            | 0.0468      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.606      |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | -9.43       |
| time/                   |             |
|    fps                  | 442         |
|    iterations           | 23          |
|    time_elapsed         | 532         |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.04551912  |
|    average_cost         | 0.044433594 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.185       |
|    cost_value_loss      | 0.13        |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.278      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    mean_cost_advantages | 0.020613655 |
|    mean_reward_advan... | 0.08214341  |
|    n_updates            | 220         |
|    nu                   | 2.07        |
|    nu_loss              | -0.0903     |
|    policy_gradient_loss | -0.00664    |
|    reward_explained_... | 0.991       |
|    reward_value_loss    | 0.0917      |
|    total_cost           | 455.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -8.52       |
|    mean_ep_length       | 17.8        |
|    mean_reward          | -8.52       |
|    true_cost            | 0.0426      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.589      |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | -8.44       |
| time/                   |             |
|    fps                  | 448         |
|    iterations           | 24          |
|    time_elapsed         | 548         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.022168618 |
|    average_cost         | 0.046777345 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.129       |
|    cost_value_loss      | 0.136       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.383      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    mean_cost_advantages | 0.02033265  |
|    mean_reward_advan... | 0.32699728  |
|    n_updates            | 230         |
|    nu                   | 2.12        |
|    nu_loss              | -0.097      |
|    policy_gradient_loss | -0.011      |
|    reward_explained_... | 0.974       |
|    reward_value_loss    | 0.184       |
|    total_cost           | 479.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -6.37         |
|    mean_ep_length       | 13.6          |
|    mean_reward          | -6.37         |
|    true_cost            | 0.0317        |
| infos/                  |               |
|    cost                 | 0.02          |
| rollout/                |               |
|    adjusted_reward      | -0.558        |
|    ep_len_mean          | 15            |
|    ep_rew_mean          | -7.26         |
| time/                   |               |
|    fps                  | 456           |
|    iterations           | 25            |
|    time_elapsed         | 560           |
|    total_timesteps      | 256000        |
| train/                  |               |
|    approx_kl            | 0.047332983   |
|    average_cost         | 0.042578124   |
|    clip_fraction        | 0.347         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.18          |
|    cost_value_loss      | 0.121         |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.406        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.122         |
|    mean_cost_advantages | -0.0102755185 |
|    mean_reward_advan... | 0.27257112    |
|    n_updates            | 240           |
|    nu                   | 2.16          |
|    nu_loss              | -0.0901       |
|    policy_gradient_loss | -0.0168       |
|    reward_explained_... | 0.963         |
|    reward_value_loss    | 0.239         |
|    total_cost           | 436.0         |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.37        |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -7.45        |
|    true_cost            | 0.0196       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | -0.522       |
|    ep_len_mean          | 14.3         |
|    ep_rew_mean          | -6.94        |
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 26           |
|    time_elapsed         | 573          |
|    total_timesteps      | 266240       |
| train/                  |              |
|    approx_kl            | 0.026500309  |
|    average_cost         | 0.03173828   |
|    clip_fraction        | 0.158        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.21        |
|    cost_value_loss      | 0.141        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.391       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0976       |
|    mean_cost_advantages | -0.034486108 |
|    mean_reward_advan... | 0.33561623   |
|    n_updates            | 250          |
|    nu                   | 2.2          |
|    nu_loss              | -0.0685      |
|    policy_gradient_loss | -0.0114      |
|    reward_explained_... | 0.95         |
|    reward_value_loss    | 0.263        |
|    total_cost           | 325.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -6.37        |
|    mean_ep_length       | 13.2         |
|    mean_reward          | -6.37        |
|    true_cost            | 0.0144       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.512       |
|    ep_len_mean          | 13.8         |
|    ep_rew_mean          | -6.65        |
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 27           |
|    time_elapsed         | 590          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.019787464  |
|    average_cost         | 0.019628907  |
|    clip_fraction        | 0.0965       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.403       |
|    cost_value_loss      | 0.0953       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.318       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.187        |
|    mean_cost_advantages | -0.059019215 |
|    mean_reward_advan... | 0.17272082   |
|    n_updates            | 260          |
|    nu                   | 2.24         |
|    nu_loss              | -0.0432      |
|    policy_gradient_loss | -0.00655     |
|    reward_explained_... | 0.951        |
|    reward_value_loss    | 0.242        |
|    total_cost           | 201.0        |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.97        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | -5.97        |
|    true_cost            | 0.0144       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | -0.511       |
|    ep_len_mean          | 13.6         |
|    ep_rew_mean          | -6.49        |
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 28           |
|    time_elapsed         | 618          |
|    total_timesteps      | 286720       |
| train/                  |              |
|    approx_kl            | 0.015719015  |
|    average_cost         | 0.014355469  |
|    clip_fraction        | 0.0923       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.12        |
|    cost_value_loss      | 0.0579       |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.262       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0856       |
|    mean_cost_advantages | -0.046500187 |
|    mean_reward_advan... | 0.05200255   |
|    n_updates            | 270          |
|    nu                   | 2.28         |
|    nu_loss              | -0.0321      |
|    policy_gradient_loss | -0.00386     |
|    reward_explained_... | 0.948        |
|    reward_value_loss    | 0.237        |
|    total_cost           | 147.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.97       |
|    mean_ep_length       | 12.8        |
|    mean_reward          | -6.1        |
|    true_cost            | 0.016       |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.509      |
|    ep_len_mean          | 13.2        |
|    ep_rew_mean          | -6.2        |
| time/                   |             |
|    fps                  | 444         |
|    iterations           | 29          |
|    time_elapsed         | 667         |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.004527609 |
|    average_cost         | 0.014355469 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.46       |
|    cost_value_loss      | 0.0545      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.241      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.083       |
|    mean_cost_advantages | 0.008503494 |
|    mean_reward_advan... | 0.008527853 |
|    n_updates            | 280         |
|    nu                   | 2.31        |
|    nu_loss              | -0.0327     |
|    policy_gradient_loss | -0.0028     |
|    reward_explained_... | 0.949       |
|    reward_value_loss    | 0.222       |
|    total_cost           | 147.0       |
-----------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -5.97         |
|    mean_ep_length       | 14            |
|    mean_reward          | -7            |
|    true_cost            | 0.0164        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | -0.511        |
|    ep_len_mean          | 12.8          |
|    ep_rew_mean          | -5.99         |
| time/                   |               |
|    fps                  | 431           |
|    iterations           | 30            |
|    time_elapsed         | 712           |
|    total_timesteps      | 307200        |
| train/                  |               |
|    approx_kl            | 0.013305692   |
|    average_cost         | 0.016015625   |
|    clip_fraction        | 0.0867        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -1.96         |
|    cost_value_loss      | 0.0499        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.204        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.101         |
|    mean_cost_advantages | -0.0021372195 |
|    mean_reward_advan... | 0.049939774   |
|    n_updates            | 290           |
|    nu                   | 2.35          |
|    nu_loss              | -0.0371       |
|    policy_gradient_loss | -0.00452      |
|    reward_explained_... | 0.959         |
|    reward_value_loss    | 0.169         |
|    total_cost           | 164.0         |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.82       |
|    mean_ep_length       | 12.4        |
|    mean_reward          | -5.82       |
|    true_cost            | 0.018       |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.511      |
|    ep_len_mean          | 12.7        |
|    ep_rew_mean          | -5.99       |
| time/                   |             |
|    fps                  | 423         |
|    iterations           | 31          |
|    time_elapsed         | 749         |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.017222112 |
|    average_cost         | 0.01640625  |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.19       |
|    cost_value_loss      | 0.0445      |
|    early_stop_epoch     | 7           |
|    entropy_loss         | -0.19       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.102       |
|    mean_cost_advantages | 0.005811546 |
|    mean_reward_advan... | 0.07135971  |
|    n_updates            | 300         |
|    nu                   | 2.38        |
|    nu_loss              | -0.0385     |
|    policy_gradient_loss | -0.00535    |
|    reward_explained_... | 0.944       |
|    reward_value_loss    | 0.213       |
|    total_cost           | 168.0       |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.38       |
|    mean_ep_length       | 12          |
|    mean_reward          | -5.38       |
|    true_cost            | 0.0443      |
| infos/                  |             |
|    cost                 | 0.15        |
| rollout/                |             |
|    adjusted_reward      | -0.555      |
|    ep_len_mean          | 12.7        |
|    ep_rew_mean          | -5.7        |
| time/                   |             |
|    fps                  | 420         |
|    iterations           | 32          |
|    time_elapsed         | 779         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.01662079  |
|    average_cost         | 0.01796875  |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -1.22       |
|    cost_value_loss      | 0.045       |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -0.165      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.291       |
|    mean_cost_advantages | -0.01612645 |
|    mean_reward_advan... | 0.03469991  |
|    n_updates            | 310         |
|    nu                   | 2.42        |
|    nu_loss              | -0.0428     |
|    policy_gradient_loss | -0.00494    |
|    reward_explained_... | 0.94        |
|    reward_value_loss    | 0.217       |
|    total_cost           | 184.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.53
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -5.38      |
|    mean_ep_length       | 12         |
|    mean_reward          | -5.45      |
|    true_cost            | 0.082      |
| infos/                  |            |
|    cost                 | 0.07       |
| rollout/                |            |
|    adjusted_reward      | -0.641     |
|    ep_len_mean          | 12.2       |
|    ep_rew_mean          | -5.38      |
| time/                   |            |
|    fps                  | 426        |
|    iterations           | 33         |
|    time_elapsed         | 792        |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.52570486 |
|    average_cost         | 0.04433594 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -7.9       |
|    cost_value_loss      | 0.754      |
|    early_stop_epoch     | 0          |
|    entropy_loss         | -0.187     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.394      |
|    mean_cost_advantages | 0.21356864 |
|    mean_reward_advan... | 0.01953735 |
|    n_updates            | 320        |
|    nu                   | 2.45       |
|    nu_loss              | -0.107     |
|    policy_gradient_loss | 0.0487     |
|    reward_explained_... | 0.959      |
|    reward_value_loss    | 0.123      |
|    total_cost           | 454.0      |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.21
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.1         |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.1         |
|    true_cost            | 0.0294       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.519       |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | -5.17        |
| time/                   |              |
|    fps                  | 431          |
|    iterations           | 34           |
|    time_elapsed         | 806          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.21472163   |
|    average_cost         | 0.08203125   |
|    clip_fraction        | 0.154        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.09        |
|    cost_value_loss      | 0.706        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0541      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.402        |
|    mean_cost_advantages | 0.24879758   |
|    mean_reward_advan... | -0.029498374 |
|    n_updates            | 330          |
|    nu                   | 2.49         |
|    nu_loss              | -0.201       |
|    policy_gradient_loss | -0.0339      |
|    reward_explained_... | 0.96         |
|    reward_value_loss    | 0.112        |
|    total_cost           | 840.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.1        |
|    mean_ep_length       | 11.8        |
|    mean_reward          | -5.27       |
|    true_cost            | 0.0275      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.517      |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | -5.25       |
| time/                   |             |
|    fps                  | 435         |
|    iterations           | 35          |
|    time_elapsed         | 823         |
|    total_timesteps      | 358400      |
| train/                  |             |
|    approx_kl            | 0.082838304 |
|    average_cost         | 0.029394532 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.188       |
|    cost_value_loss      | 0.0614      |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.123      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0346      |
|    mean_cost_advantages | -0.39322102 |
|    mean_reward_advan... | 0.075777814 |
|    n_updates            | 340         |
|    nu                   | 2.53        |
|    nu_loss              | -0.0732     |
|    policy_gradient_loss | -0.00122    |
|    reward_explained_... | 0.967       |
|    reward_value_loss    | 0.0997      |
|    total_cost           | 301.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.1         |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -5.23        |
|    true_cost            | 0.0292       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.522       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -5.13        |
| time/                   |              |
|    fps                  | 427          |
|    iterations           | 36           |
|    time_elapsed         | 863          |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.0050478824 |
|    average_cost         | 0.027539063  |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.188        |
|    cost_value_loss      | 0.0484       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0979      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.026        |
|    mean_cost_advantages | -0.09162797  |
|    mean_reward_advan... | 0.018668324  |
|    n_updates            | 350          |
|    nu                   | 2.57         |
|    nu_loss              | -0.0697      |
|    policy_gradient_loss | -0.000895    |
|    reward_explained_... | 0.975        |
|    reward_value_loss    | 0.077        |
|    total_cost           | 282.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.1         |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.13        |
|    true_cost            | 0.0315       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.526       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -5.11        |
| time/                   |              |
|    fps                  | 417          |
|    iterations           | 37           |
|    time_elapsed         | 907          |
|    total_timesteps      | 378880       |
| train/                  |              |
|    approx_kl            | 0.00501828   |
|    average_cost         | 0.029199218  |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.391       |
|    cost_value_loss      | 0.0702       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0927      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0449       |
|    mean_cost_advantages | 0.0053234934 |
|    mean_reward_advan... | -0.029925948 |
|    n_updates            | 360          |
|    nu                   | 2.61         |
|    nu_loss              | -0.0751      |
|    policy_gradient_loss | -0.00129     |
|    reward_explained_... | 0.972        |
|    reward_value_loss    | 0.0816       |
|    total_cost           | 299.0        |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.1         |
|    mean_ep_length       | 12.4         |
|    mean_reward          | -5.47        |
|    true_cost            | 0.0299       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.524       |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | -5.44        |
| time/                   |              |
|    fps                  | 414          |
|    iterations           | 38           |
|    time_elapsed         | 939          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.058262695  |
|    average_cost         | 0.031542968  |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.344       |
|    cost_value_loss      | 0.0664       |
|    early_stop_epoch     | 6            |
|    entropy_loss         | -0.0921      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.109        |
|    mean_cost_advantages | -0.015785435 |
|    mean_reward_advan... | 0.005391789  |
|    n_updates            | 370          |
|    nu                   | 2.65         |
|    nu_loss              | -0.0823      |
|    policy_gradient_loss | -0.00129     |
|    reward_explained_... | 0.984        |
|    reward_value_loss    | 0.0448       |
|    total_cost           | 323.0        |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -5.1        |
|    mean_ep_length       | 13          |
|    mean_reward          | -6.17       |
|    true_cost            | 0.03        |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | -0.548      |
|    ep_len_mean          | 12.4        |
|    ep_rew_mean          | -5.75       |
| time/                   |             |
|    fps                  | 409         |
|    iterations           | 39          |
|    time_elapsed         | 975         |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.06289841  |
|    average_cost         | 0.029882813 |
|    clip_fraction        | 0.0592      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.272      |
|    cost_value_loss      | 0.0541      |
|    early_stop_epoch     | 4           |
|    entropy_loss         | -0.153      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0608      |
|    mean_cost_advantages | 0.011228958 |
|    mean_reward_advan... | -0.0869569  |
|    n_updates            | 380         |
|    nu                   | 2.69        |
|    nu_loss              | -0.0792     |
|    policy_gradient_loss | -0.00327    |
|    reward_explained_... | 0.96        |
|    reward_value_loss    | 0.0816      |
|    total_cost           | 306.0       |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.1         |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -5.63        |
|    true_cost            | 0.0313       |
| infos/                  |              |
|    cost                 | 0.01         |
| rollout/                |              |
|    adjusted_reward      | -0.547       |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | -5.59        |
| time/                   |              |
|    fps                  | 408          |
|    iterations           | 40           |
|    time_elapsed         | 1002         |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.016399145  |
|    average_cost         | 0.02998047   |
|    clip_fraction        | 0.0653       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.687       |
|    cost_value_loss      | 0.0608       |
|    early_stop_epoch     | 4            |
|    entropy_loss         | -0.17        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0836       |
|    mean_cost_advantages | 0.04290837   |
|    mean_reward_advan... | 0.0030686709 |
|    n_updates            | 390          |
|    nu                   | 2.73         |
|    nu_loss              | -0.0807      |
|    policy_gradient_loss | -0.00437     |
|    reward_explained_... | 0.956        |
|    reward_value_loss    | 0.118        |
|    total_cost           | 307.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -5.1         |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.12        |
|    true_cost            | 0.0386       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.559       |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | -5.43        |
| time/                   |              |
|    fps                  | 397          |
|    iterations           | 41           |
|    time_elapsed         | 1056         |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.011289652  |
|    average_cost         | 0.031347655  |
|    clip_fraction        | 0.0741       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.472       |
|    cost_value_loss      | 0.059        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.136       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0646       |
|    mean_cost_advantages | 0.0142887905 |
|    mean_reward_advan... | 0.059468962  |
|    n_updates            | 400          |
|    nu                   | 2.77         |
|    nu_loss              | -0.0856      |
|    policy_gradient_loss | -0.00703     |
|    reward_explained_... | 0.973        |
|    reward_value_loss    | 0.0891       |
|    total_cost           | 321.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.95       |
|    mean_ep_length       | 11.4        |
|    mean_reward          | -4.95       |
|    true_cost            | 0.0424      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.559      |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | -5.04       |
| time/                   |             |
|    fps                  | 401         |
|    iterations           | 42          |
|    time_elapsed         | 1070        |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.018483873 |
|    average_cost         | 0.03857422  |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0334     |
|    cost_value_loss      | 0.0607      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.101      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0505      |
|    mean_cost_advantages | 0.00157702  |
|    mean_reward_advan... | 0.054016013 |
|    n_updates            | 410         |
|    nu                   | 2.81        |
|    nu_loss              | -0.107      |
|    policy_gradient_loss | -0.00787    |
|    reward_explained_... | 0.979       |
|    reward_value_loss    | 0.0609      |
|    total_cost           | 395.0       |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.95       |
|    mean_ep_length       | 12          |
|    mean_reward          | -5.18       |
|    true_cost            | 0.0634      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.616      |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | -5.13       |
| time/                   |             |
|    fps                  | 401         |
|    iterations           | 43          |
|    time_elapsed         | 1096        |
|    total_timesteps      | 440320      |
| train/                  |             |
|    approx_kl            | 0.017087644 |
|    average_cost         | 0.042382814 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.155       |
|    cost_value_loss      | 0.0585      |
|    early_stop_epoch     | 2           |
|    entropy_loss         | -0.079      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0547      |
|    mean_cost_advantages | 0.005672635 |
|    mean_reward_advan... | 0.033923246 |
|    n_updates            | 420         |
|    nu                   | 2.86        |
|    nu_loss              | -0.119      |
|    policy_gradient_loss | -0.00217    |
|    reward_explained_... | 0.991       |
|    reward_value_loss    | 0.0166      |
|    total_cost           | 434.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.95        |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -5.22        |
|    true_cost            | 0.0354       |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | -0.489       |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | -5.26        |
| time/                   |              |
|    fps                  | 406          |
|    iterations           | 44           |
|    time_elapsed         | 1109         |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.16136263   |
|    average_cost         | 0.06337891   |
|    clip_fraction        | 0.0933       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -2.2         |
|    cost_value_loss      | 0.435        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0287      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.2          |
|    mean_cost_advantages | 0.1870867    |
|    mean_reward_advan... | 0.0017539766 |
|    n_updates            | 430          |
|    nu                   | 2.9          |
|    nu_loss              | -0.181       |
|    policy_gradient_loss | -0.0178      |
|    reward_explained_... | 0.993        |
|    reward_value_loss    | 0.0134       |
|    total_cost           | 649.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 1.00
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.95       |
|    mean_ep_length       | 11.4        |
|    mean_reward          | -4.95       |
|    true_cost            | 0.0417      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.563      |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | -5.06       |
| time/                   |             |
|    fps                  | 409         |
|    iterations           | 45          |
|    time_elapsed         | 1124        |
|    total_timesteps      | 460800      |
| train/                  |             |
|    approx_kl            | 1.0011075   |
|    average_cost         | 0.035449218 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.0697     |
|    cost_value_loss      | 0.0524      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0824     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0325      |
|    mean_cost_advantages | -0.14417061 |
|    mean_reward_advan... | -0.11699022 |
|    n_updates            | 440         |
|    nu                   | 2.95        |
|    nu_loss              | -0.103      |
|    policy_gradient_loss | -0.0135     |
|    reward_explained_... | 0.987       |
|    reward_value_loss    | 0.0286      |
|    total_cost           | 363.0       |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.10
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.95       |
|    mean_ep_length       | 13          |
|    mean_reward          | -6.35       |
|    true_cost            | 0.0399      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.6        |
|    ep_len_mean          | 12.7        |
|    ep_rew_mean          | -6.08       |
| time/                   |             |
|    fps                  | 407         |
|    iterations           | 46          |
|    time_elapsed         | 1156        |
|    total_timesteps      | 471040      |
| train/                  |             |
|    approx_kl            | 0.09545597  |
|    average_cost         | 0.04169922  |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.327       |
|    cost_value_loss      | 0.0582      |
|    early_stop_epoch     | 5           |
|    entropy_loss         | -0.0655     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00628    |
|    mean_cost_advantages | -0.07976695 |
|    mean_reward_advan... | 0.06873366  |
|    n_updates            | 450         |
|    nu                   | 3           |
|    nu_loss              | -0.123      |
|    policy_gradient_loss | -0.000477   |
|    reward_explained_... | 0.996       |
|    reward_value_loss    | 0.00433     |
|    total_cost           | 427.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.95        |
|    mean_ep_length       | 12.6         |
|    mean_reward          | -5.97        |
|    true_cost            | 0.0421       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.597       |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | -5.83        |
| time/                   |              |
|    fps                  | 408          |
|    iterations           | 47           |
|    time_elapsed         | 1176         |
|    total_timesteps      | 481280       |
| train/                  |              |
|    approx_kl            | 0.015574882  |
|    average_cost         | 0.039941408  |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0651       |
|    cost_value_loss      | 0.0653       |
|    early_stop_epoch     | 1            |
|    entropy_loss         | -0.131       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0211       |
|    mean_cost_advantages | 0.0028952886 |
|    mean_reward_advan... | -0.085702285 |
|    n_updates            | 460          |
|    nu                   | 3.05         |
|    nu_loss              | -0.12        |
|    policy_gradient_loss | -0.00183     |
|    reward_explained_... | 0.95         |
|    reward_value_loss    | 0.0419       |
|    total_cost           | 409.0        |
------------------------------------------
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.95       |
|    mean_ep_length       | 12.2        |
|    mean_reward          | -5.65       |
|    true_cost            | 0.0413      |
| infos/                  |             |
|    cost                 | 0.03        |
| rollout/                |             |
|    adjusted_reward      | -0.593      |
|    ep_len_mean          | 12.2        |
|    ep_rew_mean          | -5.66       |
| time/                   |             |
|    fps                  | 400         |
|    iterations           | 48          |
|    time_elapsed         | 1227        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.01238101  |
|    average_cost         | 0.042089842 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.262       |
|    cost_value_loss      | 0.0606      |
|    early_stop_epoch     | 10          |
|    entropy_loss         | -0.137      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0346      |
|    mean_cost_advantages | 0.003132163 |
|    mean_reward_advan... | 0.06923518  |
|    n_updates            | 470         |
|    nu                   | 3.1         |
|    nu_loss              | -0.128      |
|    policy_gradient_loss | -0.00454    |
|    reward_explained_... | 0.988       |
|    reward_value_loss    | 0.0277      |
|    total_cost           | 431.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.95       |
|    mean_ep_length       | 11.4        |
|    mean_reward          | -4.95       |
|    true_cost            | 0.043       |
| infos/                  |             |
|    cost                 | 0.02        |
| rollout/                |             |
|    adjusted_reward      | -0.579      |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | -5.22       |
| time/                   |             |
|    fps                  | 403         |
|    iterations           | 49          |
|    time_elapsed         | 1244        |
|    total_timesteps      | 501760      |
| train/                  |             |
|    approx_kl            | 0.040746897 |
|    average_cost         | 0.041308593 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0502      |
|    cost_value_loss      | 0.0755      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.115      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0513      |
|    mean_cost_advantages | 0.011840826 |
|    mean_reward_advan... | 0.056220543 |
|    n_updates            | 480         |
|    nu                   | 3.15        |
|    nu_loss              | -0.128      |
|    policy_gradient_loss | -0.00735    |
|    reward_explained_... | 0.988       |
|    reward_value_loss    | 0.0374      |
|    total_cost           | 423.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11           |
|    mean_reward          | -4.58        |
|    true_cost            | 0.0445       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.582       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -5.12        |
| time/                   |              |
|    fps                  | 407          |
|    iterations           | 50           |
|    time_elapsed         | 1257         |
|    total_timesteps      | 512000       |
| train/                  |              |
|    approx_kl            | 0.017307306  |
|    average_cost         | 0.04296875   |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0502       |
|    cost_value_loss      | 0.0797       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0782      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0251       |
|    mean_cost_advantages | -0.009084093 |
|    mean_reward_advan... | 0.08345617   |
|    n_updates            | 490          |
|    nu                   | 3.21         |
|    nu_loss              | -0.136       |
|    policy_gradient_loss | -0.00368     |
|    reward_explained_... | 0.98         |
|    reward_value_loss    | 0.0191       |
|    total_cost           | 440.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -5.32         |
|    true_cost            | 0.0442        |
| infos/                  |               |
|    cost                 | 0.05          |
| rollout/                |               |
|    adjusted_reward      | -0.583        |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | -5            |
| time/                   |               |
|    fps                  | 400           |
|    iterations           | 51            |
|    time_elapsed         | 1304          |
|    total_timesteps      | 522240        |
| train/                  |               |
|    approx_kl            | 0.006347193   |
|    average_cost         | 0.04453125    |
|    clip_fraction        | 0.0342        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.151        |
|    cost_value_loss      | 0.0993        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0667       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0869        |
|    mean_cost_advantages | 0.013597943   |
|    mean_reward_advan... | -0.0051065846 |
|    n_updates            | 500           |
|    nu                   | 3.26          |
|    nu_loss              | -0.143        |
|    policy_gradient_loss | -0.00171      |
|    reward_explained_... | 0.998         |
|    reward_value_loss    | 0.0035        |
|    total_cost           | 456.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -4.77        |
|    true_cost            | 0.044        |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.584       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -5.1         |
| time/                   |              |
|    fps                  | 394          |
|    iterations           | 52           |
|    time_elapsed         | 1348         |
|    total_timesteps      | 532480       |
| train/                  |              |
|    approx_kl            | 0.010152737  |
|    average_cost         | 0.04423828   |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.208        |
|    cost_value_loss      | 0.0619       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0528      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0332       |
|    mean_cost_advantages | 0.0018591869 |
|    mean_reward_advan... | 0.01499899   |
|    n_updates            | 510          |
|    nu                   | 3.32         |
|    nu_loss              | -0.144       |
|    policy_gradient_loss | -0.000323    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00243      |
|    total_cost           | 453.0        |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.25
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 13.8         |
|    mean_reward          | -5.62        |
|    true_cost            | 0.255        |
| infos/                  |              |
|    cost                 | 0.23         |
| rollout/                |              |
|    adjusted_reward      | -1.26        |
|    ep_len_mean          | 13.5         |
|    ep_rew_mean          | -5.38        |
| time/                   |              |
|    fps                  | 392          |
|    iterations           | 53           |
|    time_elapsed         | 1383         |
|    total_timesteps      | 542720       |
| train/                  |              |
|    approx_kl            | 0.25344032   |
|    average_cost         | 0.044042967  |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.287        |
|    cost_value_loss      | 0.0573       |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.0541      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0175       |
|    mean_cost_advantages | 0.0128776925 |
|    mean_reward_advan... | 0.013533235  |
|    n_updates            | 520          |
|    nu                   | 3.37         |
|    nu_loss              | -0.146       |
|    policy_gradient_loss | -0.000999    |
|    reward_explained_... | 1            |
|    reward_value_loss    | 0.000556     |
|    total_cost           | 451.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.62
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.58       |
|    mean_ep_length       | 24.4        |
|    mean_reward          | -13.7       |
|    true_cost            | 0.0492      |
| infos/                  |             |
|    cost                 | 0.07        |
| rollout/                |             |
|    adjusted_reward      | -0.754      |
|    ep_len_mean          | 23.5        |
|    ep_rew_mean          | -13.6       |
| time/                   |             |
|    fps                  | 395         |
|    iterations           | 54          |
|    time_elapsed         | 1398        |
|    total_timesteps      | 552960      |
| train/                  |             |
|    approx_kl            | 0.61771286  |
|    average_cost         | 0.25498047  |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.65       |
|    cost_value_loss      | 1.1         |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0895     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0163     |
|    mean_cost_advantages | 1.6349137   |
|    mean_reward_advan... | -0.15298906 |
|    n_updates            | 530         |
|    nu                   | 3.45        |
|    nu_loss              | -0.86       |
|    policy_gradient_loss | -0.0517     |
|    reward_explained_... | 0.968       |
|    reward_value_loss    | 0.0137      |
|    total_cost           | 2611.0      |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.34
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -4.58      |
|    mean_ep_length       | 14.8       |
|    mean_reward          | -7.12      |
|    true_cost            | 0.0322     |
| infos/                  |            |
|    cost                 | 0.03       |
| rollout/                |            |
|    adjusted_reward      | -0.567     |
|    ep_len_mean          | 14.5       |
|    ep_rew_mean          | -6.56      |
| time/                   |            |
|    fps                  | 399        |
|    iterations           | 55         |
|    time_elapsed         | 1411       |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.33524138 |
|    average_cost         | 0.04921875 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -6.86      |
|    cost_value_loss      | 0.413      |
|    early_stop_epoch     | 0          |
|    entropy_loss         | -0.159     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.109      |
|    mean_cost_advantages | -1.0446959 |
|    mean_reward_advan... | -1.4371618 |
|    n_updates            | 540        |
|    nu                   | 3.53       |
|    nu_loss              | -0.17      |
|    policy_gradient_loss | -0.0114    |
|    reward_explained_... | 0.322      |
|    reward_value_loss    | 0.546      |
|    total_cost           | 504.0      |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.58       |
|    mean_ep_length       | 14.4        |
|    mean_reward          | -6.68       |
|    true_cost            | 0.128       |
| infos/                  |             |
|    cost                 | 0.14        |
| rollout/                |             |
|    adjusted_reward      | -0.92       |
|    ep_len_mean          | 14.2        |
|    ep_rew_mean          | -6.58       |
| time/                   |             |
|    fps                  | 402         |
|    iterations           | 56          |
|    time_elapsed         | 1424        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.117629364 |
|    average_cost         | 0.032226562 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.628       |
|    cost_value_loss      | 0.143       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.123      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    mean_cost_advantages | -0.7499929  |
|    mean_reward_advan... | 1.5016105   |
|    n_updates            | 550         |
|    nu                   | 3.6         |
|    nu_loss              | -0.114      |
|    policy_gradient_loss | 0.0103      |
|    reward_explained_... | 0.795       |
|    reward_value_loss    | 0.389       |
|    total_cost           | 330.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.23
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -4.58      |
|    mean_ep_length       | 13.6       |
|    mean_reward          | -6.38      |
|    true_cost            | 0.0362     |
| infos/                  |            |
|    cost                 | 0.03       |
| rollout/                |            |
|    adjusted_reward      | -0.604     |
|    ep_len_mean          | 14         |
|    ep_rew_mean          | -6.6       |
| time/                   |            |
|    fps                  | 405        |
|    iterations           | 57         |
|    time_elapsed         | 1437       |
|    total_timesteps      | 583680     |
| train/                  |            |
|    approx_kl            | 0.22867778 |
|    average_cost         | 0.12763672 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -3.34      |
|    cost_value_loss      | 1.06       |
|    early_stop_epoch     | 0          |
|    entropy_loss         | -0.0883    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.425      |
|    mean_cost_advantages | 0.69280756 |
|    mean_reward_advan... | 0.14708091 |
|    n_updates            | 560        |
|    nu                   | 3.69       |
|    nu_loss              | -0.46      |
|    policy_gradient_loss | -0.056     |
|    reward_explained_... | 0.975      |
|    reward_value_loss    | 0.0446     |
|    total_cost           | 1307.0     |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.58       |
|    mean_ep_length       | 13.2        |
|    mean_reward          | -5.95       |
|    true_cost            | 0.0412      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.616      |
|    ep_len_mean          | 13.3        |
|    ep_rew_mean          | -6.13       |
| time/                   |             |
|    fps                  | 408         |
|    iterations           | 58          |
|    time_elapsed         | 1453        |
|    total_timesteps      | 593920      |
| train/                  |             |
|    approx_kl            | 0.031923495 |
|    average_cost         | 0.036230467 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    cost_explained_va... | -2.17       |
|    cost_value_loss      | 0.101       |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.124      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0368      |
|    mean_cost_advantages | -0.66141605 |
|    mean_reward_advan... | 0.003918957 |
|    n_updates            | 570         |
|    nu                   | 3.77        |
|    nu_loss              | -0.134      |
|    policy_gradient_loss | 0.00207     |
|    reward_explained_... | 0.995       |
|    reward_value_loss    | 0.0208      |
|    total_cost           | 371.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.58       |
|    mean_ep_length       | 12.4        |
|    mean_reward          | -5.2        |
|    true_cost            | 0.0476      |
| infos/                  |             |
|    cost                 | 0.04        |
| rollout/                |             |
|    adjusted_reward      | -0.623      |
|    ep_len_mean          | 12          |
|    ep_rew_mean          | -5.27       |
| time/                   |             |
|    fps                  | 411         |
|    iterations           | 59          |
|    time_elapsed         | 1469        |
|    total_timesteps      | 604160      |
| train/                  |             |
|    approx_kl            | 0.049505547 |
|    average_cost         | 0.04121094  |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.536       |
|    cost_value_loss      | 0.0618      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.131      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0521      |
|    mean_cost_advantages | -0.2007952  |
|    mean_reward_advan... | 0.11222111  |
|    n_updates            | 580         |
|    nu                   | 3.84        |
|    nu_loss              | -0.155      |
|    policy_gradient_loss | -0.00601    |
|    reward_explained_... | 0.98        |
|    reward_value_loss    | 0.0641      |
|    total_cost           | 422.0       |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.8         |
|    mean_reward          | -5.32        |
|    true_cost            | 0.0434       |
| infos/                  |              |
|    cost                 | 0.04         |
| rollout/                |              |
|    adjusted_reward      | -0.605       |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | -5.17        |
| time/                   |              |
|    fps                  | 413          |
|    iterations           | 60           |
|    time_elapsed         | 1486         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.024851352  |
|    average_cost         | 0.047558594  |
|    clip_fraction        | 0.0826       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.529       |
|    cost_value_loss      | 0.161        |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.101       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.146        |
|    mean_cost_advantages | -0.011806464 |
|    mean_reward_advan... | 0.26952738   |
|    n_updates            | 590          |
|    nu                   | 3.92         |
|    nu_loss              | -0.183       |
|    policy_gradient_loss | -0.00315     |
|    reward_explained_... | 0.966        |
|    reward_value_loss    | 0.0638       |
|    total_cost           | 487.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.58       |
|    mean_ep_length       | 11.6        |
|    mean_reward          | -5.13       |
|    true_cost            | 0.0488      |
| infos/                  |             |
|    cost                 | 0.09        |
| rollout/                |             |
|    adjusted_reward      | -0.632      |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | -5.04       |
| time/                   |             |
|    fps                  | 416         |
|    iterations           | 61          |
|    time_elapsed         | 1500        |
|    total_timesteps      | 624640      |
| train/                  |             |
|    approx_kl            | 0.016144512 |
|    average_cost         | 0.043359376 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.157       |
|    cost_value_loss      | 0.0608      |
|    early_stop_epoch     | 0           |
|    entropy_loss         | -0.0631     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0307      |
|    mean_cost_advantages | -0.07121037 |
|    mean_reward_advan... | 0.10316749  |
|    n_updates            | 600         |
|    nu                   | 4           |
|    nu_loss              | -0.17       |
|    policy_gradient_loss | -0.00192    |
|    reward_explained_... | 0.989       |
|    reward_value_loss    | 0.0237      |
|    total_cost           | 444.0       |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | -4.58       |
|    mean_ep_length       | 11.4        |
|    mean_reward          | -4.95       |
|    true_cost            | 0.0456      |
| infos/                  |             |
|    cost                 | 0.05        |
| rollout/                |             |
|    adjusted_reward      | -0.623      |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | -5.02       |
| time/                   |             |
|    fps                  | 418         |
|    iterations           | 62          |
|    time_elapsed         | 1516        |
|    total_timesteps      | 634880      |
| train/                  |             |
|    approx_kl            | 0.026751954 |
|    average_cost         | 0.048828125 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.334      |
|    cost_value_loss      | 0.123       |
|    early_stop_epoch     | 1           |
|    entropy_loss         | -0.0497     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0861      |
|    mean_cost_advantages | 0.040176075 |
|    mean_reward_advan... | 0.044234507 |
|    n_updates            | 610         |
|    nu                   | 4.07        |
|    nu_loss              | -0.195      |
|    policy_gradient_loss | -0.00181    |
|    reward_explained_... | 0.998       |
|    reward_value_loss    | 0.00347     |
|    total_cost           | 500.0       |
-----------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.13        |
|    true_cost            | 0.0456       |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | -0.626       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -5.03        |
| time/                   |              |
|    fps                  | 414          |
|    iterations           | 63           |
|    time_elapsed         | 1558         |
|    total_timesteps      | 645120       |
| train/                  |              |
|    approx_kl            | 0.0024554317 |
|    average_cost         | 0.04560547   |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.266        |
|    cost_value_loss      | 0.0575       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0426      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.02         |
|    mean_cost_advantages | -0.012350022 |
|    mean_reward_advan... | 0.0070116064 |
|    n_updates            | 620          |
|    nu                   | 4.14         |
|    nu_loss              | -0.186       |
|    policy_gradient_loss | -1.87e-05    |
|    reward_explained_... | 0.999        |
|    reward_value_loss    | 0.00116      |
|    total_cost           | 467.0        |
------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | -4.95         |
|    true_cost            | 0.0454        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | -0.63         |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | -4.99         |
| time/                   |               |
|    fps                  | 416           |
|    iterations           | 64            |
|    time_elapsed         | 1571          |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.0325061     |
|    average_cost         | 0.04560547    |
|    clip_fraction        | 0.0534        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.306         |
|    cost_value_loss      | 0.057         |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0452       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0397        |
|    mean_cost_advantages | -0.015389639  |
|    mean_reward_advan... | -0.0033285595 |
|    n_updates            | 630           |
|    nu                   | 4.22          |
|    nu_loss              | -0.189        |
|    policy_gradient_loss | -0.000741     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.000568      |
|    total_cost           | 467.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.13        |
|    true_cost            | 0.043        |
| infos/                  |              |
|    cost                 | 0.06         |
| rollout/                |              |
|    adjusted_reward      | -0.622       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -5.08        |
| time/                   |              |
|    fps                  | 413          |
|    iterations           | 65           |
|    time_elapsed         | 1608         |
|    total_timesteps      | 665600       |
| train/                  |              |
|    approx_kl            | 0.005826144  |
|    average_cost         | 0.045410156  |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.254        |
|    cost_value_loss      | 0.0666       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.038       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0343       |
|    mean_cost_advantages | -0.007428439 |
|    mean_reward_advan... | 0.006068534  |
|    n_updates            | 640          |
|    nu                   | 4.29         |
|    nu_loss              | -0.192       |
|    policy_gradient_loss | 2.45e-05     |
|    reward_explained_... | 1            |
|    reward_value_loss    | 0.000557     |
|    total_cost           | 465.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -4.77        |
|    true_cost            | 0.0444       |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | -0.631       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -5.05        |
| time/                   |              |
|    fps                  | 408          |
|    iterations           | 66           |
|    time_elapsed         | 1655         |
|    total_timesteps      | 675840       |
| train/                  |              |
|    approx_kl            | 0.0016058926 |
|    average_cost         | 0.04296875   |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.222        |
|    cost_value_loss      | 0.0629       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0364      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0271       |
|    mean_cost_advantages | 0.0039860383 |
|    mean_reward_advan... | 0.0002931059 |
|    n_updates            | 650          |
|    nu                   | 4.36         |
|    nu_loss              | -0.184       |
|    policy_gradient_loss | 2.53e-05     |
|    reward_explained_... | 1            |
|    reward_value_loss    | 0.000451     |
|    total_cost           | 440.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.4         |
|    mean_reward          | -4.95        |
|    true_cost            | 0.0447       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.638       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -5.05        |
| time/                   |              |
|    fps                  | 398          |
|    iterations           | 67           |
|    time_elapsed         | 1720         |
|    total_timesteps      | 686080       |
| train/                  |              |
|    approx_kl            | 0.0014053956 |
|    average_cost         | 0.044433594  |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.289        |
|    cost_value_loss      | 0.0573       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.036       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0327       |
|    mean_cost_advantages | -0.007285607 |
|    mean_reward_advan... | -0.010557666 |
|    n_updates            | 660          |
|    nu                   | 4.43         |
|    nu_loss              | -0.194       |
|    policy_gradient_loss | 4.47e-05     |
|    reward_explained_... | 1            |
|    reward_value_loss    | 0.00108      |
|    total_cost           | 455.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 11.4          |
|    mean_reward          | -4.95         |
|    true_cost            | 0.0433        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | -0.634        |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | -5.03         |
| time/                   |               |
|    fps                  | 389           |
|    iterations           | 68            |
|    time_elapsed         | 1788          |
|    total_timesteps      | 696320        |
| train/                  |               |
|    approx_kl            | 0.00016888129 |
|    average_cost         | 0.04472656    |
|    clip_fraction        | 0.0137        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.303         |
|    cost_value_loss      | 0.0569        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0395       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0205        |
|    mean_cost_advantages | -0.024143847  |
|    mean_reward_advan... | 0.0036602232  |
|    n_updates            | 670           |
|    nu                   | 4.5           |
|    nu_loss              | -0.198        |
|    policy_gradient_loss | 0.000148      |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.000351      |
|    total_cost           | 458.0         |
-------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -5.32         |
|    true_cost            | 0.0437        |
| infos/                  |               |
|    cost                 | 0.04          |
| rollout/                |               |
|    adjusted_reward      | -0.64         |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | -5.27         |
| time/                   |               |
|    fps                  | 386           |
|    iterations           | 69            |
|    time_elapsed         | 1828          |
|    total_timesteps      | 706560        |
| train/                  |               |
|    approx_kl            | 0.020238938   |
|    average_cost         | 0.043261718   |
|    clip_fraction        | 0.0211        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.286         |
|    cost_value_loss      | 0.0571        |
|    early_stop_epoch     | 7             |
|    entropy_loss         | -0.0467       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0305        |
|    mean_cost_advantages | -0.007559302  |
|    mean_reward_advan... | -0.0013533717 |
|    n_updates            | 680           |
|    nu                   | 4.57          |
|    nu_loss              | -0.195        |
|    policy_gradient_loss | -0.000146     |
|    reward_explained_... | 1             |
|    reward_value_loss    | 0.000357      |
|    total_cost           | 443.0         |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.13        |
|    true_cost            | 0.0435       |
| infos/                  |              |
|    cost                 | 0.07         |
| rollout/                |              |
|    adjusted_reward      | -0.64        |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | -4.99        |
| time/                   |              |
|    fps                  | 389          |
|    iterations           | 70           |
|    time_elapsed         | 1841         |
|    total_timesteps      | 716800       |
| train/                  |              |
|    approx_kl            | 0.017845625  |
|    average_cost         | 0.043652344  |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.219        |
|    cost_value_loss      | 0.0555       |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0614      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0211       |
|    mean_cost_advantages | 0.0049442337 |
|    mean_reward_advan... | -0.064614296 |
|    n_updates            | 690          |
|    nu                   | 4.64         |
|    nu_loss              | -0.2         |
|    policy_gradient_loss | -0.00184     |
|    reward_explained_... | 0.979        |
|    reward_value_loss    | 0.0447       |
|    total_cost           | 447.0        |
------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | -5.1          |
|    true_cost            | 0.0408        |
| infos/                  |               |
|    cost                 | 0.03          |
| rollout/                |               |
|    adjusted_reward      | -0.634        |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | -5.08         |
| time/                   |               |
|    fps                  | 380           |
|    iterations           | 71            |
|    time_elapsed         | 1910          |
|    total_timesteps      | 727040        |
| train/                  |               |
|    approx_kl            | 0.008448045   |
|    average_cost         | 0.04345703    |
|    clip_fraction        | 0.0281        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.265         |
|    cost_value_loss      | 0.0577        |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0505       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0313        |
|    mean_cost_advantages | -0.0044827336 |
|    mean_reward_advan... | 0.03801047    |
|    n_updates            | 700           |
|    nu                   | 4.71          |
|    nu_loss              | -0.202        |
|    policy_gradient_loss | -0.000471     |
|    reward_explained_... | 0.996         |
|    reward_value_loss    | 0.00127       |
|    total_cost           | 445.0         |
-------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -4.77        |
|    true_cost            | 0.043        |
| infos/                  |              |
|    cost                 | 0.05         |
| rollout/                |              |
|    adjusted_reward      | -0.645       |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | -5.08        |
| time/                   |              |
|    fps                  | 374          |
|    iterations           | 72           |
|    time_elapsed         | 1966         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0010343987 |
|    average_cost         | 0.04082031   |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.201        |
|    cost_value_loss      | 0.0633       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0549      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0308       |
|    mean_cost_advantages | -0.035141926 |
|    mean_reward_advan... | -0.01010066  |
|    n_updates            | 710          |
|    nu                   | 4.78         |
|    nu_loss              | -0.192       |
|    policy_gradient_loss | -0.000415    |
|    reward_explained_... | 0.995        |
|    reward_value_loss    | 0.0128       |
|    total_cost           | 418.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.13        |
|    true_cost            | 0.0427       |
| infos/                  |              |
|    cost                 | 0.03         |
| rollout/                |              |
|    adjusted_reward      | -0.646       |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | -5.05        |
| time/                   |              |
|    fps                  | 371          |
|    iterations           | 73           |
|    time_elapsed         | 2014         |
|    total_timesteps      | 747520       |
| train/                  |              |
|    approx_kl            | 0.0016149202 |
|    average_cost         | 0.04296875   |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.156        |
|    cost_value_loss      | 0.058        |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0473      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0385       |
|    mean_cost_advantages | 0.012546942  |
|    mean_reward_advan... | 0.015350856  |
|    n_updates            | 720          |
|    nu                   | 4.85         |
|    nu_loss              | -0.205       |
|    policy_gradient_loss | -0.000575    |
|    reward_explained_... | 0.997        |
|    reward_value_loss    | 0.00796      |
|    total_cost           | 440.0        |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 12           |
|    mean_reward          | -5.5         |
|    true_cost            | 0.044        |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.654       |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | -4.98        |
| time/                   |              |
|    fps                  | 364          |
|    iterations           | 74           |
|    time_elapsed         | 2077         |
|    total_timesteps      | 757760       |
| train/                  |              |
|    approx_kl            | 0.0043395157 |
|    average_cost         | 0.042675782  |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.16         |
|    cost_value_loss      | 0.0621       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0399      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0312       |
|    mean_cost_advantages | 0.022382423  |
|    mean_reward_advan... | -0.008983441 |
|    n_updates            | 730          |
|    nu                   | 4.92         |
|    nu_loss              | -0.207       |
|    policy_gradient_loss | -0.000628    |
|    reward_explained_... | 0.998        |
|    reward_value_loss    | 0.00535      |
|    total_cost           | 437.0        |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.6         |
|    mean_reward          | -5.13        |
|    true_cost            | 0.0282       |
| infos/                  |              |
|    cost                 | 0.02         |
| rollout/                |              |
|    adjusted_reward      | -0.59        |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | -5.4         |
| time/                   |              |
|    fps                  | 365          |
|    iterations           | 75           |
|    time_elapsed         | 2103         |
|    total_timesteps      | 768000       |
| train/                  |              |
|    approx_kl            | 0.030479372  |
|    average_cost         | 0.044042967  |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.256        |
|    cost_value_loss      | 0.0571       |
|    early_stop_epoch     | 3            |
|    entropy_loss         | -0.0469      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0276       |
|    mean_cost_advantages | 0.0008803165 |
|    mean_reward_advan... | 0.007972835  |
|    n_updates            | 740          |
|    nu                   | 4.99         |
|    nu_loss              | -0.217       |
|    policy_gradient_loss | -0.000383    |
|    reward_explained_... | 1            |
|    reward_value_loss    | 0.000414     |
|    total_cost           | 451.0        |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 11.2         |
|    mean_reward          | -4.77        |
|    true_cost            | 0.0193       |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.551       |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | -5.53        |
| time/                   |              |
|    fps                  | 364          |
|    iterations           | 76           |
|    time_elapsed         | 2136         |
|    total_timesteps      | 778240       |
| train/                  |              |
|    approx_kl            | 0.016551642  |
|    average_cost         | 0.028222656  |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.17         |
|    cost_value_loss      | 0.0494       |
|    early_stop_epoch     | 5            |
|    entropy_loss         | -0.0482      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.101        |
|    mean_cost_advantages | -0.048389323 |
|    mean_reward_advan... | -0.09375281  |
|    n_updates            | 750          |
|    nu                   | 5.05         |
|    nu_loss              | -0.141       |
|    policy_gradient_loss | -0.000575    |
|    reward_explained_... | 0.946        |
|    reward_value_loss    | 0.0857       |
|    total_cost           | 289.0        |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | -5.62         |
|    true_cost            | 0.000879      |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.465        |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | -5.89         |
| time/                   |               |
|    fps                  | 365           |
|    iterations           | 77            |
|    time_elapsed         | 2155          |
|    total_timesteps      | 788480        |
| train/                  |               |
|    approx_kl            | 0.06978422    |
|    average_cost         | 0.019335937   |
|    clip_fraction        | 0.0306        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.256         |
|    cost_value_loss      | 0.0282        |
|    early_stop_epoch     | 1             |
|    entropy_loss         | -0.0488       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0439        |
|    mean_cost_advantages | -0.029162716  |
|    mean_reward_advan... | -0.0079859905 |
|    n_updates            | 760           |
|    nu                   | 5.11          |
|    nu_loss              | -0.0977       |
|    policy_gradient_loss | -0.00344      |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 0.145         |
|    total_cost           | 198.0         |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.53
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 13.6         |
|    mean_reward          | -6.4         |
|    true_cost            | 9.77e-05     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.468       |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | -6.52        |
| time/                   |              |
|    fps                  | 366          |
|    iterations           | 78           |
|    time_elapsed         | 2180         |
|    total_timesteps      | 798720       |
| train/                  |              |
|    approx_kl            | 0.5252892    |
|    average_cost         | 0.0008789062 |
|    clip_fraction        | 0.0659       |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.0177       |
|    cost_value_loss      | 0.0146       |
|    early_stop_epoch     | 2            |
|    entropy_loss         | -0.0531      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.159        |
|    mean_cost_advantages | -0.07846776  |
|    mean_reward_advan... | -0.11444912  |
|    n_updates            | 770          |
|    nu                   | 5.17         |
|    nu_loss              | -0.00449     |
|    policy_gradient_loss | -0.00172     |
|    reward_explained_... | 0.901        |
|    reward_value_loss    | 0.242        |
|    total_cost           | 9.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 14           |
|    mean_reward          | -6.73        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.466       |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | -6.14        |
| time/                   |              |
|    fps                  | 362          |
|    iterations           | 79           |
|    time_elapsed         | 2229         |
|    total_timesteps      | 808960       |
| train/                  |              |
|    approx_kl            | 0.007862264  |
|    average_cost         | 9.765625e-05 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.235       |
|    cost_value_loss      | 0.0006       |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0594      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0916       |
|    mean_cost_advantages | -0.012121839 |
|    mean_reward_advan... | -0.20250475  |
|    n_updates            | 780          |
|    nu                   | 5.22         |
|    nu_loss              | -0.000505    |
|    policy_gradient_loss | 0.000589     |
|    reward_explained_... | 0.892        |
|    reward_value_loss    | 0.199        |
|    total_cost           | 1.0          |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 13.6          |
|    mean_reward          | -6.23         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.465        |
|    ep_len_mean          | 13.5          |
|    ep_rew_mean          | -6.26         |
| time/                   |               |
|    fps                  | 362           |
|    iterations           | 80            |
|    time_elapsed         | 2259          |
|    total_timesteps      | 819200        |
| train/                  |               |
|    approx_kl            | 0.016338635   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.022         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0336        |
|    cost_value_loss      | 3.32e-05      |
|    early_stop_epoch     | 3             |
|    entropy_loss         | -0.0567       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0848        |
|    mean_cost_advantages | -0.0034368113 |
|    mean_reward_advan... | -0.031341493  |
|    n_updates            | 790           |
|    nu                   | 5.27          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -8.24e-05     |
|    reward_explained_... | 0.941         |
|    reward_value_loss    | 0.209         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -4.58          |
|    mean_ep_length       | 14             |
|    mean_reward          | -6.62          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.463         |
|    ep_len_mean          | 13.5           |
|    ep_rew_mean          | -6.28          |
| time/                   |                |
|    fps                  | 357            |
|    iterations           | 81             |
|    time_elapsed         | 2318           |
|    total_timesteps      | 829440         |
| train/                  |                |
|    approx_kl            | 0.00577987     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0259         |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0247        |
|    cost_value_loss      | 1.77e-05       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0699        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.161          |
|    mean_cost_advantages | -0.00054595014 |
|    mean_reward_advan... | -0.011114122   |
|    n_updates            | 800            |
|    nu                   | 5.31           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -7.53e-05      |
|    reward_explained_... | 0.945          |
|    reward_value_loss    | 0.219          |
|    total_cost           | 0.0            |
--------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 13.6          |
|    mean_reward          | -6.3          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.46         |
|    ep_len_mean          | 13.3          |
|    ep_rew_mean          | -6.12         |
| time/                   |               |
|    fps                  | 360           |
|    iterations           | 82            |
|    time_elapsed         | 2331          |
|    total_timesteps      | 839680        |
| train/                  |               |
|    approx_kl            | 0.019377537   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0562        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.05         |
|    cost_value_loss      | 1.12e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0726       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.118         |
|    mean_cost_advantages | 0.00036877612 |
|    mean_reward_advan... | 0.0020815374  |
|    n_updates            | 810           |
|    nu                   | 5.35          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0002       |
|    reward_explained_... | 0.938         |
|    reward_value_loss    | 0.247         |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 13.2          |
|    mean_reward          | -6.08         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.457        |
|    ep_len_mean          | 13.1          |
|    ep_rew_mean          | -5.94         |
| time/                   |               |
|    fps                  | 361           |
|    iterations           | 83            |
|    time_elapsed         | 2349          |
|    total_timesteps      | 849920        |
| train/                  |               |
|    approx_kl            | 0.029366469   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0823        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0327       |
|    cost_value_loss      | 1.33e-05      |
|    early_stop_epoch     | 0             |
|    entropy_loss         | -0.0777       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.09          |
|    mean_cost_advantages | 0.00022872575 |
|    mean_reward_advan... | -0.010228472  |
|    n_updates            | 820           |
|    nu                   | 5.38          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000908     |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 0.21          |
|    total_cost           | 0.0           |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 12.8         |
|    mean_reward          | -5.82        |
|    true_cost            | 0.000391     |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.463       |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | -5.81        |
| time/                   |              |
|    fps                  | 363          |
|    iterations           | 84           |
|    time_elapsed         | 2363         |
|    total_timesteps      | 860160       |
| train/                  |              |
|    approx_kl            | 0.06861816   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -0.0413      |
|    cost_value_loss      | 9.69e-06     |
|    early_stop_epoch     | 0            |
|    entropy_loss         | -0.0486      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.238        |
|    mean_cost_advantages | 0.0015964436 |
|    mean_reward_advan... | 0.06904019   |
|    n_updates            | 830          |
|    nu                   | 5.41         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00206     |
|    reward_explained_... | 0.921        |
|    reward_value_loss    | 0.292        |
|    total_cost           | 0.0          |
------------------------------------------
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | -4.58        |
|    mean_ep_length       | 12.2         |
|    mean_reward          | -5.52        |
|    true_cost            | 0            |
| infos/                  |              |
|    cost                 | 0            |
| rollout/                |              |
|    adjusted_reward      | -0.459       |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | -5.76        |
| time/                   |              |
|    fps                  | 360          |
|    iterations           | 85           |
|    time_elapsed         | 2415         |
|    total_timesteps      | 870400       |
| train/                  |              |
|    approx_kl            | 0.004375732  |
|    average_cost         | 0.000390625  |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    cost_explained_va... | -4.96        |
|    cost_value_loss      | 0.00539      |
|    early_stop_epoch     | 10           |
|    entropy_loss         | -0.0299      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.159        |
|    mean_cost_advantages | 0.0034165892 |
|    mean_reward_advan... | 0.13213784   |
|    n_updates            | 840          |
|    nu                   | 5.44         |
|    nu_loss              | -0.00211     |
|    policy_gradient_loss | -0.000224    |
|    reward_explained_... | 0.922        |
|    reward_value_loss    | 0.242        |
|    total_cost           | 4.0          |
------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -4.58          |
|    mean_ep_length       | 13.4           |
|    mean_reward          | -6.38          |
|    true_cost            | 0              |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.459         |
|    ep_len_mean          | 12.6           |
|    ep_rew_mean          | -5.83          |
| time/                   |                |
|    fps                  | 357            |
|    iterations           | 86             |
|    time_elapsed         | 2462           |
|    total_timesteps      | 880640         |
| train/                  |                |
|    approx_kl            | 0.0002342544   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00689        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.0202        |
|    cost_value_loss      | 5.21e-06       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0298        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.0816         |
|    mean_cost_advantages | -6.1381215e-06 |
|    mean_reward_advan... | 0.03435233     |
|    n_updates            | 850            |
|    nu                   | 5.47           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 2.12e-06       |
|    reward_explained_... | 0.926          |
|    reward_value_loss    | 0.254          |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | -5.8          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.46         |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | -5.58         |
| time/                   |               |
|    fps                  | 353           |
|    iterations           | 87            |
|    time_elapsed         | 2518          |
|    total_timesteps      | 890880        |
| train/                  |               |
|    approx_kl            | 0.0043148054  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0206        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0205       |
|    cost_value_loss      | 3.26e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0332       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0984        |
|    mean_cost_advantages | 0.00011074387 |
|    mean_reward_advan... | -0.01724414   |
|    n_updates            | 860           |
|    nu                   | 5.49          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.09e-05      |
|    reward_explained_... | 0.931         |
|    reward_value_loss    | 0.243         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | -5.57         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.461        |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | -5.8          |
| time/                   |               |
|    fps                  | 347           |
|    iterations           | 88            |
|    time_elapsed         | 2593          |
|    total_timesteps      | 901120        |
| train/                  |               |
|    approx_kl            | 0.00044180843 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00863       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0131       |
|    cost_value_loss      | 2.4e-06       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0319       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0673        |
|    mean_cost_advantages | -0.0005859014 |
|    mean_reward_advan... | -0.007688912  |
|    n_updates            | 870           |
|    nu                   | 5.51          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 1.62e-05      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.231         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | -5.6          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.462        |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | -5.93         |
| time/                   |               |
|    fps                  | 341           |
|    iterations           | 89            |
|    time_elapsed         | 2668          |
|    total_timesteps      | 911360        |
| train/                  |               |
|    approx_kl            | 0.0016082529  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0122        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.174        |
|    cost_value_loss      | 2.09e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.033        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.177         |
|    mean_cost_advantages | -0.0017148266 |
|    mean_reward_advan... | 0.048242826   |
|    n_updates            | 880           |
|    nu                   | 5.53          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.98e-05      |
|    reward_explained_... | 0.937         |
|    reward_value_loss    | 0.229         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -5.48         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.458        |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | -5.74         |
| time/                   |               |
|    fps                  | 335           |
|    iterations           | 90            |
|    time_elapsed         | 2746          |
|    total_timesteps      | 921600        |
| train/                  |               |
|    approx_kl            | 0.0016787194  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00836       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.122        |
|    cost_value_loss      | 2.33e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0307       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.124         |
|    mean_cost_advantages | 0.00043790153 |
|    mean_reward_advan... | -0.006940277  |
|    n_updates            | 890           |
|    nu                   | 5.55          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 1.57e-05      |
|    reward_explained_... | 0.916         |
|    reward_value_loss    | 0.297         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 13.8          |
|    mean_reward          | -6.75         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.46         |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | -5.63         |
| time/                   |               |
|    fps                  | 329           |
|    iterations           | 91            |
|    time_elapsed         | 2831          |
|    total_timesteps      | 931840        |
| train/                  |               |
|    approx_kl            | 7.6829085e-05 |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0117        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.0531        |
|    cost_value_loss      | 2.37e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0316       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0978        |
|    mean_cost_advantages | -0.0004528129 |
|    mean_reward_advan... | 0.019791415   |
|    n_updates            | 900           |
|    nu                   | 5.56          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 3.01e-05      |
|    reward_explained_... | 0.937         |
|    reward_value_loss    | 0.221         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 13.2          |
|    mean_reward          | -6.4          |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.46         |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | -5.83         |
| time/                   |               |
|    fps                  | 323           |
|    iterations           | 92            |
|    time_elapsed         | 2915          |
|    total_timesteps      | 942080        |
| train/                  |               |
|    approx_kl            | 0.0008392573  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00777       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0141       |
|    cost_value_loss      | 5.58e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0324       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.101         |
|    mean_cost_advantages | 0.00025685143 |
|    mean_reward_advan... | -0.015949255  |
|    n_updates            | 910           |
|    nu                   | 5.58          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 9.12e-06      |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 0.191         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 11.6          |
|    mean_reward          | -5.08         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.458        |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | -5.73         |
| time/                   |               |
|    fps                  | 318           |
|    iterations           | 93            |
|    time_elapsed         | 2987          |
|    total_timesteps      | 952320        |
| train/                  |               |
|    approx_kl            | 0.0010285537  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0074        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0113       |
|    cost_value_loss      | 3.79e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0285       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.178         |
|    mean_cost_advantages | 0.00012454746 |
|    mean_reward_advan... | -0.038456436  |
|    n_updates            | 920           |
|    nu                   | 5.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 1.08e-05      |
|    reward_explained_... | 0.928         |
|    reward_value_loss    | 0.245         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -5.43         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.459        |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | -5.7          |
| time/                   |               |
|    fps                  | 315           |
|    iterations           | 94            |
|    time_elapsed         | 3048          |
|    total_timesteps      | 962560        |
| train/                  |               |
|    approx_kl            | 0.001592819   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00874       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.00861      |
|    cost_value_loss      | 3.18e-06      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0269       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.023         |
|    mean_cost_advantages | 0.00018753945 |
|    mean_reward_advan... | 0.020041578   |
|    n_updates            | 930           |
|    nu                   | 5.6           |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.4e-05       |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 0.194         |
|    total_cost           | 0.0           |
-------------------------------------------
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | -4.58          |
|    mean_ep_length       | 12.4           |
|    mean_reward          | -5.58          |
|    true_cost            | 9.77e-05       |
| infos/                  |                |
|    cost                 | 0              |
| rollout/                |                |
|    adjusted_reward      | -0.459         |
|    ep_len_mean          | 12.4           |
|    ep_rew_mean          | -5.66          |
| time/                   |                |
|    fps                  | 312            |
|    iterations           | 95             |
|    time_elapsed         | 3117           |
|    total_timesteps      | 972800         |
| train/                  |                |
|    approx_kl            | 0.0011821771   |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.00923        |
|    clip_range           | 0.2            |
|    cost_explained_va... | -0.00891       |
|    cost_value_loss      | 3.02e-07       |
|    early_stop_epoch     | 10             |
|    entropy_loss         | -0.0273        |
|    learning_rate        | 0.0003         |
|    loss                 | 0.063          |
|    mean_cost_advantages | -0.00016800273 |
|    mean_reward_advan... | -0.021038365   |
|    n_updates            | 940            |
|    nu                   | 5.61           |
|    nu_loss              | -0             |
|    policy_gradient_loss | 1.11e-05       |
|    reward_explained_... | 0.94           |
|    reward_value_loss    | 0.208          |
|    total_cost           | 0.0            |
--------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.2          |
|    mean_reward          | -5.53         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.461        |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | -5.84         |
| time/                   |               |
|    fps                  | 310           |
|    iterations           | 96            |
|    time_elapsed         | 3168          |
|    total_timesteps      | 983040        |
| train/                  |               |
|    approx_kl            | 0.0012789916  |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.00499       |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.54e+03     |
|    cost_value_loss      | 0.00054       |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0222       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0979        |
|    mean_cost_advantages | 0.00042721033 |
|    mean_reward_advan... | -0.027290061  |
|    n_updates            | 950           |
|    nu                   | 5.62          |
|    nu_loss              | -0.000548     |
|    policy_gradient_loss | -2.59e-05     |
|    reward_explained_... | 0.931         |
|    reward_value_loss    | 0.224         |
|    total_cost           | 1.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 12.4          |
|    mean_reward          | -5.57         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.459        |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | -5.84         |
| time/                   |               |
|    fps                  | 307           |
|    iterations           | 97            |
|    time_elapsed         | 3229          |
|    total_timesteps      | 993280        |
| train/                  |               |
|    approx_kl            | 0.0036085881  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0078        |
|    clip_range           | 0.2           |
|    cost_explained_va... | -0.0192       |
|    cost_value_loss      | 8.66e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0226       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0852        |
|    mean_cost_advantages | 0.001615235   |
|    mean_reward_advan... | -0.0050736745 |
|    n_updates            | 960           |
|    nu                   | 5.63          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.15e-05      |
|    reward_explained_... | 0.93          |
|    reward_value_loss    | 0.247         |
|    total_cost           | 0.0           |
-------------------------------------------
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | -4.58         |
|    mean_ep_length       | 11.8          |
|    mean_reward          | -5.23         |
|    true_cost            | 0             |
| infos/                  |               |
|    cost                 | 0             |
| rollout/                |               |
|    adjusted_reward      | -0.459        |
|    ep_len_mean          | 12.5          |
|    ep_rew_mean          | -5.74         |
| time/                   |               |
|    fps                  | 304           |
|    iterations           | 98            |
|    time_elapsed         | 3290          |
|    total_timesteps      | 1003520       |
| train/                  |               |
|    approx_kl            | 0.0043012598  |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0106        |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.135         |
|    cost_value_loss      | 1.62e-07      |
|    early_stop_epoch     | 10            |
|    entropy_loss         | -0.0222       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0853        |
|    mean_cost_advantages | 0.00084156013 |
|    mean_reward_advan... | 0.010348757   |
|    n_updates            | 970           |
|    nu                   | 5.64          |
|    nu_loss              | -0            |
|    policy_gradient_loss | 2.37e-05      |
|    reward_explained_... | 0.948         |
|    reward_value_loss    | 0.186         |
|    total_cost           | 0.0           |
-------------------------------------------
/home/jovyan/icrl/stable_baselines3/common/base_class.py:348: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._last_dones = np.zeros((self.env.num_envs,), dtype=np.bool)
[32mTime taken: 55.69 minutes