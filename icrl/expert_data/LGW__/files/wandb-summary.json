{"eval/mean_reward": 60.0, "eval/mean_ep_length": 200.0, "eval/best_mean_reward": 60.0, "rollout/adjusted_reward": 0.3000223636627197, "eval/true_cost": 0.0, "time/iterations": 49, "time/fps": 537, "time/time_elapsed": 186, "time/total_timesteps": 100352, "infos/traversals_so_far": 2.06, "infos/cost": 0.0, "rollout/ep_rew_mean": 60.0, "rollout/ep_len_mean": 200.0, "_step": 100352, "_runtime": 335, "_timestamp": 1647345882, "train/learning_rate": 0.0003, "train/entropy_loss": -0.0004035548191495764, "train/policy_gradient_loss": 2.829505830027898e-10, "train/reward_value_loss": 0.24435649847146124, "train/cost_value_loss": 9.45698379939408e-08, "train/approx_kl": -8.335337042808533e-08, "train/clip_fraction": 0.0, "train/loss": 0.14973612129688263, "train/mean_reward_advantages": 0.017902247607707977, "train/mean_cost_advantages": -0.0002599842264316976, "train/reward_explained_variance": -8.578609466552734, "train/cost_explained_variance": -0.8492580652236938, "train/nu": 2.514631986618042, "train/nu_loss": -0.0, "train/average_cost": 0.0, "train/total_cost": 0.0, "train/early_stop_epoch": 10, "train/n_updates": 480, "train/clip_range": 0.2}